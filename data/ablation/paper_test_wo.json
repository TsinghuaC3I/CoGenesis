[
  {
    "tid":"0",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Finding Pairwise Intersections Inside a Query Range.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We study the following problem: preprocess a set \\(\\mathcal {O}\\) of objects into a data structure that allows us to efficiently report all pairs of objects from \\(\\mathcal {O}\\) that intersect inside an axis-aligned query range \\({Q}\\). We present data structures of size \\(O(n\\cdot {{\\mathrm{polylog\\,}}}n)\\) and with query time \\(O((k+1)\\cdot {{\\mathrm{polylog\\,}}}n)\\) time, where k is the number of reported pairs, for two classes of objects in \\({\\mathbb R}^2\\): axis-aligned rectangles and objects with small union complexity. For the 3-dimensional case where the objects and the query range are axis-aligned boxes in\u00a0\\({\\mathbb R}^3\\), we present a data structure of size \\(O(n\\sqrt{n}\\cdot {{\\mathrm{polylog\\,}}}n)\\) and query time \\(O((\\sqrt{n}+k)\\cdot {{\\mathrm{polylog\\,}}}n)\\). When the objects and query are fat, we obtain \\(O((k+1)\\cdot {{\\mathrm{polylog\\,}}}n)\\) query time using \\(O(n\\cdot {{\\mathrm{polylog\\,}}}n)\\) storage."
      }
    ]
  },
  {
    "tid":"1",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Cooperative Multicell Precoding: Rate Region Characterization and Distributed Strategies With Instantaneous and Statistical CSI' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Base station cooperation is an attractive way of increasing the spectral efficiency in multiantenna communication. By serving each terminal through several base stations in a given area, intercell interference can be coordinated and higher performance achieved, especially for terminals at cell edges. Most previous work in the area has assumed that base stations have common knowledge of both data dedicated to all terminals and full or partial channel state information (CSI) of all links. Herein, we analyze the case of distributed cooperation where each base station has only local CSI, either instantaneous or statistical. In the case of instantaneous CSI, the beamforming vectors that can attain the outer boundary of the achievable rate region are characterized for an arbitrary number of multi antenna transmitters and single-antenna receivers. This characterization only requires local CSI and justifies distributed precoding design based on a novel virtual signal-to-interference noise ratio (SINR) framework, which can handle an arbitrary SNR and achieves the optimal multiplexing gain. The local power allocation between terminals is solved heuristically. Conceptually, analogous results for the achievable rate region characterization and precoding design are derived in the case of local statistical CSI. The benefits of distributed cooperative transmission are illustrated numerically, and it is shown that most of the performance with centralized cooperation can be obtained using only local CSI."
      }
    ]
  },
  {
    "tid":"2",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Small-time scaling behaviors of Internet backbone traffic: an empirical study' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The small-time (sub-seconds) scaling behaviors of Internet backbone traffic, based on traces collected from OC3/12/48 links in a tier-1 ISP is studied. We observe that for a majority of these traces, the (second-order) scaling exponents at small time scales (1 ms - 100 ms) are fairly close to 0.5, indicating that traffic fluctuations at these time scales are (nearly) uncorrelated. In addition, the traces manifest mostly monofractal behaviors at small time scales. The objective of the paper is to understand the potential causes or factors that influence the small-time scalings of Internet backbone traffic via empirical data analysis. We analyze the traffic composition of the traces along two dimensions - flow size and flow density. Our study uncovers dense flows (i.e., flows with bursts of densely clustered packets) as the correlation-causing factor in small time scales, and reveals that the traffic composition in terms of proportions of dense vs. sparse flows plays a major role in influencing the small-time scalings of aggregate traffic."
      }
    ]
  },
  {
    "tid":"3",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Routing through networks with hierarchical topology aggregation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In the future, global networks will consist of a hierarchy ofsubnetworks called domains. For reasons of both scalability andsecurity, domains will not reveal details of their internalstructure to outside nodes. Instead, these domains will advertiseonly a summary, or aggregated view, of their internal structure,e.g., as proposed by the ATM PNNI standard.This work compares, by simulation, the performance of severaldifferent aggregation schemes in terms of network throughput (thefraction of attempted connections that are realized), and networkcontrol load (the average number of crankbacks per realizedconnection). The simulation emulate a connection oriented networkwith a PNNI-like hierarchical source routing algorithm.Our main results are: (1) minimum spanning tree is a goodaggregation scheme; (2) exponential link cost functions performbetter than min-hop routing; (3) our suggested logarithmic updatescheme that determine when re-aggregation should be computed cansignificantly reduce the computational overhead due tore-aggregation with a negligible decrease in performance."
      }
    ]
  },
  {
    "tid":"4",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A self-aware data compression system on FPGA in Hadoop' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With the exponential growth of data size, data storage and analysis have been exposed to more challenges due to the lack of disk capacity and the limited network bandwidth. Data compression technique provides a good solution to mitigate these effects. In this paper, we propose a self-aware data compression system on FPGA for typical data warehousing, such as Hive, with column stored data and multi-threading requirements. The hardware accelerators can change the degree and hierarchy of parallelism depending on the data to be compressed (during the runtime). We test the system performance on a Xilinx VC707 FPGA board and the experimental results show that, up to 16 3-parallelism accelerators can be implemented and the throughput could be improved up to 432 MB/s. It is 6.25X speedup compared with the software solution under the same number of threads."
      }
    ]
  },
  {
    "tid":"5",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Refactorings of design defects using relational concept analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Software engineers often need to identify and correct design defects, i.e., recurring design problems that hinder development and maintenance by making programs harder to comprehend and/or evolve. While detection of design defects is an actively researched area, their correction- mainly a manual and time-consuming activity- is yet to be extensively investigated for automation. In this paper, we propose an automated approach for suggesting defect-correcting refactorings using relational concept analysis (RCA). The added value of rca consists in exploiting the links between formal objects which abound in a software re-engineering context. We validated our approach on instances of the Blob design defect taken from four different open-source programs."
      }
    ]
  },
  {
    "tid":"6",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Image Coding using 2-D Anisotropic Dual-Tree Discrete Wavelet Transform' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose an image coding scheme using 2-D anisotropic dual-tree discrete wavelet transform (DDWT). First, we extend 2-D DDWT to anisotropic decomposition, and obtain more directional subbands. Second, an iterative projection-based noise shaping algorithm is employed to further sparsify anisotropic DDWT coefficients. At last, the resulting coefficients are rearranged to preserve zero-tree relationship so that they can be efficiently coded with SPIHT. Experimental results show that our proposed scheme outperforms JPEG2000 and SPIHT at low bit rates despite the redundancy of DDWT."
      }
    ]
  },
  {
    "tid":"7",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Moving Object Tracked by A Mobile Robot with Real-Time Obstacles Avoidance Capacity' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper describes a robotic application that tracks a moving object by utilizing a mobile robot with multiple sensors. The robotic platform uses a visual camera to sense the movement of the desired object and a range sensor to help the robot detect and then avoid obstacles in real time while continuing to track and follow the desired object. In terms of real-time obstacle avoidance capacity, this paper also presents a modified potential field algorithm called Dynamic Goal Potential Field algorithm (DGPF) for this robotic application specifically. Experimental results show that the robotic and intelligent system can fulfill the requirements of tracking an object and avoiding obstacles simultaneously when the object is moving."
      }
    ]
  },
  {
    "tid":"8",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Scalable virtual machine deployment using VM image caches' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In IaaS clouds, VM startup times are frequently perceived as slow, negatively impacting both dynamic scaling of web applications and the startup of high-performance computing applications consisting of many VM nodes. A significant part of the startup time is due to the large transfers of VM image content from a storage node to the actual compute nodes, even when copy-on-write schemes are used. We have observed that only a tiny part of the VM image is needed for the VM to be able to start up. Based on this observation, we propose using small caches for VM images to overcome the VM startup bottlenecks. We have implemented such caches as an extension to KVM/QEMU. Our evaluation with up to 64 VMs shows that using our caches reduces the time needed for simultaneous VM startups to the one of a single VM."
      }
    ]
  },
  {
    "tid":"9",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Routability-driven bump assignment for chip-package co-design' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In current chip and package designs, it is a bottleneck to simultaneously optimize both pin assignment and pin routing for different design domains (chip, package, and board). Usually the whole process costs a huge manual effort and multiple iterations thus reducing profit margin. Therefore, we propose a fast heuristic chip-package co-design algorithm in order to automatically obtain a bump assignment which introduces high routability both in RDL routing and substrate routing (100% in our real case). Experimental results show that the proposed method (inspired by board escape routing algorithms) automatically finishes bump assignment, RDL routing and substrate routing in a short time, while the traditional co-design flow requires weeks even months."
      }
    ]
  },
  {
    "tid":"10",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Privacy modeling for video data publication' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Video cameras are being extensively used in many applications. Huge amounts of video are being recorded and stored everyday by surveillance systems. Any proposed application of this data raises severe privacy concerns. An assessment of privacy loss is necessary before any potential application of the data. In traditional methods of privacy modeling, researchers have focused on explicit means of identity leakage like facial information, etc. However, other implicit inference channels through which individual's an identity can be learned have not been considered. For example, an adversary can observe the behavior, look at the places visited and combine that with the temporal information to infer the identity of the person in the video. In this work, we thoroughly investigate privacy issues involved with the video data considering both implicit and explicit channels. We first establish an analogy with the statistical databases and then propose a model to calculate the privacy loss that might occur due to publication of the video data. The experimental results demonstrate the utility of the proposed model."
      }
    ]
  },
  {
    "tid":"11",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Noncontact measurement of cardiac beat by using active stereo with waved-grid pattern projection.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose a method to observe cardiac beat from 3D shape information of body surface by using the active stereo with waved-grid pattern projection, and report preliminary experiments to evaluate validities of the proposed method. By comparing results of our method with those of electrocardiogram (ECG), we confirmed sufficient correspondences between peak intervals of depth changes between contiguous frames measured by the active stereo and R-R intervals measured by ECG. We proposed the visualization of the spatial distribution of depth change plotted on the 3D shape of chest surface. We confirm that the spatial phase difference, which is caused by heart pump ability, appears in the 3-D shape change of chest surface."
      }
    ]
  },
  {
    "tid":"12",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Simulation-Based Receiver Selective Opening CCA Secure PKE from Standard Computational Assumptions.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In the situation where there are one sender and multiple receivers and the sender transmits ciphertexts of correlated plaintexts, a receiver selective opening (RSO) attack for a public key encryption (PKE) scheme considers adversaries that can corrupt some of the receivers and get their secret keys and plaintexts. Security against RSO attacks for a PKE scheme ensures confidentiality of ciphertexts of uncorrupted receivers. Simulation-based RSO security against chosen ciphertext attacks (SIM-RSO-CCA) is the strongest security notion in all RSO attack scenarios. Jia, Lu, and Li (2016) [19] proposed the first SIM-RSO-CCA secure PKE scheme. However, their scheme used indistinguishability obfuscation, which is not known to be constructed from any standard computational assumption. In this paper, we give two contributions for constructing SIM-RSO-CCA secure PKE from standard computational assumptions. Firstly, we propose a generic construction of SIM-RSO-CCA secure PKE using an IND-CPA secure PKE scheme and a non-interactive zero-knowledge proof system satisfying one-time simulation soundness. Secondly, we propose an efficient and concrete construction of SIM-RSO-CCA secure PKE based on the decisional Diffie-Hellman (DDH) assumption. Moreover, we give a method for efficiently expanding the plaintext space of the DDH-based construction. By applying this method to the construction, we obtain the first DDH-based SIM-RSO-CCA secure PKE scheme supporting a super-polynomially large plaintext space with compact ciphertexts."
      }
    ]
  },
  {
    "tid":"13",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Analyzing and modeling router-level internet topology and application to routing control' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Measurement studies on the Internet topology show that connectivities of nodes exhibit power-law attribute, but it is apparent that only the degree distribution does not determine the network structure, and especially true when we study the network-related control like routing control. In this paper, we first reveal structures of the router-level topologies using the working ISP networks, which clearly indicates ISP topologies are highly clustered; a node connects two or more nodes that also connected each other, while not in the existing modeling approaches. Based on this observation, we develop a new realistic modeling method for generating router-level topologies. In our method, when a new node joins the network, the node likely connects to the nearest nodes. In addition, we add the new links based on the node utilization in the topology, which corresponds to an enhancement of network equipments in ISP networks. With appropriate parameters, important metrics, such as the a clustering coefficient and the amount of traffic that pass through nodes, exhibit the similar value of the actual ISP topology while keeping the degree distribution of resulting topology to follow power-law. We then apply the routing control method to the ISP topologies and show that the optimal routing method gives much smaller maximum link utilization (about 1/3) compared with the minimum hop routing which is often used in the operating networks. Accordingly, we examine a heuristic routing method suitable to the ISP topologies with consideration of technology constraints of IP routers. The evaluation results show that our modeling method can be actually used for evaluations on routing control."
      }
    ]
  },
  {
    "tid":"14",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Rethinking virtual network embedding: substrate support for path splitting and migration' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Network virtualization is a powerful way to run multiple architectures or experiments simultaneously on a shared infrastructure. However, making efficient use of the underlying resources requires effective techniques for virtual network embedding--mapping each virtual network to specific nodes and links in the substrate network. Since the general embedding problem is computationally intractable, past research restricted the problem space to allow efficient solutions, or focused on designing heuristic algorithms. In this paper, we advocate a different approach: rethinking the design of the substrate network to enable simpler embedding algorithms and more efficient use of resources, without restricting the problem space. In particular, we simplify virtual link embedding by: i) allowing the substrate network to split a virtual link over multiple substrate paths and ii) employing path migration to periodically re-optimize the utilization of the substrate network. We also explore node-mapping algorithms that are customized to common classes of virtual-network topologies. Our simulation experiments show that path splitting, path migration,and customized embedding algorithms enable a substrate network to satisfy a much larger mix of virtual networks"
      }
    ]
  },
  {
    "tid":"15",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Distributed Coordination Guidance in Multi-agent Reinforcement Learning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we present a distributed reinforcement learning system that leverages on expert coordination knowledge to improve learning in multi-agent problems. We focus on the scenario where agents can communicate with their neighbors but this communication structure and the number of agents may change over time. We express coordination knowledge as constraints to reduce the joint action space for exploration. We introduce an extra learning level to learn when to make use of these constraints. This extra level is decentralized among the agents, making it suitable for our communication restrictions. Experiment results on tactical real-time strategy and soccer games show that our system is effective in online learning as opposed to existing methods that use individual constraints on agents and coordinated action selection."
      }
    ]
  },
  {
    "tid":"16",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Guided mutations in cooperative coevolutionary algorithms for function optimization' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this study, a mutation method called \"guided mutation\" is proposed. In guided mutation, each individual maintains two mutation control parameters: mutation direction and mutation step size. Guidance in mutations is provided by continuous updates on these parameters during the evolutionary process. We apply guided mutations to cooperative coevolutionary algorithm (CCEA), and compare its performance of optimizing nine common problem domains with those of other CCEAs. Our results show that guided mutations can improve the performance of a CCEA in some problem domains. We discuss the implications of our results and suggest some directions for future research."
      }
    ]
  },
  {
    "tid":"17",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'GreenLA: green linear algebra software for GPU-accelerated heterogeneous computing.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"While many linear algebra libraries have been developed to optimize their performance, no linear algebra library considers their energy efficiency at the library design time. In this paper, we present GreenLA - an energy efficient linear algebra software package that leverages linear algebra algorithmic characteristics to maximize energy savings with negligible overhead. GreenLA is (1) energy efficient: it saves up to several times more energy than the best existing energy saving approaches that do not modify library source codes; (2) high performance: its performance is comparable to the highly optimized linear algebra library MAGMA; and (3) transparent to applications: with the same programming interface, existing MAGMA users do not need to modify their source codes to benefit from GreenLA. Experimental results demonstrate that GreenLA is able to save up to three times more energy than the best existing energy saving approaches while delivering similar performance compared to the state-of-the-art linear algebra library MAGMA."
      }
    ]
  },
  {
    "tid":"18",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multisource Geospatial Data Fusion via Local Joint Sparse Representation.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose an adaptive locality weighted multisource joint sparse representation classification (ALWMJ-SRC) model for the classification of multisource remote sensing data. Although the notion of multitask joint sparsity has been recently developed for data fusion and has shown to be effective for various applications, in this paper, we suggest that there are important limitations s..."
      }
    ]
  },
  {
    "tid":"19",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Neural network training and rule extraction with augmented discretized input.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The classification and prediction accuracy of neural networks can be improved when they are trained with discretized continuous attributes as additional inputs. Such input augmentation makes it easier for the network weights to form more accurate decision boundaries when the data samples of different classes in the data set are contained in distinct hyper-rectangular subregions in the original input space. In this paper, we present first how a neural network can be trained with augmented discretized inputs. The additional inputs are obtained by dividing the original interval of each continuous attribute into subintervals of equal length. The network is then pruned to remove most of the discretized inputs as well as the original continuous attributes as long as the network still achieves a minimum preset accuracy requirement. We then discuss how comprehensible classification rules can be extracted from the pruned network by analyzing the activations of the network hidden units and the weights of the network connections that remain in the pruned network. Our experiments on artificial data sets show that the rules extracted from the neural networks can perfectly replicate the class membership rules used to create the data perfectly. On real-life benchmark data sets, neural networks trained with augmented discretized inputs are shown to achieve better accuracy than neural networks trained with the original data."
      }
    ]
  },
  {
    "tid":"20",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Reflection patterns for interactive knowledge capture' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Current knowledge acquisition tools have limited understanding of how users enter knowledge and how acquired knowledge is used, and provide limited assistance in organizing various knowledge authoring tasks. In this paper, we present a novel extension to existing knowledge acquisition tools where the system 1) captures the episodes of knowledge acquisition and knowledge use through a set of declarative reflection patterns 2) performs assessment on how to improve the future knowledge acquisition and knowledge use based on captured episodes, and 3) provides assistance to the users by combining the assessment results."
      }
    ]
  },
  {
    "tid":"21",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Outsourcing Resource Selection: A Rough Set Approach' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Today's economic reality is forcing firms to become increasingly more efficient in managing their resource functions. Outsourcing has moved to the mainstream of business development and promised to be one of the many enterprise strategies for cost-effective service delivery. Proper screening and automatic selection of outsource partners are critical to the business. Resource selection is one of the most important steps in outsourcing decision-making processes. This paper considers a data intensive selection problem in outsourcing software development projects. We analyze the properties of the resource selection problem and propose some criteria for an automatic resource selection model. A naive model, a traditional rough set model, and a generalized rough set (GRS) model are introduced and the advantages and disadvantages of each model are compared. Experimental results indicate that the GRS model is superior to other models."
      }
    ]
  },
  {
    "tid":"22",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Signal-Flow-Graph Derivation Of On-Line Gradient Learning Algorithms' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, making use of the Signal-Flow-Graph (SFG) representation and its known properties, we derive a new general method for backward gradient computation of a system output or cost function with respect to past (or present) system parameters. The system can be any causal, in general non-linear and time-variant, dynamic system represented by a SFG, in particular any feed forward or recurrent neural network. In this work we use discrete time notation, but the same theory holds for the continuos time case. The gradient is obtained by the analysis of two SFGs, the original one and its adjoint. This method can be used both for on-line and off-line learning. In the latter case using the Mean Square Error cost function, our approach particularises to E. Wan's method that is not suited for online training of recurrent networks. Computer simulations of non-linear dynamic systems identification will also be presented to assess the performance of the algorithm resulting from the application of the proposed method in the case of locally recurrent neural networks."
      }
    ]
  },
  {
    "tid":"23",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Validating the Functional Design of Embedded Systems against Stakeholder Intentions.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In the embedded systems industry, function-centered engineering is commonly applied to address the increasing number and complexity of system functions. During function-centered engineering, the functional design that is created based on the defined requirements for the system is the main artifact that serves as a basis for subsequent development activities. If stakeholder intentions change and modifications become necessary, they are frequently incorporated directly into the functional design without updating the behavioral requirements accordingly. As a consequence, the correctness of the interplay of system functions as defined in the functional design cannot be assessed by checking it against the defined requirements (since they are outdated) but needs to be checked against the current stakeholder intentions. More precisely, the requirements engineer has to validate the functional design against the stakeholder intentions because he is the expert concerning the stakeholder intentions and can communicate with the stakeholders regarding them, if necessary. However, the requirements engineer is typically not familiar with the functional design and its notation on the one hand, and, on the other hand, the overall behavior of the system is spread across various diagrams in the functional design. Therefore, the requirements engineer needs a more abstract and consolidated view of the functional design in order to be able to validate its correctness with regard to the current stakeholder intentions. In this paper, we present an approach which is based on a specific kind of review model that is automatically generated from the functional design and supports the requirements engineer in her task. The approach that is presented in this paper is subject of ongoing research."
      }
    ]
  },
  {
    "tid":"24",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fluid Annotation: a human-machine collaboration interface for full image annotation.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We introduce Fluid Annotation, an intuitive human-machine collaboration interface for annotating the class label and outline of every object and background region in an image. Fluid annotation is based on three principles:(I) Strong Machine-Learning aid. We start from the output of a strong neural network model, which the annotator can edit by correcting the labels of existing regions, adding new regions to cover missing objects, and removing incorrect regions.The edit operations are also assisted by the model.(II) Full image annotation in a single pass. As opposed to performing a series of small annotation tasks in isolation [51,68], we propose a unified interface for full image annotation in a single pass.(III) Empower the annotator.We empower the annotator to choose what to annotate and in which order. This enables concentrating on what the ma-chine does not already know, i.e. putting human effort only on the errors it made. This helps using the annotation budget effectively.\n\nThrough extensive experiments on the COCO+Stuff dataset [11,51], we demonstrate that Fluid Annotation leads to accurate an-notations very efficiently, taking 3x less annotation time than the popular LabelMe interface [70].\n\n"
      }
    ]
  },
  {
    "tid":"25",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Learning visually grounded words and syntax for a scene description task' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A spoken language generation system has been developed that learns to describe objects in computer-generated visual scenes. The system is trained by a \u2018show-and-tell\u2019 procedure in which visual scenes are paired with natural language descriptions. Learning algorithms acquire probabilistic structures which encode the visual semantics of phrase structure, word classes, and individual words. Using these structures, a planning algorithm integrates syntactic, semantic, and contextual constraints to generate natural and unambiguous descriptions of objects in novel scenes. The system generates syntactically well-formed compound adjective noun phrases, as well as relative spatial clauses. The acquired linguistic structures generalize from training data, enabling the production of novel word sequences which were never observed during training. The output of the generation system is synthesized using word-based concatenative synthesis drawing from the original training speech corpus. In evaluations of semantic comprehension by human judges, the performance of automatically generated spoken descriptions was comparable to human-generated descriptions. This work is motivated by our long-term goal of developing spoken language processing systems which grounds semantics in machine perception and action."
      }
    ]
  },
  {
    "tid":"26",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Developing secure data warehouses with a UML extension' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Data Warehouses (DWs), Multidimensional (MD) Databases, and On-Line Analytical Processing Applications are used as a very powerful mechanism for discovering crucial business information. Considering the extreme importance of the information managed by these kinds of applications, it is essential to specify security measures from the early stages of the DW design in the MD modeling process, and enforce them. In the past years, some proposals for representing main MD modeling properties at the conceptual level have been stated. Nevertheless, none of these proposals considers security issues as an important element in its model, so they do not allow us to specify confidentiality constraints to be enforced by the applications that will use these MD models. In this paper, we will discuss the specific confidentiality problems regarding DWs as well as present an extension of the Unified Modeling Language for specifying security constraints in the conceptual MD modeling, thereby allowing us to design secure DWs. One key advantage of our approach is that we accomplish the conceptual modeling of secure DWs independently of the target platform where the DW has to be implemented, allowing the implementation of the corresponding DWs on any secure commercial database management system. Finally, we will present a case study to show how a conceptual model designed with our approach can be directly implemented on top of Oracle 10g."
      }
    ]
  },
  {
    "tid":"27",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Improved Contextual Advertising Matching Approach based on Wikipedia Knowledge' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The current boom of the Web is associated with the revenues originated from Web advertising. As one prevalent type of Web advertising, contextual advertising refers to the placement of the most relevant commercial textual ads within the content of a Web page, so as to provide a better user experience and thereby increase the revenues of Web site owners and an advertising platform. Therefore, in contextual advertising, the relevance of selected ads with a Web page is essential. However, some problems, such as homonymy and polysemy, low intersection of keywords and context mismatch, can lead to the selection of irrelevant textual ads for a Web page, making that a simple keyword matching technique generally gives poor accuracy. To overcome these problems and thus to improve the relevance of contextual ads, in this paper we propose a novel Wikipedia-based matching technique which, using selective matching strategies, selects a certain amount of relevant articles from Wikipedia as an intermediate semantic reference model for matching Web pages and textual ads. We call this technique SIWI: Selective Wikipedia Matching, which, instead of using the whole Wikipedia articles, only matches the most relevant articles for a page (or a textual ad), resulting in the effective improvement of the overall matching performance. An experimental evaluation is conducted, which runs over a set of real textual ads, a set of Web pages from the Internet and a dataset of more than 260\u8117\u807d000 articles from Wikipedia. The experimental results show that our method performs better than existing matching strategies, which can deal with the matching over the large dataset of Wikipedia articles efficiently, and achieve a satisfactory contextual advertising effect."
      }
    ]
  },
  {
    "tid":"28",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Game Of Registrars: An Empirical Analysis Of Post-Expiration Domain Name Takeovers' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Every day, hundreds of thousands of Internet domain names are abandoned by their owners and become available for re-registration. Yet, there appears to be enough residual value and demand from domain speculators to give rise to a highly competitive ecosystem of drop-catch services that race to be the first to re-register potentially desirable domain names in the very instant the old registration is deleted. To pre-empt the competitive (and uncertain) race to re-registration, some registrars sell their own customers' expired domains pre-release, that is, even before the names are returned to general availability.These practices are not without controversy, and can have serious security consequences. In this paper, we present an empirical analysis of these two kinds of post expiration domain ownership changes. We find that 10 % of all com domains are re-registered on the same day as their old registration is deleted. In the case of org, over 50 % of re-registrations on the deletion day occur during only 30 s. Furthermore, drop-catch services control over 75 % of accredited domain registrars and cause more than 80 % of domain creation attempts, but represent at most 9.5 % of successful domain creations. These findings highlight a significant demand for expired domains, and hint at highly competitive re-registrations.Our work sheds light on various questionable practices in an opaque ecosystem. The implications go beyond the annoyance of websites turned into \"Internet graffiti\" [26], as domain ownership changes have the potential to circumvent established security mechanisms."
      }
    ]
  },
  {
    "tid":"29",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Pixy: A Static Analysis Tool for Detecting Web Application Vulnerabilities (Short Paper)' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The number and the importance of Web applications have increased rapidly over the last years. At the same time, the quantity and impact of security vulnerabilities in such applications have grown as well. Since manual code reviews are time-consuming, error-prone and costly, the need for automated solutions has become evident. In this paper, we address the problem of vulnerable Web applications by means of static source code analysis. More precisely, we use flow-sensitive, interprocedural and context-sensitive data flow analysis to discover vulnerable points in a program. In addition, alias and literal analysis are employed to improve the correctness and precision of the results. The presented concepts are targeted at the general class of taint-style vulnerabilities and can be applied to the detection of vulnerability types such as SQL injection, cross-site scripting, or command injection. Pixy, the open source prototype implementation of our concepts, is targeted at detecting cross-site scripting vulnerabilities in PHP scripts. Using our tool, we discovered and reported 15 previously unknown vulnerabilities in three web applications, and reconstructed 36 known vulnerabilities in three other web applications. The observed false positive rate is at around 50% (i.e., one false positive for each vulnerability) and therefore, low enough to permit effective security audits."
      }
    ]
  },
  {
    "tid":"30",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Similarity Measures for Closed General Type-2 Fuzzy Sets: Overview, Comparisons, and a Geometric Approach' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The similarity between two fuzzy sets (FSs) is an important concept in fuzzy logic. As the research interest on general type-2 (GT2) FSs has increased recently, many similarity measures for them have also been proposed. This paper gives a comprehensive overview of existing similarity measures for GT2 FSs, points out their limitations, and, by using an intuitive geometric explanation, proposes a Jaccard similarity measure for GT2 FSs that is an extension of the popular Jaccard similarity measure for type-1 and interval type-2 FSs. The fundamental difference between the proposed Jaccard similarity measure for GT2 FSs and all existing similarity measures is that the Jaccard similarity measure considers the overall geometries of two GT2 FSs and does not depend on a specific representation of the GT2 FSs, whereas all existing similarity measures for GT2 FSs depend either on the vertical slice representation or the \n<inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\alpha$</tex-math></inline-formula>\n-plane representation. We show that the Jaccard similarity measure for GT2 FSs satisfies four properties of a similarity measure and demonstrate its reasonableness using two examples."
      }
    ]
  },
  {
    "tid":"31",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Fuzzy-Based Cluster-Head Selection System for WSNs Considering Sensor Node Movement' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cluster formation and cluster head selection are important problems in sensor network applications and can drastically affect the network's communication energy dissipation. However, selecting of the cluster head is not easy in different environments which may have different characteristics. In order to deal with this problem, we propose a power reduction algorithm for sensor networks based on fuzzy logic and node movement. The proposed system uses 3 input linguistic parameters: remaining battery power of sensor, degree of number of neighbor nodes and sensor speed for cluster-head decision. By considering the moving speed of the sensor node, we are able to find the nodes which leave the cluster. We evaluate the proposed system by simulations and show that the proposed system has a good cluster-head selection."
      }
    ]
  },
  {
    "tid":"32",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Joint Cooperative-Transmit and Receive FDE for Single-Carrier Incremental Relaying' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose joint cooperative-transmit/receive frequency-domain equalization (cooperative-Tx/Rx FDE) with incremental relaying (IR) for broadband single-carrier (SC) transmission under the total and individual transmit power constraint at a source node ( S ) and a relay node ( R ). We derive the optimum cooperative-Tx/Rx FDE weights to minimize the mean square error (MSE) after packet combining at a destination node (\\ssr D ). We show that the optimum cooperative-Tx FDE weights allocate the transmit power in the minimum MSE (MMSE) sense in the frequency domain and in the maximal ratio transmission (MRT) sense in the spatial domain. To relax the condition that the complete channel state information (CSI) needs to be shared among all nodes, selection-based suboptimum cooperative-Tx FDE weights are derived. Computer simulation verifies the effectiveness of the proposed schemes and shows that the 5%-outage throughput of the system can be increased by around 30% over the conventional IR with only Rx FDE."
      }
    ]
  },
  {
    "tid":"33",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Differential Fault Attack against Grain family with very few faults and minimal assumptions.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The series of published works, related to differential fault attack (DFA) against the Grain family, require quite a large number (hundreds) of faults and also several assumptions on the locations and the timings of the faults injected. In this paper, we present a significantly improved scenario from the adversarial point of view for DFA against the Grain family of stream ciphers. Our model is the most realistic one so far as it considers that the cipher has to be re-keyed only a few times and faults can be injected at any random location and at any random point of time, i.e., no precise control is needed over the location and timing of fault injections. We construct equations based on the algebraic description of the cipher by introducing new variables so that the degrees of the equations do not increase. In line of algebraic cryptanalysis, we accumulate such equations based on the fault-free and faulty key-stream bits and solve them using the SAT Solver Cryptominisat-2.9.5 installed with SAGE 5.7. In a few minutes we can recover the state of Grain v1, Grain-128 and Grain-128a with as little as 10, 4 and 10 faults respectively."
      }
    ]
  },
  {
    "tid":"34",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Providing QoS strategies and cloud-integration to web servers by means of aspects' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The main responsibilities of a web server are to listen from the communication channel and to prepare replies to requests. Additional responsibilities include adapting processing activities, for example, through scheduling or request filtering, so as to satisfy Quality of Service QoS requirements. Typical QoS-related concerns address behavioural constraints e.g. response time bounds, satisfiable by scheduling the most urgent requests first and resource monitoring, for optimal use. Although such concerns are spread across several web server components, they should be handled separately from communication-related ones, for the sake of modularity."
      }
    ]
  },
  {
    "tid":"35",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Supporting User-Defined Granularities in a Spatiotemporal Conceptual Model' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Granularities are integral to spatial and temporal data. A large number of applications require storage of facts along with their temporal and spatial context, which needs to be expressed in terms of appropriate granularities. For many real-world applications, a single granularity in the database is insufficient. In order to support any type of spatial or temporal reasoning, the semantics related to granularities needs to be embedded in the database. Specifying granularities related to facts is an important part of conceptual database design because under-specifying the granularity can restrict an application, affect the relative ordering of events and impact the topological relationships. Closely related to granularities is indeterminacy, i.e., an occurrence time or location associated with a fact that is not known exactly. In this paper, we present an ontology for spatial granularities that is a natural analog of temporal granularities. We propose an upward-compatible, annotation-based spatiotemporal conceptual model that can comprehensively capture the semantics related to spatial and temporal granularities, and indeterminacy without requiring new spatiotemporal constructs. We specify the formal semantics of this spatiotemporal conceptual model via translation to a conventional conceptual model. To underscore the practical focus of our approach, we describe an on-going case study. We apply our approach to a hydrogeologic application at the United States Geologic Survey and demonstrate that our proposed granularity-based spatiotemporal conceptual model is straightforward to use and is comprehensive."
      }
    ]
  },
  {
    "tid":"36",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Performance measure of information systems (IS) in evolving computing environments: an empirical investigation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper argues that the contingent evaluation approach can help an organization to evaluate the impact of information systems (IS) on business performance. The structure of IS is one of the most important factors in applying the contingent approach, because it reflects the evolution of the computing environment and is also aligned with business objectives and strategies. We empirically derived four taxonomics of IS structure and tested the relationships between the derived typologies and the prioritized or adopted measures of their performance. Firms with different IS structure were found to consider some measures to be more appropriate than others. The empirical results support the idea that the contingent measurement approach has the potential to be an even more useful framework if it incorporates more contingency factors, such as environmental, organizational contexts, and other characteristics."
      }
    ]
  },
  {
    "tid":"37",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The minimum code length for clustering using the gray code' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose new approaches to exploit compression algorithms for clustering numerical data. Our first contribution is to design a measure that can score the quality of a given clustering result under the light of a fixed encoding scheme. We call this measure the Minimum Code Length (MCL). Our second contribution is to propose a general strategy to translate any encoding method into a cluster algorithm, which we call COOL (COding-Oriented cLustering). COOL has a low computational cost since it scales linearly with the data set size. The clustering results of COOL is also shown to minimize MCL. To illustrate further this approach, we consider the Gray Code as the encoding scheme to present GCOOL. G-COOL can find clusters of arbitrary shapes and remove noise. Moreover, it is robust to change in the input parameters; it requires only two lower bounds for the number of clusters and the size of each cluster, whereas most algorithms for finding arbitrarily shaped clusters work well only if all parameters are tuned appropriately. G-COOL is theoretically shown to achieve internal cohesion and external isolation and is experimentally shown to work well for both synthetic and real data sets."
      }
    ]
  },
  {
    "tid":"38",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title '5th international workshop on Traceability in Emerging Forms of Software Engineering (TEFSE 2009)' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Traceability of Emerging Forms of Software Engineering (TEFSE) 2009 will bring together researchers and practitioners to examine the challenges of recovering and maintaining traceability for the myriad forms of software engineering, from user needs to models to source code. In the 2007 instalment, TEFSE focused on the grand challenges of traceability. The 2009 instalment will focus on these and other emerging challenges in traceability."
      }
    ]
  },
  {
    "tid":"39",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient Partial Monitoring with Prior Information.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Partial monitoring is a general model for online learning with limited feedback: a learner chooses actions in a sequential manner while an opponent chooses outcomes. In every round, the learner suffers some loss and receives some feedback based on the action and the outcome. The goal of the learner is to minimize her cumulative loss. Applications range from dynamic pricing to label-efficient prediction to dueling bandits. In this paper, we assume that we are given some prior information about the distribution based on which the opponent generates the outcomes. We propose BPM, a family of new efficient algorithms whose core is to track the outcome distribution with an ellipsoid centered around the estimated distribution. We show that our algorithm provably enjoys near-optimal regret rate for locally observable partial-monitoring problems against stochastic opponents. As demonstrated with experiments on synthetic as well as real-world data, the algorithm outperforms previous approaches, even for very uninformed priors, with an order of magnitude smaller regret and lower running time."
      }
    ]
  },
  {
    "tid":"40",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Joint Charge and Thermal Management for Batteries in Portable Systems With Hybrid Power Sources.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper introduces a joint charge and thermal management problem for batteries in a battery-supercapacitor hybrid power source of a portable system, which has been equipped with a forced convection cooling technique, such as a fan. A key consideration in such a system is that the battery aging depends strongly on the battery temperature, which is in turn a function of the workload running on th..."
      }
    ]
  },
  {
    "tid":"41",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Mobile Vision System for Urban Detection with Informative Local Descriptors' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present a computer vision system for the detection and identification of urban objects from mobile phone imagery, e.g., for the application of tourist information services. Recognition is based on MAP decision making over weak object hypotheses from local descriptor responses in the mobile imagery. We present an improvement over the standard SIFT key detector [7] by selecting only informative (i-SIFT) keys for descriptor matching. Selection is applied first to reduce the complexity of the object model and second to accelerate detection by selective filtering. We present results on the MPG-20 mobile phone imagery with severe illumination, scale and viewpoint changes in the images, performing with \u2248 98% accuracy in identification, efficient (100%) background rejection, efficient (0%) false alarm rate, and reliable quality of service under extreme illumination conditions, significantly improving standard SIFT based recognition in every sense, providing - important for mobile vision - runtimes which are \u2248 8 (\u224824) times faster for the MPG-20 (ZuBuD) database."
      }
    ]
  },
  {
    "tid":"42",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Ontology maintenance in a hierarchical federated collaborative product development environment' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a novel approach aiming to dynamically maintain the collaboration ontology in the execution of an ontology-based federated collaborative product development system when a federate joins or has resigned from a given federation. The proposed approach includes two algorithms: ontology maintenance (+) and ontology maintenance (-), corresponding to joining and resigning situations. It adopts an axiom-based deduction ontology fusion strategy, and takes heavy-weighted ontologies into consideration. It can find all the explicit and derived inter-ontology relations, and furthermore it reaches the active upper bounds of implicit equivalent inter-ontology relations searching. This paper also discusses some implementation issues on the basis of TH_RTI, a RTI (Run Time Infrastructure) version developed by National CIMS ERC, Tsinghua University. The proposed approach has great potential to improve the efficiency of ontology-based federated collaboration executions, reduce the work load for adaptive adjustment of ever-existing platforms, and enhance the applicability and flexibility of collaborative product development systems."
      }
    ]
  },
  {
    "tid":"43",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Temporal rule induction for clinical outcome analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Clinical outcomes analysis normally covers a particular time period. The sample under study is constantly changing as patients are censored, leave the study or die. In this paper, we present a novel data mining approach to mine temporal rules that reflect characteristics of outcomes analysis. We apply our temporal rule induction algorithm to a set of cancer patients, clinical records that were prospectively collected for 20 years. We analyse clinical data not only based on the static event, such as local recurrence for survival analysis, but also based on the temporal event with censored data for each time unit. The rules extracted from our temporal rule induction algorithm are compared to results from statistical analysis. The importance of this paper is that this novel temporal rule induction algorithm provides valuable insights for clinical data assessment and complements traditional statistical analysis."
      }
    ]
  },
  {
    "tid":"44",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A New Sampling Technique for Tensors.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"  In this paper we propose new techniques to sample arbitrary third-order tensors, with an objective of speeding up tensor algorithms that have recently gained popularity in machine learning. Our main contribution is a new way to select, in a biased random way, only $O(n^{1.5}/\\epsilon^2)$ of the possible $n^3$ elements while still achieving each of the three goals: \\\\ {\\em (a) tensor sparsification}: for a tensor that has to be formed from arbitrary samples, compute very few elements to get a good spectral approximation, and for arbitrary orthogonal tensors {\\em (b) tensor completion:} recover an exactly low-rank tensor from a small number of samples via alternating least squares, or {\\em (c) tensor factorization:} approximating factors of a low-rank tensor corrupted by noise. \\\\ Our sampling can be used along with existing tensor-based algorithms to speed them up, removing the computational bottleneck in these methods. "
      }
    ]
  },
  {
    "tid":"45",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A moment-based variational approach to tomographic reconstruction.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We describe a variational framework for the tomographic reconstruction of an image from the maximum likelihood (ML) estimates of its orthogonal moments. We show how these estimated moments and their (correlated) error statistics can be computed directly, and in a linear fashion from given noisy and possibly sparse projection data. Moreover, thanks to the consistency properties of the Radon transform, this two-step approach (moment estimation followed by image reconstruction) can be viewed as a statistically optimal procedure. Furthermore, by focusing on the important role played by the moments of projection data, we immediately see the close connection between tomographic reconstruction of nonnegative valued images and the problem of nonparametric estimation of probability densities given estimates of their moments. Taking advantage of this connection, our proposed variational algorithm is based on the minimization of a cost functional composed of a term measuring the divergence between a given prior estimate of the image and the current estimate of the image and a second quadratic term based on the error incurred in the estimation of the moments of the underlying image from the noisy projection data. We show that an iterative refinement of this algorithm leads to a practical algorithm for the solution of the highly complex equality constrained divergence minimization problem. We show that this iterative refinement results in superior reconstructions of images from very noisy data as compared with the classical filtered back-projection (FBP) algorithm."
      }
    ]
  },
  {
    "tid":"46",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Developing an optimized application hosting framework in Clouds' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A Cloud system, which is also often called a data center, may host a large number of application servers (termed as applications for short in this paper) that consume various types of resource, such as CPU, memory, and I/O. In such a scenario, the application placement strategy, which specifies how to host applications across the physical servers in the Cloud, is critical to achieve good system- and/or client-oriented performance. The existing application placement algorithms in literature assume that the consumption of system resources is proportional to the level of workloads submitted to the system. In this paper, we reveal that it may not be the case in some circumstances. Based on this observation, we design and implement an application placement framework, called EAPAC, for Clouds. The developed framework is able to achieve the optimized performance by judiciously allocating to the applications a proper mixture of different types of application request as well as an appropriate number of requests in each type. Further, we investigate the issue of resource conflicts among different applications when there exist concurrent requests in the system. We have conducted extensive experiments to evaluate the performance of the developed framework. The experiment results show that compared with the existing method in literature, EAPAC can improve the performance by 35% in terms of the reply rate."
      }
    ]
  },
  {
    "tid":"47",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'CRP: Cluster head Re-election Protocol for heterogeneous wireless sensor networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Sensor nodes in a network consume energy in a non-uniform manner. Designing clustering protocols that are heterogeneity aware is still an open issue in sensor networks. This work focuses on hierarchically clustered heterogeneous sensor networks. Sensor nodes organize themselves in self organized groups called clusters. Each cluster consists of a cluster head and some member nodes. The sensor network consists of nodes of two different energy levels. We show that current heterogeneity aware protocols (such as SEP [1], DEEC [2] etc.) are unable to distribute the usage of energy amongst the sensor nodes uniformly. We propose CRP, Cluster Re-election Protocol, that is heterogeneity aware and enhances the network stability period (period before which the first node dies) over current protocols. We show, by simulation, that CRP improves network stability by reducing the variation in the residual energy of sensor nodes. Therefore. CRP is able to exploit the heterogeneity present in a sensor network in an efficient manner."
      }
    ]
  },
  {
    "tid":"48",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'JavaGI: generalized interfaces for java' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"JavaGI is an experimental language that extends Java 1.5 by generalizing the interface concept to incorporate the essential features of Haskell's type classes. In particular, generalized interfaces cater for retroactive and constrained interface implementations, binary methods, static methods in interfaces, default implementations for interface methods, interfaces over families of types, and existential quantification for interface-bounded types. As a result, many anticipatory uses of design patterns such as Adapter, Factory, and Visitor become obsolete; several extension and integration problems can be solved more easily. JavaGI's interface capabilities interact with subtyping (and subclassing) in interesting ways that go beyond type classes. JavaGI can be translated to Java 1.5. Its formal type system is derived from Featherweight GJ."
      }
    ]
  },
  {
    "tid":"49",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient Routing in Intermittently Connected Mobile Networks: The Multiple-Copy Case' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Intermittently connected mobile networks are wireless networks where most of the time there does not exist a complete path from the source to the destination. There are many real networks that follow this model, for example, wildlife tracking sensor networks, military networks, vehicular ad hoc networks, etc. In this context, conventional routing schemes fail, because they try to establish complete end-to-end paths, before any data is sent. To deal with such networks researchers have suggested to use flooding-based routing schemes. While flooding-based schemes have a high probability of delivery, they waste a lot of energy and suffer from severe contention which can significantly degrade their performance. Furthermore, proposed efforts to reduce the overhead of flooding-based schemes have often been plagued by large delays. With this in mind, we introduce a new family of routing schemes that \"spray\" a few message copies into the network, and then route each copy independently towards the destination. We show that, if carefully designed, spray routing not only performs significantly fewer transmissions per message, but also has lower average delivery delays than existing schemes; furthermore, it is highly scalable and retains good performance under a large range of scenarios. Finally, we use our theoretical framework proposed in our 2004 paper to analyze the performance of spray routing. We also use this theory to show how to choose the number of copies to be sprayed and how to optimally distribute these copies to relays."
      }
    ]
  },
  {
    "tid":"50",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The PoSIM middleware for translucent and context-aware integrated management of heterogeneous positioning systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The widespread availability of devices with multiple wireless interfaces and of heterogeneous positioning techniques open new market potentials for Location Based Services (LBSs), even if complicating their development. The paper claims the need for novel middlewares to manage dynamically retrieved client-side positioning systems in a synergic way and depending on context, i.e., LBS requirements, user preferences, device characteristics, and overall system state. To that purpose, we have designed and implemented PoSIM, a context-aware middleware for the synergic exploitation and control of heterogeneous positioning systems that facilitates the development and portability of LBSs. PoSIM is translucent, i.e., can provide LBS developers with differentiated visibility of data characteristics and control possibilities, thus dynamically adapting to application-specific deployment requirements and enabling cross-layer management decisions. The paper describes the translucent PoSIM architecture, some primary implementation insights, and how to practically use it to simplify LBS development via events, filters, and policies."
      }
    ]
  },
  {
    "tid":"51",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'MDSI: Signal Integrity Interconnect Fault Modeling and Testing for SoCs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Unacceptable loss of signal integrity may cause permanent or intermittent harm to the functionality and performance of SoCs. In this paper, we present an abstract model and a new test pattern generation method of signal integrity problems on interconnects. This approach is achieved by considering the effects for testing inputs and parasitic RLC elements of interconnects. We also develop a framework to deal with arbitrary interconnection topology. Experimental results show that the proposed signal integrity fault model is more exact and more powerful for long interconnects than previous approaches."
      }
    ]
  },
  {
    "tid":"52",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Enhancement of Residual Echo for Robust Acoustic Echo Cancellation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper examines the technique of using a noise-suppressing nonlinearity in the adaptive filter error feedback-loop of an acoustic echo canceler (AEC) based on the least mean square (LMS) algorithm when there is an interference at the near end. The source of distortion may be linear, such as local speech or background noise, or nonlinear due to speech coding used in the telecommunication networks. Detailed derivation of the error recovery nonlinearity (ERN), which \u201cenhances\u201d the filter estimation error prior to the adaptation in order to assist the linear adaptation process, will be provided. Connections to other existing AEC and signal enhancement techniques will be revealed. In particular, the error enhancement technique is well-founded in the information-theoretic sense and has strong ties to independent component analysis (ICA), which is the basis for blind source separation (BSS) that permits unsupervised adaptation in the presence of multiple interfering signals. The single-channel AEC problem can be viewed as a special case of semi-blind source separation (SBSS) where one of the source signals is partially known, i.e., the far-end microphone signal that generates the near-end acoustic echo. The system approach to robust AEC will be motivated, where a proper integration of the LMS algorithm with the ERN into the AEC \u201csystem\u201d allows for continuous and stable adaptation even during double talk without precise estimation of the signal statistics. The error enhancement paradigm encompasses many traditional signal enhancement techniques and opens up an entirely new avenue for solving the AEC problem in a real-world setting."
      }
    ]
  },
  {
    "tid":"53",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Fault-Tolerant Environment For Large-Scale Query Processing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"As datasets are increasing in size, the data management and processing needs are being met with added parallelism, i.e, by involving more nodes and/or cores in the system. This, in turn, is increasing the chances of failures during processing. In this paper, we present the design and implementation of a fault-tolerant environment for processing queries on large scientific dataset. Our systems meet the following three requirements that we consider essential for any such environment: 1) high efficiency of execution of a particular data analysis task or query, when there are no failures, 2) ability to handle failure of up to a certain number of nodes, and 3) only a modest slowdown in processing times of data analysis task or a query when there are failures. We address these challenges by developing a new data replication scheme, which we refer to as subchunk or subpartition replication. Our system currently supports two types of queries: range queries on spatial data and aggregation queries on point datasets, but the underlying ideas can be extended to other query types as well. Our extensive evaluation shows that we can handle single and rack failures with only modest slowdowns, and particularly, clearly outperform the traditional (chunk or partition replication) schemes."
      }
    ]
  },
  {
    "tid":"54",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Towards Building an Intelligent Traffic Simulation Platform' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Traffic congestion has become a major concern for many cities throughout the world. Simulations provide useful tools for engineer to plan traffic systems and government to make decisions. The microscopic traffic simulation approach defines the behavior of each interactive object in the traffic network, such as a vehicle and a traffic light through an individual model so that we can observe the detail traffic information of a scenario. Since microscopic model requires large computational power and data storage power, new simulation system architecture is needed. In the paper, the background of this research work is introduced. The simulation platform which combines grid, agent and HLA is given. Some considerations about microscopic traffic simulation models and technical issues of this platform are also discussed."
      }
    ]
  },
  {
    "tid":"55",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Real-world testing of a multi-robot team' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Multi-robot systems (MRS) have received a great deal of attention recently due to their potential to address complex distributed tasks such as environmental monitoring, search and rescue, agriculture, and security [3, 4, 5, 1, 2]. One specific type of multi-robot system that has significant near term promise is fleets of autonomous watercraft for applications such as flood response, water monitoring and bathymetry. Small watercraft are an attractive option for real world multi-robot systems because some of the most critical robotic problems are minimized on water - movement is relatively simple and dangers are relatively low."
      }
    ]
  },
  {
    "tid":"56",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'W3W: Energy Management of Hybrid Energy Supplied Sensors for Internet of Things.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The usage of hybrid energy supplied sensors in the Internet of Things has enabled longer lifetime of sensors and expanded scope of applications. These sensors can combine advantages of environmental energy harvesting techniques and wireless energy harvesting techniques. However, how to coordinate them is still a challenge and has not been studied extensively. In this article, we present a system based on mobile crowd wireless charging to manage energy of hybrid energy supplied sensors. When environmental energy is insufficient, the system will utilize smart devices carried by mobile users as chargers to provide wireless energy. We construct and study a W3W problem in the system: <underline>w</underline>hen to leverage mobile crowd wireless charging to support rechargeable sensors, <underline>w</underline>here to perform wireless energy transfer, and <underline>w</underline>hom to allocate and incentivize as chargers to maximize useful energy value over all sensors subject to a budget. In order to control the actual quality of wireless energy charging, we propose a design principle named task completion trustfulness. We consider offline and online conditions and design corresponding algorithms with incentive allocations. Extensive simulations are conducted to demonstrate the effectiveness of our algorithms, which also validates our theoretical results.\n\n"
      }
    ]
  },
  {
    "tid":"57",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Word Order Correction For Language Transfer Using Relative Position Language Modeling' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Sentence correction has been an important and emerging issue in computer-assisted language learning. However, existing techniques based on grammar rules or statistical machine translation are still not robust enough to tackle the common incorrect word order errors in sentences produced by second language learners of Chinese. In this paper, a novel relative position. language model is proposed to address this problem, for which a corpus of erroneous English-Chinese language transfer sentences along with their corrected counterparts is created and manually judged by human annotators. Experimental results show that compared to a scoring approach based on an n-gram language model and a phrase-based machine translation system, the performance in terms of BLEU scores of the proposed approach achieved improvements of 20.3% and 26.5% for the correction of word order errors resulting from language transfer, respectively."
      }
    ]
  },
  {
    "tid":"58",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Provably good solutions for the traveling salesman problem' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The determination of true optimum solutions of combinatorial optimization problems is seldomly required in practical applications. The majority of users of optimization software would be satisfied with solutions of guaranteed quality in the sense that it can be proven that the given solution is at most a few percent off an optimum solution. This paper presents a general framework for practical problem solving with emphasis on this aspect. A detailed discussion along with a report about extensive computational experiments is given for the traveling salesman problem."
      }
    ]
  },
  {
    "tid":"59",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A stable target-tracking control for unicycle mobile robots' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper deals with target tracking control of unicycle type mobile robots. Target tracking function is essential for autonomous robots such as guide robots, security guard robots, etc. In the field of mobile robot control, many control schemes for posture stabilization and trajectory tracking problem have been proposed. Target tracking control, however, cannot be achieved using these kinds of control laws. Therefore, a new global asymptotic stable controller for this problem is designed using the backstepping method. The stability of the system is proved using the Lyapunov function. Various simulation results validate the performance and theoretical analysis"
      }
    ]
  },
  {
    "tid":"60",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Analysis Of Deficit Round Robin Scheduling For Future Aeronautical Data Link' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The aviation community is currently working on the standardization of data communication systems for the future air traffic management. In this context, the International Civil Aviation Organization (ICAO) has initiated a work on standardization of an IPv6-based aeronautical telecommunications network and on future radio access technologies, respectively. In this paper, we integrate L-Band Digital Aeronautical Communications System Option 1 (L-DACS 1), which is one candidate for future radio access technologies, with realistic IPv6-based network layer functionality and analyze Deficit Round Robin (DRR) with fragmentation algorithm for the forward and return link in L-DACS 1. Our analysis mainly covers two application domains: file transfer and real-time services. We show that DRR with fragmentation scheduler provides good performance results in terms of throughput, delay, and bandwidth fairness."
      }
    ]
  },
  {
    "tid":"61",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimal coding for transmission energy minimization in wireless nanosensor networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Wireless nanosensor networks (WNSNs), which consist of nanosensors with the size of just a few hundred nanometers, can sense new types of events at nanoscale and thus are promising for lots of important applications, like intrabody drug delivery, air pollution surveillance, etc. Since nanosensors are highly energy-constrained, it is critical to adopt energy-efficient protocols for communications in WNSNs. This paper focuses on WNSNs adopting the on\u2013off keying modulator which transmits a signal for a high bit and keeps silence for a low bit and explores the optimal coding design for transmission energy minimization in such networks. Specifically, we focus on the design of low-weight code which maps m-bit symbols into n-bit codewords with fewer high bits. First, for given symbol length m and codeword length n, we determine the optimal codebook with the least average number of high bits per codeword and give the corresponding average codeword weight as a function of m and n. Then, we study the minimum transmission energy (MTE) code design problem with codeword length and codeword rate constraints and investigate how the transmission energy changes when m and n are varied. Finally, the corresponding optimal settings of m and n are determined for the MTE code design problem to achieve the minimum transmission energy consumption per data bit."
      }
    ]
  },
  {
    "tid":"62",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Pinball Loss Minimization for One-bit Compressive Sensing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"  The one-bit quantization can be implemented by one single comparator, which operates at low power and a high rate. Hence one-bit compressive sensing (\\emph{1bit-CS}) becomes very attractive in signal processing. When the measurements are corrupted by noise during signal acquisition and transmission, 1bit-CS is usually modeled as minimizing a loss function with a sparsity constraint. The existing loss functions include the hinge loss and the linear loss. Though 1bit-CS can be regarded as a binary classification problem because a one-bit measurement only provides the sign information, the choice of the hinge loss over the linear loss in binary classification is not true for 1bit-CS. Many experiments show that the linear loss performs better than the hinge loss for 1bit-CS. Motivated by this observation, we consider the pinball loss, which provides a bridge between the hinge loss and the linear loss. Using this bridge, two 1bit-CS models and two corresponding algorithms are proposed. Pinball loss iterative hard thresholding improves the performance of the binary iterative hard theresholding proposed in [6] and is suitable for the case when the sparsity of the true signal is given. Elastic-net pinball support vector machine generalizes the passive model proposed in [11] and is suitable for the case when the sparsity of the true signal is not given. A fast dual coordinate ascent algorithm is proposed to solve the elastic-net pinball support vector machine problem, and its convergence is proved. The numerical experiments demonstrate that the pinball loss, as a trade-off between the hinge loss and the linear loss, improves the existing 1bit-CS models with better performances. "
      }
    ]
  },
  {
    "tid":"63",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Robust MIMO Cognitive Radio Systems Under Interference Temperature Constraints' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cognitive Radio (CR) systems are built on the coexistence of primary users (PUs) and secondary users (SUs), the latter being allowed to share spectral resources with the PUs but under strict interference limitations. However, such limitations may easily be violated by SUs if perfect SU-to-PU channel state information (CSI) is not available at the secondary transmitters, which always happens in practice. In this paper, we propose a distributed design of MIMO CR networks under global interference temperature constraints that is robust (in the worst-case sense) against SU-to-PU channel uncertainties. More specifically, we consider two alternative formulations that are complementary to each other in terms of signaling and system performance, namely: a game-theoretical design and a social-oriented optimization. To study and solve the proposed formulations we hinge on the new theory of finite-dimensional variational inequalities (VI) in the complex domain and a novel parallel decomposition technique for nonconvex sum-utility problems with coupling constraints, respectively. A major contribution of this paper is to devise a new class of distributed best-response algorithms with provable convergence. The algorithms differ in computational complexity, convergence speed, communication overhead, and achievable performance; they are thus applicable to a variety of CR scenarios, either cooperative or non-cooperative, which allow the SUs to explore the trade-off between signaling and performance."
      }
    ]
  },
  {
    "tid":"64",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Zombie Chasing: Efficient Flash Management Considering Dirty Data in the Buffer Cache' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a novel technique, called Zombie Chasing, for efficient flash management in solid state drives (SSDs). Due to the unique characteristics of NAND flash memory, SSDs need to accurately understand the liveness of the data stored in themselves. Recently, the TRIM command has been introduced to notify SSDs of dead data caused by file deletions, which otherwise could not be tracked by SSDs. This paper goes one step further and proposes a new liveness state, called the zombie state, to denote live data that will be dead shortly due to the corresponding dirty data in the buffer cache. We also devise new zombie-aware garbage collection algorithms which utilize the information about such zombie data inside SSDs. To evaluate Zombie Chasing, we implement zombie-aware garbage collection algorithms in the prototype SSD and modify the Linux kernel and the Oracle DBMS to deliver the information on the zombie data to the prototype SSD. Through comprehensive evaluations using our in-house micro-benchmark and the TPC-C benchmark, we observe that Zombie Chasing improves SSD performance effectively by reducing garbage collection overhead. Especially, our evaluation with the TPC-C benchmark on the Oracle DBMS shows that Zombie Chasing enhances the Transactions Per Second (TPS) value by up to 22% with negligible overhead."
      }
    ]
  },
  {
    "tid":"65",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Subset Sum in the Absence of Concentration.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We study the exact time complexity of the Subset Sum problem. Our focus is on instances that lack additive structure in the sense that the sums one can form from the subsets of the given integers are not strongly concentrated on any particular integer value. We present a randomized algorithm that runs in O(2(0.3399n) B-4) time on instances with the property that no value can arise as a sum of more than B different subsets of the n given integers."
      }
    ]
  },
  {
    "tid":"66",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An empirical study of information contribution to online feedback systems: A motivation perspective' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Online feedback systems (OFSs) are increasingly available on online shopping websites; they allow consumers to post their ratings and consumption reviews for products. We employed motivation theory and a goal attainment perspective to model a set of motivating and inhibiting factors that could influence a consumer's intention to contribute to an OFS. Our experiment, which involved 168 university students, showed that a consumer's intention to contribute product reviews is influenced by perceived satisfaction gained in helping other consumers, perceived satisfaction gained in influencing the merchant, perceived probability of enhancing self-image, and perceived executional costs. In addition, the presence of an economic rewarding mechanism was found to promote a contribution when a consumer's perceived probability of enhancing self-image was relatively high or when perceived cognitive cost was relatively low. Implications of our findings are discussed."
      }
    ]
  },
  {
    "tid":"67",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Finding parametric representations of the cortical sulci using an active contour model' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The cortical sulci are brain structures resembling thin convoluted ribbons embedded in three dimensions. The importance of the sulci lies primarily in their relation to the cytoarchitectonic and functional organization of the underlying cortex and in their utilization as features in non-rigid registration methods. This paper presents a methodology for extracting parametric representations of the cerebral sulci from magnetic resonance images. The proposed methodology is based on deformable models utilizing characteristics of the cortical shape. Specifically, a parametric representation of a sulcus is determined by the motion of an active contour along the medial surface of the corresponding cortical fold. The active contour is initialized along the outer boundary of the brain and deforms toward the deep root of a sulcus under the influence of an external force field, restricting it to lie along the medial surface of the particular cortical fold. A parametric representation of the medial surface of the sulcus is obtained as the active contour traverses the sulcus. Based on the first fundamental form of this representation, the location and degree of an interruption of a sulcus can be readily quantified; based on its second fundamental form, shape properties of the sulcus can be determined. This methodology is tested on magnetic resonance images and it is applied to three medical imaging problems: quantitative morphological analysis of the central sulcus; mapping of functional activation along the primary motor cortex and non-rigid registration of brain images."
      }
    ]
  },
  {
    "tid":"68",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Improving the Energy Efficiency of Two-Tier Heterogeneous Cellular Networks through Partial Spectrum Reuse' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Partial Spectrum Reuse (PSR) in the second tier of two-tier heterogeneous cellular networks has a potential to improve spectrum efficiency by reducing inter-cell interference, and thus energy efficiency as well by deploying less or switching off more Base Stations (BSs). In this paper, we analyze the optimal PSR factor, defined as the portion of spectrum reused by micro cells in two-tier heterogeneous networks, which is not in an explicit form generally. Then, a closed-form limit of the optimal PSR factor is derived as the ratio of the user rate requirement over the whole system spectrum bandwidth is approaching zero, based on which a threshold of the micro-BS energy cost is also derived to determine which type of BSs is preferable. Specifically, one should deploy more micro BSs or switch off more macro BSs if the micro-BS energy cost is lower than the threshold. Otherwise, the optimal choice is the opposite. This threshold with the PSR scheme is higher than that without PSR scheme, i.e., PSR can improve both spectrum efficiency and energy efficiency. Numerical results show that adopting PSR can reduce the network energy consumption by up to 50% when the transmit power of macro BSs is 10dB higher than that of micro BSs."
      }
    ]
  },
  {
    "tid":"69",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'RGMP-ROS: A real-time ROS architecture of hybrid RTOS and GPOS on multi-core processor' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recently, the open-source robot operating system (ROS) has been growing rapidly in the robotics community. However, the ROS runs on Linux, which does not provide timing guarantees for robot motion. This paper present a hybrid real-time ROS architecture on multi-core processor \u201cRGMP-ROS\u201d, which consists of two parts including the non-real-time subsystem \u201cGPOS (General Operating system)\u201d and the real-time one \u201cRTOS (Real-time Operating system)\u201d. The GPOS is comprised of non-real-time ROS nodes running in Linux, while the RTOS only contains real-time ROS nodes running in Nuttx. To get higher operational efficiency, the RGMP-ROS system is executed by a dual-core processor, one CPU for GPOS and the other for RTOS. The RGMP-ROS has used in the controller of a 6-DOF modular manipulator, and its effectiveness and efficiency are demonstrated by software testing and experiments. The main contributions of the present work lie in the realization of real-time ROS architecture and the application of multi-core processor in the hybrid control of an industrial robot."
      }
    ]
  },
  {
    "tid":"70",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'IMPROVING MUSICAL CONCEPT DETECTION BY ORDINAL REGRESSION AND CONTEXT FUSION' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Tofacilitateinformationretrievaloflarge-scalemusicdata- bases, the detection of musical concepts, or auto-tagging, has been an active research topic. This paper concerns the use of concept correlations to improve musical concept de- tection. We propose to formulate concept detection as an ordinal regression problem to explicitly take advantage of the ordinal relationship between concepts and avoid the data imbalance problem of conventional multi-label clas- sification methods. To further improve the detection ac- curacy, we propose to leverage the co-occurrence patterns of concepts for context fusion and employ concept selec- tion to remove irrelevant or noisy concepts. Evaluation on the cal500 dataset shows that we are able to improve the detection accuracy of 174 concepts from 0.2513 to 0.2924."
      }
    ]
  },
  {
    "tid":"71",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Min-Actor algorithm for connected dominating set formation in WSAN' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"As the virtual backbone of the network, connected dominating set (CDS) plays an important role in supporting data communication, reducing the routing overhead, and enhancing the scalability of the network. Traditional CDS construction algorithms are usually applied to the networks with homogeneous nodes, e.g., a pure wireless sensor network. In the past few years, Wireless Sensor and Actor Networks (WSANs) have attracted growing attentions due to its capabilities of sensing and gathering information, as well as performing certain tasks by actor nodes. One of the most critical issues in WSAN is how to construct an appropriate CDS with the consideration of heterogeneous nodes. Specifically, how to construct the CDS that contains minimum number of actors, so that the movement of actors poses a minimal threat to the integrity of the CDS. In this paper, we take the nature of the actor nodes into consideration and propose a CDS construction algorithm named Min-Actor algorithm (MIA), which can reduce nodes' movement constraints in WSAN. Simulation results confirm that MIA can effectively reduce the number of the actors in CDS, thus reducing the impact of actors' movement on the network."
      }
    ]
  },
  {
    "tid":"72",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title '\"do you see yonder cloud?\"-on priming concepts, a new test, and a familiar outcome. reply to lucas et al.: \"familiarity or conceptual priming? good question! comment on stenberg, hellman, johansson, and ros\u00e9n (2009)\"' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Lucas, Voss, and Paller sympathize with our intentions but disagree with our findings. They argue that a relation between frequency and conceptual priming may have been obscured by methodological details in our second experiment, therefore failing to complete a bridge between conceptual priming and FN400 with name frequency as the mediator. However, renewed inspections of our experiment and a new additional experiment, designed to meet the objections, fail to find any role for name frequency in conceptual priming and therefore re-establish the dissociation of priming and the FN400. On closer inspection, our differing views seem to derive from different interpretations of the term \u201cconcept.\u201d"
      }
    ]
  },
  {
    "tid":"73",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A complete algorithm for synthesizing modular fixtures for polygonal parts' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Commercially-available modular fixturing systems typically include a square lattice of tapped and bushed holes with precise spacing and an assortment of precision locating and clamping elements that can be rigidly attached to the lattice using dowel pins or expanding mandrels. Currently, human expertise is required to synthesize a suitable arrangement of these elements to hold a given part. Besides being time consuming, if the set of alternatives is not systematically explored, the designer may fail to find an acceptable fixture or may settle upon a suboptimal fixture. The authors consider a class of modular fixtures that prevent a part from translating or rotating in the plane using four point contacts on the part's boundary. These fixtures are based on three round locators, each centered on a lattice point, and one translating clamp. The authors present an algorithm that accepts a polygonal part shape as input and synthesizes the set of all fixture designs that achieve form closure for the given part. The algorithm also allows the user to specify geometric access constraints on fixtures. If the part has n edges and its maximal diameter is d lattice units, the asymptotic running time of the algorithm is O(n5d5). The authors have implemented the algorithm and present example fixtures that it has synthesized. This implementation includes a metric to rank fixtures based on their ability to resist applied forces. The authors believe this is the first fixture synthesis algorithm that is complete in the sense that it is guaranteed to find an admissible fixture if one exists. Furthermore, the algorithm is guaranteed to find the optimal fixture, relative to any well-defined quality metric"
      }
    ]
  },
  {
    "tid":"74",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'CI-Graph: An efficient approach for large scale SLAM' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"When solving the Simultaneous Localization and Mapping (SLAM) problem, submapping and graphical methods have shown to be valuable approaches that provide significant advantages over the standard EKF solution: they are faster and can produce more consistent estimates when using local coordinates. In this paper we present CI-Graph, a submapping method for SLAM that uses a graph structure to efficiently solve complex trajectories reducing the computational cost. Unlike other submapping SLAM approaches, we are able to transmit and share information through maps in the graph in a consistent manner by using conditionally independent submaps. In addition, the current submap always summarizes, without further computations, all information available making CI-Graph be an intrinsically \u201cup to date\u201d algorithm. Moreover, the technique is also efficient in memory requirements since it does not need to recover the full covariance matrix. To evaluate CI-Graph performance, the method has been tested using a synthetic Manhattan world and Victoria Park data set."
      }
    ]
  },
  {
    "tid":"75",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Preserving the Context of Interrupted Business Process Activities' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The capability to safely interrupt business process activities is an important requirement for advanced process-aware information systems. Indeed, exceptions stemming from the application environment often appear while one or more application-related process activities are running. Safely interrupting an activity consists of preserving its context, i.e., saying the data associated with this activity. This is important since possible solutions for an exceptional situation are often based on the current data context of the interrupted activity. In this paper, a data classification scheme based on data relevance and on data update frequency is proposed and discussed with respect to two different real-world applications. Taking into account this classification. a correctness criterion for interrupting running activities while preserving their context is proposed and analyzed."
      }
    ]
  },
  {
    "tid":"76",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Automata Evaluation and Text Search Protocols with Simulation-Based Security.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents efficient protocols for securely computing the following two problems: (1) The fundamental problem of pattern matching. This problem is defined in the two-party setting, where party $$P_1$$P1 holds a pattern and party $$P_2$$P2 holds a text. The goal of $$P_1$$P1 is to learn where the pattern appears in the text, without revealing it to $$P_2$$P2 or learning anything else about $$P_2$$P2's text. This problem has been widely studied for decades due to its broad applicability. We present several protocols for several notions of security. We further generalize one of our solutions to solve additional pattern matching-related problems of interest. (2) Our construction from above, in the malicious case, is based on a novel protocol for secure oblivious automata evaluation which is of independent interest. In this problem, party $$P_1$$P1 holds an automaton and party $$P_2$$P2 holds an input string, and they need to decide whether the automaton accepts the input, without learning anything else. Our protocol obtains full security in the face of malicious adversaries."
      }
    ]
  },
  {
    "tid":"77",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'TriSI: A Distinctive Local Surface Descriptor for 3D Modeling and Object Recognition.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Local surface description is a critical stage for surface matching. This paper presents a highly distinctive local surface descriptor, namely TriSI. From a keypoint, we first construct a unique and repeatable local reference frame (LRF) using all the points lying on the local surface. We then generate three spin images from the three coordinate axes of the LRF. These spin images are concatenated and further compressed into a TriSI descriptor using the principal component analysis technique. We tested our TriSI descriptor on the Bologna Dataset and compared it to several existing methods. Experimental results show that TriSI outperformed existing methods under all levels of noise and varying mesh resolutions. The TriSI was further tested to demonstrate its effectiveness in 3D modeling. Experimental results show that it can accurately perform pairwise and multiview range image registration. We finally used the TriSI descriptor for 3D object recognition. The results on the UWA Dataset show that TriSI outperformed the state-of-the-art methods including spin image, tensor and exponential map. The TriSI based method achieved a high recognition rate of 98.4%."
      }
    ]
  },
  {
    "tid":"78",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Conformance Testing of Hybrid Systems with Qualitative Reasoning Models' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Embedded systems are of growing importance in industry. For example, in a today's vehicle a huge number of embedded and communicating systems can be found. Exhaustive testing of such systems is a requirement, because changes after delivery and use are expensive and sometimes even impossible. In this paper we propose the use of qualitative models, which are an abstraction of quantitative physical models, for test case generation and test execution. In particular, we show how Simulink models from which control programs are automatically extracted can be tested with respect to qualitative models. Since Simulink models are heavily used in industry, the approach is of practical interest."
      }
    ]
  },
  {
    "tid":"79",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Parallel Algorithm for Timing-driven Global Routing for Standard Cells' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The timing-driven global routing problem is an extremely important and time consuming phase of any automated layout system. In this paper, by integrating high performance interconnection tree construction, wire-sizing, and switch-able segment channel optimization together, we propose an adaptive timing-driven global routing algorithm which minimizes the timing delay as well as circuit area. Our experiments on MONO benchmarks show that our timing-driven global routing algorithm reduces the maximum path delays significantly from the global router TimberWolfSC. Based on this adaptive timing-driven global routing algorithm, a parallel algorithm on timing-driven global routing for standard cells is given. This algorithm has been implemented on an 8 processor IBM J-40 shared memory multi-processor by using the Message Passing Interface (MPI). Our experimental results show good speedup and circuit delay results for this parallel algorithm using MONO benchmark circuits."
      }
    ]
  },
  {
    "tid":"80",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Case-based retrieval to support the treatment of end stage renal failure patients.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In the present paper, we describe an application of case-based retrieval to the domain of end stage renal failure patients, treated with hemodialysis.Defining a dialysis session as a case, retrieval of past similar cases has to operate both on static and on dynamic features, since most of the monitoring variables of a dialysis session are time series. Retrieval is then articulated as a two-step procedure: (1) classification, based on static features and (2) intra-class retrieval, in which dynamic features are considered. As regards step (2), we concentrate on a classical dimensionality reduction technique for time series allowing for efficient indexing, namely discrete Fourier transform (DFT). Thanks to specific index structures (i.e. k -d trees), range queries (on local feature similarity) can be efficiently performed on our case base, allowing the physician to examine the most similar stored dialysis sessions with respect to the current one.The retrieval tool has been positively tested on real patients' data, coming from the nephrology and dialysis unit of the Vigevano hospital, in Italy.The overall system can be seen as a means for supporting quality assessment of the hemodialysis service, providing a useful input from the knowledge management perspective."
      }
    ]
  },
  {
    "tid":"81",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'cAnt-Miner: An Ant Colony Classification Algorithm to Cope with Continuous Attributes' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents an extension to Ant-Miner, named cAnt-Miner (Ant-Miner coping with continuous attributes), which incorporates an entropy-based discretization method in order to cope with continuous attributes during the rule construction process. By having the ability to create discrete intervals for continuous attributes \"on-the-fly\", cAnt-Miner does not requires a discretization method in a preprocessing step, as Ant-Miner requires. cAnt-Miner has been compared against Ant-Miner in eight public domain datasets with respect to predictive accuracy and simplicity of the discovered rules. Empirical results show that creating discrete intervals during the rule construction process facilitates the discovery of more accurate and significantly simpler classification rules."
      }
    ]
  },
  {
    "tid":"82",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Exclusive Strategy for Generalization Algorithms in Micro-data Disclosure' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"When generalization algorithms are known to the public, an adversary can obtain a more precise estimation of the secret table than what can be deduced from the disclosed generalization result. Therefore, whether a generalization algorithm can satisfy a privacy property should be judged based on such an estimation. In this paper, we show that the computation of the estimation is inherently a recursive process that exhibits a high complexity when generalization algorithms take a straightforward inclusive strategy. To facilitate the design of more efficient generalization algorithms, we suggest an alternative exclusive strategy, which adopts a seemingly drastic approach to eliminate the need for recursion. Surprisingly, the data utility of the two strategies are actually not comparable and the exclusive strategy can provide better data utility in certain cases."
      }
    ]
  },
  {
    "tid":"83",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'FAUST: Dataset and Evaluation for 3D Mesh Registration' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"New scanning technologies are increasing the importance of 3D mesh data and the need for algorithms that can reliably align it. Surface registration is important for building full 3D models from partial scans, creating statistical shape models, shape retrieval, and tracking. The problem is particularly challenging for non-rigid and articulated objects like human bodies. While the challenges of real-world data registration are not present in existing synthetic datasets, establishing ground-truth correspondences for real 3D scans is difficult. We address this with a novel mesh registration technique that combines 3D shape and appearance information to produce high-quality alignments. We define a new dataset called FAUST that contains 300 scans of 10 people in a wide range of poses together with an evaluation methodology. To achieve accurate registration, we paint the subjects with high-frequency textures and use an extensive validation process to ensure accurate ground truth. We find that current shape registration methods have trouble with this real-world data. The dataset and evaluation website are available for research purposes at http://faust.is.tue.mpg.de."
      }
    ]
  },
  {
    "tid":"84",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Multimodal Approach to Understanding Human Vocal Expressions and Beyond.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Human verbal and nonverbal expressions carry crucial information not only about intent but also emotions, individual identity, and the state of health and wellbeing. From a basic science perspective, understanding how such rich information is encoded in these signals can illuminate underlying production mechanisms including the variability therein, within and across individuals. From a technology perspective, finding ways for automatically processing and decoding this complex information continues to be of interest across a variety of applications.\n\nThe convergence of sensing, communication and computing technologies is allowing access to data, in diverse forms and modalities, in ways that were unimaginable even a few years ago. These include data that afford the multimodal analysis and interpretation of the generation of human expressions.\n\nThe first part of the talk will highlight advances that allow us to perform investigations on the dynamics of vocal production using real-time imaging and audio modeling to offer insights about how we produce speech and song with the vocal instrument. The second part of the talk will focus on the production of vocal expressions in conjunction with other signals from the face and body especially in encoding affect. The talk will draw data from various domains notably in health to illustrate some of the applications.\n\n"
      }
    ]
  },
  {
    "tid":"85",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Using measures of vocal entrainment to inform outcome-related behaviors in marital conflicts' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Behavioral entrainment is an important, naturally-occurring dynamic phenomenon in human interactions. In this paper, we carry out two quantitative analyses of the vocal entrainment phenomenon in the context of studying conflictual marital interactions. We investigate the role of vocal entrainment in reflecting different dimensions of couple-specific behaviors, such as withdrawal, that are commonly-used in assessing the effectiveness on the outcome of couple therapy. The results indicate a statistically-significant relation between these behaviors and vocal entrainment, as quantified using our proposed unsupervised signal-derived computational framework. We further demonstrate the potential of the signal-based vocal entrainment framework in characterizing influential factors in distressed couples relationship satisfaction outcomes."
      }
    ]
  },
  {
    "tid":"86",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Automated physical design in database caches' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The ability of a database management system (DBMS) to detect problems or problem trends (which have not yet manifested as problems) and either take corrective actions, and/or provide advice or an automated notification to the database administrator (DBA) ..."
      }
    ]
  },
  {
    "tid":"87",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An atlas-based deep brain structure segmentation method: from coarse positioning to fine shaping' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Segmentation of deep brain structures is a challenging task for MRI images due to blurry structure boundaries, small object size and irregular shapes. In this paper, we present a new atlas-based segmentation method. It first uses a prior spatial dependency tree to constrain the relative positions between different deep brain structures and determine an optimal sequence for the structure by-structure segmentation. After positioning the structures, the segmentation result is further fine tuned by a non-rigid registration procedure between the atlas image and the target image using the histogram of the gradient magnitudes lying on the structure boundaries. The pro posed method has been applied on a publicly available MRI brain database and can achieve comparatively high segmentation accuracy."
      }
    ]
  },
  {
    "tid":"88",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'More Constructions Of Re-Splittable Threshold Public Key Encryption' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The concept of threshold public key encryption (TPKE) with the special property called key re-splittability (re-splittable TPKE, for short) was introduced by Hanaoka et al. (CT-RSA 2012), and used as one of the building blocks for constructing their proxy re-encryption scheme. In a re-splittable TPKE scheme, a secret key can be split into a set of secret key shares not only once, but also multiple times, and the security of the TPKE scheme is guaranteed as long as the number of corrupted secret key shares under the same splitting is smaller than the threshold. In this paper, we show several new constructions of a re-splittable TPKE scheme by extending the previous (ordinary) TPKE schemes. All of our proposed schemes are based on discrete logarithm (DL)-type assumptions. Therefore, our results suggest that key re-splittability is a very natural property for DL-type TPKE schemes."
      }
    ]
  },
  {
    "tid":"89",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Sicherheit moderner Funkt\u00fcr\u00f6ffnersysteme' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Zusammenfassung\u00a0\u00a0Durch Automatisierung gelangt immer mehr Komfort in unseren Alltag. Ein System, das in den vergangenen Jahren rasante Verbreitung\n gefunden hat, sind Funkt\u00fcr\u00f6ffner. Garagentor, Haust\u00fcr und Auto entriegeln sich auf Knopfdruck aus mehreren Metern Entfernung.\n Wie sicher aber sind diese Verfahren? Die Autoren entwickelten einen erfolgreichen Seitenkanalangriff auf Funkt\u00fcr\u00f6ffnersysteme.\n "
      }
    ]
  },
  {
    "tid":"90",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Evaluating snapping-to-photos virtual travel interfaces for 3D reconstructed visual reality' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Navigating through a virtual, 3D reconstructed scene has recently become very important in many applications. A popular approach is to virtually travel to the photos used in reconstructing the scene; such an approach may be generally termed a \"snapping-to-photos\" virtual travel interface. While previous work has either used fully constrained interfaces (always at the photos) or minimally constrained interfaces (free-flight navigation), in this paper we introduce new snapping-to-photos interfaces that lie in between these two extremes. Our snapping-to-photos interfaces snap the view to a photo in 3D based on viewpoint similarity and optionally the user's mouse cursor or finger-tap position. Experimental results, with both indoor and outdoor scene reconstructions, found that our snapping-to-photos interfaces are preferred over the baseline fully constrained-to-photos interface, that there exist differences between indoor and outdoor scenes, and that users preferred and were able to reach target photos better with click-to-snap point-of-interest snapping compared to automatic point-of-view snapping.\n\n"
      }
    ]
  },
  {
    "tid":"91",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Two key properties of dimensionality reduction methods' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Dimensionality reduction aims at providing faithful low-dimensional representations of high-dimensional data. Its general principle is to attempt to reproduce in a low-dimensional space the salient characteristics of data, such as proximities. A large variety of methods exist in the literature, ranging from principal component analysis to deep neural networks with a bottleneck layer. In this cornucopia, it is rather difficult to find out why a few methods clearly outperform others. This paper identifies two important properties that enable some recent methods like stochastic neighborhood embedding and its variants to produce improved visualizations of high-dimensional data. The first property is a low sensitivity to the phenomenon of distance concentration. The second one is plasticity, that is, the capability to forget about some data characteristics to better reproduce the other ones. In a manifold learning perspective, breaking some proximities typically allow for a better unfolding of data. Theoretical developments as well as experiments support our claim that both properties have a strong impact. In particular, we show that equipping classical methods with the missing properties significantly improves their results."
      }
    ]
  },
  {
    "tid":"92",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Implications of autonomy for the expressiveness of policy routing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Thousands of competing autonomous systems must cooperate with each other to provide global Internet connectivity. Each autonomous system (AS) encodes various economic, business, and performance decisions in its routing policy. The current interdomain routing system enables each AS to express policy using rankings that determine how each router in the AS chooses among different routes to a destination, and filters that determine which routes are hidden from each neighboring AS. Because the Internet is composed of many independent, competing networks, the interdomain routing system should provide autonomy, allowing network operators to set their rankings independently, and to have no constraints on allowed filters. This paper studies routing protocol stability under these conditions. We first demonstrate that \"next-hop rankings,\" commonly used in practice, may not ensure routing stability. We then prove that, when providers can set rankings and filters autonomously, guaranteeing that the routing system will converge to a stable path assignment imposes strong restrictions on the rankings ASes are allowed to choose. We discuss the implications of these results for the future of interdomain routing."
      }
    ]
  },
  {
    "tid":"93",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fast Spectral Clustering for Unsupervised Hyperspectral Image Classification.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Hyperspectral image classification is a challenging and significant domain in the field of remote sensing with numerous applications in agriculture, environmental science, mineralogy, and surveillance. In the past years, a growing number of advanced hyperspectral remote sensing image classification techniques based on manifold learning, sparse representation and deep learning have been proposed and reported a good performance in accuracy and efficiency on state-of-the-art public datasets. However, most existing methods still face challenges in dealing with large-scale hyperspectral image datasets due to their high computational complexity. In this work, we propose an improved spectral clustering method for large-scale hyperspectral image classification without any prior information. The proposed algorithm introduces two efficient approximation techniques based on Nystrom extension and anchor-based graph to construct the affinity matrix. We also propose an effective solution to solve the eigenvalue decomposition problem by multiplicative update optimization. Experiments on both the synthetic datasets and the hyperspectral image datasets were conducted to demonstrate the efficiency and effectiveness of the proposed algorithm."
      }
    ]
  },
  {
    "tid":"94",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Synthesizing style-preserving cartoons via non-negative style factorization' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present a complete framework for synthesizing style-preserving 2D cartoons by learning from traditional Chinese cartoons. In contrast to reusing-based approaches which rely on rearranging or retrieving existing cartoon sequences, we aim to generate stylized cartoons with the idea of style factorization. Specifically, starting with 2D skeleton features of cartoon characters extracted by an improved rotoscoping system, we present a non-negative style factorization (NNSF) algorithm to obtain style basis and weights and simultaneously preserve class separability. Thus, factorized style basis can be combined with heterogeneous weights to re-synthesize style-preserving features, and then these features are used as the driving source in the character reshaping process via our proposed subkey-driving strategy. Extensive experiments and examples demonstrate the effectiveness of the proposed framework."
      }
    ]
  },
  {
    "tid":"95",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Identification of multi-resolution network structures with multi-objective immune algorithm' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Community structure is one of the most important properties in complex networks, and the field of community detection has received an enormous amount of attention in the past several years. Many quality metrics and methods have been proposed for revealing community structures at multiple resolution levels, while most existing methods need a tunable parameter in their quality metrics to determine the resolution level in advance. In this study, a multi-objective evolutionary algorithm (MOEA) for revealing multi-resolution community structures is proposed. The proposed MOEA-based community detection algorithm aims to find a set of tradeoff solutions which represent network partitions at different resolution levels in a single run. It adopts an efficient multi-objective immune algorithm to simultaneously optimize two contradictory objective functions, Modified Ratio Association and Ratio Cut. The optimization of Modified Ratio Association tends to divide a network into small communities, while the optimization of Ratio Cut tends to divide a network into large communities. The simultaneous optimization of these two contradictory objectives returns a set of tradeoff solutions between the two objectives. Each of these solutions corresponds to a network partition at one resolution level. Experiments on artificial and real-world networks show that the proposed method has the ability to reveal community structures of networks at different resolution levels in a single run."
      }
    ]
  },
  {
    "tid":"96",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Adaptive sensing and inference for single-photon imaging' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In recent years, there have been increasing efforts to develop solid-state sensors with single-photon sensitivity, with applications ranging from bio-imaging to 3D computer vision. In this paper, we present adaptive sensing models, theory and algorithms for these single-photon sensors, aiming to improve their dynamic ranges. Mapping different sensor configurations onto a finite set of states, we represent adaptive sensing schemes as finite-state parametric Markov chains. After deriving an asymptotic expression for the Fisher information rate of these Markovian systems, we propose a design criterion for sensing policies based on minimax ratio regret. We also present a suboptimal yet effective sensing policy based on random walks. Numerical experiments demonstrate the strong performance of the proposed scheme, which expands the sensor dynamic ranges of existing nonadaptive approaches by several orders of magnitude."
      }
    ]
  },
  {
    "tid":"97",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Parallel Read Error Correction for Big Genomic Datasets.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Genome sequencing, using instruments in voguetoday, deciphers in the order of a billion short genomic fragmentsper run. These fragments are a few hundred bases long andare commonly referred to as 'reads'. Reads contain errors dueto limitations of sequencing technology. Read error correctionenhances the quality of results produced by applications inareas such as genomics, metagenomics, and transcriptomics. Useof error corrected reads also improves the runtime and thememory usage of such applications. Sequential error correctiontools cannot cope with the large number of reads producedby modern day sequencing instruments. A distributed-memoryParallel Spectrum-based Error Correction (PSbEC) algorithmwas proposed to overcome this drawback [1].In this work, we propose techniques to address three majorshortcomings of the PSbEC algorithm. Our optimizations enhancethe scope and the speedup of the PSbEC algorithm, therebyenabling error correction of big genomic datasets. More specifically, by combining our optimizations, we are able to achieve a cumulative speedup of up to 11 X. Further, we demonstrate error correction of a human dataset containing nearly 1.55 billion reads. This work stands as the first demonstration of distributed-memorygenomic read error correction for a dataset consistingof more than a billion reads."
      }
    ]
  },
  {
    "tid":"98",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Simulating multiple class urban land-use/cover changes by RBFN-based CA model' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Land use systems are complex adaptive systems, and they are characterized by emergence, nonlinearity, feedbacks, self organization, path dependence, adaptation, and multiple-scale characteristics. Land use/cover change has been recognized as one of the major drivers of global environmental change. This paper presents a coupled Cellular Automata (CA) and Radial Basis Function Neural (RBFN) Network model, which combines Geographic Information Systems (GIS) to contribute to the understanding of the complex land use/cover change process. In this model, GIS analysis is used to generate spatial drivers of land use/cover changes, and RBFN is trained to extract model parameters. Through the RBFN-CA model, the conversion probabilities of each cell from its initial land use state to the target type can be generated automatically. Future land use/cover scenarios are projected by using generated parameters in the model training process. This RBFN-CA model is tested based on the comparison of model output and the real data. A BPN-CA model is also built and compared with the RBFN-CA model by using a variety of calibration metrics, including confusion matrix, figure of merit, and landscape metrics. Both the location and landscape metrics based assessment for model simulation indicate that the RBFN-CA model performs better than the BPN-CA model for simulating land use changes in the study area. Therefore the RBFN-CA model is capable of simulating multiple classes of land use/cover changes and can be used as a useful communication environment for stakeholders involved in land use decision-making."
      }
    ]
  },
  {
    "tid":"99",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multilayer optimization of heterogeneous networks using grammatical genetic programming' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Wireless communications networks are a global trillion dollar industry, where small improvements can scale to provide significant cost savings to networks operators. In a field full of NP-hard optimisation problems, heuristic optimisation techniques such as Evolutionary Computation offer a means to provide bespoke, scalable solutions. Grammatical Genetic Programming is applied to optimise three aspects of an LTE Heterogeneous Network: setting optimal Small Cell powers and biases, Macro Cell ABS patterns, and Small Cell scheduling. The evolved heuristics yield minimum downlink rates three times greater than a baseline technique, and twice that of a state-of-the-art industry standard benchmark. This work appears in full in Fenton et al., \\\"Multilayer Optimization of Heterogeneous Networks using Grammatical Genetic Programming\\\", IEEE Transactions on Cybernetics, 2017. DOI: 10.1109/TCYB.2017.2688280."
      }
    ]
  },
  {
    "tid":"100",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Traffic-Aware Online Network Selection in Heterogeneous Wireless Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We focus on the network selection problem in heterogeneous wireless networks. Many traditional approaches select the best network according to quality of service (QoS) related criteria, which neglects diverse user demands. We aim to select networks maximizing the quality of experience (QoE) of users. When the availability and dynamics of network state information are considered, most of the existing approaches can not make effective selection decisions since they are vulnerable to the uncertainty in network state information. To address this issue, we introduce the idea of online learning for network selection. In this paper, we formulate the network selection problem as a continuous time multi-armed bandit (CT-MAB) problem. A traffic-aware online network selection algorithm (ONES) is designed to match typical traffic types of users with respective optimal networks in terms of QoE. Moreover, we found that the correlation among multiple traffic network selections can be exploited to improve the learning capability. This motivates us to propose another two more efficient algorithms: decoupled online network selection algorithm (D-ONES) and virtual multiplexing ONES (VM-ONES). Simulation results demonstrate that our online network selection algorithms attain around 10% gain in QoE reward rate over non learning-based algorithms and learning based algorithms without QoE considerations."
      }
    ]
  },
  {
    "tid":"101",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'FIVR: Fine-grained Incident Video Retrieval.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper introduces the problem of Fine-grained Incident Video Retrieval (FIVR). Given a query video, the objective is to retrieve all associated videos, considering several types of associations that range from duplicate videos to videos from the same incident. FIVR offers a single framework that contains several retrieval tasks as special cases. To address the benchmarking needs of all such ta..."
      }
    ]
  },
  {
    "tid":"102",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On trajectory optimization for active sensing in Gaussian process models.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We consider the problem of optimizing the trajec- tory of a mobile sensor with perfect localization whose task is to estimate a stochastic, perhaps multidimensional field modeling the environment. When the estimator is the Kalman filter, and for certain classes of objective functions capturing the informativeness of the sensor paths, the sensor trajectory op- timization problem is a deterministic optimal control problem. This estimation problem arises in many applications besides the field estimation problem, such as active mapping with mobile robots. The main difficulties in solving this problem are computational, since the Gaussian process of interest is usually high dimensional. We review some recent work on this problem and propose a suboptimal non-greedy trajectory optimization scheme with a manageable computational cost, at least in static field models based on sparse graphical models."
      }
    ]
  },
  {
    "tid":"103",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A radial basis function neural network adaptive controller to drive a powered lower limb knee joint orthosis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Graphical abstractDisplay Omitted HighlightsHuman safety is ensured for the wearer of exoskeleton.Adaptive controller without prior knowledge.Stability proved theoretically using Lyapunov approach.No chattering phenomena in the input control.External disturbances are rejected and the wearer efforts are taken into account. This paper deals with the rehabilitation purposes using an active orthosis driven by an adaptive neural controller based on a radial basis function neural network (RBFNN). Two essential conditions are required in our study: ensuring the wearer safety and the good trajectory tracking. We consider for our experiments the same movements often recommended by the doctor during therapy sessions. In this context, it is possible to add some trivial prior knowledge as the dynamic model structure and all dynamical identified parts. The unknown or the uncertainty part of the inertia term of the knee-shank-orthosis system is identified online using an adaptive term. All other uncertainties or unknown dynamics are identified online by the RBFNN. The Lyapunov approach has been used to derive adaptation laws of the neural parameters and the inertia term. These adaptation laws ensure the stability of the system composed of the exoskeleton and its wearer. The wearer can be completely inactive or applying either a resistive or an assistive effort. Experimental results have been conducted on a real exoskeleton that is used for rehabilitation reasons. Based on these results we conclude with the effectiveness of the proposed approach."
      }
    ]
  },
  {
    "tid":"104",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Experiential Sampling in Multimedia Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Multimedia systems must deal with multiple data streams. Each data stream usually contains significant volume of redundant noisy data. In many real-time applications, it is essential to focus the computing resources on a relevant subset of data streams at any given time instant and use it to build the model of the environment. We formulate this problem as an experiential sampling problem and propose an approach to utilize computing resources efficiently on the most informative subset of data streams. First, in this paper, we focus on theoretical background and develop a theoretical framework for a single data stream. We generalize the notion of static visual attention in a dynamical systems setting and propose a dynamical attention-orientated analysis method. This is achieved by a sampling representation that utilizes the current context and past experience for attention evolution. Hence, the multimedia analysis task at hand can select its data of interest while immediately discarding the irrelevant data to achieve efficiency and adaptability"
      }
    ]
  },
  {
    "tid":"105",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimal energy-aware control policies for FIFO servers.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We study the optimal control of sleep states in an energy-aware M/G/1-FIFO queue. Two families of policies are considered, referred to as randomized and sequential policies. Under a randomized policy, the server upon becoming idle can wait for a random period before entering, potentially randomly, any of the possible sleep states. The server enters setup after a possibly random number of jobs have arrived. A server controlled by a sequential policy waits for a random period upon becoming idle and starts sequentially transiting the sleep states in a decreasing order of power consumption until it reaches the deepest sleep state, where it waits for a threshold number of jobs to arrive before setup. We show for Poisson arrivals, generally distributed service times, setup delays and idle times that the optimal control of the idle time and sleep states neither randomizes nor transits sleep states sequentially. Either the system only uses the idle state and no sleep states, or the server immediately goes to some fixed sleep state and waits until a fixed number of jobs have arrived before setup. We prove this result for two cost metrics, namely weighted sum of energy and response time (ERWS) and their product (ERP)."
      }
    ]
  },
  {
    "tid":"106",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Position estimation using principal components of range data' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper describes a new approach to mobile robot position estimation, based on principal component analysis of laser range data. An eigenspace is constructed from the principal components of a large number of range data sets. The structure of an environment, as seen by a range sensor, is represented as a family of surfaces in this space. Subsequent range data sets from the environment project as a point in this space. Associating this point to the family of surfaces gives a set of candidate positions and orientations (poses) for the sensor. These candidate poses correspond to positions and orientations in the environment which have similar range profiles. A Kalman filter can be used to select the most likely candidate pose based on coherence with small movements. The first part of this paper describes how a relatively small number of depth profiles of an environment can be used to generate a complete eigenspace. This space is used to build a representation of the range scan profiles obtained from a regular grid of positions and orientations (poses). This representation has the form of a family of surfaces (a manifold). This representation converts the problem of associating a range profile to possible positions and orientations into a table lookup. As a side benefit, the method provides a simple means to detect obstacles in a range profile. The final section of the paper reviews the use of estimation theory to determine the correct pose hypothesis by tracking. (C) 1998 Elsevier Science B.V. All rights reserved."
      }
    ]
  },
  {
    "tid":"107",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mining Yeast Gene Microarray Data with Latent Variable Models.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Gene-expression microarrays make it possible to simultaneously measure the rate at which a cell or tissue is expressing each of its thousands of genes. One can use these comprehensive snapshots of biological activity to infer regulatory pathways in cells, identify novel targets for drug design, and improve diagnosis, prognosis, and treatment planning for those suffering from disease. However, the amount of data this new technology produces is more than one can manually analyze. Hence, the need for automated analysis of microarray data offers an opportunity for machine learning to have a significant impact on biology and medicine. Probabilistic Principal Surfaces defines a unified theoretical framework for nonlinear latent variable models embracing the Generative Topographic Mapping as a special case. This article describes the use of PPS for the analysis of yeast gene expression levels from microarray chips showing its effectiveness for high-D data visualization and clustering."
      }
    ]
  },
  {
    "tid":"108",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Depth perception of the surfaces in occluded scenic images' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The adaptive disparity filter is refined, along with the monocular feature filter designed based on neuron dynamics, and a new stereopsis model is set up in this study. The disparities of the matched binocular features in stereo image pair are detected by the adaptive disparity filter, and removed by the monocular feature filter, only leaving those unmatched monocular features to be added to all depth planes determined by the disparities of the matched binocular features. Finally, visible surface perception is generated by the closed boundaries in the corresponding depth plane during the filling-in processing. By the above mechanism, the depth perception of surfaces in the occluded scenic images is realized, also, the figure-ground segmentation."
      }
    ]
  },
  {
    "tid":"109",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Beyond the Words: Predicting User Personality from Heterogeneous Information.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"An incisive understanding of user personality is not only essential to many scientific disciplines, but also has a profound business impact on practical applications such as digital marketing, personalized recommendation, mental diagnosis, and human resources management. Previous studies have demonstrated that language usage in social media is effective in personality prediction. However, except for single language features, a less researched direction is how to leverage the heterogeneous information on social media to have a better understanding of user personality. In this paper, we propose a Heterogeneous Information Ensemble framework, called HIE, to predict users' personality traits by integrating heterogeneous information including self-language usage, avatar, emoticon, and responsive patterns. In our framework, to improve the performance of personality prediction, we have designed different strategies extracting semantic representations to fully leverage heterogeneous information on social media. We evaluate our methods with extensive experiments based on a real-world data covering both personality survey results and social media usage from thousands of volunteers. The results reveal that our approaches significantly outperform several widely adopted state-of-the-art baseline methods. To figure out the utility of HIE in a real-world interactive setting, we also present DiPsy, a personalized chatbot to predict user personality through heterogeneous information in digital traces and conversation logs."
      }
    ]
  },
  {
    "tid":"110",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient and scalable filtering of graph-based metadata' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"RDF Site Summaries constitute an application of RDF on the Web that has considerably grown in popularity. However, the way RSS systems operate today limits their scalability. Current RSS feed arregators follow a pull-based architecture model, which is not going to scale with the increasing number of RSS feeds becoming available on the Web. In this paper, we introduce G-ToPSS, a scalable publish/subscribe system for selective information dissemination. G-ToPSS only sends newly updated information to the interested user and follows a push-based architecture model. G-ToPSS is particularly well suited for applications that deal with large-volume content distribution from diverse sources. G-ToPSS allows use of an ontology as a way to provide additional information about the data disseminated. We have implemented and experimentally evaluated G-ToPSS and we provide results demonstrating its scalability compared to alternative approaches. In addition, we describe an application of G-ToPSS and RSS to a Web-based content management system that provides an expressive, efficient, and convenient update notification dissemination system."
      }
    ]
  },
  {
    "tid":"111",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Adaptive sampling for Bayesian geospatial models' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Bayesian hierarchical modeling with Gaussian process random effects provides a popular approach for analyzing point-referenced spatial data. For large spatial data sets, however, generic posterior sampling is infeasible due to the extremely high computational burden in decomposing the spatial correlation matrix. In this paper, we propose an efficient algorithm--the adaptive griddy Gibbs (AGG) algorithm--to address the computational issues with large spatial data sets. The proposed algorithm dramatically reduces the computational complexity. We show theoretically that the proposed method can approximate the real posterior distribution accurately. The sufficient number of grid points for a required accuracy has also been derived. We compare the performance of AGG with that of the state-of-the-art methods in simulation studies. Finally, we apply AGG to spatially indexed data concerning building energy consumption."
      }
    ]
  },
  {
    "tid":"112",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Path-Optimal GAC Algorithm for Table Constraints.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Filtering by Generalized Arc Consistency (GAC) is a fundamental technique in Constraint Programming. Recent advances in GAC algorithms for extensional constraints rely on direct manipulation of tables during search. Simple Tabular Reduction (STR), which systematically removes invalid tuples from tables, has been shown to be a simple yet efficient approach. STR2, a refinement of STR, is considered to be among the best filtering algorithms for positive table constraints. In this paper, we introduce a new GAC algorithm called STR3 that is specifically designed to enforce GAC during search. STR3 can completely avoid unnecessary traversal of tables, making it optimal along any path of the search tree. Our experiments show that STR3 is much faster than STR2 when the average size of the tables is not reduced drastically during search."
      }
    ]
  },
  {
    "tid":"113",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A QoS-aware exception handling method in scientific workflow execution' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Scientific workflow is gaining ever-increasing attention as it integrates a wide range of heterogeneous services to enable and accelerate the scientific discovery processes. However, the exception occurred in a scientific workflow execution often decreases the robustness of a scientific workflow system. For example, if a service engaged in a scientific workflow schedule becomes unavailable during its execution, an exception might be raised and the whole workflow execution may be interrupted unexpectedly. In this situation, it is a challenge to smooth the interrupted workflow execution. In view of this challenge, an exception handling method, named RelaxingMe (constraints Relaxing Method, RelaxingMe) is proposed in this paper. This method aims at relaxing the original QoS constraint values requested by the interrupted task node, in order to find a near-to-optimal candidate service to replace the unavailable one. At last, the feasibility of our method is evaluated by an experiment. Copyright \u00a9 2011 John Wiley & Sons, Ltd."
      }
    ]
  },
  {
    "tid":"114",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'TNT: a numeric keypad based text input method' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With the evolving functionality in television-based (TV-based) information and entertainment appliances, there is an increased need to enable users input text through remote control devices. We present a novel text input method, The Numpad Typer (TNT), for interactive TV, multimedia home terminals or other similar applications. Embodied in a TV remote control and guided by a visual map on the TV screen, TNT was designed for consistent spatial Stimuli-Response (S-R) compatibility and consistency of use. Five users tested TNT in ten sessions of 45-minutes. This initial investigation showed that users on average could type 9.3 and 17.7 correct words per minute with TNT doing the slowest and the fastest session respectively. The study also showed that the users found the TNT method easy to grasp and fun to use. Subjectively the participants felt they mastered the method rather quickly in comparison to their actual speed improvement."
      }
    ]
  },
  {
    "tid":"115",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Approximate logic circuits for low overhead, non-intrusive concurrent error detection' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper describes a scalable, technology-independent algorithm for the synthesis of approximate logic circuits. A low overhead, non-intrusive solution for concurrent error detection (CED) based on such circuits is described in this paper. CED based on approximate logic circuits does not impose any performance penalty on the original design. The proposed synthesis algorithm for approximate logic circuits scales with circuit size, and provides fine-grained trade-offs between area-power overhead and CED coverage."
      }
    ]
  },
  {
    "tid":"116",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Versatility and Unix semantics in namespace unification' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Administrators often prefer to keep related sets of files in different locations or media, as it is easier to maintain them separately. Users, however, prefer to see all files in one location for convenience. One solution that accommodates both needs is virtual namespace unification---providing a merged view of several directories without physically merging them. For example, namespace unification can merge the contents of several CD-ROM images without unpacking them, merge binary directories from different packages, merge views from several file servers, and more. Namespace unification can also enable snapshotting by marking some data sources read-only and then utilizing copy-on-write for the read-only sources. For example, an OS image may be contained on a read-only CD-ROM image---and the user's configuration, data, and programs could be stored in a separate read-write directory. With copy-on-write unification, the user need not be concerned about the two disparate file systems.It is difficult to maintain Unix semantics while offering a versatile namespace unification system. Past efforts to provide such unification often compromised on the set of features provided or Unix compatibility---resulting in an incomplete solution that users could not use.We designed and implemented a versatile namespace unification system called Unionfs. Unionfs maintains Unix semantics while offering advanced namespace unification features: dynamic insertion and removal of namespaces at any point in the merged view, mixing read-only and read-write components, efficient in-kernel duplicate elimination, NFS interoperability, and more. Since releasing our Linux implementation, it has been used by thousands of users and over a dozen Linux distributions, which helped us discover and solve many practical problems."
      }
    ]
  },
  {
    "tid":"117",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Synthesizing view definitions from data' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Given a database instance and a corresponding view instance, we address the view definitions problem (VDP): Find the most succinct and accurate view definition, when the view query is restricted to a specific family of queries. We study the tradeoffs among succintness, level of approximation, and the family of queries through algorithms and complexity results. For each family of queries, we address three variants of the VDP: (1) Does there exist an exact view definition, and if so find it. (2) Find the best view definition, i.e., one as close to the input view instance as possible, and as succinct as possible. (3) Find an approximate view definition that satisfies an input approximation threshold, and is as succinct as possible."
      }
    ]
  },
  {
    "tid":"118",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A specification of the Agent Reputation and Trust (ART) testbed: experimentation and competition for trust in agent societies' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A diverse collection of trust-modeling algorithms for multi-agent systems has been developed in recent years, resulting in significant breadth-wise growth without unified direction or benchmarks. Based on enthusiastic response from the agent trust community, the Agent Reputation and Trust (ART) Testbed initiative has been launched, charged with the task of establishing a testbed for agent trust- and reputation-related technologies. This testbed serves in two roles: (1) as a competition forum in which researchers can compare their technologies against objective metrics, and (2) as a suite of tools with flexible parameters, allowing researchers to perform customizable, easily-repeatable experiments. This paper first enumerates trust research objectives to be addressed in the testbed and desirable testbed characteristics, then presents a competition testbed specification that is justified according to these requirements. In the testbed's artwork appraisal domain, agents, who valuate paintings for clients, may gather opinions from other agents to produce accurate appraisals. The testbed's implementation architecture is discussed briefly, as well."
      }
    ]
  },
  {
    "tid":"119",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Novel Index Supporting High Volume Data Warehouse Insertion' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"While the desire to support fast, ad hoc query processing for large data warehouses has motivated the recent introduction of many new indexing structures, with a few notable exceptions (namely, the LSM-Tree [4] and the Stepped Merge Method ill) little attention has been given to developing new indexing schemes that allow fast insertions. Since additions to a large warehouse may number in the millions per day, indices that require a disk seek (or even a significant fraction of a seek) per insertion are not acceptable.In this paper, we offer an alternative to the B+-tree called the Y-tree for indexing huge warehouses having frequent insertions. The Y-tree is a new indexing structure supporting both point and range queries over a single attribute, with retrieval performance comparable to the B+-tree. For processing insertions, however, the Y-tree may exhibit a speedup of 100 times over hatched insertions into a B+-tree."
      }
    ]
  },
  {
    "tid":"120",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Bargaining with limited computation: deliberation equilibrium' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We develop a normative theory of interaction-negotiation in particular-among self-interested computationally limited agents where computational actions are game theoretically treated as part of an agent's strategy. We focus on a 2-agent setting where each agent has an intractable individual problem, and there is a potential gain from pooling the problems, giving rise to an intractable joint problem. At any time, an agent can compute to improve its solution to its own problem, its opponent's problem, or the joint problem. At a deadline the agents then decide whether to implement the joint solution, and if so, how to divide its value (or cost). We present a fully normative model for controlling anytime algorithms where each agent has statistical performance profiles which are optimally conditioned on the problem instance as well as on the path of results of the algorithm run so far. Using this model, we introduce a solution concept, which we call deliberation equilibrium. It is the perfect Bayesian equilibrium of the game where deliberation actions are part of each agent's strategy. The equilibria differ based on whether the performance profiles are deterministic or stochastic, whether the deadline is known or not, and whether the proposer is known in advance or not. We present algorithms for finding the equilibria. Finally, we show that there exist instances of the deliberation-bargaining problem where no pure strategy equilibria exist and also instances where the unique equilibrium outcome is not Pareto efficient. Copyright 2001 Elsevier Science B.V."
      }
    ]
  },
  {
    "tid":"121",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Boosting Color Saliency in Image Feature Detection' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The aim of salient feature detection is to find distinctive local events in images. Salient features are generally determined from the local differential structure of images. They focus on the shape-saliency of the local neighborhood. The majority of these detectors are luminance-based, which has the disadvantage that the distinctiveness of the local color information is completely ignored in determining salient image features. To fully exploit the possibilities of salient point detection in color images, color distinctiveness should be taken into account in addition to shape distinctiveness. In this paper, color distinctiveness is explicitly incorporated into the design of saliency detection. The algorithm, called color saliency boosting, is based on an analysis of the statistics of color image derivatives. Color saliency boosting is designed as a generic method easily adaptable to existing feature detectors. Results show that substantial improvements in information content are acquired by targeting color salient features."
      }
    ]
  },
  {
    "tid":"122",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Hybrid object-based/block-based coding in video compression at very low bit-rate' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, an algorithmic architecture of video coding at very low bit-rate is presented. This architecture incorporates block- and object-based coding algorithms into the same coder and manages them in a cooperative way: I and P frames are coded by block-based algorithm and B frames are coded by bi-directional object-based algorithm. An extra degree of freedom in video coding is introduced to achieve higher compression ratios while maintaining good visual quality. We give details of the object-based coding algorithm which is based on a source model of rigid 2-D objects with 3-D motions. Each moving object in B frame is coded with its contour and a pair of motion parameter sets (forward and backward motions). The bi-directional object-based motion parameters are obtained by a hierarchical estimation. The object image is interpolated using two neighboring pictures, which are block-based coded. ITU-T H.263 is chosen as the block-based coding scheme and the simulations are performed on MPEG-4 test sequences, which contain low-to-medium spatial detail and low amount of movement, but do not restrict to head_and_shoulder images. Experiment results show that the coder provides higher image quality with smooth and satisfactory motion. (C) 1997 Elsevier Science B.V."
      }
    ]
  },
  {
    "tid":"123",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Towards self-authenticable smart cards' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Traditionally, the smart cards have been seen as security devices, but as soon as they could be integrated into distributed and networked environments their vulnerabilities could be attempted and countermeasures against new security threats in an open-access internet were required. In this work, our target could be represented by an end-to-end mutual authentication scenario where the smart card could authenticate by itself to a Network Access Server by means of link layer protocols and therefore in absence of IP connectivity. Some previous related models based on the Extensible Authentication Protocol are presented. However, in these works the smart card and terminal implement jointly the supplicant functionality (split supplicant). We consider the native EAP multiplexing model specified by the IETF to propose a new approach in order to avoid this split and to achieve an autonomous and highly independent smart card in the authentication scheme: a self-authenticable smart card."
      }
    ]
  },
  {
    "tid":"124",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Training-free indexing refinement for visual media via multi-semantics.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Indexing of visual media based on content analysis has now moved beyond using individual concept detectors and there is now a focus on combining concepts by post-processing the outputs of individual concept detection. Due to the limitations and availability of training corpora which are usually sparsely and imprecisely labeled with concept groundtruth, training-based refinement methods for semantic indexing of visual media suffer in correctly capturing relationships between concepts, including co-occurrence and ontological relationships. In contrast to training-dependent methods which dominate this field, this paper presents a training-free refinement (TFR) algorithm for enhancing semantic indexing of visual media based purely on concept detection results, making the refinement of initial concept detections based on semantic enhancement, practical and flexible. This is achieved using what can be called multi-semantics, factoring in semantics from multiple sources. In the case of this paper, global and temporal neighbourhood information inferred from the original concept detections in terms of weighted non-negative matrix factorization and neighbourhood-based graph propagation are both used in the refinement of semantics. Furthermore, any available ontological concept relationships among concepts can also be integrated into this model as an additional source of external a priori knowledge. Extended experiments on two heterogeneous datasets, images from wearable cameras and videos from TRECVid, demonstrate the efficacy of the proposed TFR solution."
      }
    ]
  },
  {
    "tid":"125",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Testing container classes: random or systematic?' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Container classes such as lists, sets, or maps are elementary data structures common to many programming languages. Since they are a part of standard libraries, they are important to test, which led to research on advanced testing techniques targeting such containers and research on comparing testing techniques using such containers. However, these techniques have not been thoroughly compared to simpler techniques such as random testing. We present the results of a larger case study in which we compare random testing with shape abstraction, a systematic technique that showed the best results in a previous study. Our experiments show that random testing is about as effective as shape abstraction for testing these containers, which raises the question whether containers are well suited as a benchmark for comparing advanced testing techniques."
      }
    ]
  },
  {
    "tid":"126",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Broadcasting With Side Information: Bounding and Approximating the Broadcast Rate' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Index coding has received considerable attention recently motivated in part by applications such as fast video-on-demand and efficient communication in wireless networks and in part by its connection to network coding. Optimal encoding schemes and efficient heuristics were studied in various settings, while also leading to new results for network coding such as improved gaps between linear and non-linear capacity as well as hardness of approximation. The problem of broadcasting with side information, a generalization of the index coding problem, begins with a sender and sets of users and messages. Each user possesses a subset of the messages and desires an additional message from the set. The sender wishes to broadcast a message so that on receipt of the broadcast each user can compute her desired message. The fundamental parameter of interest is the broadcast rate, $\\\\beta $, the average communication cost for sufficiently long broadcasts. Though there have been many new nontrivial bounds on $\\\\beta $ by Bar-Yossef (2006), Lubetzky and Stav (2007), Alon (2008), and Blasiak (2011) there was no known polynomial-time algorithm for approximating $\\\\beta $ within a nontrivial factor, and the exact value of $\\\\beta $ remained unknown for all nontrivial instances. Using the information theoretic linear program introduced in Blasiak (2011), we give a polynomial-time algorithm for recognizing instances with $\\\\beta = 2$ and pinpoint $\\\\beta $ precisely for various classes of graphs (e.g., various Cayley graphs of cyclic groups). Further, extending ideas from Ramsey theory, we give a polynomial-time algorithm with a nontrivial approximation ratio for computing $\\\\beta $. Finally, we provide insight into the quality of previous bounds by giving constructions showing separations between $\\\\beta $ and the respective bounds. In particular, we construct graphs where $\\\\beta $ is uniformly bounded while its upper bound derived from the na\u00efve encoding scheme is polynomially worse."
      }
    ]
  },
  {
    "tid":"127",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Active Imitation Learning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Imitation learning, also called learning by watching or programming by demonstration, has emerged as a means of accelerating many reinforcement learning tasks. Previous work has shown the value of imitation in domains where a single mentor demonstrates execution of a known optimal policy for the benefit of a learning agent. We consider the more general scenario of learning from mentors who are themselves agents seeking to maximize their own rewards. We propose a new algorithm based on the concept of transferable utility for ensuring that an observer agent can learn efficiently in the context of a selfish, not necessarily helpful, mentor. We also address the questions of when an imitative agent should request help from a mentor, and when the mentor can be expected to acknowledge a request for help. In analogy with other types of active learning, we call the proposed approach active imitation learning."
      }
    ]
  },
  {
    "tid":"128",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Rapid Feature Space Speaker Adaptation for Multi-Stream HMM-Based Audio-Visual Speech Recognition' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Multi-stream hidden Markov models (HMMs) have recently been very successful in audio-visual speech recognition, where the audio and visual streams are fused at the final decision level. In this paper we investigate fast feature space speaker adaptation using multi-stream HMMs for audio-visual speech recognition. In particular, we focus on studying the performance of feature-space maximum likelihood linear regression (fMLLR), a fast and effective method for estimating feature space transforms. Unlike the common speaker adaptation techniques of MAP or MLLR, fMLLR does not change the audio or visual HMM parameters, but simply applies a single transform to the testing features. We also address the problem of fast and robust on-line fMLLR adaptation using feature space maximum a posterior linear regression (fMAPLR). Adaptation experiments are reported on the IBM infrared headset audio-visual database. On average for a 20-speaker 1 hour independent test set, the multi-stream fMLLR achieves 31% relative gain on the clean audio condition, and 59% relative gain on the noisy audio condition (approximately 7 dB) as compared to the baseline multi-stream system"
      }
    ]
  },
  {
    "tid":"129",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Programming with non-determinism in deductive databases' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"While non-determinism has long been established as a key concept in logic pro-gramming, its importance in the context of deductive databases was recognized only recently. This paper provides an overview of recent results on this topic with the aim of providing an introduction to the theory and practice of non-determinism in deductive databases. In particular we (i) recall the main results linking non-deterministic constructs in database languages to the theory of data complexity and the expressibility hierarchy of query languages; (ii) provide a reasoned introduction to effective programming with non-deterministic constructs; (iii) compare the usage of non-deterministic constructs in languages such as LDL++ to that of traditional logic programming languages; (iv) discuss the link between the semantics of logic programs with non-deterministic constructs and the stable-model semantics of logic programs with negation."
      }
    ]
  },
  {
    "tid":"130",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Distance measurement for tower crane obstacle based on multi-ultrasonic sensors' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Aiming at the anti-collision monitoring of tower crane, the models of state equation and observation equation for obstacle are established, and distance measurement method based on ultrasonic sensor for tower crane obstacle is proposed. According to the multi-sensor information fusion theory and obstacle kinematic model, Kalman filter principle is used to respectively deal with single ultrasonic sensor information and multi-ultrasonic sensor information fusion, then those simulation and experiments results are analyzed and contrasted. The results show that multi-sensor distance measurement is more accurate and stable than single sensor distance measurement, and verify the feasibility and superiority of multi-sensor information fusion theory in tower crane anti-collision applications."
      }
    ]
  },
  {
    "tid":"131",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Accuracy and efficiency of algorithms for the demarcation of bacterial ecotypes from DNA sequence data' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Identification of closely related, ecologically distinct populations of bacteria would benefit microbiologists working in many fields including systematics, epidemiology and biotechnology. Several laboratories have recently developed algorithms aimed at demarcating such 'ecotypes'. We examine the ability of four of these algorithms to correctly identify ecotypes from sequence data. We tested the algorithms on synthetic sequences, with known history and habitat associations, generated under the stable ecotype model and on data from Bacillus strains isolated from Death Valley where previous work has confirmed the existence of multiple ecotypes. We found that one of the algorithms ecotype simulation performs significantly better than the others AdaptML, GMYC, BAPS in both instances. Unfortunately, it was also shown to be the least efficient of the four. While ecotype simulation is the most accurate, it is by a large margin the slowest of the algorithms tested. Attempts at improving its efficiency are underway."
      }
    ]
  },
  {
    "tid":"132",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Verifying Architectural Variabilities In Software Fault Tolerance Techniques' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper considers the representation of different software fault tolerance techniques as a product line architecture (PLA) for promoting the muse of software artifact. The proposed PLA enables to specify a series of closely related architectural applications, which is obtained by identifying variation points associated with design decisions regarding software fault tolerance. These decisions are used to choose the appropriate technique depending on the features selected, e.g, the number of redundant resources, or the type of adjudicator. The proposed approach also comprises the formalisation of the PLA, using B-Method and CSP, for systematising the verification of fault-tolerant software systems at the architectural level. The properties verified cover two complementary contexts: the selection of the correct architectural variabilities for instantiating the PLA, and also the properties of the chosen fault tolerance techniques."
      }
    ]
  },
  {
    "tid":"133",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Self-healing network for scalable fault-tolerant runtime environments' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The number of processors embedded on high performance computing platforms is growing daily to satisfy the user desire for solving larger and more complex problems. Scalable and fault-tolerant runtime environments are needed to support and adapt to the underlying libraries and hardware which require a high degree of scalability in dynamic large-scale environments. This paper presents a self-healing network (SHN) for supporting scalable and fault-tolerant runtime environments. The SHN is designed to support transmission of messages across multiple nodes while also protecting against recursive node and process failures. It will automatically recover itself after a failure occurs. SHN is implemented on top of a scalable fault-tolerant protocol (SFTP). The experimental results show that both the latest multicast and broadcast routing algorithms used in SHN are faster and more reliable than the original SFTP routing algorithms."
      }
    ]
  },
  {
    "tid":"134",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'CommentSpace: structured support for collaborative visual analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Collaborative visual analysis tools can enhance sensemaking by facilitating social interpretation and parallelization of effort. These systems enable distributed exploration and evidence gathering, allowing many users to pool their effort as they discuss and analyze the data. We explore how adding lightweight tag and link structure to comments can aid this analysis process. We present CommentSpace, a collaborative system in which analysts comment on visualizations and websites and then use tags and links to organize findings and identify others'\" contributions. In a pair of studies comparing CommentSpace to a system without support for tags and links, we find that a small, fixed vocabulary of tags (question, hypothesis, to-do) and links (evidence-for, evidence-against) helps analysts more consistently and accurately classify evidence and establish common ground. We also find that managing and incentivizing participation is important for analysts to progress from exploratory analysis to deeper analytical tasks. Finally, we demonstrate that tags and links can help teams complete evidence gathering and synthesis tasks and that organizing comments using tags and links improves analytic results."
      }
    ]
  },
  {
    "tid":"135",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Representative entry selection for profiling blogs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many applications on blog search and mining often meet the challenge of handling huge volume of blog data, in which one single blog could contain hundreds or even thousands of entries. We investigate novel techniques for profiling blogs by selecting a subset of representative entries for each blog. We propose two principles for guiding the entry selection task: representativeness and diversity. Further, we formulate the entry selection task into a combinatorial optimization problem and propose a greedy yet effective algorithm for finding a good approximate solution by exploiting the theory of submodular functions. We suggest blog classification for judging the performance of the proposed entry selection techniques and evaluate their performance on a real blog dataset, in which encouraging results were obtained."
      }
    ]
  },
  {
    "tid":"136",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On the Consistency of Ranking Algorithms' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present a theoretical analysis of super- vised ranking, providing necessary and suf- ficient conditions for the asymptotic consis- tency of algorithms based on minimizing a surrogate loss function. We show that many commonly used surrogate losses are incon- sistent; surprisingly, we show inconsistency even in low-noise settings. We present a new value-regularized linear loss, establish its consistency under reasonable assumptions on noise, and show that it outperforms conven- tional ranking losses in a collaborative filter- ing experiment."
      }
    ]
  },
  {
    "tid":"137",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Seed, Expand And Constrain: Three Principles For Weakly-Supervised Image Segmentation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We introduce a new loss function for the weakly-supervised training of semantic image segmentation models based on three guiding principles: to seed with weak localization cues, to expand objects based on the information about which classes can occur in an image, and to constrain the segmentations to coincide with object boundaries. We show experimentally that training a deep convolutional neural network using the proposed loss function leads to substantially better segmentations than previous state-of-the-art methods on the challenging PASCAL VOC 2012 dataset. We furthermore give insight into the working mechanism of our method by a detailed experimental study that illustrates how the segmentation quality is affected by each term of the proposed loss function as well as their combinations."
      }
    ]
  },
  {
    "tid":"138",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Checking memory safety with blast' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Blast is an automatic verification tool for checking temporal safety properties of C programs. Given a C program and a temporal safety property, Blast statically proves that either the program satisfies the safety property or the program has an execution trace that exhibits a violation of the property. Blast constructs, explores, and refines abstractions of the program state space based on lazy predicate abstraction and interpolation-based predicate discovery. We show how Blast can be used to statically prove memory safety for C programs. We take a two-step approach. First, we use Ccured, a type-based memory safety analyzer, to annotate with run-time checks all program points that cannot be proved memory safe by the type system. Second, we use Blast to remove as many of the run-time checks as possible (by proving that these checks never fail), and to generate for the remaining run-time checks execution traces that witness them fail. Our experience shows that Blast can remove many of the run-time checks added by Ccured and provide useful information to the programmer about many of the remaining checks."
      }
    ]
  },
  {
    "tid":"139",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Automatic verification of pointer programs using grammar-based shape analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present a program analysis that can automatically discover the shape of complex pointer data structures. The discovered invariants are, then, used to verify the absence of safety errors in the program, or to check whether the program preserves the data consistency. Our analysis extends the shape analysis of Sagiv et al. with grammar annotations, which can precisely express the shape of complex data structures. We demonstrate the usefulness of our analysis with binomial heap construction and the Schorr-Waite tree traversal. For a binomial heap construction algorithm, our analysis returns a grammar that precisely describes the shape of a binomial heap; for the Schorr-Waite tree traversal, our analysis shows that at the end of the execution, the result is a tree and there are no memory leaks."
      }
    ]
  },
  {
    "tid":"140",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'HoIP: A point-to-point haptic data communication protocol and its evaluation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Telehaptics applications are usually characterized by a strict imposition of a round trip haptic data latency of less than 30 ms. In this paper, we present Haptics over Internet Protocol (HoIP) - a low latency application layer protocol that enables haptic, audio and video data transmission over a network between two remotely connected nodes. The evaluation of the protocol is carried out through a set of three experiments, each with distinct objectives. First, a haptic-audio-visual (HAV) interactive application, involving two remotely located human personnel communicating via haptic, auditory and visual media, to evaluate the Quality of Service (QoS) violation due to the protocol. Second, a haptic sawing experiment with the goal of assessing the impact of HoIP and network delays in telehaptics applications, by taking the example of a typical telesurgical activity. Third, a telepottery system to determine the protocol's ability in reproducing a real-time interactive user experience with a remote virtual object, in presence of perceptual data compression and reconstruction techniques. Our experiments reveal that the transmission scheduling of multimedia packets performs well in terms of maintaining the latencies well under the QoS thresholds."
      }
    ]
  },
  {
    "tid":"141",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Dynamic Planar Convex Hull' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we determine the computational complexity of the dynamic convex hull problem in the planar case. We present a data structure that maintains a finite set of n points in the plane under insertion and deletion of points in amortized O(log n) time per operation. Thespace usage of the data structure is O(n). The data structure supports extreme point queries in a given direction, tangent queries through a given point, and queries for the neighboring points on the convex hull in O(log n) time. The extreme point queries can be used to decide whether or not a given line intersects the convex hull, and the tangent queries to determine whether a given point is inside the convex hull. We give a lower bound on the amortized asymptotic time complexity that matches the performance of this data structure."
      }
    ]
  },
  {
    "tid":"142",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Evaluating intertwined effects in e-learning programs: A novel hybrid MCDM model based on factor analysis and DEMATEL' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Internet evolution has affected all industrial and commercial activity and accelerated e-learning growth. Due to cost, time, or flexibility for designer courses and learners, e-learning has been adopted by corporations as an alternative training method. E-learning effectiveness evaluation is vital, and evaluation criteria are diverse. A large effort has been made regarding e-learning effectiveness evaluation; however, a generalized quantitative evaluation model, which considers both the interaffected relation between criteria and the fuzziness of subjective perception concurrently, is lacking. In this paper, the proposed new novel hybrid MCDM model addresses the independent relations of evaluation criteria with the aid of factor analysis and the dependent relations of evaluation criteria with the aid of DEMATEL. The AHP and the fuzzy integral methods are used for synthetic utility in accordance with subjective perception environment. Empirical experimental results show the proposed model is capable of producing effective evaluation of e-learning programs with adequate criteria that fit with respondent's perception patterns, especially when the evaluation criteria are numerous and intertwined."
      }
    ]
  },
  {
    "tid":"143",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Log-Determinant Divergences Revisited: Alpha-Beta and Gamma Log-Det Divergences.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This work reviews and extends a family of log-determinant (log-det) divergences for symmetric positive definite (SPD) matrices and discusses their fundamental properties. We show how to use parameterized Alpha-Beta (AB) and Gamma log-det divergences to generate many well-known divergences; in particular, we consider the Stein's loss, the S-divergence, also called Jensen-Bregman LogDet (JBLD) divergence, Logdet Zero (Bhattacharyya) divergence, Affine Invariant Riemannian Metric (AIRM), and other divergences. Moreover, we establish links and correspondences between log-det divergences and visualise them on an alpha-beta plane for various sets of parameters. We use this unifying framework to interpret and extend existing similarity measures for semidefinite covariance matrices in finite-dimensional Reproducing Kernel Hilbert Spaces (RKHS). This paper also shows how the Alpha-Beta family of log-det divergences relates to the divergences of multivariate and multilinear normal distributions. Closed form formulas are derived for Gamma divergences of two multivariate Gaussian densities; the special cases of the Kullback-Leibler, Bhattacharyya, Renyi, and Cauchy-Schwartz divergences are discussed. Symmetrized versions of log-det divergences are also considered and briefly reviewed. Finally, a class of divergences is extended to multiway divergences for separable covariance (or precision) matrices."
      }
    ]
  },
  {
    "tid":"144",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Frequency domain normal map filtering' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Filtering is critical for representing detail, such as color textures or normal maps, across a variety of scales. While MIP-mapping texture maps is commonplace, accurate normal map filtering remains a challenging problem because of nonlinearities in shading---we cannot simply average nearby surface normals. In this paper, we show analytically that normal map filtering can be formalized as a spherical convolution of the normal distribution function (NDF) and the BRDF, for a large class of common BRDFs such as Lambertian, microfacet and factored measurements. This theoretical result explains many previous filtering techniques as special cases, and leads to a generalization to a broader class of measured and analytic BRDFs. Our practical algorithms leverage a significant body of work that has studied lighting-BRDF convolution. We show how spherical harmonics can be used to filter the NDF for Lambertian and low-frequency specular BRDFs, while spherical von Mises-Fisher distributions can be used for high-frequency materials."
      }
    ]
  },
  {
    "tid":"145",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Exploring variable neighborhood search for automatic algorithm configuration.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Methods for automatic algorithm configuration integrate some search mechanism for generating candidate algorithm configurations and mechanisms for handling the stochasticity of the algorithm configuration problem. One popular algorithm configurator is ParamILS, which searches the configuration space using an iterated local search algorithm. In our research, we explore variable neighborhood search mechanisms as an alternative for the one-exchange neighborhood that is searched in the local search phase of ParamILS. In this article, we explore a reduced variable neighborhood search for automatic configuration. Our experimental results are promising and indicate directions for extending our work."
      }
    ]
  },
  {
    "tid":"146",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Terminating Evaluation of Logic Programs with Finite Three-Valued Models' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"As evaluation methods for logic programs have become more sophisticated, the classes of programs for which termination can be guaranteed have expanded. From the perspective of ar set programs that include function symbols, recent work has identified classes for which grounding routines can terminate either on the entire program [Calimeri et al. 2008] or on suitable queries [Baselice et al. 2009]. From the perspective of tabling, it has long been known that a tabling technique called subgoal abstraction provides good termination properties for definite programs [Tamaki and Sato 1986], and this result was recently extended to stratified programs via the class of bounded term-size programs [Riguzzi and Swift 2013]. In this article, we provide a formal definition of tabling with subgoal abstraction resulting in the SLGSA algorithm. Moreover, we discuss a declarative characterization of the queries and programs for which SLGSA terminates. We call this class strongly bounded term-size programs and show its equivalence to programs with finite well-founded models. For normal programs, strongly bounded term-size programs strictly includes the finitely ground programs of Calimeri et al. [2008]. SLGSA has an asymptotic complexity on strongly bounded term-size programs equal to the best known and produces a residual program that can be sent to an answer set programming system. Finally, we describe the implementation of subgoal abstraction within the SLG-WAM of XSB and provide performance results."
      }
    ]
  },
  {
    "tid":"147",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'From object fields to local variables: a practical approach to field-sensitive analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Static analysis which takes into account the value of data stored in the heap is typically considered complex and computationally intractable in practice. Thus, most static analyzers do not keep track of object fields (or fields for short), i.e., they are field-insensitive. In this paper, we propose locality conditions for soundly converting fields into local variables. This way, field-insensitive analysis over the transformed program can infer information on the original fields. Our notion of locality is context-sensitive and can be applied both to numeric and reference fields. We propose then a polyvariant transformation which actually converts object fields meeting the locality condition into variables and which is able to generate multiple versions of code when this leads to increasing the amount of fields which satisfy the locality conditions. We have implemented our analysis within a termination analyzer for Java bytecode."
      }
    ]
  },
  {
    "tid":"148",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Analysis of the Convergence and Generalization of AA1' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"AA1 is an incremental learning algorithm for Adaptive Self-Organizing Concurrent Systems (ASOCS). ASOCS are self-organizing, dynamically growing networks of computing nodes. AA1 learns by discrimination and implements knowledge in a distributed fashion over all the nodes. This paper reviews AA1 from the perspective of convergence and generalization. A formal proof that AA1 converges on any arbitrary Boolean instance set is given. A discussion of generalization and other aspects of AA1, including the problem of handling inconsistency, follows. Results of simulations with real-world data are presented. They show that AA1 gives promising generalization."
      }
    ]
  },
  {
    "tid":"149",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Type-Constrained Representation Learning in Knowledge Graphs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Large knowledge graphs increasingly add value to various applications that require machines to recognize and understand queries and their semantics, as in search or question answering systems. Latent variable models have increasingly gained attention for the statistical modeling of knowledge graphs, showing promising results in tasks related to knowledge graph completion and cleaning. Besides storing facts about the world, schema-based knowledge graphs are backed by rich semantic descriptions of entities and relation-types that allow machines to understand the notion of things and their semantic relationships. In this work, we study how type-constraints can generally support the statistical modeling with latent variable models. More precisely, we integrated prior knowledge in form of type-constraints in various state of the art latent variable approaches. Our experimental results show that prior knowledge on relation-types significantly improves these models up to 77% in link-prediction tasks. The achieved improvements are especially prominent when a low model complexity is enforced, a crucial requirement when these models are applied to very large datasets. Unfortunately, type-constraints are neither always available nor always complete e.g., they can become fuzzy when entities lack proper typing. We show that in these cases, it can be beneficial to apply a local closed-world assumption that approximates the semantics of relation-types based on observations made in the data."
      }
    ]
  },
  {
    "tid":"150",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Realizing chain-wide transparency in meat supply chains based on global standards and a reference architecture.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A reference architecture for chain-wide transparency systems is presented.Concrete architectures that provide different transparency levels are discussed.A systematic approach for deriving concrete architectures is presented.The approach is illustrated by realizing a chain-wide transparency system. One of the key concerns in meat supply chains is to provide chain-wide transparency, whereby food operators capture and share transparency data across the supply chain. To meet this concern a chain-wide transparency software system is needed that is able to address the desired stakeholder requirements. Unfortunately, designing and implementing a chain-wide transparency system is not straightforward. In this paper we provide a systematic approach for designing and implementing chain wide transparency systems. To this end, we first present a reference architecture that represents a generic design of such systems. Secondly, we discuss the systematic approach for deriving concrete architectures from the reference architecture based on stakeholders' requirements. Finally, we illustrate our approach with the design and implementation of a transparency system for beef supply chains."
      }
    ]
  },
  {
    "tid":"151",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Unifying dependent clustering and disparate clustering for non-homogeneous data' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Modern data mining settings involve a combination of attribute-valued descriptors over entities as well as specified relationships between these entities. We present an approach to cluster such non-homogeneous datasets by using the relationships to impose either dependent clustering or disparate clustering constraints. Unlike prior work that views constraints as boolean criteria, we present a formulation that allows constraints to be satisfied or violated in a smooth manner. This enables us to achieve dependent clustering and disparate clustering using the same optimization framework by merely maximizing versus minimizing the objective function. We present results on both synthetic data as well as several real-world datasets."
      }
    ]
  },
  {
    "tid":"152",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Muse: a system for understanding and designing mappings' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Schema mappings are logical assertions that specify the relationships between a source and a target schema in a declarative way. The specification of such mappings is a fundamental problem in information integration. Mappings can be generated by existing mapping systems (semi-)automatically from a visual specification between two schemas. In general, the well-known 80-20 rule applies for mapping generation tools. They can automate 80% of the work, covering common cases and creating a mapping that is close to correct. However, ensuring complete correctness can still require intricate manual work to perfect portions of the mapping. Previous research on mapping understanding and refinement and anecdotal evidence from mapping designers suggest that the mapping design process can be perfected by using data examples to explain the mapping and alternative mappings. We demonstrate Muse, a data example driven mapping design tool currently implemented on top of the Clio schema mapping system. Muse leverages data examples that are familiar to a designer to illustrate nuances of how a small change to a mapping specification changes its semantics. We demonstrate how Muse can differentiate between alternative mapping specifications and infer the desired mapping semantics based on the designer's actions on a short sequence of simple data examples."
      }
    ]
  },
  {
    "tid":"153",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Design, implementation and evaluation of an efficient opportunistic retransmission protocol' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents an efficient opportunistic retransmission protocol (PRO, Protocol for Retransmitting Opportunistically) to improve the performance of IEEE 802.11 WLANs. PRO is a link-layer protocol that allows overhearing nodes to function as relays that retransmit on behalf of the source after they learn about a failed transmission. Relays with stronger connectivity to the destination have a higher chance of delivering the packet than the source does, thereby resulting in a more efficient use of the channel. PRO has four main features. First, channel reciprocity coupled with a run-time calibration process is used to estimate the instantaneous link quality to the destination. Second, a local qualification process filters out poor relays early. Third, a distributed relay selection algorithm chooses the best set of eligible relays among all qualified relays and prioritizes them. Finally, 802.11e Enhanced Distributed Channel Access (EDCA) is leveraged to make sure high priority relays transmit with high probability. PRO is designed to coexist with legacy 802.11 stations. We have implemented PRO in the driver of a commodity wireless card. Our extensive evaluation on both a controlled testbed and in the real world shows that PRO boosts throughput in diverse wireless environments, and especially in when there is significant contention for the channels, under fading, and with user mobility."
      }
    ]
  },
  {
    "tid":"154",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Adaptive NormalHedge for robust visual tracking.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a novel visual tracking framework, based on a decision-theoretic online learning algorithm namely NormalHedge. To make NormalHedge more robust against noise, we propose an adaptive NormalHedge algorithm, which exploits the historic information of each expert to perform more accurate prediction than the standard NormalHedge. Technically, we use a set of weighted experts to predict the state of the target to be tracked over time. The weight of each expert is online learned by pushing the cumulative regret of the learner towards that of the expert. Our simulation experiments demonstrate the effectiveness of the proposed adaptive NormalHedge, compared to the standard NormalHedge method. Furthermore, the experimental results of several challenging video sequences show that the proposed tracking method outperforms several state-of-the-art methods."
      }
    ]
  },
  {
    "tid":"155",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Beep: 3D Indoor Positioning Using Audible Sound' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Rapid growth in the number of wireless enabled devices has led to an increased interest in location-aware applications. The backbone of such applications is provided by a location system. In this paper we present Beep, an indoor location system that senses audible sound. The use of audible sound makes our system cheap and easily deployable to most existing roaming devices. Unlike positioning systems using ultrasound and infrared signals, Beep does not require the user to carry any kind of specialized hardware. Our system is based on standard 3D multilateration algorithms. However, the requirement of being able to locate existing devices, whose sound cards were not designed for high-precision signaling, introduces additional challenges to the location problem. This paper describes how those problems were solved and presents experimental results. Beep works with an accuracy of about 2 feet in more than 97% cases. The paper also describes a sensor deployment strategy that requires low sensor density and consequently low installation costs."
      }
    ]
  },
  {
    "tid":"156",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mapping Structures for Flash Memories: Techniques and Open Problems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Flash memory is a type of electrically erasable programmable read-only memory (EEPROM). Because flash memories are nonvolatile and relatively dense, they are now used to store files and other persistent objects in handheld computers, mobile phones, digital cameras, portable music players, and many other computer systems in which magnetic disks are inappropriate. Flash, like earlier EEPROM devices, suffers from two limitations. First, bits can only be cleared by erasing a large block of memory. Second, each block can only sustain a limited number of erasures, after which it can no longer reliably store data. Due to these limitations, sophisticated data structures and algorithms are required to effectively use flash memories. These algorithms and data structures support efficient not-in-place updates of data, reduce the number of erasures, and level the wear of the blocks in the device. This survey presents these algorithms and data structures as well as open theoretical problems that arise in this area."
      }
    ]
  },
  {
    "tid":"157",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Sensor fusion-based exploration in home environments using information, driving and localization gains' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We proposed a sensor fusion-based exploration scheme in home environments.The goal is to optimize the exploration efficiency and improve the map quality.In complex environments, the proposed method showed excellent performance.The localization gain enables the robot to accurately estimate its pose.The driving gain enables the robot to explore unknown environments very efficiently. Exploration is one of the most important functions for a mobile service robot because a map is required to carry out various tasks. A suitable strategy is needed to efficiently explore an environment and to build an accurate map. This study proposed the use of several gains (information, driving, localization) that, if considered during exploration, can simultaneously improve the efficiency of the exploration process and quality of the resulting map. Considering the information and driving gains reduces behavior that leads a robot to explore a previously visited place, and thus the exploration distance is reduced. In addition, the robot can select a favorable path for localization by considering the localization gain during exploration, and the robot can estimate its pose more robustly than other methods that do not consider localizability during exploration. This proposed exploration method was verified by various experiments, which verified that a robot can build an accurate map fully autonomously and efficiently in various home environments using the proposed method."
      }
    ]
  },
  {
    "tid":"158",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Transformative Reality: improving bionic vision with robotic sensing.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Implanted visual prostheses provide bionic vision with very low spatial and intensity resolution when compared against healthy human vision. Vision processing converts camera video to low resolution imagery for bionic vision with the aim of preserving salient features such as edges. Transformative Reality extends and improves upon traditional vision processing in three ways. Firstly, a combination of visual and non-visual sensors are used to provide multi-modal data of a person's surroundings. This enables the sensing of features that are difficult to sense with only a camera. Secondly, robotic sensing algorithms construct models of the world in real time. This enables the detection of complex features such as navigable empty ground or people. Thirdly, models are visually rendered so that visually complex entities such as people can be effectively represented in low resolution. Preliminary simulated prosthetic vision trials, where a head mounted display is used to constrain a subject's vision to 25\u00d725 binary phosphenes, suggest that Transformative Reality provides functional bionic vision for tasks such as indoor navigation, object manipulation and people detection in scenes where traditional processing is unusable."
      }
    ]
  },
  {
    "tid":"159",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Where should i turn: moving from individual to collaborative navigation strategies to inform the interaction design of future navigation systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The design of in-vehicle navigation systems fails to take into account the social nature of driving and automobile navigation. In this paper, we consider navigation as a social activity among drivers and navigators to improve design of such systems. We explore the implications of moving from a map-centered, individually-focused design paradigm to one based upon collaborative human interaction during the navigation task. We conducted a qualitative interaction design study of navigation among three types of teams: parents and their teenage children, couples, and unacquainted individuals. We found that collaboration varied among these different teams, and was influenced by social role, as well as the task role of driver or navigator. We also found that patterns of prompts, maneuvers, and confirmations varied among the three teams. We identify overarching practices that differ greatly from the literature on individual navigation. From these discoveries, we present design implications that can be used to inform future navigation systems."
      }
    ]
  },
  {
    "tid":"160",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'id-Privacy in large scale biometric systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Balancing privacy and security concerns in biometric systems is an area of growing importance. While important work has gone on in template protection and revocable biometric tokens, these avenues of research address only one aspect of the problem. Such research does not address a critical issue: balancing the need government and anti-fraud programs to do deduplication (ensure one identity per person) against the potential for abuse using that data. Any existing system capable of deduplication, even if using a template protection scheme, would allow function creep or abuse by searching with latent prints. This paper introduces the concept of id-privacy, requiring at least i items (e.g. fingers) to be provided to resolve identity to better than d above random chance. We show how using cross-finger representation on unsegmented fingerprint slap data, we can address what may be the single most important \u201cprivacy\u201d issue in biometrics, privacy enhanced deduplication. We prove we can achieve (2,0)-id-privacy for fingerprint-based deduplication while preventing searching with a latent print. We introduce the Forest Finger algorithm - an approach for matching unsegmented slaps and cross-finger representations. Our results on the largest public slap database shows superior accuracy when compared with existing NIST Bozorth matcher when tested on unsegmented slaps, segmented prints or fused rolled prints."
      }
    ]
  },
  {
    "tid":"161",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fast firewall implementations for software and hardware-based routers' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Routers must perform packet classification at high speeds to efficiently implement functions such as firewalls and diffserv. Classification can be based on an arbitrary number of fields in the packet header. Performing classification quickly on an arbitrary number of fields is known to be difficult, and has poor worst-case complexity.In this paper, we re-examine two basic mechanisms that have been dismissed in the literature as being too inefficient: backtracking search and set pruning tries. We find using real databases that the time for backtracking search is much better than the worst-case bound; instead of \u03a9((logN)k-1), the search time is only roughly twice the optimal search time1. Similarly, we find that set pruning tries (using a DAG optimization) have much better storage costs than the worstcase bound. We also propose several new techniques to further improve the two basic mechanisms. Our major ideas are (i) backtracking search on a small memory budget, (ii) a novel compression algorithm, (iii) pipelining the search, (iv) the ability to trade-off smoothly between backtracking and set pruning, and (v) algorithms to effectively make use of hardware if hardware is available. We quantify the performance gain of each technique using real databases. We show that on real firewall databases our schemes, with the accompanying optimizations, are close to optimal in time and storage."
      }
    ]
  },
  {
    "tid":"162",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Automatically Labeling Video Data Using Multi-class Active Learning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Labeling video data is an essential prerequisite for many visionapplications that depend on training data, such as visualinformation retrieval, object recognition, and human activitymodeling. However, manually creating labels is not onlytime-consuming but also subject to human errors, and eventually,becomes impossible for a very large amount of data (e.g. 24/7surveillance video). To minimize the human effort in labeling, wepropose a unified multi-class active learning approach forautomatically labeling video data. The contributions of this paperinclude extending active learning from binary classes to multipleclasses and evaluating several practical sample selectionstrategies. The experimental results show that the proposedapproach works effectively even with a significantly reduced amountof labeled data. The best sample selection strategy can achievemore than a 50% error reduction over random sample selection."
      }
    ]
  },
  {
    "tid":"163",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Classification Pruning for Web-request Prediction' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"N-gram and repeating pattern based prediction rules have been used for next-web request prediction. However, there is no rigorous study of how to select the best rule for a given observation. The longer patter may not be the best pattern, because such patters are also more rare. In this paper, we propose several rule-pruning methods that enable us to build efficient, compact and high-quality classifiers for web- request prediction."
      }
    ]
  },
  {
    "tid":"164",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Frequent grams based embedding for privacy preserving record linkage' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we study the problem of privacy preserving record linkage which aims to perform record linkage without revealing anything about the non-linked records. We propose a new secure embedding strategy based on frequent variable length grams which allows record linkage on the embedded space. The frequent grams used for constructing the embedding base are mined from the original database under the framework of differential privacy. Compared with the state-of-the-art secure matching schema [15], our approach provides formal, provable privacy guarantees and achieves better scalability while providing comparable utility."
      }
    ]
  },
  {
    "tid":"165",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'TokDoc: a self-healing web application firewall' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The growing amount of web-based attacks poses a severe threat to the security of web applications. Signature-based detection techniques increasingly fail to cope with the variety and complexity of novel attack instances. As a remedy, we introduce a protocol-aware reverse HTTP proxy TokDoc (the token doctor), which intercepts requests and decides on a per-token basis whether a token requires automatic \"healing\". In particular, we propose an intelligent mangling technique, which, based on the decision of previously trained anomaly detectors, replaces suspicious parts in requests by benign data the system has seen in the past. Evaluation of our system in terms of accuracy is performed on two real-world data sets and a large variety of recent attacks. In comparison to state-of-the-art anomaly detectors, TokDoc is not only capable of detecting most attacks, but also significantly outperforms the other methods in terms of false positives. Runtime measurements show that our implementation can be deployed as an inline intrusion prevention system."
      }
    ]
  },
  {
    "tid":"166",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Using reputation and adaptive coalitions to support collaboration in competitive environments' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Internet-based scenarios, like co-working, e-freelancing, or crowdsourcing, usually need supporting collaboration among several actors that compete to service tasks. Moreover, the distribution of service requests, i.e., the arrival rate, varies over time, as well as the service workload required by each customer. In these scenarios, coalitions can be used to help agents to manage tasks they cannot tackle individually. In this paper we present a model to build and adapt coalitions with the goal of improving the quality and the quantity of tasks completed. The key contribution is a decision making mechanism that uses reputation and adaptation to allow agents in a competitive environment to autonomously enact and sustain coalitions, not only its composition, but also its number, i.e., how many coalitions are necessary. We provide empirical evidence showing that when agents employ our mechanism it is possible for them to maintain high levels of customer satisfaction. First, we show that coalitions keep a high percentage of tasks serviced on time despite a high percentage of unreliable workers. Second, coalitions and agents demonstrate that they successfully adapt to a varying distribution of customers' incoming tasks. This occurs because our decision making mechanism facilitates coalitions to disband when they become non-competitive, and individual agents detect opportunities to start new coalitions in scenarios with high task demand."
      }
    ]
  },
  {
    "tid":"167",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fast And Robust Motion Tracking For Time-Varying Mesh Featuring Reeb-Graph-Based Skeleton Fitting And Its Application To Motion Retrieval' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, an algorithm for motion extraction from Time-Varying Mesh (TVM) is proposed. TVM is a sequence of 3D models made for real-world dynamic objects. In TVM, detailed information of 3D objects such as shape, color, and motion is stored. Therefore, TVM is an attractive technology for the next-generation multimedia. However, the management of TVM database for archiving and retrieving is a significant problem because it is difficult to locate and track feature points in TVM. For efficient and effective archive and retrieval systems, motion extraction and tracking is essential. In our approach, skeletons are extracted from TVM using Reeb graph, and motion tracking is achieved by fitting a reference skeleton to the others. We defined a geodesic function using principal component analysis (PCA) for fast and noiseless skeleton extraction. Robust fitting has been realized by an end node tracking strategy. In addition, we have developed an efficient motion retrieval system compatible to conventional motion capture (mocap) data using the extracted skeleton. As a result, low-cost query generation using mocap systems and cross search between TVM and mocap data have been achieved."
      }
    ]
  },
  {
    "tid":"168",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Study and Comparison of Schema Versioning and Database Conversion Techniques for Bi-Temporal Databases' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we describe a new system of relations for reasoning about general, i.e., both convex and nonconvex, intervals of time. This system is essentially an integration of Allen's theory [1,2] and a one-dimensional version of the Region Connection ..."
      }
    ]
  },
  {
    "tid":"169",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Distance Oracles beyond the Thorup-Zwick Bound' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We give the first improvement to the space/approximation trade-off of distance oracles since the seminal result of Thorup and Zwick [STOC'01]. For unweighted graphs, our distance oracle has size $O(n^{5/3}) = O(n^{1.66\\cdots})$ and, when queried about vertices at distance $d$, returns a path of length $2d+1$. For weighted graphs with $m=n^2/\\alpha$ edges, our distance oracle has size $O(n^2 / \\sqrt[3]{\\alpha})$ and returns a factor 2 approximation. Based on a plausible conjecture about the hardness of set intersection queries, we show that a 2-approximate distance oracle requires space $\\tOmega(n^2 / \\sqrt{\\alpha})$. For unweighted graphs, this implies a $\\tOmega(n^{1.5})$ space lower bound to achieve approximation $2d+1$."
      }
    ]
  },
  {
    "tid":"170",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A refined cut-through buffer management scheme for layered protocol stacks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Avoiding recopying of user data as it is passed between protocol layers helps break the protocol-execution bottleneck. The authors use a use a simple pointer-variable updating technique in the sending phase of their refined buffer cut-through scheme. Binary tree traversal is not required. Time is saved in allocating (and later deallocating) buffers for storing usually short data transfer protocol headers. Their scheme saves working buffer space for segmenting and linking of protocol data units. Because a simple linked list is constructed in the receiving phase, significant working buffer space can be saved. Each node of the linked list points to a user data segment. During the linked-list traversal, user data is copied directly to the user process when a node is visited.<>"
      }
    ]
  },
  {
    "tid":"171",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Statistical Comparison Of Classifiers For Script Identification From Multi-Script Handwritten Documents' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Script identification for handwritten document image is an open document analysis problem especially for multilingual optical character recognition (OCR) system. To design the OCR system for multi-script document pages, it is essential to recognise different scripts before running a particular OCR system of a script. The present work reports an intelligent feature-based technique for word-level script identification in multi-script handwritten document pages. At first, the text lines and then the words are extracted from the document pages. A set of 39 distinctive features have been designed of which eight features are topological and the rest (31) are based on convex hull for each word image. For selection of a suitable classifier, performances of multiple classifiers are evaluated with the designed feature set on multiple subsets of freely available database CMATERdb1.5.1 (http://www. code. google. com/p/cmaterdb), which comprises of 150 handwritten document pages containing both Devnagari and Roman script words. Statistical significance tests on these performance measures declare MLP to be the best performing one. The overall word-level script identification accuracy with MLP classifier on the said database is observed as 99.74%."
      }
    ]
  },
  {
    "tid":"172",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A multi-task adaptive monitoring system combining different sampling primitives' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Traffic measurement and analysis are crucial management activities for network operators. With the increase in traffic volume, operators resort to sampling primitives to reduce the measurement load. Unfortunately, existing systems use sampling primitives separately and configure them statically to achieve some performance objective. It becomes then important to design a new system that combines different existing sampling primitives together to support a large spectrum of monitoring tasks while providing the best possible accuracy by spatially correlating measurements and adapting the configuration to traffic variability. In this paper, and to prove the interest of the joint approach, we introduce an adaptive system that combines two sampling primitives, packet sampling and flow sampling, and that is able to satisfy multiple monitoring tasks. Our system consists of two main functions: (i) a global estimator that investigates measurements done by the different sampling primitives in order to deal with multiple monitoring tasks and to construct a more reliable global estimator while providing visibility over the entire network; (ii) an optimization method based on overhead prediction that allows to reconfigure monitors according to accuracy requirements and monitoring constraints. We present an exhaustive experimental methodology with different monitoring tasks in order to assess the performance of our system. Our experimentations are done on our MonLab platform that we developed for the purpose of this research.\n\n"
      }
    ]
  },
  {
    "tid":"173",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Using complete-1-distinguishability for FSM equivalence checking' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper introduces the use of the Complete-1-Distinguishability (C-1-D) property for simplifying FSM verification. This property eliminates the need for a traversal of the product machine for the implementation and the specification. Instead, a much simpler check suffices. This check consists of first obtaining a 1-equivalence mapping between states of the two machines, and then checking that it is a bisimulation relation. The C-1-D property can be used directly on specifications for which it naturally holds -- a condition that has not been exploited thus far in FSM verification. We also show how this property can be enforced on arbitrary FSMs by exposing some of the latch outputs as pseudo-primary outputs during synthesis and verification. In this sense, our synthesis/verification methodology provides another point in the tradeoff curve between constraints-on-synthesis versus complexity-of-verification. Practical experiences with using this methodology have resulted in success with several examples for which it is not possible to complete verification using existing implicit state space traversal techniques."
      }
    ]
  },
  {
    "tid":"174",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Supporting Semantic Concept Retrieval With Negative Correlations In A Multimedia Big Data Mining System' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With the extensive use of smart devices and blooming popularity of social media websites such as Flickr, YouTube, Twitter, and Facebook, we have witnessed an explosion of multimedia data. The amount of data nowadays is formidable without effective big data technologies. It is well-acknowledged that multimedia high-level semantic concept mining and retrieval has become an important research topic; while the semantic gap (i.e., the gap between the low-level features and high-level concepts) makes it even more challenging. To address these challenges, it requires the joint research efforts from both big data mining and multimedia areas. In particular, the correlations among the classes can provide important context cues to help bridge the semantic gap. However, correlation discovery is computationally expensive due to the huge amount of data. In this paper, a novel multimedia big data mining system based on the MapReduce framework is proposed to discover negative correlations for semantic concept mining and retrieval. Furthermore, the proposed multimedia big data mining system consists of a big data processing platform with Mesos for efficient resource management and with Cassandra for handling data across multiple data centers. Experimental results on the TRECVID benchmark datasets demonstrate the feasibility and the effectiveness of the proposed multimedia big data mining system with negative correlation discovery for semantic concept mining and retrieval."
      }
    ]
  },
  {
    "tid":"175",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Semidefinite relaxation based beamforming in clustered cooperative multicell MISO systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a novel coordinated downlink beamforming strategy based on the semidefinite relaxation (SDR) approach for a clustered multicell cellular system. As base station (BS) cooperation can provide significant performance improvement for the system capacity and interference suppression, we divide the whole network into a number of clusters. In each cluster, several BSs share the common user data and cooperatively design their signaling strategies. In this system, maximizing the sum-rate requires solving a global nonconvex optimization problem. We apply the semidefinite programming (SDP) relaxation to convert the original problem into distributed convex SDP which can be solved efficiently and optimally with convex optimization techniques. Numerical results show that the proposed beamforming algorithm provides a remarkable sum-rate enhancement and outperforms some existing schemes."
      }
    ]
  },
  {
    "tid":"176",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A constant-factor approximation algorithm for the k-median problem' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present the first constant-factor approximation algorithm for the metric k-median problem. The k-median problem is one of the most well-studied clustering problems, i.e., those problems in which the aim is to partition a given set of points into clusters so that the points within a cluster are relatively close with respect to some measure. For the metric k-median problem, we are given n points in a metric space. We select k of these to be cluster centers and then assign each point to its closest selected center. If point j is assigned to a center i, the cost incurred is proportional to the distance between i and j. The goal is to select the k centers that minimize the sum of the assignment costs. We give a 62/3-approximation algorithm for this problem. This improves upon the best previously known result of O(log k log log k), which was obtained by refining and derandomizing a randomized O(log n log log n)-approximation algorithm of Bartal."
      }
    ]
  },
  {
    "tid":"177",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Equivalence probability and sparsity of two sparse solutions in sparse representation.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper discusses the estimation and numerical calculation of the probability that the 0-norm and 1-norm solutions of underdetermined linear equations are equivalent in the case of sparse representation. First, we define the sparsity degree of a signal. Two equivalence probability estimates are obtained when the entries of the 0-norm solution have different sparsity degrees. One is for the case in which the basis matrix is given or estimated, and the other is for the case in which the basis matrix is random. However, the computational burden to calculate these probabilities increases exponentially as the number of columns of the basis matrix increases. This computational complexity problem can be avoided through a sampling method. Next, we analyze the sparsity degree of mixtures and establish the relationship between the equivalence probability and the sparsity degree of the mixtures. This relationship can be used to analyze the performance of blind source separation (BSS). Furthermore, we extend the equivalence probability estimates to the small noise case. Finally, we illustrate how to use these theoretical results to guarantee a satisfactory performance in underdetermined BSS."
      }
    ]
  },
  {
    "tid":"178",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Group versus individual use of power-only EPMcreate as a creativity enhancement technique for requirements elicitation.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Creativity is often needed in requirements elicitation, i.e., generating ideas for requirements, and therefore, techniques to enhance creativity are believed to be useful. How does the size of a group using the Power-Only EPMcreate (POEPMcreate) creativity enhancement technique affect the group's and each member of the group's effectiveness in generating requirement ideas? This paper describes an experiment in which individuals and two-person and four-person groups used POEPMcreate to generate ideas for requirements for enhancing a high school's public Web site. The data of this experiment combined with the data of two previous experiments involving two-person and four-person groups using POEPMcreate show that, similar to what has been observed for brainstorming, the size of a group using POEPMcreate does affect the number of raw and new requirement ideas generated by the group and by the average member of the group. The data allow concluding that a two-person group using POEPMcreate generates more raw and new requirement ideas, both per group and per group member or individual, than does a four-person group and than does an individual. This conclusion is partially corroborated by qualitative data gathered from a survey of professional business or requirements analysts about group sizes and creativity enhancement techniques."
      }
    ]
  },
  {
    "tid":"179",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Challenge-response-based biometric image scrambling for secure personal identification' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a challenge/response-based biometric image scrambling scheme which attempts to overcome the liveness and retransmission issues of biometric image transmission over insecure communication channel. An intelligent biometric sensor is proposed which has computational power to receive challenges from authentication server and generates response against the challenge with encrypted biometric image. The fractional Fourier transform (FRT) has been used for a biometric image encryption process with its scaling factors and random phase masks as the secret keys. The random phase masks of FRT are chaotically generated by piecewise linear chaotic map (PWLCM) to further improve the encryption security. Experimental and simulation results have demonstrated that the proposed system is secure, robust, and deters the risks of attacks for secure biometric image transmission."
      }
    ]
  },
  {
    "tid":"180",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Stereoscopic video synthesis from a monocular video.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents an automatic and robust approach to synthesize stereoscopic videos from ordinary monocular videos acquired by commodity video cameras. Instead of recovering the depth map, the proposed method synthesizes the binocular parallax in stereoscopic video directly from the motion parallax in monocular video. The synthesis is formulated as an optimization problem via introducing a cost function of the stereoscopic effects, the similarity, and the smoothness constraints. The optimization selects the most suitable frames in the input video for generating the stereoscopic video frames. With the optimized selection, convincing and smooth stereoscopic video can be synthesized even by simple constant-depth warping. No user interaction is required. We demonstrate the visually plausible results obtained given the input clips acquired by ordinary handheld video camera."
      }
    ]
  },
  {
    "tid":"181",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Hybrid visibility compositing and masking for illustrative rendering.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we introduce a novel framework for the compositing of interactively rendered 3D layers tailored to the needs of scientific illustration. Currently, traditional scientific illustrations are produced in a series of composition stages, combining different pictorial elements using 2D digital layering. Our approach extends the layer metaphor into 3D without giving up the advantages of 2D methods. The new compositing approach allows for effects such as selective transparency, occlusion overrides, and soft depth buffering. Furthermore, we show how common manipulation techniques such as masking can be integrated into this concept. These tools behave just like in 2D, but their influence extends beyond a single viewpoint. Since the presented approach makes no assumptions about the underlying rendering algorithms, layers can be generated based on polygonal geometry, volumetric data, point-based representations, or others. Our implementation exploits current graphics hardware and permits real-time interaction and rendering."
      }
    ]
  },
  {
    "tid":"182",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The natural-constraint representation of the path space for efficient light transport simulation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The path integral formulation of light transport is the basis for (Markov chain) Monte Carlo global illumination methods. In this paper we present half vector space light transport (HSLT), a novel approach to sampling and integrating light transport paths on surfaces. The key is a partitioning of the path space into subspaces in which a path is represented by its start and end point constraints and a sequence of generalized half vectors. We show that this representation has several benefits. It enables importance sampling of all interactions along paths in between two endpoints. Based on this, we propose a new mutation strategy, to be used with Markov chain Monte Carlo methods such as Metropolis light transport (MLT), which is well-suited for all types of surface transport paths (diffuse/glossy/specular interaction). One important characteristic of our approach is that the Fourier-domain properties of the path integral can be easily estimated. These can be used to achieve optimal correlation of the samples due to well-chosen mutation step sizes, leading to more efficient exploration of light transport features. We also propose a novel approach to control stratification in MLT with our mutation strategy."
      }
    ]
  },
  {
    "tid":"183",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Allowing users to weight search terms' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":" Information retrieval systems typically weight the importance of search terms according todocument and collection statistics (such as by using tf \\Theta idf scores, where less common termsare given higher weight). We consider here the scenario where a user can express her ownsubjective weighting of the importance of the terms that form the query on top of the systemgeneratedweighting, and show how this should modify the relevance scores of documents.This has been allowed before, but... "
      }
    ]
  },
  {
    "tid":"184",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Distributed Computing with Adaptive Heuristics.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"  We use ideas from distributed computing to study dynamic environments in which computational nodes, or decision makers, follow adaptive heuristics (Hart 2005), i.e., simple and unsophisticated rules of behavior, e.g., repeatedly \"best replying\" to others' actions, and minimizing \"regret\", that have been extensively studied in game theory and economics. We explore when convergence of such simple dynamics to an equilibrium is guaranteed in asynchronous computational environments, where nodes can act at any time. Our research agenda, distributed computing with adaptive heuristics, lies on the borderline of computer science (including distributed computing and learning) and game theory (including game dynamics and adaptive heuristics). We exhibit a general non-termination result for a broad class of heuristics with bounded recall---that is, simple rules of behavior that depend only on recent history of interaction between nodes. We consider implications of our result across a wide variety of interesting and timely applications: game theory, circuit design, social networks, routing and congestion control. We also study the computational and communication complexity of asynchronous dynamics and present some basic observations regarding the effects of asynchrony on no-regret dynamics. We believe that our work opens a new avenue for research in both distributed computing and game theory. "
      }
    ]
  },
  {
    "tid":"185",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Catching the Head, Tail, and Everything in Between: A Streaming Algorithm for the Degree Distribution' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The degree distribution is one of the most fundamental graph properties of interest for real-world graphs. It has been widely observed in numerous domains that graphs typically have a tailed or scale-free degree distribution. While the average degree is usually quite small, the variance is quite high and there are vertices with degrees at all scales. We focus on the problem of approximating the degree distribution of a large streaming graph, with small storage. We design an algorithm headtail, whose main novelty is a new estimator of infrequent degrees using truncated geometric random variables. We give a mathematical analysis of headtail and show that it has excellent behavior in practice. We can process streams will millions of edges with storage less than 1% and get extremely accurate approximations for all scales in the degree distribution. We also introduce a new notion of Relative Hausdorff distance between tailed histograms. Existing notions of distances between distributions are not suitable, since they ignore infrequent degrees in the tail. The Relative Hausdorff distance measures deviations at all scales, and is a more suitable distance for comparing degree distributions. By tracking this new measure, we are able to give strong empirical evidence of the convergence of headtail."
      }
    ]
  },
  {
    "tid":"186",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Linear-time Detection of Non-linear Changes in Massively High Dimensional Time Series.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Change detection in multivariate time series has applications in many domains, including health care and network monitoring. A common approach to detect changes is to compare the divergence between the distributions of a reference window and a test window. When the number of dimensions is very large, however, the naive approach has both quality and efficiency issues: to ensure robustness the window size needs to be large, which not only leads to missed alarms but also increases runtime.  this end, we propose LIGHT, a linear-time algorithm for robustly detecting non-linear changes in massively high dimensional time series. Importantly, LIGHT provides high flexibility in choosing the window size, allowing the domain expert to fit the level of details required. To do such, we 1) perform scalable PCA to reduce dimensionality, 2) perform scalable factorization of the joint distribution, and 3) scalably compute divergences between these lower dimensional distributions. Extensive empirical evaluation on both synthetic and real-world data show that LIGHT outperforms state of the art with up to 100% improvement in both quality and efficiency."
      }
    ]
  },
  {
    "tid":"187",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Conceptual Model of the Graphical Editor GENGED for the Visual Definition of Visual Languages' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This contribution presents a conceptual model ofGENGED, an editor supporting the visual definition of visual lan- guages (VLs). As usual a VL consists of an alphabet and a gramm ar. Given a grammar of a specific VLGENGED generates a syntax-directed graphical editor allowing themanipulation of visual sentences over this language. The conceptual framework of GENGED is based on algebraic graph transformation and algebraic sp ecification techniques, especially typed algebraic specifications. Starting with a type signatureMETAVI SUAL for visual alpha- bets, the user of GENGED can define a specific VL-alphabet and some graphical constrai nts. The VL-alphabet and the constraints are transformed into an algebraic speci fication whose signature is typed over METAVI SUAL. The VL-grammar the user can define on top of the VL-alphabet consi sts of a start sentence and a set of VL-rules. Each VL-rule is defined by a graph grammar rule and comprises a left - as well as a right-hand-side. Both sides are again visual sentences. Like the VL-alphabet these visual s entences can be transformed into algebraic specifica- tions whose signature is typed over the VL-alphabet. Becaus e the typing is transitiv, all visual sentences are also typed over METAVI SUAL. The same is true according to the visual sentences which can be manipulated using a gen- erated graphical editor."
      }
    ]
  },
  {
    "tid":"188",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Parallel QZ Algorithm for Distributed Memory HPC Systems.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Appearing frequently in applications, generalized eigenvalue problems represent one of the core problems in numerical linear algebra. The QZ algorithm of Moler and Stewart is the most widely used algorithm for addressing such problems. Despite its importance, little attention has been paid to the parallelization of the QZ algorithm. The purpose of this work is to fill this gap. We propose a parallelization of the QZ algorithm that incorporates all modern ingredients of dense eigensolvers, such as multishift and aggressive early deflation techniques. To deal with (possibly many) infinite eigenvalues, a new parallel deflation strategy is developed. Numerical experiments for several random and application examples demonstrate the effectiveness of our algorithm on two different distributed memory HPC systems."
      }
    ]
  },
  {
    "tid":"189",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title '/facet: a browser for heterogeneous semantic web repositories' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Facet browsing has become popular as a user friendly interface to data repositories. The Semantic Web raises new challenges due to the heterogeneous character of the data. First, users should be able to select and navigate through facets of resources of any type and to make selections based on properties of other, semantically related, types. Second, where traditional facet browsers require manual configuration of the software, a semantic web browser should be able to handle any RDFS dataset without any additional configuration. Third, hierarchical data on the semantic web is not designed for browsing: complementary techniques, such as search, should be available to overcome this problem. We address these requirements in our browser, /facet. Additionally, the interface allows the inclusion of facet-specific display options that go beyond the hierarchical navigation that characterizes current facet browsing. /facet is a tool for Semantic Web developers as an instant interface to their complete dataset. The automatic facet configuration generated by the system can then be further refined to configure it as a tool for end users. The implementation is based on current Web standards and open source software. The new functionality is motivated using a scenario from the cultural heritage domain."
      }
    ]
  },
  {
    "tid":"190",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Nominals, inverses, counting, and conjunctive queries or: why infinity is your friend!' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Description Logics are knowledge representation formalisms that provide, for example, the logical underpinning of the W3C OWL standards. Conjunctive queries, the standard query language in databases, have recently gained significant attention as an expressive formalism for querying Description Logic knowledge bases. Several different techniques for deciding conjunctive query entailment are available for a wide range of DLs. Nevertheless, the combination of nominals, inverse roles, and number restrictions in OWL 1 and OWL 2 DL causes unsolvable problems for the techniques hitherto available. We tackle this problem and present a decidability result for entailment of unions of conjunctive queries in the DL ALCHOIQb that contains all three problematic constructors simultaneously. Provided that queries contain only simple roles, our result also shows decidability of entailment of (unions of) conjunctive queries in the logic that underpins OWL 1 DL and we believe that the presented results will pave the way for further progress towards conjunctive query entailment decision procedures for the Description Logics underlying the OWL standards."
      }
    ]
  },
  {
    "tid":"191",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Defining and Supporting Pipelined Executions in OpenMP' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes a set of extensions to the OpenMP programming model to express complex pipelined computations. This is accomplished by defining, in the form of directives, precedence relations among the tasks originated from work-sharing constructs. The proposal is based on the definition of a name space that identifies the work parceled out by these work-sharing constructs. Then the programmer defines the precedence relations using this name space. This relieves the programmer from the burden of defining complex synchronization data structures and the insertion of explicit synchronization actions in the program that make the program difficult to understand and maintain. The paper focuses on the runtime support required to support this feature and the code generated by the NanosCompiler."
      }
    ]
  },
  {
    "tid":"192",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A genetic algorithm for solving patient-priority- based elective surgery scheduling problem' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Surgery generates the largest cost and revenue in the hospital. The quality of operation directly affects the level of patient satisfaction and economic benefit. This paper focuses on partitioning patients into different priorities according to the state of illness, an optimization model with the aim of maximizing customer satisfaction is established under the consideration of a three-dimensional parameter constraint related patients, operating rooms and medical staffs. An Genetic algorithm is proposed with two-dimensional 0-1 encoding for solving the surgery scheduling problem with the data derived from an upper first-class hospital, the experimental results show the efficiency of the model and algorithm."
      }
    ]
  },
  {
    "tid":"193",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Modeling Parallel Scientific Applications through their Input/Output Phases' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The increase in computational power in processing units and the complexity of scientific applications that use high performance computing require more efficient Input/Output (I/O) systems. To use the I/O systems more efficiently it is necessary to know its performance capacity to determine if it fulfills applications' I/O requirements. Evaluating the I/O system performance capacity is difficult due to the diversity of I/O architectures and the complexity of its I/O software stack. Furthermore, parallel scientific applications have different behavior depending on their access patterns. Then, it is necessary to have some method to evaluate the I/O subsystem capacity taking into account the applications access patterns without executing the application in each I/O subsystem. Here, we propose a methodology to evaluate the I/O subsystem performance capacity through an I/O model of the parallel application independent of the I/O subsystem. This I/O model is composed of I/O phases representing \"where\" and \"when\" the I/O operations are performed into application logic. This approach encompasses the I/O subsystem evaluation at I/O library level for the application I/O model. The I/O phases are replicated by benchmark IOR which is executed in the target subsystem. This approach was used to estimate the I/O time of an application in different subsystems. The results show an relative error of estimation lower than 10%. This approach was also utilized to select the I/O subsystem that provide less I/O time for the application."
      }
    ]
  },
  {
    "tid":"194",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Configuring information systems and work practices for each other: What competences are needed locally?' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"\u2022Design is increasingly completed locally as system configurations after go-live.\u2022The configuration of systems and practices for each other is demanding in competences.\u2022Understanding practice is the largest competence issue in the studied project.\u2022Preparing and making change are probably the competences most thinly spread locally.\u2022The catalog of competences needed locally can inform training and project staffing."
      }
    ]
  },
  {
    "tid":"195",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Lattice Basis Reduction Attack against Physically Unclonable Functions' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Due to successful modeling attacks against arbiter PUFs (Physically Unclonable Functions), the trend towards consideration of XOR arbiter PUFs has emerged. Nevertheless, it has already been demonstrated that even this new non-linear structure, with a restricted number of parallel arbiter chains, is still vulnerable to more advanced modeling attacks and side channel analyses. However, so far the security of XOR arbiter PUFs with a large number of parallel arbiter chains has not been appropriately assessed. Furthermore, as another countermeasure against modeling and physical attacks, the concept of controlled PUFs, i.e., with a limited access to challenges and responses, has also been developed. Towards a better understanding of the security of XOR arbiter PUFs, the present paper simultaneously addresses all above mentioned countermeasures by introducing a novel attack, which is a combination of a lattice basis reduction attack and a photonic side channel analysis. We present how our new attack can be successfully launched against XOR arbiter PUFs with an arbitrarily large number of parallel arbiter chains. Most interestingly, our attack does not require any access to challenges or responses. Finally, by conducting an exhaustive discussion on our experimental results, the practical feasibility of our attack scenario is proved as well."
      }
    ]
  },
  {
    "tid":"196",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'f-Sensitivity Distance Oracles and Routing Schemes' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"An f-sensitivity distance oracle for a weighted undirected graph G(V,E) is a data structure capable of answering restricted distance queries between vertex pairs, i.e., calculating distances on a subgraph avoiding some forbidden edges. This paper presents an efficiently constructible f-sensitivity distance oracle that given a triplet (s,t,F), where s and t are vertices and F is a set of forbidden edges such that |F|\u2264f, returns an estimate of the distance between s and t in G(V,E\u2216F). For an integer parameter k\u22651, the size of the data structure is O(fkn 1+1/k log\u2009(nW)), where W is the heaviest edge in G, the stretch (approximation ratio) of the returned distance is (8k\u22122)(f+1), and the query time is O(|F|\u22c5log\u20092 n\u22c5log\u2009log\u2009n\u22c5log\u2009log\u2009d), where d is the distance between s and t in G(V,E\u2216F). Our result differs from previous ones in two major respects: (1) it is the first to consider approximate oracles for general graphs (and thus obtain a succinct data structure); (2) our result holds for an arbitrary number of forbidden edges. In contrast, previous papers concern f-sensitive exact distance oracles, which consequently have size \u03a9(n 2). Moreover, those oracles support forbidden sets F of size |F|\u22642. The paper also considers f-sensitive compact routing schemes, namely, routing schemes that avoid a given set of forbidden (or failed) edges. It presents a scheme capable of withstanding up to two edge failures. Given a message M destined to t at a source vertex s, in the presence of a forbidden edge set F of size |F|\u22642 (unknown to s), our scheme routes M from s to t in a distributed manner, over a path of length at most O(k) times the length of the optimal path (avoiding F). The total amount of information stored in vertices of G is O(kn 1+1/k log\u2009(nW)log\u2009n). To the best of our knowledge, this is the first result obtaining an f-sensitive compact routing scheme for general graphs."
      }
    ]
  },
  {
    "tid":"197",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Deterministic distributed construction of linear stretch spanners in polylogarithmic time' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The paper presents a deterministic distributed algorithm that given an n node unweighted graph constructs an O(n3/2) edge 3- spanner for it in O(log n) time. This algorithm is then extended into a deterministic algorithm for computing an O(k n1+1/k) edge O(k)-spanner in 2O(k) logk-1 n time for every integer parameter k \u2265 1. This establishes that the problem of the deterministic construction of a linear (in k) stretch spanner with few edges can be solved in the distributed setting in polylogarithmic time. The paper also investigates the distributed construction of sparse spanners with almost pure additive stretch (1 + \u03b5, \u03b2), i.e., such that the distance in the spanner is at most 1 + \u03b5 times the original distance plus \u03b2. It is shown, for every \u03b5 0, that in O(\u03b5-1 log n) time one can deterministically construct a spanner with O(n3/2) edges that is both a 3-spanner and a (1+\u03b5, 8 log n)-spanner. Furthermore, it is shown that in nO(1/\u221alog n) + O(1/\u03b5) time one can deterministically construct a spanner with O(n3/2) edges which is both a 3-spanner and a (1 + \u03b5, 4)-spanner. This algorithm can be transformed into a Las Vegas randomized algorithm with guarantees on the stretch and time, running in O(\u03b5-1 + log n) expected time."
      }
    ]
  },
  {
    "tid":"198",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Truth discovery and copying detection in a dynamic world' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Modern information management applications often require integrating data from a variety of data sources, some of which may copy or buy data from other sources. When these data sources model a dynamically changing world (e.g., people's contact information changes over time, restaurants open and go out of business), sources often provide out-of-date data. Errors can also creep into data when sources are updated often. Given out-of-date and erroneous data provided by different, possibly dependent, sources, it is challenging for data integration systems to provide the true values. Straightforward ways to resolve such inconsistencies (e.g., voting) may lead to noisy results, often with detrimental consequences. In this paper, we study the problem of finding true values and determining the copying relationship between sources, when the update history of the sources is known. We model the quality of sources over time by their coverage, exactness and freshness. Based on these measures, we conduct a probabilistic analysis. First, we develop a Hidden Markov Model that decides whether a source is a copier of another source and identifies the specific moments at which it copies. Second, we develop a Bayesian model that aggregates information from the sources to decide the true value for a data item, and the evolution of the true values over time. Experimental results on both real-world and synthetic data show high accuracy and scalability of our techniques."
      }
    ]
  },
  {
    "tid":"199",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A unified supervised codebook learning framework for classification.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we investigate a discriminative visual dictionary learning method for boosting the classification performance. Tied to the K-means clustering philosophy, those popular algorithms for visual dictionary learning cannot guarantee the well-separation of the normalized visual word frequency vectors from distinctive classes or large label distances. The rationale of this work is to harness sample label information for learning visual dictionary in a supervised manner, and this target is then formulated as an objective function, where each sample element, e.g., SIFT descriptor, is expected to be close to its assigned visual word, and at the same time the normalized aggregative visual word frequency vectors are expected to possess the property that kindred samples shall be close to each other while inhomogeneous samples shall be far away. By relaxing the hard binary constraints to soft nonnegative ones, a multiplicative nonnegative update procedure is proposed to optimize the objective function along with theoretic convergence proof. Extensive experiments on classification tasks (i.e., natural scene and sports event classifications) all demonstrate the superiority of this proposed framework over conventional clustering based visual dictionary learning."
      }
    ]
  },
  {
    "tid":"200",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Modeling Energy Consumption in Single-Hop IEEE 802.11 Ad Hoc Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents an analytical model to predict energy consumption in saturated IEEE 802.11 single-hop ad hoc networks under ideal channel conditions. The model we introduce takes into account the different operational modes of the IEEE 802.11 DCF MAC, and is validated against packet-level simulations. In contrast to previous works that attempted to characterize the energy consumption of IEEE 802.11 cards in isolated, contention-free channels (i.e., single sender/receiver pair), this paper investigates the extreme opposite case, i.e., when nodes need to contend for channel access under saturation conditions. In such scenarios, our main findings include: (1) contrary to what most previous results indicate, the radio's transmit mode has marginal impact on overall energy consumption, while other modes (receive, idle, etc.) are responsible for most of the energy consumed; (2) the energy cost to transmit useful data increases almost linearly with the network size; and (3) transmitting large payloads is more energy efficient under saturation conditions."
      }
    ]
  },
  {
    "tid":"201",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Pollux: towards scalable distributed real-time search on microblogs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The last few years have witnessed a meteoric rise of microblogging platforms, such as Twitter and Tumblr. The sheer volume of the microblog data and its highly dynamic nature present unique technical challenges for the platforms that provide search services. In particular, the search service must provide real-time response to queries, and continuously update the results as new microblogs are posted. Conventional approaches either cannot keep up with the high update rate, or cannot scale well to handle the large volume of data. We propose Pollux, a system that provides distributed real-time indexing and search service on microblogs. It adopts the distributed stream processing paradigm advocated by the recently developed platforms that are designed for real-time processing of large volume of data, such as Apache S4 and Twitter Storm. Although those open-source platforms have found successful applications in production environments, they lack some critical features required for real-time search. In particular: (1) they only implement partial fault tolerance, and do not provide lossless recovery in the event of a node failure, and (2) they do not have a facility for storing global data, which is necessary in efficiently ranking search results. Addressing those problems, Pollux extends current platforms in two important ways. First, we propose a failover strategy that can ensure high system availability and no data/state loss in the event of a node failure. Second, Pollux adds a global storage facility that supports convenient, efficient, and reliable data storage for shared data. We describe how to apply Pollux to the task of real-time search. We implement Pollux based on Apache S4, and show through extensive experiments on a Twitter dataset that the proposed solutions are effective, and Pollux can achieve excellent scalability."
      }
    ]
  },
  {
    "tid":"202",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Energy-Efficient Resource Allocation Scheme for Mobile Ad Hoc Computational Grids' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Due to recent advancements in mobile computing and communication technologies, mobile ad hoc computational Grids are emerging as a new computing paradigm, enabling innovative applications through sharing of computing resources among mobile devices without any pre-existing network infrastructure. Energy-efficient resource allocation is one of the key issues in mobile ad hoc computational Grids due to limited battery life of mobile nodes. To reduce energy consumption, we propose a hybrid power-based resource allocation scheme for allocation of interdependent tasks to nodes within mobile ad hoc computational Grid. The basic idea is to exploit dependencies and task type, and allocate interdependent tasks to nodes accessible at minimum transmission power. We also propose a power-based algorithm to search a group of closest nodes to allocate a set of interdependent tasks. Compared to traditional algorithms, complexity of proposed algorithm depends on number of transmission power levels rather than number of nodes within a Grid. The scheme is validated in a simulation environment using various workloads and parameters."
      }
    ]
  },
  {
    "tid":"203",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Robust and stable small-world topology of brain intrinsic organization during pre- and post-task resting states' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Brain functional network studies have demonstrated the small-world topology as the nature of large-scale spontaneous brain activity. Studies have also revealed that the temporal coherence of spontaneous activity could be reshaped during task-dependent (or post-task) resting states within local spatial patterns such as task-related and the default-mode networks. However, to our best knowledge, it is still a lack of rigorous investigations that whether the small-world topology of spontaneous intrinsic organization remains robust and stable during different resting states. To address the problem, we recorded blood oxygen level-dependent (BOLD) signals from two rests (namely, pre- and posttask resting states) before and after a simple semantic-matching task, and investigated the preceding task influences on the topology of the large-scale spontaneous intrinsic organization during the post-task resting state. The major findings are that the small-world configuration of spontaneous intrinsic organization remains robust and stable during resting states regardless of preceding task influences."
      }
    ]
  },
  {
    "tid":"204",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Importance sampling strategy for non-convex randomized block-coordinate descent' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"As the number of samples and dimensionality of optimization problems related to statistics and machine learning explode, block coordinate descent algorithms have gained popularity since they reduce the original problem to several smaller ones. Coordinates to be optimized are usually selected randomly according to a given probability distribution. We introduce an importance sampling strategy that helps randomized coordinate descent algorithms to focus on blocks that are still far from convergence. The framework applies to problems composed of the sum of two possibly non-convex terms, one being separable and non-smooth. We have compared our algorithm to a full gradient proximal approach as well as to a randomized block coordinate algorithm that considers uniform sampling and cyclic block coordinate descent. Experimental evidences show the clear benefit of using an importance sampling strategy."
      }
    ]
  },
  {
    "tid":"205",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Imbalance is fixed parameter tractable' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In the Imbalance Minimization problem we are given a graph G=(V,E) and an integer b and asked whether there is an ordering v\"1...v\"n of V such that the sum of the imbalance of all the vertices is at most b. The imbalance of a vertex v\"i is the absolute value of the difference between the number of neighbors to the left and right of v\"i. The problem is also known as the Balanced Vertex Ordering problem and it finds many applications in graph drawing. We show that this problem is fixed parameter tractable and provide an algorithm that runs in time 2^O^(^b^l^o^g^b^)@?n^O^(^1^). This resolves an open problem of Kara et al. [On the complexity of the balanced vertex ordering problem, in: COCOON, in: Lecture Notes in Comput. Sci., vol. 3595, 2005, pp. 849-858]."
      }
    ]
  },
  {
    "tid":"206",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'SelInv---An Algorithm for Selected Inversion of a Sparse Symmetric Matrix' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We describe an efficient implementation of an algorithm for computing selected elements of a general sparse symmetric matrix A that can be decomposed as A = LDLT, where L is lower triangular and D is diagonal. Our implementation, which is called SelInv, is built on top of an efficient supernodal left-looking LDLT factorization of A. We discuss how computational efficiency can be gained by making use of a relative index array to handle indirect addressing. We report the performance of SelInv on a collection of sparse matrices of various sizes and nonzero structures. We also demonstrate how SelInv can be used in electronic structure calculations."
      }
    ]
  },
  {
    "tid":"207",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Hybrid Approach for Individual and Group Activity Analysis in Crowded Scene.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents an efficient hybrid (top-down and bottom-up) framework for activity recognition based on analyzing group context in crowded scenes. The approach presented starts by discovering interacting groups of people using a graph based clustering algorithm. Given the interacting groups, a novel group context activity descriptor is computed that captures not only the focal person's activity but also the behaviors of neighbors in the group. Finally, for a high-level of understanding of human activities, we propose a bottom-up approach using a random field model to encode activity relationships between people in the scene. We evaluate our approach on two public benchmark datasets and compare the utility of our proposed descriptor with other descriptors using the same baseline recognition framework. The results of both the steps show that our approach with the proposed descriptor achieves recognition rates comparable to state-of-the-art methods for activity recognition in crowded scenes."
      }
    ]
  },
  {
    "tid":"208",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Versatile lightweight key distribution for big data privacy in vehicular ad hoc networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Vehicular ad hoc networks VANETs continually produce large scale of data shared among vehicles nearby to improve driving experience and safety. The vehicular communications contain a large amount of private information. It is well known that group key management is a fundamental cryptographic primitive for providing secure group communication. Although many proposals with regard to group key management have been introduced, they cannot well support secure subgroup and intergroup communications in VANETs. In this paper, we exploit the specific features of vehicular communications and propose two group key distribution schemes providing efficient solutions to these two problems. Our schemes do not require interaction between users. Storage and computation analyses show that compared with existing schemes, our proposals are more efficient and versatile. Based on these basic schemes, we present extensions for multipartite groups, by which the efficiency is greatly improved in this scenario. We also provide the extensions for two-level multipartite groups, which enable fine-grained real-world applications. The experimental results confirm that our group key distribution schemes are practical in security, computation, and communication. Copyright \u00a9 2015 John Wiley & Sons, Ltd."
      }
    ]
  },
  {
    "tid":"209",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Publicly Verifiable Privacy-Preserving Group Decryption' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Anonymity is one of the main concerns in group-oriented cryptography. However, most efforts, for instance, group signatures and ring signatures, are only made to provide anonymity on the sender's side. There are merely few works done to ensure anonymity in a cryptographic sense on the recipient's side in group-oriented communications. This paper formalizes the notion of group decryption (GD) . It can be viewed as an analog of group signatures in the context of public key encryptions. In this notion, a sender can encrypt a committed message intended to any member of a group, managed by a group manager, while the recipient of the ciphertext remains anonymous . The sender can convince a verifier about this fact without leaking the plaintext or the identity of the recipient. If required, the group manager can verifiably open the identity of the recipient. We propose an efficient GD scheme that is proven secure in the random oracle model. The overhead in both computation and communication is independent of the group size . A full ciphertext is about 0.2K bytes in a typical implementation and the scheme is practical."
      }
    ]
  },
  {
    "tid":"210",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Personalizing map content to improve task completion efficiency' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Significant interaction challenges arise in both developing and using interactive map applications. Users encounter problems of information overload in using interactive maps to complete tasks. This is further exacerbated by device limitations and interaction constraints in increasingly popular mobile platforms. Application developers must then address restrictions related to screen size and limited bandwidth in order to effectively display maps on mobile devices. In order to address issues of user information overload and application efficiency in interactive map applications, we have developed a novel approach for delivering personalized vector maps. Ongoing task interactions between users and maps are monitored and captured implicitly in order to infer individual and group preferences related to specific map feature content. Personalized interactive maps that contain spatial feature content tailored specifically to users' individual preferences are then generated. Our approach addresses spatial information overload by providing only the map information necessary and sufficient to suit user interaction preferences, thus simplifying the completion of tasks performed with interactive maps. In turn, tailoring map content to specific user preferences considerably reduces the size of vector data sets necessary to transmit and render maps on mobile devices. We have developed a geographic information system prototype, MAPPER (MAP PERsonalization), that implements our approach. Experimental evaluations show that the use of personalized maps helps users complete their tasks more efficiently and can reduce information overload."
      }
    ]
  },
  {
    "tid":"211",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Blind separation of audio signals using trigonometric transforms and wavelet denoising' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper deals with the problem of blind separation of audio signals from noisy mixtures. It proposes the application of\n a blind separation algorithm on the discrete cosine transform (DCT) or the discrete sine transform (DST) of the mixed signals,\n instead of performing the separation on the mixtures in the time domain. Wavelet denoising of the noisy mixtures is recommended\n in this paper as a preprocessing step for noise reduction. Both the DCT and the DST have an energy compaction property, which\n concentrates most of the signal energy in a few coefficients in the transform domain, leaving most of the transform domain\n coefficients close to zero. As a result, the separation is performed on a few coefficients in the transform domain. Another\n advantage of signal separation in transform domains is that the effect of noise on the signals in the transform domains is\n smaller than that in the time domain due to the averaging effect of the transform equations, especially when the separation\n algorithm is preceded by a wavelet denoising step. The simulation results confirm the superiority of transform domain separation\n to time domain separation and the importance of the wavelet denoising step."
      }
    ]
  },
  {
    "tid":"212",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Manifold Based Local Classifiers: Linear and Nonlinear Approaches' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In case of insufficient data samples in high-dimensional classification problems, sparse scatters of samples tend to have\n many \u2018holes\u2019\u2014regions that have few or no nearby training samples from the class. When such regions lie close to inter-class\n boundaries, the nearest neighbors of a query may lie in the wrong class, thus leading to errors in the Nearest Neighbor classification\n rule. The K-local hyperplane distance nearest neighbor (HKNN) algorithm tackles this problem by approximating each class with\n a smooth nonlinear manifold, which is considered to be locally linear. The method takes advantage of the local linearity assumption\n by using the distances from a query sample to the affine hulls of query\u2019s nearest neighbors for decision making. However,\n HKNN is limited to using the Euclidean distance metric, which is a significant limitation in practice. In this paper we reformulate\n HKNN in terms of subspaces, and propose a variant, the Local Discriminative Common Vector (LDCV) method, that is more suitable\n for classification tasks where the classes have similar intra-class variations. We then extend both methods to the nonlinear\n case by mapping the nearest neighbors into a higher-dimensional space where the linear manifolds are constructed. This procedure\n allows us to use a wide variety of distance functions in the process, while computing distances between the query sample and\n the nonlinear manifolds remains straightforward owing to the linear nature of the manifolds in the mapped space. We tested\n the proposed methods on several classification tasks, obtaining better results than both the Support Vector Machines (SVMs)\n and their local counterpart SVM-KNN on the USPS and Image segmentation databases, and outperforming the local SVM-KNN on the\n Caltech visual recognition database."
      }
    ]
  },
  {
    "tid":"213",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A competing risks analysis of the duration of federal target funds rates' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The behaviour of short term interest rates has been the focus of extensive studies in economics and finance. This paper analyses the duration between changes in the Intended Federal Funds Rate (IFFR) taking into account different factors and the possibility of two outcomes (rate increase and rate decrease). Due to the lack of independence between these outcomes, the use of traditional survival analysis was ruled out and a novel approach based on competing risks is used. The estimation results show the influence of the current interest rate level, US GDP growth rate, US inflation rate, inflation rate differential with Euro-zone and the sign of the previous interest rate change. Test results also confirm that the duration previous to small changes (of a quarter point) and to large changes (above a quarter point) are statistically different for both rate hikes and rate cuts. Accurate confidence intervals for the Cumulative Incidence Function (CIF) are provided even with small sample size under non-normality."
      }
    ]
  },
  {
    "tid":"214",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient polyhedral enclosures for the reachable set of nonlinear control systems.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This work presents a general theory for the construction of a polyhedral outer approximation of the reachable set (\u201cpolyhedral bounds\u201d) of a dynamic system subject to time-varying inputs and uncertain initial conditions. This theory is inspired by the efficient methods for the construction of interval bounds based on comparison theorems. A numerically implementable instance of this theory leads to an auxiliary system of differential equations which can be solved with standard numerical integration methods. Meanwhile, the use of polyhedra provides greater flexibility in defining tight enclosures on the reachable set. These advantages are demonstrated with a few examples, which show that tight bounds can be efficiently computed for general, nonlinear systems. Further, it is demonstrated that the ability to use polyhedra provides a means to meaningfully distinguish between time-varying and constant, but uncertain, inputs."
      }
    ]
  },
  {
    "tid":"215",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Cost considerations in network on chip' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Systems on Chip (SoCs) require efficient inter-module interconnection providing for the required communications at a low cost. We analyze the generic cost in area and power of Networks on Chip (NoCs) and alternative interconnect architectures: a shared bus, a segmented bus and a point-to-point interconnect. For each architecture we derive analytical expressions for area, power dissipation and operating frequency as well as asymptotic limits of these functions. The analysis quantifies the intuitive NoC scalability advantages.Next we turn to NoC cost optimization. We explore cost tradeoffs between the number of buffers and the link speed. We use a reference architecture, termed QNoC (Quality-of-Service NoC), which is based on a grid of wormhole switches, shortest path routing and multiple QoS classes. Two traffic scenarios are considered, one dominated by short packets sensitive to queuing delays and the other dominated by large block-transfers. Our simulations show that network cost can be minimized while maintaining quality of service, by trading off buffers with links in the first scenario but not in the second."
      }
    ]
  },
  {
    "tid":"216",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Structuring heterogeneous biological information using fuzzy clustering of k-partite graphs.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"BACKGROUND: Extensive and automated data integration in bioinformatics facilitates the construction of large, complex biological networks. However, the challenge lies in the interpretation of these networks. While most research focuses on the unipartite or bipartite case, we address the more general but common situation of k-partite graphs. These graphs contain k different node types and links are only allowed between nodes of different types. In order to reveal their structural organization and describe the contained information in a more coarse-grained fashion, we ask how to detect clusters within each node type. RESULTS: Since entities in biological networks regularly have more than one function and hence participate in more than one cluster, we developed a k-partite graph partitioning algorithm that allows for overlapping (fuzzy) clusters. It determines for each node a degree of membership to each cluster. Moreover, the algorithm estimates a weighted k-partite graph that connects the extracted clusters. Our method is fast and efficient, mimicking the multiplicative update rules commonly employed in algorithms for non-negative matrix factorization. It facilitates the decomposition of networks on a chosen scale and therefore allows for analysis and interpretation of structures on various resolution levels. Applying our algorithm to a tripartite disease-gene-protein complex network, we were able to structure this graph on a large scale into clusters that are functionally correlated and biologically meaningful. Locally, smaller clusters enabled reclassification or annotation of the clusters' elements. We exemplified this for the transcription factor MECP2. CONCLUSIONS: In order to cope with the overwhelming amount of information available from biomedical literature, we need to tackle the challenge of finding structures in large networks with nodes of multiple types. To this end, we presented a novel fuzzy k-partite graph partitioning algorithm that allows the decomposition of these objects in a comprehensive fashion. We validated our approach both on artificial and real-world data. It is readily applicable to any further problem."
      }
    ]
  },
  {
    "tid":"217",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Spherical Catadioptric Arrays: Construction, Multi-View Geometry, and Calibration' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper introduces a novel imaging system composed of an array of spherical mirrors and a single high-resolution digital camera. We describe the mechanical design and construction of a prototype, analyze the geometry of image formation, present a tailored calibration algorithm, and discuss the effect that design decisions had on the calibration routine. This system is presented as a unique platform for the development of efficient multi-view imaging algorithms which exploit the combined properties of camera arrays and non-central projection catadioptric systems. Initial target applications include data acquisition for image-based rendering and 3D scene reconstruction. The main advantages of the proposed system include: a relatively simple calibration procedure, a wide field of view, and a single imaging sensor which eliminates the need for color calibration and guarantees time synchronization."
      }
    ]
  },
  {
    "tid":"218",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimizing Alloy for Multi-objective Software Product Line Configuration.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Software product line (SPL) engineering involves the modeling, analysis, and configuration of variability-rich systems. We improve the performance of the multi-objective optimization of SPLs in Alloy by several orders of magnitude with two techniques. First, we rewrite the model to remove binary relations that map to integers, which enables removing most of the integer atoms from the universe. SPL models often require using large bitwidths, hence the number of integer atoms in the universe can be orders of magnitude more than the other atoms. In our approach, the tuples for these integer-valued relations are computed outside the sat solver before returning the solution to the user. Second, we add a checkpointing facility to Kodkod, which allows the multi-objective optimization algorithm to reuse previously computed internal sat solver state, after backtracking. Together these result in orders of magnitude improvement in using Alloy as a multi-objective optimization tool for software product lines."
      }
    ]
  },
  {
    "tid":"219",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Perfect Structure on the Edge of Chaos.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We construct trapdoor permutations based on sub-exponential indistinguishability obfuscation and one-way functions, thereby providing the first candidate that is not based on the hardness of factoring.Our construction shows that even highly structured primitives, such as trapdoor permutations, can be potentially based on hardness assumptions with noisy structures such as those used in candidate constructions of indistinguishability obfuscation. It also suggest a possible way to construct trapdoor permutations that resist quantum attacks, and that their hardness may be based on problems outside the complexity class $$text{ SZK } $$ i\u00be?-- indeed, while factoring-based candidates do not possess such security, future constructions of indistinguishability obfuscation might.As a corollary, we eliminate the need to assume trapdoor permutations and injective one-way function in many recent constructions based on indistinguishability obfuscation."
      }
    ]
  },
  {
    "tid":"220",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Queue Layouts Of Toroidal Grids' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A queue layout of a graph G consists of a linear order of its vertices, and a partition of its edges into queues, such that no two edges in the same queue are nested. The queuenumber qn(G) is the minimum number of queues required in a queue layout of G. The Cartesian product of two graphs G(1) = (V-1 E-1) and G(2) = (V-2, E-2), denoted by G(1) x G(2), is the graph with {< v(1), v(2)>: v(1) is an element of V-1 and v(2) is an element of V-2) as its vertex set and an edge (< u(1),u(2)>), < u(1),u(2)>) belongs to G(1) x G(2) if and only if either (u(1), v(1)) E El and u(2) = v(2) or (u(2), v(2)) is an element of E-2 and u(1) = v(1). Let T-k1,(k2,...,kn) denote the ndimensional toroidal grid defined by the Cartesian product of n cycles with varied lengths, i.e. T-k1,(k2,...,kn) = C-k1 x C-k2 x ... x C-kn where C-k1 is a cycle of length k(i) >= 3. If k(1) =k(2) = ... = k(n) = k, the graph is also called the k-ary n-cube and is denoted by Q(n)(k). In this paper, we deal with queue layouts of toroidal grids and show the following bound: qn(T-k1,(k2,...,kn)) <= 2n - 2 if n >= 2 and k(i) >= 3 for all i = 1, 2,...,n. In particular, for n = 2 and k(1), k(2) >= 3, we acquire qn(T-k1,(k2)) = 2. Recently, Pai et al. (Inform. Process. Lett. 110 (2009) pp.50-56) showed that qn(Q(n)(k)) <= 2n - 1 if n >= 1 and k >= 9. Thus, our result improves the bound of qn(Q(n)(k)) when n >= 2 and k >= 9."
      }
    ]
  },
  {
    "tid":"221",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multi-Label Image Classification via Knowledge Distillation from Weakly-Supervised Detection.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Multi-label image classification is a fundamental but challenging task towards general visual understanding. Existing methods found the region-level cues (e.g., features from RoIs) can facilitate multi-label classification. Nevertheless, such methods usually require laborious object-level annotations (i.e., object labels and bounding boxes) for effective learning of the object-level visual features. In this paper, we propose a novel and efficient deep framework to boost multi-label classification by distilling knowledge from weakly-supervised detection task without bounding box annotations. Specifically, given the image-level annotations, (1) we first develop a weakly-supervised detection (WSD) model, and then (2) construct an end-to-end multi-label image classification framework augmented by a knowledge distillation module that guides the classification model by the WSD model according to the class-level predictions for the whole image and the object-level visual features for object RoIs. The WSD model is the teacher model and the classification model is the student model. After this cross-task knowledge distillation, the performance of the classification model is significantly improved and the efficiency is maintained since the WSD model can be safely discarded in the test phase. Extensive experiments on two large-scale datasets (MS-COCO and NUS-WIDE) show that our framework achieves superior performances over the state-of-the-art methods on both performance and efficiency.\n\n"
      }
    ]
  },
  {
    "tid":"222",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Incomplete Directed Perfect Phylogeny' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Abstract: Perfect phylogeny is one of the fundamental models for studying evolution. We investigate thefollowing variant of the model: The input is a species-characters matrix. The characters are binary anddirected, i.e., a species can only gain characters. The dierence from standard perfect phylogeny is thatfor some species the states of some characters are unknown. The question is whether one can completethe missing states in a way that admits a perfect phylogeny. The problem arises in classical..."
      }
    ]
  },
  {
    "tid":"223",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Chaotic gravitational constants for the gravitational search algorithm.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Display Omitted Chaotic maps have been embedded into Gravitational Search Algorithms (GSA) for the first time.The problem of trapping in local minima in GSA has been improved by the chaotic maps.The convergence rate of GSA has been improved.The statistical test allowed us to judge about the significance of the results.An adaptive normalization is proposed to smoothly transit from the exploration phase to the exploitation phase. In a population-based meta-heuristic, the search process is divided into two main phases: exploration versus exploitation. In the exploration phase, a random behavior is fruitful to explore the search space as extensive as possible. In contrast, a fast exploitation toward the promising regions is the main objective of the latter phase. It is really challenging to find a proper balance between these two phases because of the stochastic nature of population-based meta-heuristic algorithms. The literature shows that chaotic maps are able to improve both phases. This work embeds ten chaotic maps into the gravitational constant (G) of the recently proposed population-based meta-heuristic algorithm called Gravitational Search Algorithm (GSA). Also, an adaptive normalization method is proposed to transit from the exploration phase to the exploitation phase smoothly. As case studies, twelve shifted and biased benchmark functions evaluate the performance of the proposed chaos-based GSA algorithms in terms of exploration and exploitation. A statistical test called Wilcoxon rank-sum is done to judge about the significance of the results as well. The results demonstrate that sinusoidal map is the best map for improving the performance of GSA significantly."
      }
    ]
  },
  {
    "tid":"224",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Motor Current Regulation Based Direct DC-link Current Control of Wide Range Load Condition for IPMSM Drive System Without Passive Component' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In a single-phase to three-phase electrolytic capacitor-less inverter, the inverter must control not only the motor but also the source current because a power factor correction circuit or energy buffer does not exist. To obtain a sinusoidal source current, a control method that modifies the output voltage directly is proposed. However, the motor current control occasionally becomes unstable because the control method modifies the output voltage to control the source current. To control the source current while considering the motor current response, this paper proposes a motor current regulation based direct DC-link current control. The proposed method enhances the stability of the motor current control system by realizing the sinusoidal source current with the motor current reference modification. The effectiveness of the proposed method is experimentally verified."
      }
    ]
  },
  {
    "tid":"225",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On the Design of Target Beampatterns for Differential Microphone Arrays' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"\nDifferential microphone arrays DMAs have many interesting properties and have been widely used in acoustic, audio, and speech applications. A critical part of a DMA is the differential beamformer, which is generally designed in two important steps: 1 specifying a target beampattern based on what differential sound pressure field the DMA is expected to respond to and 2 designing the differential beamforming filter so that the resulting beampattern matches the target one. Most efforts in the study of DMAs so far have focused on the second step while choosing one of the limited patterns available in the literature as the target beampattern. Since it governs how the array performs, how to design the target beampattern is an important problem, which this paper addresses. The major contributions of this paper consists of the following four aspects. First, a positive superposition theorem is presented, which shows that the linear combination of effective beampatterns with non-negative coefficients is always an effective beampattern. Second, we propose a general approach to the design of target DMA beampatterns based on the positive superposition theorem. Third, an overview of the classical target beampatterns is provided and discussion is made on how to form effective base patterns. Fourth, we show that the smallest first null of a DMA is $\\pi /2N$ with $N$ being the DMA order, which provides the rule of setting nulls in practice. Finally, with examples, we show that with the use of the alternating-direction-method-of-multipliers algorithm, the proposed approach is able to generate useful DMA target beampatterns.\n"
      }
    ]
  },
  {
    "tid":"226",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Single-channel noise reduction via semi-orthogonal transformations and reduced-rank filtering.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A general framework is developed that combines semi-orthogonal transformation and reduced-rank filtering for noise reduction.Under this new framework, several optimal reduced-rank filters are derived, including the maximum SNR, the Wiener, the tradeoff, and the MVDR filters.Discussions are also provided on how to derive different semi-orthogonal transformations under four estimation criteria, including minimum correlation, minimum MSE, minimum distortion, and minimum residual noise.Simulations are performed and the results show the properties of the deduced optimal reduced-rank filters. This paper investigates the problem of single-channel noise reduction in the time domain. The objective is to find a lower dimensional filter that can yield a noise reduction performance as close as possible to or even better than that obtained by the full-rank solution. This is achieved in three steps. First, we transform the observation signal vector sequence, through a semi-orthogonal matrix, into a sequence of transformed signal vectors with a reduced dimension. Second, a reduced-rank filter is applied to get an estimate of the clean speech in the transformed domain. Third, the estimate of the clean speech in the time domain is obtained by an inverse semi-orthogonal transformation. The focus of this paper is on the derivation of semi-orthogonal transformations under certain estimation criteria in the first step and the design of the reduced-rank optimal filters that can be used in the second step. We show how noise reduction using the principle of rank reduction can be cast as an optimal filtering problem, and how different semi-orthogonal transformations affect the noise reduction performance. Simulations are performed under various conditions to validate the deduced filters for noise reduction."
      }
    ]
  },
  {
    "tid":"227",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Theoretical and Empirical Analysis of Reward Shaping in Reinforcement Learning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Reinforcement learning suffers scalability problems due to the state space explosion and the temporal credit assignment problem. Knowledge-based approaches have received a significant attention in the area. Reward shaping is a particular approach to incorporate domain knowledge into reinforcement learning. Theoretical and empirical analysis of this paper reveals important properties of this principle, especially the influence of the reward type, MDP discount factor, and the way of evaluating the potential function on the performance."
      }
    ]
  },
  {
    "tid":"228",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Method for Resource and Service Discovery in MANETs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The paper presents a novel resource and service discovery mechanism for MANETs using Routing Intelligent Mobile Agents(RIMAs). RIMAs periodically collect routing, resource and service availability information and index the same in appropriate RIMA nodes. Every mobile node is close to atleast one RIMA node. The mobile node running an application in need of a resource or a service dispatches a Discovery agent to its nearest RIMA node. The index available at the RIMA node will guide the Discoverer agent to the resource or service node. Analysis of the method reveals the computational complexity to be of the order of square of number of RIMA nodes. Communication overheads are the product of number of generated agents, agent size and number of hops travelled by an agent. The method has been simulated and tested on MANETs of varying sizes from 50 to 800 nodes. Simulation snapshots and results show the proper working and success of the algorithm."
      }
    ]
  },
  {
    "tid":"229",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Runs based traffic estimator (RATE): a simple, memory efficient scheme for per-flow rate estimation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Per-flow network traffic measurements are needed for effective network traffic management, network performance assessment, and detection of anomalous network events such as incipient DoS attacks. Explicit measurement of per-flow traffic statistics is difficult in backbone networks because tracking the possibly hundreds of thousands of flows needs correspondingly large high-speed memories. To reduce the measurement over- head, many previous papers have proposed the use of random sampling and this is also used in commercial routers (Cisco's NetFlow). Our goal is to develop a new scheme that has very low memory requirements and has quick convergence to within a pre-specified accuracy. We achieve this by use of a novel approach based on sampling two-runs to estimate per-flow traffic. (A flow has a two-run when two consecutive samples belong to the same flow). Sampling two-runs automatically biases the samples towards the larger flows thereby making the estimation of these sources more accurate. This biased sampling leads to significantly smaller memory requirement compared to random sampling schemes. The scheme is very simple to implement and performs extremely well. I. INTRODUCTION In this paper, we address the problem of accurate measure- ment of traffic in a packet network. Measurement of traffic is an important component for traffic management, accounting, detecting DoS attacks, and in traffic engineering (2), (3), (6). The traffic in the network can typically be classified into flows, and measurements are required on a per-flow basis. The definition of flows can be very flexible. Examples are specific application to application traffic characterized by 5-tuples in the IP packet header, all traffic destined toward a a destination network (characterized by a destination address prefix), etc. The standard approach used for measuring traffic is to sample the traffic arriving at the router, keep counts of the traffic arrivals on a per-flow basis and then use this counter to estimate the traffic. This approach has been suggested by the Real-Time Flow Measurement Working Group of the IETF. The main problem with this approach is scalability. If the number of flows are large, then keeping per-flow counts"
      }
    ]
  },
  {
    "tid":"230",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'iOptimize: A software capability for analyzing and optimizing connection-oriented data networks in real time.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper describes a service called iOptimize that analyzes and optimizes service providers' connection-oriented data networks. In these networks, online connection routing is used to set up connections quickly, but the simple path selection scheme and the limited information available for online routing can cause network capacity to be used inefficiently. This may result in the loss or disruption of user data transmissions, leading to a degraded quality of service (QoS) in a network capable of supporting much higher throughput. The objective of iOptimize is to detect and analyze such inefficiencies and to offset them by the occasional rerouting of a selected set of connections, while causing little or no disruption of the existing data services and network operations. By helping service providers support data transport services at the QoS levels desired, iOptimize lets them make the most of their existing network resources and helps them defer capital expenditure. Moreover, by tuning their networks to perform at optimal efficiency, it improves end users' experiences, resulting in lower churn/turnover and reduced maintenance costs. (C) 2005 Lucent Technologies Inc."
      }
    ]
  },
  {
    "tid":"231",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The complexity of welfare maximization in congestion games' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We investigate issues of complexity related to welfare maximization in congestion games. In particular, we provide a full classification of complexity results for the problem of finding a minimum cost solution to a congestion game, under the model of Rosenthal. We consider both network and general congestion games, and we examine several variants of the problem concerning the structure of the game and the properties of its associated cost functions. Many of these problem variants turn out to be NP-hard, and some are hard to approximate to within any finite factor, unless P = NP. We also identify several versions of the problem that are solvable in polynomial time. \u00a9 2011 Wiley Periodicals, Inc. NETWORKS, 2012 \u00a9 2012 Wiley Periodicals, Inc."
      }
    ]
  },
  {
    "tid":"232",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mining \"Hidden phrase\" definitions from the web' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Keyword searching is the most common form of document search on the Web. Many Web publishers manually annotate the META tags and titles of their pages with frequently queried phrases in order to improve their placement and ranking. A \"hidden phrase\" is defined as a phrase that occurs in the META tag of a Web page but not in its body. In this paper we present an algorithm that mines the definitions of hidden phrases from the Web documents. Phrase definitions allow (i) publishers to find relevant phrases with high query frequency, and, (ii) search engines to test if the content of the body of a document matches the phrases. We use co-occurrence clustering and association rule mining algorithms to learn phrase definitions from high-dimensional data sets. We also provide experimental results."
      }
    ]
  },
  {
    "tid":"233",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Modeling sensor detectability with the VANTAGE geometric/sensor modeler' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A G-source, an abstract sensor component representing either a light source or a TV camera, is defined. G-source illumination conditions are investigated to describe the condition under which each G-source illuminates (or observes) surface regions with respect to its illumination/observation directions. Sensor detectability is defined as a function that takes object boundaries and produces a set of points on the object from which the sensor can obtain useful data. It is shown that sensor detectability can be represented by G-source illumination conditions and/or operations between them. The way in which G-sources are combined is described by a sensor-composition tree in which leaf nodes represent G-source illumination conditions and branch nodes represent set operations. Applications of the sensor composition tree to object boundaries are investigated and a set of detectable points is obtained using the VANTAGE geometric modeler. Several 2D structures are proposed to represent these detected points given by VANTAGE. A brief overview of how to use these capabilities in building model-based vision systems is given"
      }
    ]
  },
  {
    "tid":"234",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Music, Heart Rate, and Emotions in the Context of Stimulating Technologies' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The present aim was to explore heart rate responses when stimulating participants with technology primarily aimed at the rehabilitation of older adults. Heart rate responses were measured from 31 participants while they listened to emotionally provoking negative, neutral, and positive musical clips. Ratings of emotional experiences were also collected. The results showed that heart rate responses to negative musical stimuli differed significantly from responses to neutral stimuli. The use of emotion-related physiological responses evoked by stimulating devices offers a possibility to enhance, for example, emotionally stimulating or otherwise therapeutic sessions."
      }
    ]
  },
  {
    "tid":"235",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mesh Generation from Layered Depth Images Using Isosurface Raycasting.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents an approach for the fast generation of meshes from Layered Depth Images (LDI), a representation that is independent of the underlying data structure and widely used in image-based rendering. LDIs can be quickly generated from high-quality, yet computationally expensive isosurface raycasters that are available for a wide range of different types of data. We propose a fast technique to extract meshes from one or several LDIs which can then be rendered for fast, yet high-quality analysis with comparatively low hardware requirements. To further improve quality, we also investigate mesh geometry merging and adaptive refinement, both for triangle and quad meshes. Quality and performance are evaluated using simulation data and analytic functions."
      }
    ]
  },
  {
    "tid":"236",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Bridging the gap between the total and additional test-case prioritization strategies' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In recent years, researchers have intensively investigated various topics in test-case prioritization, which aims to re-order test cases to increase the rate of fault detection during regression testing. The total and additional prioritization strategies, which prioritize based on total numbers of elements covered per test, and numbers of additional (not-yet-covered) elements covered per test, are two widely-adopted generic strategies used for such prioritization. This paper proposes a basic model and an extended model that unify the total strategy and the additional strategy. Our models yield a spectrum of generic strategies ranging between the total and additional strategies, depending on a parameter referred to as the p value. We also propose four heuristics to obtain differentiated p values for different methods under test. We performed an empirical study on 19 versions of four Java programs to explore our results. Our results demonstrate that wide ranges of strategies in our basic and extended models with uniform p values can significantly outperform both the total and additional strategies. In addition, our results also demonstrate that using differentiated p values for both the basic and extended models with method coverage can even outperform the additional strategy using statement coverage."
      }
    ]
  },
  {
    "tid":"237",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The design of an acquisitional query processor for sensor networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices."
      }
    ]
  },
  {
    "tid":"238",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Communication Lower Bounds for Statistical Estimation Problems via a Distributed Data Processing Inequality.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We study the tradeoff between the statistical error and communication cost of distributed statistical estimation problems in high dimensions. In the distributed sparse Gaussian mean estimation problem, each of the m machines receives n data points from a d-dimensional Gaussian distribution with unknown mean \u03b8 which is promised to be k-sparse. The machines communicate by message passing and aim to estimate the mean \u03b8. We provide a tight (up to logarithmic factors) tradeoff between the estimation error and the number of bits communicated between the machines. This directly leads to a lower bound for the distributed sparse linear regression problem: to achieve the statistical minimax error, the total communication is at least \u03a9(min{n,d}m), where n is the number of observations that each machine receives and d is the ambient dimension. These lower results improve upon Shamir (NIPS'14) and Steinhardt-Duchi (COLT'15) by allowing multi-round iterative communication model. We also give the first optimal simultaneous protocol in the dense case for mean estimation. As our main technique, we prove a distributed data processing inequality, as a generalization of usual data processing inequalities, which might be of independent interest and useful for other problems."
      }
    ]
  },
  {
    "tid":"239",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Towards understanding the symmetry of human ears: A biometric perspective' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, an analysis of the symmetry of human ears is presented. Such an analysis is essential in order to understand the possibility of matching the left and right ears of an individual, or to reconstruct portions of the ear that may be occluded in a surveillance video. Ear symmetry is assessed geometrically using symmetry operators and Iannarelli's measurements, where the contribution of individual ear regions to the overall symmetry of the ear is studied. Next, to assess the ear symmetry (or asymmetry) from a biome\u0301trie recognition system perspective, several experiments were conducted on the WVU Ear Database. Our experiments suggest the existence of some degree of symmetry in the human ears that can perhaps be systematically exploited in the design of commercial ear recognition systems. At the same time, the degree of asymmetry it offers may be used in designing effective fusion schemes that combine the face information with the two ears."
      }
    ]
  },
  {
    "tid":"240",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Towards detecting anomalous user behavior in online social networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Users increasingly rely on crowdsourced information, such as reviews on Yelp and Amazon, and liked posts and ads on Facebook. This has led to a market for blackhat promotion techniques via fake (e.g., Sybil) and compromised accounts, and collusion networks. Existing approaches to detect such behavior relies mostly on supervised (or semisupervised) learning over known (or hypothesized) attacks. They are unable to detect attacks missed by the operator while labeling, or when the attacker changes strategy. We propose using unsupervised anomaly detection techniques over user behavior to distinguish potentially bad behavior from normal behavior. We present a technique based on Principal Component Analysis (PCA) that models the behavior of normal users accurately and identifies significant deviations from it as anomalous. We experimentally validate that normal user behavior (e.g., categories of Facebook pages liked by a user, rate of like activity, etc.) is contained within a low-dimensional subspace amenable to the PCA technique. We demonstrate the practicality and effectiveness of our approach using extensive ground-truth data from Facebook: we successfully detect diverse attacker strategies--fake, compromised, and colluding Facebook identities--with no a priori labeling while maintaining low false-positive rates. Finally, we apply our approach to detect click-spam in Facebook ads and find that a surprisingly large fraction of clicks are from anomalous users."
      }
    ]
  },
  {
    "tid":"241",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Is SC + ILP = RC?' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Sequential consistency (SC) is the simplest programming interface for shared-memory systems but imposes program order among all memory operations, possibly precluding high performance implementations. Release consistency (RC), however, enables the highest performance implementations but puts the burden on the programmer to specify which memory operations need to be atomic and in program order. This paper shows, for the first time, that SC implementations can perform as well as RC implementations if the hardware provides enough support for speculation. Both SC and RC implementations rely on reordering and overlapping memory operations for high performance. To enforce order when necessary, an RC implementation uses software guarantees, whereas an SC implementation relies on hardware speculation. Our SC implementation, called SC++, closes the performance gap because: (1) the hardware allows not just loads, as some current SC implementations do, but also stores to bypass each other speculatively to hide remote latencies, (2) the hardware provides large speculative state for not just processor, as previously proposed, but also memory to allow out-of-order memory operations, (3) the support for hardware speculation does not add excessive overheads to processor pipeline critical paths, and (4) well-behaved applications incur infrequent rollbacks of speculative execution. Using simulation, we show that SC++ achieves an RC implementation's performance in all the six applications we studied.\n\n"
      }
    ]
  },
  {
    "tid":"242",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Taming the flying cable monster: a topology design and optimization framework for data-center networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Data-center network designers now have many choices for high-bandwidth, multi-path network topologies. Some of these topologies also allow the designer considerable freedom to set parameters (for example, the number of ports on a switch, link bandwidths, or switch-to-switch wiring patterns) at design time. This freedom of choice, however, requires the designer to balance among bandwidth, latency, reliability, parts cost, and other real-world details. Designers need help in exploring this design space, and especially in finding optimal network designs. We describe the specific challenges that designers face, and we present Perseus, a framework for quickly guiding a network designer to a small set of candidate designs. We identify several optimization problems that can be accommodated within this framework, and present solution algorithms for many of these problems."
      }
    ]
  },
  {
    "tid":"243",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The MediaEval 2015 Affective Impact of Movies Task.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper provides a description of the MediaEval 2015 \\Aective Impact of Movies Task\", which is running for the fth year, previously under the name \\Violent Scenes Detection\". In this year\u2019s task, participants are expected to create systems that automatically detect video content that depicts violence, or predict the aective impact that video content will have on viewers. Here we provide insights on the use case, task challenges, data set and ground truth, task run requirements and evaluation metrics."
      }
    ]
  },
  {
    "tid":"244",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Error-resistance and Low-complexity Integer Inverse Discrete Cosine Transform' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a low-complexity multiplication-free integer inverse discrete cosine transform (IDCT) based on data flow structure of improved AAN's fast IDCT is proposed for error drifting resistance of decoder. Two algorithms are used in this integer IDCT improvement. One is common factor extraction which extracts the complicated common factors from transform kernel to scale; the other is two-stage scale which splits a more than 16-bit scale into two less than 16-bit scales. With the two algorithms, high-accuracy integer IDCT is implemented in lower complexity. The experimental results show that the proposed transform exceeds the requirements of IEEE1180-1990 significantly (about 10 times). The results of the proposed IDCT implemented into MPEG-2 and MPEG-4 decoders instead of original 64-bit floating-point IDCT also show that it reduces the error drifting of decoders efficiently."
      }
    ]
  },
  {
    "tid":"245",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'REMIX: Automated Exploration for Interactive Outlier Detection.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Outlier detection is the identification of points in a dataset that do not conform to the norm. Outlier detection is highly sensitive to the choice of the detection algorithm and the feature subspace used by the algorithm. Extracting domain-relevant insights from outliers needs systematic exploration of these choices since diverse outlier sets could lead to complementary insights. This challenge is especially acute in an interactive setting, where the choices must be explored in a time-constrained manner. In this work, we present REMIX, the first system to address the problem of outlier detection in an interactive setting. REMIX uses a novel mixed integer programming (MIP) formulation for automatically selecting and executing a diverse set of outlier detectors within a time limit. This formulation incorporates multiple aspects such as (i) an upper limit on the total execution time of detectors (ii) diversity in the space of algorithms and features, and (iii) meta-learning for evaluating the cost and utility of detectors. REMIX provides two distinct ways for the analyst to consume its results: (i) a partitioning of the detectors explored by REMIX into perspectives through low-rank non-negative matrix factorization; each perspective can be easily visualized as an intuitive heatmap of experiments versus outliers, and (ii) an ensembled set of outliers which combines outlier scores from all detectors. We demonstrate the benefits of REMIX through extensive empirical validation on real-world data."
      }
    ]
  },
  {
    "tid":"246",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'AMPol: Adaptive Messaging Policy' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Interoperability in a large-scale distributed system is challenged by by the diversity of node policies. We introduce AMPol (Adaptive Messaging Policy), a serviceoriented architecture that facilitates policy-aware, end-toend adaptive messaging between multiple parties. AMPol provides services for expressing non-functional QoS policies, finding them, and carrying out system extensions to adapt to them. We implement this approach with a web service middleware that allows parties to use policies for features like attachments, payment, encryption, and signatures. Our implementation demonstrates how AMPol can enhance the function of email messaging by enabling automatic deployment and use of features like cycle exhaustion puzzles, reverse Turing tests and identity based encryption without the need for global deployment or changes to the baseline messaging system."
      }
    ]
  },
  {
    "tid":"247",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Location-Aware Waypoint-Based Routing Protocol for Airborne DTNs in Search and Rescue Scenarios.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose GeoSaW, a delay-tolerant routing protocol for Airborne Networks in Search and Rescue scenarios. The protocol exploits the geographical information of UAVs to make appropriate message forwarding decisions. More precisely, the information about the future UAV's motion path is exploited to select the best UAV carrying the message towards the destination. Simulation results show that the proposed solution outperforms the classic DTN routing protocols in terms of several performance metrics."
      }
    ]
  },
  {
    "tid":"248",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Information-Theoretic Approach to Individual Sequential Data Sanitization.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Fine-grained, personal data has been largely, continuously generated nowadays, such as location check-ins, web histories, physical activities, etc. Those data sequences are typically shared with untrusted parties for data analysis and promotional services. However, the individually-generated sequential data contains behavior patterns and may disclose sensitive information if not properly sanitized. Furthermore, the utility of the released sequence can be adversely affected by sanitization techniques. In this paper, we study the problem of individual sequence data sanitization with minimum utility loss, given user-specified sensitive patterns. We propose a privacy notion based on information theory and sanitize sequence data via generalization. We show the optimization problem is hard and develop two efficient heuristic solutions. Extensive experimental evaluations are conducted on real-world datasets and the results demonstrate the efficiency and effectiveness of our solutions.\n\n"
      }
    ]
  },
  {
    "tid":"249",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Stiffness discrimination with visual and proprioceptive cues' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This study compares the Weber fraction for human perception of stiffness among three conditions: vision, proprioceptive motion feedback, and their combination. To make comparisons between these feedback conditions, a novel haptic device was designed that senses the spring behavior through encoder and force measurements, and implements a controller to render linear virtual springs so that the stimuli displayed haptically could be compared with their visual counterparts. The custom-designed, torque-controlled haptic interface non-invasively controls the availability of proprioceptive motion feedback in unimpaired individuals using a virtual environment. When proprioception is available, the user feels an MCP joint rotation that is proportional to his or her finger force. When proprioception is not available, the actual finger is not allowed to move, but a virtual finger displayed graphically moves in proportion to the user's applied force. Visual feedback is provided and removed by turning on and off this graphical display. Weber fractions were generated from an experiment in which users examined pairs of springs and attempted to identify the spring with higher stiffness. To account for slight trial-to-trial variations in the relationship between force and position in the proprioceptive feedback conditions, our analysis uses measurements of the actual rendered stiffness, rather than the commanded stiffness. Results for 10 users give average Weber fractions of 0.056 for vision, 0.036 for proprioception, and 0.039 for their combination, indicating that proprioception is important for stiffness perception for this experimental setup. The long-term goal of this research is to motivate and develop methods for proprioception feedback to wearers of dexterous upper-limb prostheses."
      }
    ]
  },
  {
    "tid":"250",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Sketch-Based Interface For Classifying And Visualizing Vector Fields' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In flow visualization, field lines are often used to convey both global and local structure and movement of the flow. One challenge is to find and classify the representative field lines. Most existing solutions follow an automatic approach that generates field lines characterizing the flow and arranges these lines into a single picture. In our work, we advocate a user-centric approach to exploring 3D vector fields. Our method allows the user to sketch 2D curves for pattern matching in 2D and field lines clustering in 3D. Specifically, a 3D field line whose view-dependent 2D projection is most similar to the user drawing will be identified and utilized to extract all similar 3D field lines. Furthermore, we employ an automatic clustering method to generate field-line templates for the user to locate sub-fields of interest. This semi-automatic process leverages the user's knowledge about the flow field through intuitive user interaction, resulting in a promising alternative to existing flow visualization solutions. With our sketch-based interface, the user can effectively dissect the flow field and make more structured visualization for analysis or presentation."
      }
    ]
  },
  {
    "tid":"251",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Estimating surface normals from spherical stokes reflectance fields' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we introduce a novel technique for estimating surface normals from the four Stokes polarization parameters of specularly reflected light under a single spherical incident lighting condition that is either unpolarized or circularly polarized. We illustrate the practicality of our technique by estimating surface normals under uncontrolled outdoor illumination from just four observations from a fixed viewpoint."
      }
    ]
  },
  {
    "tid":"252",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Tackling Common Due Window Problem with a Two-Layered Approach.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This work presents a polynomial algorithm to optimize any given job sequence for the Common Due-Window (CDW) Problem. The CDW problem comprises of scheduling and sequencing a set of jobs against a due-window to minimize the total weighted earliness/tardiness penalty. This due-window is defined by the left and right common due-dates. Jobs that finish before (after) the left (right) due-date are termed as early (tardy) jobs. We present an exact polynomial algorithm for optimally scheduling a given fixed job sequence for a single machine with the runtime complexity of O(n), where n is the number of jobs. The linear algorithm and a heuristic based on the V-shaped property are then incorporated with a modified Simulated Annealing (SA) algorithm to obtain the optimal/near-optimal solutions. We carry out computational experiments to demonstrate the utility of our approach over the benchmark instances and previous work on this problem."
      }
    ]
  },
  {
    "tid":"253",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Immigrant schemes for evolutionary algorithms in dynamic environments: Adapting the replacement rate' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"One approach for evolutionary algorithms (EAs) to address dynamic optimization problems (DOPs) is to maintain diversity of the population via introducing immigrants. So far all immigrant schemes developed for EAs have used fixed replacement rates. This paper examines the impact of the replacement rate on the performance of EAs with immigrant schemes in dynamic environments, and proposes a self-adaptive mechanism for EAs with immigrant schemes to address DOPs. Our experimental study showed that the new approach could avoid the tedious work of fine-tuning the parameter and outperformed other immigrant schemes using a fixed replacement rate with traditionally suggested values in most cases."
      }
    ]
  },
  {
    "tid":"254",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'ExpanD: a stereoscopic expanding technique for compound graphs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this work, we present a new technique, called ExpanD, for expanding compound nodes in stereoscopic platforms. In this technique, stereoscopic depth is used to encode the structural relationships in compound graphs using the containment representation, while explicit edges are used to represent the adjacency relations between the graph nodes. Further, stereoscopic depth is used to show the parent-child relations in such a manner that the children level is rendered in a plane closer to the viewer than the parent node. The technique provides a novel interactive operation for expanding or contracting nodes in order to align graph nodes in the 3D space with minimum occlusion. Different visual cues are used together to encode other data aspects, e.g., color to encode node status, shape to encode node type, and stereoscopic depth to encode hierarchical relations between nodes."
      }
    ]
  },
  {
    "tid":"255",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Towards intuitive exploration tools for data visualization in VR' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we present a basic set of intuitive exploration tools for the data visualization in a Virtual Environment on the Responsive Workbench. First, we introduce the Plexipad, a transparent acrylic panel which allows two-handed interaction in combination with a stylus. After a description of various interaction scenarios with these two devices, we present a basic set of interaction tools, which support the user in the process of exploring volumetric datasets. Besides the interaction tools for navigation and selection we present tools that are closely coupled with probing tools. These interactive probing tools are used as input for complex visualization tools and for performing virtual measurements. We illustrate the use of our tools in two applications from different research areas which use volumetric and particle data."
      }
    ]
  },
  {
    "tid":"256",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Trajectory planning for optimal robot catching in real-time' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many real-world tasks require fast planning of highly dynamic movements for their execution in real-time. The success often hinges on quickly finding one of the few plans that can achieve the task at all. A further challenge is to quickly find a plan which optimizes a desired cost. In this paper, we will discuss this problem in the context of catching small flying targets efficiently. This can be formulated as a non-linear optimization problem where the desired trajectory is encoded by an adequate parametric representation. The optimizer generates an energy-optimal trajectory by efficiently using the robot kinematic redundancy while taking into account maximal joint motion, collision avoidance and local minima. To enable the resulting method to work in real-time, examples of the global planner are generalized using nearest neighbour approaches, Support Vector Machines and Gaussian process regression, which are compared in this context. Evaluations indicate that the presented method is highly efficient in complex tasks such as ball-catching."
      }
    ]
  },
  {
    "tid":"257",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Performance and energy efficient cache migrationapproach for thermal management in embedded systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we propose an approach for performance and power aware warm start for the data cache during core level migration events that originate from overheating. We utilize the concept of reuse in the references to eliminate unnecessary information from being migrated. Furthermore, we exploit the temperature predictability to trigger the cache migration slightly before the actual thermal limit to allow sufficient time for the extraction of the reuse information and transfer of it to the destination cache while the execution is resuming normally. The suggested hardware not only is cost efficient but is also programmable so as to maintain flexibility in targeting application particularities. The experimental results we provide confirm the applicability of this approach."
      }
    ]
  },
  {
    "tid":"258",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Beyond bug-finding: sound program analysis for Linux' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"It is time for us to focus on sound analyses for our critical systems software--that is, we must focus on analyses that ensure the absence of defects of particular known types, rather than best-effort bug-finding tools. This paper presents three sample analyses for Linux that are aimed at eliminating bugs relating to type safety, deallocation, and blocking. These analyses rely on lightweight programmer annotations and run-time checks in order to make them practical and scalable. Sound analyses of this sort can check a wide variety of properties and will ultimately yield more reliable code than bug-finding alone."
      }
    ]
  },
  {
    "tid":"259",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Research on distributed intrusion detection system based on multi-living agent.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a novel distributed intrusion detection system based on multi-living agent inspired by the research\n method of multi-living agent complex information system for improving the security of distributed intrusion detection system\n using mobile agent. The previous two-layer distributed intrusion detection system has been expanded into a three-layer structure,\n the new layer of which can monitor the livelihood of each detection agent and utilize the negotiation-coordination mechanism\n to adjust the working way of each agent for protecting the security of system. In this paper, we propose a quantitative analysis\n method of agent livelihood in distributed intrusion detection system and further present the design method of living agent.\n Finally, through the analysis of system working flow and the detection effect of distributed denial of service attack, we\n have shown that the security and stability of the novel distributed intrusion detection system is effectively enhanced and\n the agent can work normally in strict restraint strong countermeasure environment."
      }
    ]
  },
  {
    "tid":"260",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Interactive volume segmentation with differential image foresting transforms.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The absence of object information very often asks for considerable human assistance in medical image segmentation. Many interactive two-dimensional and three-dimensional (3-D) segmentation methods have been proposed, but their response time to user&#39;s actions should be considerably reduced to make them viable from the practical point of view. We circumvent this problem in the framework of the image..."
      }
    ]
  },
  {
    "tid":"261",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Agent warp engine: formula based shape warping for networked applications' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Computer visualization and networking have advanced dramatically. 3D hardware acceleration has reached the point where even low-power handheld computers can render and animate complex 3D graphics efficiently. Unfortunately, end-user computing does not yet provide the necessary tools and conceptual frameworks to let end-users access these technologies and build their own networked interactive 2D and 3D applications such as rich visualizations, animations and simulations. The Agent Warp Engine (AWE) is a formula-based shape-warping framework that combines end-user visualization and end-user networking. AWE is a spreadsheet-inspired framework based on Web sharable variables. To build visualizations, users define these variables, relate them through equations and connect them to 2D and 3D shapes. In addition to basic shape control such as rotation, size, and location, AWE enables the creation of rich shape warping visualizations. We motivate the AWE approach with the Mr. Vetro human physiology simulation supporting collaborative learning through networked handheld computers."
      }
    ]
  },
  {
    "tid":"262",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Assessment of navigation cues with proximal force sensing during endovascular catheterization.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Despite increased use of robotic catheter navigation systems for endovascular intervention procedures, current master-slave platforms have not yet taken into account dexterous manipulation skill used in traditional catheterization procedures. Information on tool forces applied by operators is often limited. A novel force/torque sensor is developed in this paper to obtain behavioural data across different experience levels and identify underlying factors that affect overall operator performance. The miniature device can be attached to any part of the proximal end of the catheter, together with a position sensor attached to the catheter tip, for relating tool forces to catheter dynamics and overall performance. The results show clear differences in manipulation skills between experience groups, thus providing insights into different patterns and range of forces applied during routine endovascular procedures. They also provide important design specifications for ergonomically optimized catheter manipulation platforms with added haptic feedback while maintaining natural skills of the operators."
      }
    ]
  },
  {
    "tid":"263",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Hydra: a scalable proteomic search engine which utilizes the Hadoop distributed computing framework.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"For shotgun mass spectrometry based proteomics the most computationally expensive step is in matching the spectra against an increasingly large database of sequences and their post-translational modifications with known masses. Each mass spectrometer can generate data at an astonishingly high rate, and the scope of what is searched for is continually increasing. Therefore solutions for improving our ability to perform these searches are needed.We present a sequence database search engine that is specifically designed to run efficiently on the Hadoop MapReduce distributed computing framework. The search engine implements the K-score algorithm, generating comparable output for the same input files as the original implementation. The scalability of the system is shown, and the architecture required for the development of such distributed processing is discussed.The software is scalable in its ability to handle a large peptide database, numerous modifications and large numbers of spectra. Performance scales with the number of processors in the cluster, allowing throughput to expand with the available resources."
      }
    ]
  },
  {
    "tid":"264",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Solving Problems with Unknown Solution Length at (Almost) No Extra Cost' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Most research in the theory of evolutionary computation assumes that the problem at hand has a fixed problem size. This assumption does not always apply to real-world optimization challenges, where the length of an optimal solution may be unknown a priori. Following up on previous work of Cathabard, Lehre, and Yao [FOGA 2011] we analyze variants of the (1+1) evolutionary algorithm for problems with unknown solution length. For their setting, in which the solution length is sampled from a geometric distribution, we provide mutation rates that yield an expected optimization time that is of the same order as that of the (1+1) EA knowing the solution length. We then show that almost the same run times can be achieved even if no a priori information on the solution length is available. Finally, we provide mutation rates suitable for settings in which neither the solution length nor the positions of the relevant bits are known. Again we obtain almost optimal run times for the OneMax and LeadingOnes test functions, thus solving an open problem from Cathabard et al."
      }
    ]
  },
  {
    "tid":"265",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Duality, Rigidity and Planar Parallax' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We investigate the geometry of two views of seven points,four of which are coplanar, and the geometry of three views of six points,four of which are coplanar. We prove that the two are dual, and that the fundamental geometric constraints in each case are encapsulated by aplanar homology. The work unifies a number of previously diverse resultsrelated to planar parallax, duality and planar homologies.In addition, we make a number of practical contributions, including formulae for computing ... "
      }
    ]
  },
  {
    "tid":"266",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Investigations into the Effect of Multiobjectivization in Protein Structure Prediction' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Physics-based potential energy functions used in protein structure prediction are composed of several energy terms combined in a weighted sum. `Multiobjectivization' -- splitting up the energy function into its components and optimizing the components as a vector using multiobjective methods -- may have beneficial effects for tackling these difficult problems. In this paper we investigate the hypotheses that multiobjectivization can (i) reduce the number of local optima in the landscapes, as seen by hillclimbers, and (ii) equalize the influence of different energy components that range over vastly different energy scales and hence usually swamp each other's search gradients. The investigations use models of two real molecules, the alanine dipeptide and Metenkephalin under the Amber99 energy function, and consider hillclimbers with a range of mutation step sizes. Our findings support the hypotheses and also indicate that multiobjectivization is competitive with alternative methods of escaping local optima."
      }
    ]
  },
  {
    "tid":"267",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Equilibrium Strategies for Task Allocation in Dynamic Multi-Agent Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We address a model of self interested agents competing to perform tasks. The agents are situated in an uncertain environment while dif-ferent tasks dynamically arrive from a central manager. The agents differ in their capabilities to perform a task under different world states. The tasks are allocated according to a pre-defined protocol set by a central manager such as a government, a municipality, a com-pany. The central manager lacks the required resources to perform the tasks by itself. The protocol defines the rules for selecting a per-former for a task and the appropriate payment to this performer. The goal of the central manager is to maximize his expected utility, de-fined as a function of the number of tasks being performed and the total payment. Previous models concerning cooperative agents aim-ing for a joint goal are not applicable in such environments, since self interested agents have a motivation to deviate from the joint alloca-tion strategy, in order to increase their own benefits. Given the allo-cation protocol set by the central manager, a stable solution is a set of strategies derived from an equilibrium where no agent can benefit from changing its strategy given the other agents'strategies. Hence, a major challenge in task allocation process for self-interested agents in such environments is to identify the agents'equilibrium strategies, for a given protocol and environmental settings. We suggest a methodology for calculating the agents'equilibrium strategies. Specifically, we focus on a protocol in which, upon arrival of a new task, the central manager starts a Vickrey auction, and the agent who bids the lowest cost wins. In our domains, the number of competing agents is relatively small, and their overall capabilities can be estimated with some probability. Thus, an agent's strategy must consider the long term strategies of the other agents in the en-vironment as part of its own bid strategy determination. An example of such an application can be found in an environment where self interested servers, with different configurations and changing loads, compete for the execution of jobs arriving from an external source. The servers set their strategies on-the-fly, according to their current information of the world sate, and their evaluation of their competing agents'capabilities. The main difference from e-commerce domains is that in e-commerce the rate of new agents entering the environment is relatively high so it is unfeasible to consider the modulation of competitors'future strategies when setting an agent's bid (e. g. [2]). The concept of task allocation in a competitive environment is dis-cussed in several works (e. g., [1, 3]). The main focus of these works is on the commitments and the communication problems that emerge "
      }
    ]
  },
  {
    "tid":"268",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Beyond \"Web Of Trust\": Enabling P2p E-Commerce*' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The huge success of eBay has proven the demand for customer-to-customer (C2C) electronic commerce. eBay is a centralized infrastructure with all its scalability problems (network bandwidth, server load, availability, etc.). In this paper we argue that C2C e-commerce is an application domain that maps naturally onto the emergent field of P2P systems simply by its underlying interaction model of customers, i.e., peers. This offers the opportunity to take P2P systems beyond mere file sharing systems into interesting new application domains. The long-term goal would be to design a fully, functional decentralized system which resembles eBay without eBay's dedicated, centralized infrastructure. Since security (authenticity, non-repudiation, trust, etc.) is key to any e-commerce infrastructure, our envisioned P2P e-commerce platform has to address this adequately. As the first step in this direction we present an approach for a completely decentralized P2P public key infrastructure (PKI) which can serve as the basis for higher-level security service. In contrast to other systems in this area, such as PGP which uses a \"web of trust\" concept, we use a statistical approach which allows us to provide an analytical model with provable guarantees, and quantify the behavior and specific properties of the PKI. To justify our claims we provide a first-order analysis and discuss its resilience against various known threats and attack scenarios. In support of our belief that C2C E-commerce is one of the potential killer applications of the emerging structured P2P systems, we provide a layered model for P2P E-commerce, demonstrating the dependencies of various security related issues that can be built on top of a decentralized PKI."
      }
    ]
  },
  {
    "tid":"269",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Design Strategies for Building Mobile Crowdsourcing Services' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we present an overview of three community-based mobile crowd sourcing services that we have developed as case studies. We then extract four lessons learned from our experiences to show that motivating people is an important factor in designing mobile crowd sourcing service. The extracted lessons are essential to successful design strategies for developing future crowd sourcing services."
      }
    ]
  },
  {
    "tid":"270",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Persistent Bloom Filter: Membership Testing for the Entire History.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Membership testing is the problem of testing whether an element is in a set of elements. Performing the test exactly is expensive space-wise, requiring the storage of all elements in a set. In many applications, an approximate testing that can be done quickly using small space is often desired. Bloom filter (BF) was designed and has witnessed great success across numerous application domains. But there is no compact structure that supports set membership testing for temporal queries, e.g., has person A visited a web server between 9:30am and 9:40am? And has the same person visited the web server again between 9:45am and 9:50am? It is possible to support such \"temporal membership testing\" using a BF, but we will show that this is fairly expensive. To that end, this paper designs persistent bloom filter (PBF), a novel data structure for temporal membership testing with compact space.\n\n"
      }
    ]
  },
  {
    "tid":"271",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Online sequential reduced kernel extreme learning machine.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we present an Online Sequential Reduced Kernel Extreme Learning Machine (OS-RKELM). In OS-RKELM, only a small part of the instances in the original training samples is employed for training the kernel neurons, while the output weights are attained analytically. Similar to the Online Sequential Extreme Learning Machine (OS-ELM), OS-RKELM learns data samples in a chunk-by-chunk or one-by-one mode and does not require an archival of the data sample once it has been learned. OS-RKELM also contains few control parameters, thus avoiding the need for cumbersome fine-tuning of the algorithm. OS-RKELM supports a widespread types of kernels as hidden neurons and is capable of addressing the singular problem that arises when the initial training samples are smaller than the neuron size. A comprehensive performance evaluation of the OS-RKELM against other state-of-the-art sequential learning algorithms, including OS-ELM, Large-scale Active Support Vector Machine (LASVM) and Budgeted Stochastic Gradient Descent Support Vector Machine (BSGD) using popular time series, regression and classification benchmarks have been conducted. Experimental results obtained indicate that the proposed OS-RKELM showcases improved prediction accuracy and efficiency over the OS-ELM, LASVM and BSGD in many cases."
      }
    ]
  },
  {
    "tid":"272",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Distributed Dynamic Mobility Management Scheme Designed for Flat IP Architectures' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Current mobility management approaches are highly centralized and hierarchical. They make use of centralized anchors located in core networks, responsible for forwarding traffic to/from mobile nodes' location. Emerging networks standards developed for new broadband cellular radio interfaces are use flat architecture where control functions are deployed in network edges such as evolved base stations. In order to better fit to such distributed logic while optimizing handover efficiency and minimizing traffic encapsulation, we propose a new scheme designed for mobility management in flat architectures. Our scheme dynamically anchors mobile nodes' traffic in access nodes, depending on their location when sessions are set up. A first estimation shows that we can obtain low handover delays, promising for future adoption and development of our distributed and dynamic approach."
      }
    ]
  },
  {
    "tid":"273",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Truthful Online Auction Toward Maximized Instance Utilization in the Cloud.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Although infrastructure as a service (IaaS) users are busy scaling up/out their cloud instances to meet the ever-increasing demands, the dynamics of their demands, as well as the coarse-grained billing options offered by leading cloud providers, have led to substantial instance underutilization in both temporal and spatial domains. This paper systematically examines an instance subletting service,..."
      }
    ]
  },
  {
    "tid":"274",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Sharing small optical buffers between real-time and TCP traffic' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Buffers in emerging optical packet routers are expensive resources, and it is expected that they would be able to store at most a few tens of KiloBytes of data in the optical domain. When TCP and real-time (UDP) traffic multiplex at an optical router with such small buffers (less than 50 KB), we recently showed that UDP packet loss can increase with increasing buffer size. This anomalous loss behaviour can negatively impact the investment made in larger buffers and degrade quality of service. In this paper we explore if this anomalous behaviour can be alleviated by dedicating (i.e., pre-allocating) buffers to UDP traffic. Our contributions within this context are two fold. First, we show using extensive simulations that there would seem to be a critical buffer size above which UDP benefits with dedicated buffers that protect it from the aggressive nature of TCP. However, for smaller buffers that are below this critical value, UDP can benefit by time-sharing the buffers with TCP. Second, we develop a simple linear model that quantitatively captures the combined utility of TCP and real-time traffic for shared and dedicated buffers, and propose a method to optimise the buffer split ratio with the objective of maximising the overall network utility. Our study equips designers of optical packet switched networks with quantitative tools to tune their buffer allocation strategies subject to various system parameters such as the ratio of traffic mix and relative weights associated with TCP and UDP traffic."
      }
    ]
  },
  {
    "tid":"275",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Audio-Visual processing in meetings: seven questions and current AMI answers' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The project Augmented Multi-party Interaction (AMI) is concerned with the development of meeting browsers and remote meeting assistants for instrumented meeting rooms \u2013 and the required component technologies R&D themes: group dynamics, audio, visual, and multimodal processing, content abstraction, and human-computer interaction. The audio-visual processing workpackage within AMI addresses the automatic recognition from audio, video, and combined audio-video streams, that have been recorded during meetings. In this article we describe the progress that has been made in the first two years of the project. We show how the large problem of audio-visual processing in meetings can be split into seven questions, like \u201cWho is acting during the meeting?\u201d. We then show which algorithms and methods have been developed and evaluated for the automatic answering of these questions."
      }
    ]
  },
  {
    "tid":"276",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Developing Ubiquitous Multi-touch Sensing and Displaying Systems with Vision-Based Finger Detection and Event Identification Techniques' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This study presents efficient vision-based finger detection, tracking, and event identification techniques, as well as a low-cost hardware framework for multi-touch sensing and display applications. A fast bright-blob segmentation process based on automatic multilevel histogram thresholding is performed to extract pixels of touch blobs from the captured image sequences obtained from the scattered infrared lights by the video camera. Given the touch blobs extracted from each of the captured frames, a blob tracking and event recognition process is then conducted to analyze the spatial and temporal information of these touch blobs from consecutive frames and determine the possible touch events issued by users. This process also refines the detection results and corrects for errors and occlusions caused by noise and errors during the blob extraction processes. Our proposed blob tracking and touch event recognition process includes two phases. First, the phase of blob tracking associates the motion correspondence of blobs in succeeding frames by analyzing their spatial and temporal features. Then the phase of touch event recognition process can identify meaningful touch events activated by users from the motion information of touch blobs. Experimental results demonstrate that the proposed vision-based finger detection, tracking, and event identification system is feasible and effective for multi-touch sensing applications in various operational environments and conditions."
      }
    ]
  },
  {
    "tid":"277",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An asymmetric communications platform for knowledge sharing with low-end mobile phones' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present Awaaz.De (\"give voice\"), a social platform for communities to access and share knowledge using low-end mobile phones. Awaaz.De features a configurable mobile voice application organized into asynchronous voice mes-sage boards. For poor, remote and marginal communities, the voice-touchtone interface addresses the constraints of low literacy, language diversity, and affordability of only basic mobile devices. Voice content also presents a low barrier to content authoring, encouraging otherwise disconnected communities to actively participate in knowledge exchange. Awaaz.De includes a web-based administration interface for Internet-connected community managers to moderate, annotate, categorize, route, and narrow-cast voice messages. In this paper we describe the platform's design, implementation, and future directions."
      }
    ]
  },
  {
    "tid":"278",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'OmniVoice: a mobile voice solution for small-scale enterprises' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We consider the problem of providing mobility support for Voice-over-IP (VoIP) traffic in small-scale enterprises. There is considerable interest in providing on-the-go support for VoIP through the use of WiFi-enabled smart phones. However, existing solutions either do not support client mobility or require client modifications, making them difficult to deploy in practice. In this paper, we present OmniVoice, an 802.11 compliant solution that supports mobility for VoIP traffic without any client modifications. To effectively support such traffic, OmniVoice eliminates client handoff delays and manages interference from non-VoIP background traffic. It achieves this by using (a) a single-channel WLAN design and (b) a lightweight central controller for scheduling non-interfering AP-to-client transmissions and dynamically associating clients. The controller minimizes interference with the help of an interference map that models potential exposed and hidden terminal conflicts in the WLAN, while allowing for the serialization of transmissions that would otherwise compete for medium access. We have implemented and extensively evaluated OmniVoice on a 40 node wireless testbed. OmniVoice meets the QoS requirements for VoIP in all operating scenarios and is unaffected by interference from non-VoIP traffic. In particular, OmniVoice provides 100% greater throughput and 130% greater uninterrupted connectivity time to mobile VoIP users as compared to an off-the-shelf 802.11 solution."
      }
    ]
  },
  {
    "tid":"279",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient computation of causal behavioural profiles using structural decomposition' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Identification of behavioural contradictions is an important aspect of software engineering, in particular for checking the consistency between a business process model used as system specification and a corresponding workflow model used as implementation. In this paper, we propose causal behavioural profiles as the basis for a consistency notion, which capture essential behavioural information, such as order, exclusiveness, and causality between pairs of activities. Existing notions of behavioural equivalence, such as bisimulation and trace equivalence, might also be applied as consistency notions. Still, they are exponential in computation. Our novel concept of causal behavioural profiles provides a weaker behavioural consistency notion that can be computed efficiently using structural decomposition techniques for sound free-choice workflow systems if unstructured net fragments are acyclic or can be traced back to S- or T-nets."
      }
    ]
  },
  {
    "tid":"280",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Sensor Allocation in Diverse Environments' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"\n Sensor coverage varies with location due to factors such as weather, terrain, and obstacles. If a field can be partitioned\n into zones of homogeneous sensing areas, then coverage by a random deployment of sensors can be optimized by controlling the\n number of sensors deployed in each zone. We derive expressions to directly calculate the optimal sensor partition in runtime\n asymptotically equal to the number of zones. We further derive expressions to determine the minimum sensor count required\n to achieve a specific coverage threshold. We bound the maximum increase in coverage over a strategy oblivious to differences\n in sensing areas, which our results show is no greater than 13% for a field with two zones. While the analytical solutions\n assume that each zone is covered independently, we allow sensors to affect neighboring zones in simulations. Nevertheless,\n the simulations support the optimality of our solution.\n \n "
      }
    ]
  },
  {
    "tid":"281",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Honey Encryption: Security Beyond the Brute-Force Bound.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We introduce honey encryption (HE), a simple, general approach to encrypting messages using low min-entropy keys such as passwords. HE is designed to produce a ciphertext which, when decrypted with any of a number of incorrect keys, yields plausible-looking but bogus plaintexts called honey messages. A key benefit of HE is that it provides security in cases where too little entropy is available to withstand brute-force attacks that try every key; in this sense, HE provides security beyond conventional brute-force bounds. HE can also provide a hedge against partial disclosure of high min-entropy keys. HE significantly improves security in a number of practical settings. To showcase this improvement, we build concrete HE schemes for password-based encryption of RSA secret keys and credit card numbers. The key challenges are development of appropriate instances of a new type of randomized message encoding scheme called a distribution-transforming encoder (DTE), and analyses of the expected maximum loading of bins in various kinds of balls-and-bins games."
      }
    ]
  },
  {
    "tid":"282",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Delay Estimation in the Presence of Timing Noise' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We consider the problem of delay estimation in the presence of timing noise. We introduce an iterative algorithm with superior performance compared to the traditional method of using only cross-correlation. This method can exploit statistical knowledge of the timing noise such as loop bandwidth, giving further improvement in performance."
      }
    ]
  },
  {
    "tid":"283",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Transactional actors: communication in transactions.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Developers often require different concurrency models to fit the various concurrency needs of the different parts of their applications. Many programming languages, such as Clojure, Scala, and Haskell, cater to this need by incorporating different concurrency models. It has been shown that, in practice, developers often combine these concurrency models. However, they are often combined in an ad hoc way and the semantics of the combination is not always well-defined. The starting hypothesis of this paper is that different concurrency models need to be carefully integrated such that the properties of each individual model are still maintained.     This paper proposes one such combination, namely the combination of the actor model and software transactional memory. In this paper we show that, while both individual models offer strong safety guarantees, these guarantees are no longer valid when they are combined. The main contribution of this paper is a novel hybrid concurrency model called transactional actors that combines both models while preserving their guarantees. This paper also presents an implementation in Clojure and an experimental evaluation of the performance of the transactional actor model."
      }
    ]
  },
  {
    "tid":"284",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Secrets Of Optical Flow Estimation And Their Principles' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The accuracy of optical flow estimation algorithms has been improving steadily as evidenced by results on the Middlebury optical flow benchmark. The typical formulation, however, has changed little since the work of Horn and Schunck. We attempt to uncover what has made recent advances possible through a thorough analysis of how the objective function, the optimization method, and modern implementation practices influence accuracy. We discover that \"classical\" flow formulations perform surprisingly well when combined with modern optimization and implementation techniques. Moreover, we find that while median filtering of intermediate flow fields during optimization is a key to recent performance gains, it leads to higher energy solutions. To understand the principles behind this phenomenon, we derive a new objective that formalizes the median filtering heuristic. This objective includes a non-local term that robustly integrates flow estimates over large spatial neighborhoods. By modifying this new term to include information about flow and image boundaries we develop a method that ranks at the top of the Middlebury benchmark."
      }
    ]
  },
  {
    "tid":"285",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Double-Layered Hybrid Neural Network Approach for Solving Mixed Integer Quadratic Bilevel Problems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we build a double-layered hybrid neural network method to solve mixed integer quadratic bilevel programming problems. Bilevel programming problems arise when one optimization problem, the upper problem, is constrained by another optimization, the lower problem. In this paper, mixed integer quadratic bilevel programming problem is transformed into a double-layered hybrid neural network. We propose an efficient method for solving bilevel programming problems which employs a double-layered hybrid neural network. A two-layered neural network is formulate by comprising a Hopfield network, genetic algorithm, and a Boltzmann machine in order to effectively and efficiently select the limited number of units from those available. The Hopfield network and genetic algorithm are employed in the upper layer to select the limited number of units, and the Boltzmann machine is employed in the lower layer to decide the optimal solution/units from the limited number of units selected by the upper layer. The proposed method leads the mixed integer quadratic bilevel programming problem to a global optimal solution. To illustrate this approach, several numerical examples are solved and compared."
      }
    ]
  },
  {
    "tid":"286",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Evaluating Trust in a Public Key Certification Authority' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With the growth of many different public key infrastructures on the Internet, relying parties have the difficult task of deciding whether the sender of digitally signed message is really who the public key certificate says they are. We have built an expert system that calculates the amount of trust, or trust quotient, that one can place in the name to public key binding in a certificate. The structure of the expert system is based on the CPS framework of Chokhani and Ford (RFC 2527), whilst the relative importance of the various factors that comprise the trust quotient, were determined by interviewing PKI experts from around the globe. This paper discusses the knowledge analysis strategy employed to collect this expert information and how we used it to develop the KBS. The analysis of the results of the interviews are also presented, and they can be summarised succinctly as \u201cthere are some factors concerning trust in a PKI which nearly all experts agree upon, and there are other factors in which there is very little agreement at all\u201d. The importance of identifying contextual factors when building a knowledge base is very important. In many cases, a disagreement between experts, as shown by a bimodal split in importance, was traced to differences in context and we show how this can be a source of new knowledge."
      }
    ]
  },
  {
    "tid":"287",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'SICA: subjectively interesting component analysis.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The information in high-dimensional datasets is often too complex for human users to perceive directly. Hence, it may be helpful to use dimensionality reduction methods to construct lower dimensional representations that can be visualized. The natural question that arises is  We study this question from an information-theoretic perspective and introduce a new method for linear dimensionality reduction. The obtained model that quantifies the informativeness also allows us to flexibly account for prior knowledge a user may have about the data. This enables us to provide representations that are . We title the method Subjectively Interesting Component Analysis (SICA) and expect it is mainly useful for iterative data mining. SICA is based on a model of a user\u2019s belief state about the data. This belief state is used to search for surprising views. The initial state is chosen by the user (it may be empty up to the data format) and is updated automatically as the analysis progresses. We study several types of prior beliefs: if a user only knows the scale of the data, SICA yields the same cost function as Principal Component Analysis (PCA), while if a user expects the data to have outliers, we obtain a variant that we term -PCA. Finally, scientifically more interesting variants are obtained when a user has more complicated beliefs, such as knowledge about similarities between data points. The experiments suggest that SICA enables users to find subjectively more interesting representations."
      }
    ]
  },
  {
    "tid":"288",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Method Construction By Goal Analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Method engineering proposes the construction of methodologies by selecting method fragments from a repository and assembling them in an appropriate way. However, the rules by which the \"optimal\" method fragments are chosen are not clear, and such chores are usually done manually by an expert. This paper presents a goal analysis technique for the selection of the optimal method fragments from a repository, using backward reasoning to obtain the set of fragments that satisfy the desired goals with minimum effort. By using this technique, a methodologist can determine the goals that the organisation wants the methodology to satisfy, and then, preferably, rely on automated tools for the selection of the optimal solution."
      }
    ]
  },
  {
    "tid":"289",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Principal component based diffeomorphic surface mapping.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present a new diffeomorphic surface mapping algorithm under the framework of large deformation diffeomorphic metric mapping (LDDMM). Unlike existing LDDMM approaches, this new algorithm reduces the complexity of the estimation of diffeomorphic transformations by incorporating a shape prior in which a nonlinear diffeomorphic shape space is represented by a linear space of initial momenta of diff..."
      }
    ]
  },
  {
    "tid":"290",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Simple Countermeasures to Mitigate the Effect of Pollution Attack in Network Coding-Based Peer-to-Peer Live Streaming' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Network coding (NC)-based peer-to-peer (P2P) streaming represents an effective solution to aggregate user capacities and to increase system throughput in live multimedia streaming. Nonetheless, such systems are vulnerable to pollution attacks where a handful of malicious peers can disrupt the communication by transmitting just a few bogus packets which are then recombined and relayed by unaware honest nodes, further spreading the pollution over the network. Whereas previous research focused on malicious nodes identification schemes and pollution-resilient coding, in this paper we show pollution countermeasures which make a standard NC scheme resilient to pollution attacks. Thanks to a simple yet effective analytical model of a reference node collecting packets by malicious and honest neighbors, we demonstrate that: i) packets received earlier are less likely to be polluted, and ii) short generations increase the likelihood to recover a clean generation. Therefore, we propose a recombination scheme where nodes draw packets to be recombined according to their age in the input queue, paired with a decoding scheme able to detect the reception of polluted packets early in the decoding process and short generations. The effectiveness of our approach is experimentally evaluated in a real system we developed and deployed on hundreds to thousands of peers. Experimental evidence shows that, thanks to our simple countermeasures, the effect of a pollution attack is almost canceled and the video quality experienced by the peers is comparable to pre-attack levels."
      }
    ]
  },
  {
    "tid":"291",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Generalised Kernel Sets for Inverse Entailment' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The task of inverting logical entailment is of central importance to the disciplines of Abductive and Inductive Logic Programming (ALP & ILP). Bottom Generalisation (BG) is a widely applied approach for Inverse Entailment (IE), but is limited to deriving single clauses from a hypothesis space restricted by Plotkin's notion of C-derivation. Moreover, known practical applications of BG are confined to Horn clause logic. Recently, a hybrid ALP-ILP proof procedure, called HAIL, was shown to generalise existing BG techniques by deriving multiple clauses in response to a single example, and constructing hypotheses outside the semantics of BG. The HAIL proof procedure is based on a new semantics, called Kernel Set Subsumption (KSS), which was shown to be a sound generalisation of BG. But so far KSS is defined only for Horn clauses. This paper extends the semantics of KSS from Horn clause logic to general clausal logic, where it is shown to remain a sound extension of BG. A generalisation of the C-derivation, called a K*-derivation, is introduced and shown to provide a sound and complete characterisation of KSS. Finally, the K*-derivation is used to provide a systematic comparison of existing proof procedures based on IE."
      }
    ]
  },
  {
    "tid":"292",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Least-Squares Conditional Density Estimation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Estimating the conditional mean of an input-output relation is the goal of regression. However, regression analysis is not sufficiently informative if the conditional distribution has multi-modality, is highly asymmetric, or contains heteroscedastic noise. In such scenarios, estimating the conditional distribution itself would be more useful. In this paper. we propose a novel method of conditional density estimation that is suitable for multi-dimensional continuous variables. The basic idea of the proposed method is to express the conditional density in terms of the density ratio and the ratio is directly estimated without going through density estimation. Experiments using benchmark and robot transition datasets illustrate the usefulness of the proposed approach."
      }
    ]
  },
  {
    "tid":"293",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Using Announce-Listen With Global Events To Develop Distributed Control Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We specify an abstract model for dynamic distributed control systems in which the component objects make local decisions based on system-wide constraints and approximate global state. We focus an the issue of distributed resource management, exploring a solution that is both compositional and scalable because it builds global events into the Java infrastructure by exploiting its multicast facilities. (C) 1998 John Wiley & Sons, Ltd."
      }
    ]
  },
  {
    "tid":"294",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Polyglots: crossing origins by crossing formats' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In a heterogeneous system like the web, information is exchanged between components in versatile formats. A new breed of attacks is on the rise that exploit the mismatch between the expected and provided content. This paper focuses on the root cause of a large class of attacks: polyglots. A polyglot is a program that is valid in multiple programming languages. Polyglots allow multiple interpretation of the content, providing a new space of attack vectors. We characterize what constitutes a dangerous format in the web setting and identify particularly dangerous formats, with PDF as the prime example. We demonstrate that polyglot-based attacks on the web open up for insecure communication across Internet origins. The paper presents novel attack vectors that infiltrate the trusted origin by syntax injection across multiple languages and by content smuggling of malicious payload that appears formatted as benign content. The attacks lead to both cross-domain leakage and cross-site request forgery. We perform a systematic study of PDF-based injection and content smuggling attacks. We evaluate the current practice in client/server content filtering and PDF readers for polyglot-based attacks, and report on vulnerabilities in the top 100 Alexa web sites. We identify five web sites to be vulnerable to syntax injection attacks. Further, we have found two major enterprise cloud storage services to be susceptible to content smuggling attacks. Our recommendations for protective measures on server side, in browsers, and in content interpreters (in particular, PDF readers) show how to mitigate the attacks."
      }
    ]
  },
  {
    "tid":"295",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Declarative representation of programming access to ontologies' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Using ontologies in software applications is a challenging task due to the chasm between the logics-based world of ontologies and the object-oriented world of software applications. The logics-based representation emphasizes the meaning of concepts and properties, i.e., their semantics. The modeler in the object-oriented paradigm also takes into account the pragmatics, i.e., how the classes are used, by whom, and why. To enable a comprehensive use of logics-based representations in object-oriented software systems, a seamless integration of the two paradigms is needed. However, the pragmatic issues of using logic-based knowledge in object-oriented software applications has yet not been considered sufficiently. Rather, the pragmatic issues that arise in using an ontology, e.g., which classes to instantiate in which order, remains a task to be carefully considered by the application developer. In this paper, we present a declarative representation for designing and applying programming access to ontologies. Based on this declarative representation, we have build OntoMDE, a model-driven engineering toolkit that we have applied to several example ontologies with different Characteristics."
      }
    ]
  },
  {
    "tid":"296",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Backward-Forward Combined Convolutive Blind Source Separation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many blind source separation algorithms only try to find the estimation of original signals and the de-mixing system. However, finding the mixing system is also an important task, which is a representation for the recording environment. Direct inversing the mixing system will introduce many noises. We proposed a correlation based backward-forward combined algorithm to do blind source separation. In the proposed algorithm, an alternative optimization strategy is applied. The estimation of mixing/de-mixing system is used as an initial guess to optimize the de-mixing/mixing system iteratively. This alternative optimization algorithm can obtain better estimation for mixing and de-mixing system at the same time. In addition, it can reduce the chance to be tracked into a local minimal. As a result, we can get a better estimation for the sources. Good experimental results were obtained on both simulated and real signals."
      }
    ]
  },
  {
    "tid":"297",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Hierarchy-Regularized Latent Semantic Indexing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Organizing textual documents into a hierarchical taxonomy is a common practice in knowledge management. Beside textual features, the hierarchical structure of directories reflect additional and important knowledge annotated by experts. It is generally desired to incorporate this information into text mining processes. In this paper, we propose hierarchy-regularized latent semantic indexing, which encodes the hierarchy into a similarity graph of documents and then formulates an optimization problem mapping each document into a low dimensional vector space. The new feature space preserves the intrinsic structure of the original taxonomy and thus provides a meaningful basis for various learning tasks like visualization and classification. Our approach employs the information about class proximity and class specificity, and can naturally cope with multi-labeled documents. Our empirical studies show very encouraging results on two real-world data sets, the new Reuters (RCV1) benchmark and the Swissprot protein database."
      }
    ]
  },
  {
    "tid":"298",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Profile UML2.0 for Specification of the SCA Architectures' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Service Component Architecture (SCA) aims to simplify the construction of service oriented architecture (SOA) to encourage better the reuse and to be independent from used technologies. In the other hand, UML is a standard for modeling especially in industrial environments. To face this situation we recommend an adaptation of UML2.0 to SCA. It is in this context that we have defined a profile UML2.0 for SCA contain a set of stereotypes applied to metaclasses stemming from the metamodel UML2.0. These stereotypes are completed by formal constraints in OCL. Our profile introduces new elements to reflect the architectural concepts of SCA."
      }
    ]
  },
  {
    "tid":"299",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multi-Robot Team Response To A Multi-Robot Opponent Team' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Adversarial multi-robot problems, where teams of robots compete with one another, require the development of approaches that span all levels of control and integrate algorithms ranging from low-level robot motion control, through to planning, opponent modeling, and multiagent learning. Small-size robot soccer, a league within the RoboCup initiative, is a prime example of this multi-robot team adversarial environment. In this paper, we describe some of the algorithms and approaches of our robot soccer team, CMDragons'02, developed for RoboCup 2002. Our team represents an integration of many components, several of which that are in themselves state-of-the-art, into a framework designed for fast adaptation and response to the changing environment."
      }
    ]
  },
  {
    "tid":"300",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mining temporal rules for software maintenance' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Software evolution incurs difficulties in program comprehensionand software verification, and hence it increases the cost ofsoftware maintenance. In this study, we propose a novel techniqueto mine from program execution traces a sound and complete set ofstatistically significant temporal rules of arbitrary lengths. Theextracted temporal rules reveal invariants that the programobserves, and will consequently guide developers to understand theprogram behaviors, and facilitate all downstream applications suchas verification and debugging. Different from previous studies thatwere restricted to mining two-event rules (e.g., (lock)\u2192(unlock)), our algorithm discovers rules of arbitrarylengths. In order to facilitate downstream applications, werepresent the mined rules as temporal logic expressions, so thatexisting model checkers or other formal analysis toolkit canreadily consume our mining results. Performance studies onbenchmark data sets and a case study on an industrial system havebeen performed to show the scalability and utility of our approach.We performed case studies on JBoss application server and a buggyconcurrent versions system application, and the result clearlydemonstrates the usefulness of our technique in recoveringunderlying program designs and detecting bugs. Copyright \u00a92008 John Wiley & Sons, Ltd.This work was done while the author was with School ofComputing, National University of Singapore."
      }
    ]
  },
  {
    "tid":"301",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The Triangular Pyramid Scheduling Model and algorithm for PDES in Grid' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Grid is a perfect environment for the large scale Parallel Discrete Event Simulation (PDES), because its distribution and collaboration features match the PDES applications well. The PDES tasks or applications are modeled as a Directed Acyclic Graph (DAG), in which the simulation resources consist of three critical factors, simulation hosting machine (SHM), simulation service (SS) and simulation data (SD) in Grid environment. By solving the model we attempt to obtain an optimized triangular matching of the simulation resources on Grid, so that it can support the PDES activities better. We name the algorithm of solving the model Triangular Pyramid Scheduling (TPS). The PDES DAG is divided into three basic graph structures: Sequential structure, Fork structure, and Join structure. The TPS algorithm is developed based on these graph structures. The simulation results show that TPS algorithm can reduce the makespan and congestion, improve the simulation efficiency, and increase the resource utilization efficiency, compared to the existing algorithms."
      }
    ]
  },
  {
    "tid":"302",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Bypassing Space Explosion in High-Speed Regular Expression Matching' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Network intrusion detection and prevention systems commonly use regular expression (RE) signatures to represent individual security threats. While the corresponding deterministic finite state automata (DFA) for any one RE is typically small, the DFA that corresponds to the entire set of REs is usually too large to be constructed or deployed. To address this issue, a variety of alternative automata implementations that compress the size of the final automaton have been proposed such as extended finite automata (XFA) and delayed input DFA (D 2FA). The resulting final automata are typically much smaller than the corresponding DFA. However, the previously proposed automata construction algorithms do suffer from some drawbacks. First, most employ a \u201cUnion then Minimize\u201d framework where the automata for each RE are first joined before minimization occurs. This leads to an expensive nondeterministic finite automata (NFA) to DFA subset construction on a relatively large NFA. Second, most construct the corresponding large DFA as an intermediate step. In some cases, this DFA is so large that the final automaton cannot be constructed even though the final automaton is small enough to be deployed. In this paper, we propose a \u201cMinimize then Union\u201d framework for constructing compact alternative automata focusing on the D 2FA. We show that we can construct an almost optimal final D 2FA with small intermediate parsers. The key to our approach is a space- and time-efficient routine for merging two compact D 2FA into a compact D 2FA. In our experiments, our algorithm runs on average 155 times faster and uses 1500 times less memory than previous algorithms. For example, we are able to construct a D 2FA with over 80 000 000 states using only 1 GB of main memory in only 77 min."
      }
    ]
  },
  {
    "tid":"303",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Coordination of Security Levels for Internet Architectures' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Internet systems provide a variety of ways for exchanging information, contain a large amount and variety of data, and have become quite complex, making them vulnerable to attacks from determined hackers. There are many products currently being used to stop these attacks but they suffer from a lack of completeness, only applying to one type of attack. Several mechanisms are needed for a comprehensive defense but this incurs the problem of their lack of coordination, which can be exploited for attacks. We propose a way to coordinate different mechanisms based on a unified object-oriented modeling approach and a hierarchical architecture whose layers define the scope of each security mechanism"
      }
    ]
  },
  {
    "tid":"304",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The Relevance of Semantic Subtyping' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We compare Meyer and Routley's minimal relevant logic B+ with the recent semantics-based approach to subtyping introduced by Frisch, Castagna and Benzaken in the definition of a type system with intersection and union. We show that \u2013 for the functional core of the system \u2013 such notion of subtyping, which is defined in purely set-theoretical terms, coincides with the relevant entailment of the logic B+."
      }
    ]
  },
  {
    "tid":"305",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Scientific Workflow Provenance Querying with Security Views' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Provenance, the metadata that pertains to the derivation history of a data product, has become increasingly important in scientific workflow environments. In many cases, both data products and their provenance can be sensitive and effective access control mechanisms are essential to protect their confidentiality. In this paper, we propose i) a formalization of scientific workflow provenance as the basis for querying and access control; ii) a security specification mechanism for provenance at various granularity levels and the derivation of a full security specification based on inheritance, overriding, and conflict resolution rules; iii) a formalization of security views that are derived from a scientific workflow run provenance for different roles of users; and iv) a framework that integrates abstraction views and security views such that a user can examine provenance at different abstraction levels while respecting the security policy prescribed for her. We have developed the SecProv prototype to validate the effectiveness of our approach."
      }
    ]
  },
  {
    "tid":"306",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Enhanced Indexing and Querying of Trajectories in Road Networks via String Algorithms.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this article, we propose a novel indexing and querying method for trajectories constrained in a road network. We aim to provide efficient algorithms for various types of spatiotemporal queries that involve routing in road networks, such as (1) finding moving objects that have traveled along a given path during a given time interval, (2) extracting all paths traveled after a given spatiotemporal context, and (3) enumerating all paths between two locations traveled during a certain time interval. Unlike the existing methods in spatial database research, we employ indexing techniques and algorithms from string processing. This idea is based on the fact that we can represent spatial paths as strings, because trajectories in a network are represented as sequences of road segment IDs. The proposed SNT-index (<u>s</u>uffix-array-based <u>n</u>etwork-constrained <u>t</u>rajectory index) introduces two novel concepts to trajectory indexing. The first is FM-index, which is a compact in-memory data structure for pattern matching. The second is an inverse suffix array, which allows the FM-index to be integrated with the temporal information stored in a forest of B+-trees. Thanks to these concepts, we can reduce the number of B+-tree accesses required by the query processing algorithms to a constant number, something that cannot be achieved with existing methods. Although an FM-index is essentially a static index, we also propose a practical method of appending new data to the index. Finally, experiments show that our method can process the target queries for more than 1 million trajectories in a few tens of milliseconds, which is significantly faster than what the baseline algorithms can achieve without string algorithms.\n\n"
      }
    ]
  },
  {
    "tid":"307",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Generic side-channel countermeasures for reconfigurable devices' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this work, we propose and evaluate generic hardware countermeasures against DPA attacks for recent FPGA devices. The proposed set of FPGA-specific countermeasures can be combined to resist a large variety of first-order DPA attacks, even with 100 million recorded power traces. This set includes generic and resource-efficient countermeasures for on-chip noise generation, random-data processing delays and S-box scrambling using dual-ported block memories. In particular, it is possible to build many of these countermeasures into a single IP-core or hard macro that then provides basic protection for any cryptographic implementation just by its inclusion in the design process - what is particularly useful for engineers with no or little background on security and side-channel attacks."
      }
    ]
  },
  {
    "tid":"308",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Let's Talk About X: Combining Image Recognition and Eye Gaze to Support Conversation for People with ALS.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Communicating at a natural speed is a significant challenge for users of augmentative and alternative communication (AAC) devices, especially when input is provided by eye gaze, as is common for people with ALS and similar conditions. One way to improve AAC throughput is by drawing on contextual information from the outside world. Toward this goal, we present SceneTalk, a prototype gaze-based AAC system that uses computer vision to identify objects in the user's field of view and suggests words and phrases related to the current scene. We conducted a formative evaluation of SceneTalk with six people with ALS, in which we evaluated their preference for user interface modes and output preferences. Participants agreed that integrating contextual awareness into their AAC device could be helpful across a diverse range of situations."
      }
    ]
  },
  {
    "tid":"309",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Vehicle Orientation Detection Using Vehicle Color and Normalized Cut Clustering' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes a novel approach for vehicle orientation detection using \"vehicle color\" and edge information based on clustering framework. To extract the \"vehicle color\", this paper proposes a novel color transform model which is global and does not need to be re-estimated for any new vehicles or new images. This model is invariant to various situations like contrast changes, background and lighting. Compared with traditional methods which use motion feature to determine vehicle orientations, this paper uses only one still image to finish this task. After feature extraction, the normalized cut spectral clustering (N-cut) is used for vehicle orientation clustering. The N-cut criterion tries to minimize the ratio of the total dissimilarity between groups to the total similarity within the groups. Then, the vehicle orientation can be detected using the eigenvector derived from the N-cut result. Experimental results reveal the superior performances in vehicle orientation estimation."
      }
    ]
  },
  {
    "tid":"310",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Security of erasable memories against adaptive adversaries' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We study cryptographic modeling and encryption-based design techniques for guaranteeing privacy of data that is first stored in some type of computer memory and then deleted. We continue the investigation started in [3] by presenting an enhanced privacy notion that captures practical scenarios of adversaries repeatedly and adaptively attacking the memory to inspect its entire content before trying to obtain information about deleted data. We prove that the new notion is strictly stronger than the previous one considered in [3] (allowing the adversary a single intrusion), and show then that the efficient protocol in [3] still satisfies the new notion. One question implicitly raised by the previous work was whether it is indeed possible to define one meaningful and applicable notion of security even against adversaries that can repeatedly and adaptively obtain total control of the memory. Perhaps unexpectedly, our paper affirmatively answers this question."
      }
    ]
  },
  {
    "tid":"311",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Assessing the accuracy of prediction algorithms for classification: an overview.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We provide a unified overview of methods that currently are widely used to assess the accuracy of prediction algorithms, from raw percentages, quadratic error measures and other distances, ann correlation coefficients, and to information theoretic measures such as relative entropy and mutual information. We briefly discuss the advantages and disadvantages of each approach. For classification tasks, we derive new learning algorithms for the design of prediction systems by directly optimising the correlation coefficient. We observe and prove several results relating sensitivity nod specificity of optimal systems. While the principles are general, we illustrate the applicability on specific problems such as protein secondary structure and signal peptide prediction."
      }
    ]
  },
  {
    "tid":"312",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Improving Student Modeling Through Partial Credit and Problem Difficulty' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Student modeling within intelligent tutoring systems is a task largely driven by binary models that predict student knowledge or next problem correctness (i.e., Knowledge Tracing (KT)). However, using a binary construct for student assessment often causes researchers to overlook the feedback innate to these platforms. The present study considers a novel method of tabling an algorithmically determined partial credit score and problem difficulty bin for each student's current problem to predict both binary and partial next problem correctness. This study was conducted using log files from ASSISTments, an adaptive mathematics tutor, from the 2012-2013 school year. The dataset consisted of 338,297 problem logs linked to 15,253 unique student identification numbers. Findings suggest that an efficiently tabled model considering partial credit and problem difficulty performs about as well as KT on binary predictions of next problem correctness. This method provides the groundwork for modifying KT in an attempt to optimize student modeling."
      }
    ]
  },
  {
    "tid":"313",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Machine learning for event selection in high energy physics' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The field of high energy physics aims to discover the underlying structure of matter by searching for and studying exotic particles, such as the top quark and Higgs boson, produced in collisions at modern accelerators. Since such accelerators are extraordinarily expensive, extracting maximal information from the resulting data is essential. However, most accelerator events do not produce particles of interest, so making effective measurements requires event selection, in which events producing particles of interest (signal) are separated from events producing other particles (background). This article studies the use of machine learning to aid event selection. First, we apply supervised learning methods, which have succeeded previously in similar tasks. However, they are suboptimal in this case because they assume that the selector with the highest classification accuracy will yield the best final analysis; this is not true in practice, as such analyses are more sensitive to some backgrounds than others. Second, we present a new approach that uses stochastic optimization techniques to directly search for selectors that maximize either the precision of top quark mass measurements or the sensitivity to the presence of the Higgs boson. Empirical results confirm that stochastically optimized selectors result in substantially better analyses. We also describe a case study in which the best selector is applied to real data from the Fermilab Tevatron accelerator, resulting in the most precise top quark mass measurement of this type to date. Hence, this new approach to event selection has already contributed to our knowledge of the top quark's mass and our understanding of the larger questions upon which it sheds light."
      }
    ]
  },
  {
    "tid":"314",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Arbitrary Modulus Indexing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Modern high performance processors require memory systems that can provide access to data at a rate that is well matched to the processor's computation rate. Common to such systems is the organization of memory into local high speed memory banks that can be accessed in parallel. Associative look up of values is made efficient through indexing instead of associative memories. These techniques lose effectiveness when data locations are not mapped uniformly to the banks or cache locations, leading to bottlenecks that arise from excess demand on a subset of locations. Address mapping is most easily performed by indexing the banks using a mod (2 N) indexing scheme, but such schemes interact poorly with the memory access patterns of many computations, making resource conflicts a significant memory system bottleneck. Previous work has assumed that prime moduli are the best choices to alleviate conflicts and has concentrated on finding efficient implementations for them. In this paper, we introduce a new scheme called Arbitrary Modulus Indexing (AMI) that can be implemented efficiently for all moduli, matching or improving the efficiency of the best existing schemes for primes while allowing great flexibility in choosing a modulus to optimize cost/performance trade-offs. We also demonstrate that, for a memory-intensive workload on a modern replay-style GPU architecture, prime moduli are not in general the best choices for memory bank and cache set mappings. Applying AMI to set of memory intensive benchmarks eliminates 98% of bank and set conflicts, resulting in an average speedup of 24% over an aggressive baseline system and a 64% average reduction in memory system replays at reasonable implementation cost."
      }
    ]
  },
  {
    "tid":"315",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Partially Ordered Constraint Optimization Problems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The Constraint Problems usually addressed fall into one of two models: the Constraint Satisfaction Problem (CSP) and the Constraint Op- timization Problem (COP). However, in many real-life applications, more functions should be optimized at the same time, and solutions are ranked by means of a Partial Order. Also, when dealing with Null values, the solution set is ranked through a Partial Order, and only the non dominated solutions are interesting for the user. For example, in 3D visual recognition, from any viewpoint there are always hidden features: assignments that are less expressive (that take more Null values than others) should be discarded. For these reasons, we propose the model of Partially-Ordered Constraint Optimization Problem and provide an algorithm for solving it, derived from the widely used Branch and Bound."
      }
    ]
  },
  {
    "tid":"316",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Hybrid fuzzy probabilistic data association filter and joint probabilistic data association filter' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Multitarget tracking problems are theoretically interesting because, unlike other estimation problems, the origins of the measurements are not identified. This involves hypothesis generation and their evaluation in terms of degree of agreement between the given measurements and the underlying tracks. Typical algorithms to deal with such problems are the probabilistic data association filter (PDAF) in the case of single target tracking and joint probabilistic data association filter (JPDAF) in the case of multiple target tracking proposed by Bar-Shalom and his team. The basis of JPDAF is the calculus of the joint probabilities over all targets and hits. The algorithm assigns weights for reasonable hits and uses a weighted centroid of those hits to update the track. In this paper, we propose a new weight assignment based on fuzzy c-means methodology. Particularly, in order to take account for the false alarms (clutter) where none of the measurements is target originated, a new noisy fuzzy c-means algorithm is elaborated. The latter contrasts with that provided by Dave regarding the location of the noise prototype as well as the meaning of the universality of the noise class. The treatment of conflictual situations where, for instance, more than one hit fail in a target extension gate is accomplished using some weighted based procedure with respect to all feasible joint matrices involved in the construction of joint probabilities in JPDAF. In the meantime, the general methodology of PDAF and JPDAF remains unchanged. This leads to Hybrid Fuzzy PDAF in the case of single target tracking and Hybrid Fuzzy JPDAF in the case of multiple target tracking. This investigation shows a fruitful combination between fuzzy and probabilistic approaches in order to accomplish target tracking tasks."
      }
    ]
  },
  {
    "tid":"317",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Key Derivation Without Entropy Waste.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We revisit the classical problem of converting an imperfect source of randomness into a usable cryptographic key. Assume that we have some cryptographic application P that expects a uniformly random m-bit key R and ensures that the best attack (in some complexity class) against P(R) has success probability at most delta. Our goal is to design a key-derivation function (KDF) h that converts any random source X of min-entropy k into a sufficiently \"good\" key h(X), guaranteeing that P(h(X)) has comparable security delta' which is 'close' to delta. Seeded randomness extractors provide a generic way to solve this problem for all applications P, with resulting security delta' = 0(6), provided that we start with entropy k >= m + 2 log (1/delta) - O(1). By a result of Radhakrishnan and Ta-Shma, this bound on k (called the \"RT-bound\") is also known to be tight in general. Unfortunately, in many situations the loss of 2 log (1/delta) bits of entropy is unacceptable. This motivates the study KDFs with less entropy waste by placing some restrictions on the source X or the application P. In this work we obtain the following new positive and negative results in this regard: Efficient samplability of the source X does not help beat the RT-bound for general applications. This resolves the SRI (samplable RT) conjecture of Dachman-Soled et al. [DGKM12] in the affirmative, and also shows that the existence of computationally-secure extractors beating the RT-bound implies the existence of one-way functions. We continue in the line of work initiated by Barak et al. [BDK+11] and construct new information-theoretic KDFs which beat the RIbound for large but restricted classes of applications. Specifically, we design efficient KDFs that work for all unpredictability applications P (e.g., signatures, MACs, one-way functions, etc.) and can either: (1) extract all of the entropy k = m with a very modest security loss delta' = 0(delta center dot log (1/delta)), or alternatively, (2) achieve essentially optimal security delta' = 0(delta) with a very modest entropy loss k >= m + loglog (1/delta). In comparison, the best prior results from [BDK(+)11] for this class of applications would only guarantee delta' = O(root/delta) when k = m, and would need k >= m + log (1/delta) to get delta' = O(delta). The weaker bounds of [BDK(+)11] hold for a larger class of so-called \"square-friendly\" applications (which includes all unpredictability, but also some important indistinguishability, applications). Unfortunately, we show that these weaker bounds are tight for the larger class of applications. We abstract out a clean, information-theoretic notion of (k,delta,delta')'unpredictability extractors, which guarantee \"induced\" security delta' for any delta-secure unpredictability application P, and characterize the parameters achievable for such unpredictability extractors. Of independent interest, we also relate this notion to the previously-known notion of (min-entropy) condensers, and improve the state-of-the-art parameters for such condensers."
      }
    ]
  },
  {
    "tid":"318",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multiple Kernel Clustering With Neighbor-Kernel Subspace Segmentation.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Multiple kernel clustering (MKC) has been intensively studied during the last few decades. Even though they demonstrate promising clustering performance in various applications, existing MKC algorithms do not sufficiently consider the intrinsic neighborhood structure among base kernels, which could adversely affect the clustering performance. In this paper, we propose a simple yet effective neighbor-kernel-based MKC algorithm to address this issue. Specifically, we first define a neighbor kernel, which can be utilized to preserve the block diagonal structure and strengthen the robustness against noise and outliers among base kernels. After that, we linearly combine these base neighbor kernels to extract a consensus affinity matrix through an exact-rank-constrained subspace segmentation. The naturally possessed block diagonal structure of neighbor kernels better serves the subsequent subspace segmentation, and in turn, the extracted shared structure is further refined through subspace segmentation based on the combined neighbor kernels. In this manner, the above two learning processes can be seamlessly coupled and negotiate with each other to achieve better clustering. Furthermore, we carefully design an efficient iterative optimization algorithm with proven convergence to address the resultant optimization problem. As a by-product, we reveal an interesting insight into the exact-rank constraint in ridge regression by careful theoretical analysis: it back-projects the solution of the unconstrained counterpart to its principal components. Comprehensive experiments have been conducted on several benchmark data sets, and the results demonstrate the effectiveness of the proposed algorithm."
      }
    ]
  },
  {
    "tid":"319",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Generic Storage API' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present a generic API suitable for provision of highly generic storage facilities that can be tailored to produce various individually customised storage infrastructures. The paper identifies a candidate set of minimal storage system building blocks, which are sufficiently simple to avoid encapsulating policy where it cannot be customised by applications, and composable to build highly flexible storage architectures. Four main generic components are defined: the store, the namer , the caster and the interpreter. It is hypothesised that these are sufficiently general that they could act as building blocks for any information storage and retrieval system. The essential characteristics of each are defined by an interface, which may be implemented by multiple implementing classes . ."
      }
    ]
  },
  {
    "tid":"320",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A GML-Based mobile device trace monitoring system' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recently, the demands on information services have been increasing significantly. This is mainly due to the popularization of computer and mobile telecommunication devices and the rapid improvements on wireless communication technology. Specially, information services and their corresponding management for mobile devices, such as Location Based Service (LBS) and Telematics, become more and more important. However, the standard for geographical space data has not been finalized. Many commercial monitoring systems are using their own independent geographical information without making them compatible to others. Much efforts and resources have been wasted on managing and operating those different monitoring systems\u2019 geographical Information System (GIS) databases. Accordingly, a standard format called GML, based on the most commonly used geographical data format such as DXF, DWG and SHP, has been emerged. In this paper, our work on GML\u2019s visualization in Trace Monitoring Systems (TMS) is described fully. The details on how to trace and manage data moving among different mobile terminals are presented as well."
      }
    ]
  },
  {
    "tid":"321",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Adaptive schemes for distributed web caching' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In distributed web caching architectures, institutional proxies take advantage of their neighbors' contents in order to reduce the number of requests forwarded to the server. Intuitively, the maximum benefit from this cooperation is expected when the proxies that exhibit similar requests are grouped together. The current practice is to follow a static and manual configuration of neighbors. Such an approach has a number of drawbacks: (i) static allocation may not determine the best neighbors, especially if global knowledge of the participating proxies is not available, (ii) a manual allocation places significant administrative burden, (iii) static schemes are insensitive to changes in access patterns, and (iv) they cannot deal with the introduction of new, potentially useful, proxies. In this paper, we propose a set of algorithms that allow proxies to independently explore the network for better neighbors and continuously update their configuration in an adaptive fashion. The simulation experiments illustrate that dynamic neighbor reconfiguration leads to significantly higher hit ratios compared to the static approach. Although some researchers in the past have recognized the need for adaptive caching, to the best of our knowledge this is the first study to propose concrete algorithms and evaluate their efficacy."
      }
    ]
  },
  {
    "tid":"322",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Bidirectional Search That Is Guaranteed to Meet in the Middle.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present MM, the first bidirectional heuristic search algorithm whose forward and backward searches are guaranteed to \\\"meet in the middle\\\", i.e. never expand a node beyond the solution midpoint. We also present a novel framework for comparing MM, A*, and brute-force search, and identify conditions favoring each algorithm. Finally, we present experimental results that support our theoretical analysis."
      }
    ]
  },
  {
    "tid":"323",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Finding Underlying Connections: A Fast Graph-Based Method for Link Analysis and Collaboration Queries' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many techniques in the social sciences and graph theory deal with the problem of examining and analyzing patterns found in the underlying struc- ture and associations of a group of entities. How- ever, much of this work assumes that this un- derlying structure is known or can easily be in- ferred from data, which may often be an unreal- istic assumption for many real-world problems. Below we consider the problem of learning and querying a graph-based model of this underlying structure. The model is learned from noisy ob- servations linking sets of entities. We explicitly allow different types of links (representing differ- ent types of relations) and temporal information indicating when a link was observed. We quanti- tatively compare this representation and learning method against other algorithms on the task of predicting future links and new \"friendships\" in a variety of real world data sets."
      }
    ]
  },
  {
    "tid":"324",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Querying the Web of Data: A Formal Approach' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The increasing amount of interlinked RDF data has finally made available the necessary building blocks for the web of data. This in turns makes it possible (and interesting) to query such a collection of graphs as an open and decentralized knowledge base. However, despite the fact that there are already implementations of query answering algorithms for the web of data, there is no formal characterization of what a satisfactory answer is expected to be. In this paper, we propose a preliminary model for such an open collection of graphs which goes beyond the standard single-graph RDF semantics, describes three different ways in which a query can be answered, and characterizes them semantically in terms of three incremental restrictions on the relation between the domain of interpretation of each single component graph."
      }
    ]
  },
  {
    "tid":"325",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Who is the expert? combining intention and knowledge of online discussants in collaborative RE tasks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Large, distributed software development projects rely on the collaboration of culturally heterogeneous and geographically distributed stakeholders. Software requirements, as well as solution ideas are elicited in distributed processes, which increasingly use online forums and mailing lists, in which stakeholders mainly use free or semi-structured natural language text. The identification of contributors of key information about a given topic --called experts, in both the software domain and code-- and in particular an automated support for retrieving information from available online resources, are becoming of crucial importance. In this paper, we address the problem of expert finding in mailing-list discussions, and propose an approach which combines content- and intent-based information extraction for ranking online discussants with respect to their expertise in the discussed topics. We illustrate its application on an example."
      }
    ]
  },
  {
    "tid":"326",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Adaptive Q-Learning for Data-Based Optimal Output Regulation With Experience Replay.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, the data-based optimal output regulation problem of discrete-time systems is investigated. An off-policy adaptive Q-learning (QL) method is developed by using real system data without requiring the knowledge of system dynamics and the mathematical model of utility function. By introducing the Q-function, an off-policy adaptive QL algorithm is developed to learn the optimal Q-functio..."
      }
    ]
  },
  {
    "tid":"327",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mosaicing-by-recognition: a technique for video-based text recognition' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a mosaicing-by-recognition technique is proposed where video mosaicing and text recognition are simultaneously and collaboratively optimized in a one-step manner. Specifically, multiple frames capturing a long text line are optimally concatenated with a guide of the text recognition framework. In this optimization process, rotation, scaling, vertical shift, and speed fluctuation, which often appear in video frames captured by hand-held cameras, are compensated. The optimization is performed by a DPbased algorithm. The results of experiments to evaluate not only the accuracy of text recognition but also that of video mosaicing indicates that the proposed technique is practical and can provide reasonable results in most cases."
      }
    ]
  },
  {
    "tid":"328",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An efficient Earth Mover's Distance algorithm for robust histogram comparison.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose EMD-L1: a fast and exact algorithm for computing the Earth Mover's Distance (EMD) between a pair of histograms. The efficiency of the new algorithm enables its application to problems that were previously prohibitive due to high time complexities. The proposed EMD-L1 significantly simplifies the original linear programming formulation of EMD. Exploiting the L1 metric structure, the number of unknown variables in EMD-L1 is reduced to O(N) from O(N2) of the original EMD for a histogram with N bins. In addition, the number of constraints is reduced by half and the objective function of the linear program is simplified. Formally, without any approximation, we prove that the EMD-L1 formulation is equivalent to the original EMD with a L1 ground distance. To perform the EMD-L1 computation, we propose an efficient tree-based algorithm, Tree-EMD. Tree-EMD exploits the fact that a basic feasible solution of the simplex algorithm-based solver forms a spanning tree when we interpret EMD-L1 as a network flow optimization problem. We empirically show that this new algorithm has an average time complexity of O(N2), which significantly improves the best reported supercubic complexity of the original EMD. The accuracy of the proposed methods is evaluated by experiments for two computation-intensive problems: shape recognition and interest point matching using multidimensional histogram-based local features. For shape recognition, EMD-L1 is applied to compare shape contexts on the widely tested MPEG7 shape data set, as well as an articulated shape data set. For interest point matching, SIFT, shape context and spin image are tested on both synthetic and real image pairs with large geometrical deformation, illumination change, and heavy intensity noise. The results demonstrate that our EMD-L1-based solutions outperform previously reported state-of-the-art features and distance measures in solving the two tasks."
      }
    ]
  },
  {
    "tid":"329",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Constants Count: Practical Improvements to Oblivious RAM' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Oblivious RAM (ORAM) is a cryptographic primitive that hides memory access patterns as seen by untrusted storage. This paper proposes Ring ORAM, the most bandwidth-efficient ORAM scheme for the small client storage setting in both theory and practice. Ring ORAM is the first tree-based ORAM whose bandwidth is independent of the ORAM bucket size, a property that unlocks multiple performance improvements. First, Ring ORAM's overall bandwidth is 2.3\u00d7 to 4\u00d7 better than Path ORAM, the prior-art scheme for small client storage. Second, if memory can perform simple untrusted computation, Ring ORAM achieves constant online bandwidth (\u223c 60\u00d7 improvement over Path ORAM for practical parameters). As a case study, we show Ring ORAM speeds up program completion time in a secure processor by 1.5\u00d7 relative to Path ORAM. On the theory side, Ring ORAM features a tighter and significantly simpler analysis than Path ORAM."
      }
    ]
  },
  {
    "tid":"330",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Deterministic divide-and-conquer algorithm for decomposable black-box optimization problems with bounded difficulty' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"There is a class of GA-hard problems for which classical genetic algorithms often fail to obtain optimal solutions. In this paper, we focus on a class of very typical GA-hard problems that we call decomposable black-box optimization problems (DBBOP). Different from random methods in GA literature, two \u201cdeterministic\u201d divide-and-conquer algorithms DA1 and DA2 are proposed respectively for non-overlapping and overlapping DBBOP, in which there are no classical genetic operations and even no random operations. Given any DBBOP with dimension l and bounded order k, our algorithms can always reliably and accurately obtain the optimal solutions in deterministic way using O(lk) function evaluations."
      }
    ]
  },
  {
    "tid":"331",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'AIDE: Fast and Communication Efficient Distributed Optimization.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we present two new communication-efficient methods for distributed minimization of an average of functions. The first algorithm is an inexact variant of the DANE algorithm that allows any local algorithm to return an approximate solution to a local subproblem. We show that such a strategy does not affect the theoretical guarantees of DANE significantly. In fact, our approach can be viewed as a robustification strategy since the method is substantially better behaved than DANE on data partition arising in practice. It is well known that DANE algorithm does not match the communication complexity lower bounds. To bridge this gap, we propose an accelerated variant of the first method, called AIDE, that not only matches the communication lower bounds but can also be implemented using a purely first-order oracle. Our empirical results show that AIDE is superior to other communication efficient algorithms in settings that naturally arise in machine learning applications."
      }
    ]
  },
  {
    "tid":"332",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Quasi-asynchronous migration: a novel migration protocol for PVM tasks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Process migration is a desirable and useful facility for the Network of Workstations (NOW). This paper presents a novel migration protocol called quasi-asynchronous migration for PVM tasks, which allows non-migrating tasks to execute during most of the time of migration. Message flushing and message delaying are the key mechanisms used in quasi-asynchronous migration. Because the protocol is implemented on top of PVM and at user-level, it is both transparent to users and portable. Both the analysis of the migration protocol and the experimental results show that quasi-asynchronous migration has lower overhead than other migration protocols."
      }
    ]
  },
  {
    "tid":"333",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Finding the event structure of neuronal spike trains.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Neurons in sensory systems convey information about physical stimuli in their spike trains. In vitro, single neurons respond precisely and reliably to the repeated injection of the same fluctuating current, producing regions of elevated firing rate, termed events. Analysis of these spike trains reveals that multiple distinct spike patterns can be identified as trial-to-trial correlations between spike times (Fellous, Tiesinga, Thomas, & Sejnowski, 2004 ). Finding events in data with realistic spiking statistics is challenging because events belonging to different spike patterns may overlap. We propose a method for finding spiking events that uses contextual information to disambiguate which pattern a trial belongs to. The procedure can be applied to spike trains of the same neuron across multiple trials to detect and separate responses obtained during different brain states. The procedure can also be applied to spike trains from multiple simultaneously recorded neurons in order to identify volleys of near-synchronous activity or to distinguish between excitatory and inhibitory neurons. The procedure was tested using artificial data as well as recordings in vitro in response to fluctuating current waveforms."
      }
    ]
  },
  {
    "tid":"334",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Completeness theorems for the Abadi-Rogaway language of encrypted expressions' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We show that the Abadi-Rogaway logic of indistinguishability for cryptographic expressions is not complete by giving a natural example of a secure encryption function and a pair of expressions, such that the distributions associated to the two expressions are computationally indistinguishable, but equality cannot be proved within the logic. We then introduce a new property for encryption schemes, which we call confusion freeness, and show that the Abadi-Rogaway logic is sound and complete, whenever the encryption scheme used satisfies this property. We relate confusion freeness with standard cryptographic security notions, showing that any authenticated encryption scheme is confusion free. We also consider two extensions of the basic logic. The first is a refinement of the Abadi-Rogaway logic that overcomes certain limitations of the original proposal, allowing for encryption functions that do not hide the length of the message being sent. Both the soundness theorem of Abadi and Rogaway, and our completeness result for confusion free (or authenticated) encryption easily extend to this more realistic notion of secrecy. The second is an extension of the logic due to Abadi and J\u00fcrjens that allows to study more complex protocols in the presence of a passive adversary. Our completeness results holds for this extended logic as well."
      }
    ]
  },
  {
    "tid":"335",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Comparing Clustering with Pairwise and Relative Constraints: A Unified Framework.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Clustering can be improved with the help of side information about the similarity relationships among instances. Such information has been commonly represented by two types of constraints: pairwise constraints and relative constraints, regarding similarities about instance pairs and triplets, respectively. Prior work has mostly considered these two types of constraints separately and developed individual algorithms to learn from each type. In practice, however, it is critical to understand/compare the usefulness of the two types of constraints as well as the cost of acquiring them, which has not been studied before. This paper provides an extensive comparison of clustering with these two types of constraints. Specifically, we compare their impacts both on human users that provide such constraints and on the learning system that incorporates such constraints into clustering. In addition, to ensure that the comparison of clustering is performed on equal ground (without the potential bias introduced by different learning algorithms), we propose a probabilistic semi-supervised clustering framework that can learn from either type of constraints. Our experiments demonstrate that the proposed semi-supervised clustering framework is highly effective at utilizing both types of constraints to aid clustering. Our user study provides valuable insights regarding the impact of the constraints on human users, and our experiments on clustering with the human-labeled constraints reveal that relative constraint is often more efficient at improving clustering."
      }
    ]
  },
  {
    "tid":"336",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Location-based algorithms for finding sets of corresponding objects over several geo-spatial data sets' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"When integrating geo-spatial data sets, a join algorithm is used for finding sets of corresponding objects (i.e., objects that represent the same real-world entity). This article investigates location-based join algorithms for integration of several data sets. First, algorithms for integration of two data sets are presented and their performances, in terms of recall and precision, are compared. Then, two approaches for integration of more than two data sets are described. In one approach, all the integrated data sets are processed simultaneously. In the second approach, a join algorithm for two data sets is applied sequentially, either in a serial manner, where in each join at least one of the joined data sets is a single source, or in a hierarchical manner, where two join results can be joined. For the two approaches, join algorithms are given. The algorithms are designed to perform well even when location of objects are imprecise and each data set represents only some of the real-world entities. Results of extensive experiments with the different approaches are provided and analyzed. The experiments show the differences, in accuracy and efficiency, between the approaches, under different circumstances. The results also show that all our algorithms have much better accuracy than applying the commonly used one-sided nearest-neighbor join."
      }
    ]
  },
  {
    "tid":"337",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Content-based image retrieval using growing hierarchical self-organizing quadtree map' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a growing hierarchical self-organizing quadtree map (GHSOQM) is proposed and used for a content-based image retrieval (CBIR) system. The incorporation of GHSOQM in a CBIR system organizes images in a hierarchical structure. The retrieval time by GHSOQM is less than that by using direct image comparison using a flat structure. Furthermore, the ability of incremental learning enables GHSOQM to be a prospective neural-network-based approach for CBIR systems. We also propose feature matrices, image distance and relevance feedback for region-based images in the GHSOQM-based CBIR system. Experimental results strongly demonstrate the effectiveness of the proposed system."
      }
    ]
  },
  {
    "tid":"338",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Persistent ET0L systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A new class of parallel rewriting devices called persistent ET0L systems is presented. It is shown that these systems are equivalent (in their language-generating power) to some other classes of grammars investigated in the literature. Also this new mechanism sheds some light on the nature of nondeterminism in (variations of) ET0L systems."
      }
    ]
  },
  {
    "tid":"339",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Evolutions equations in computational anatomy.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"One of the main purposes in computational anatomy is the measurement and statistical study of anatomical variations in organs, notably in the brain or the heart. Over the last decade, our group has progressively developed several approaches for this problem, all related to the Riemannian geometry of groups of diffeomorphisms and the shape spaces on which these groups act. Several important shape evolution equations that are now used routinely in applications have emerged over time."
      }
    ]
  },
  {
    "tid":"340",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The Effect Of Spine Actuation And Stiffness On A Pneumatically-Driven Quadruped Robot For Cheetah-Like Locomotion' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Biological research has concluded that the actuation of the spine contributes significantly to the performance of quadrupeds in terms of controlling body posture, and integrating limbs and trunk actions. Inspired by this biological findings, we develop a pneumatically-driven quadruped robot called Renny with configurable spine morphology to study how the spine contributes to cheetah-like running. Three spine morphologies: rigid spine, passive spine, and actuated spine, are introduced and tested in Renny robot. In addition, we investigate the effect of the stiffness distribution of the spine muscles in the passive case. The experimental results show that the passive one where the dorsal stiffness is higher than the ventral stiffness can run faster, even faster than the rigid case. Moreover, the coordination between the legs and the actuated spine is studied in actuated spine morphology. We found that when the spinal movements are synchronized with the legs movements, the speed is much faster. In the actuated case, both flexion and extension benefit the increase of the speed by advancing limbs rapidly and increasing the limb swing."
      }
    ]
  },
  {
    "tid":"341",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Social Q&A and virtual reference\u2014comparing apples and oranges with the help of experts and users' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Online question-answering (Q&A) services are becoming increasingly popular among information seekers. We divide them into two categories, social Q&A (SQA) and virtual reference (VR), and examine how experts (librarians) and end users (students) evaluate information within both categories. To accomplish this, we first performed an extensive literature review and compiled a list of the aspects found to contribute to a \u201cgood\u201d answer. These aspects were divided among three high-level concepts: relevance, quality, and satisfaction. We then interviewed both experts and users, asking them first to reflect on their online Q&A experiences and then comment on our list of aspects. These interviews uncovered two main disparities. One disparity was found between users\u2019 expectations with these services and how information was actually delivered among them, and the other disparity between the perceptions of users and experts with regard to the aforementioned three characteristics of relevance, quality, and satisfaction. Using qualitative analyses of both the interviews and relevant literature, we suggest ways to create better hybrid solutions for online Q&A and to bridge the gap between experts\u2019 and users\u2019 understandings of relevance, quality, and satisfaction, as well as the perceived importance of each in contributing to a good answer. \u00a9 2012 Wiley Periodicals, Inc."
      }
    ]
  },
  {
    "tid":"342",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The effects of group composition on decision quality in a social production community' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Online social production communities allow efficient construction of valuable and high-quality information sources. To be successful, community members must be effective at collaboration, including makink collective decisions in the presence of disagreement. We examined over 100,000 decisions made by small working groups in Wikipedia, and analyzed how decision quality in these online groups is influenced by four group composition factors: the size of the group, how members were invited to the group, the prior experience of group members, and apparent bias shown by the group administrator. Our findings lead us to recommendations for designers of social production communities."
      }
    ]
  },
  {
    "tid":"343",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimized Anchors Placement: An Analytical Approach In Uwb-Based Tdoa Localization' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we consider the problem of locating an Automated Guided Vehicle (AGV) which moves on a plane in an industrial environment by means of Ultra-Wide Band (UWB) signaling from fixed Anchors Nodes (ANs) situated in the (three-dimensional) space. An analytical approach to optimize, under proper (realistic) constraints, the placement of the ANs used to locate the AGV is proposed. Analytical results are confirmed by simulations."
      }
    ]
  },
  {
    "tid":"344",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'IPv6 Conformance and Interoperability Testing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The fear of incompatibility issues between legacy IPv4 systems and new IPv6 implementations can delay the migration towards a full IPv6 native Internet. Network engineers need to ensure that heterogeneous IPv6 devices will behave consistently to the IPv6 specifications and will interoperate correctly in a multi-vendor environment. This paper analyzes IPv6 Conformance Testing, a set of rigorous test methodologies that allows to throughout verify the quality of IPv6 devices during their entire life cycle, and can give confidence to the customers. We analyze the TAHI IPv6 Conformance and Interoperability Test Suite and present our Conformance Test results for the latest versions of IPv6 host implementations."
      }
    ]
  },
  {
    "tid":"345",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Sliding Mode Congestion Control for Data Center Ethernet Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recently, Ethernet is enhanced as the unified switch fabric of data centers, called Data Center Ethernet. One of the indispensable enhancements is end-to-end congestion management, and currently Quantized Congestion Notification (QCN) has been ratified as the corresponding standard. However, our experiments show that QCN suffers from large oscillations of the queue length at the bottleneck link such that the buffer is emptied frequently and accordingly the link utilization degrades, with certain system parameters and network configurations. This phenomenon is corresponding to our theoretical analysis result that QCN fails to enter into the Sliding Mode Motion (SMM) pattern with certain system parameters and network configurations. Knowing the drawbacks of QCN and realizing the advantage that congestion management system is insensitive to the changes of parameters and network configurations in the SMM pattern, we present Sliding Mode Congestion Control (SMCC), which can enter into the SMM pattern under any conditions. SMCC is simple, stable, fair, has short response time, and can be easily used to replace QCN because both of them follow the framework developed by the IEEE 802.1Qau work group. Experiments on the NetFPGA platform show that SMCC is superior to QCN, especially when traffic pattern and network states are variable."
      }
    ]
  },
  {
    "tid":"346",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Information Utilization Ratio in Heuristic Optimization Algorithms.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Heuristic algorithms are able to optimize objective functions efficiently because they use intelligently the information about the objective functions. Thus, information utilization is critical to the performance of heuristics. However, the concept of information utilization has remained vague and abstract because there is no reliable metric to reflect the extent to which the information about the objective function is utilized by heuristic algorithms. In this paper, the metric of information utilization ratio (IUR) is defined, which is the ratio of the utilized information quantity over the acquired information quantity in the search process. The IUR proves to be well-defined. Several examples of typical heuristic algorithms are given to demonstrate the procedure of calculating the IUR. Empirical evidences on the correlation between the IUR and the performance of a heuristic are also provided. The IUR can be an index of how finely an algorithm is designed and guide the invention of new heuristics and the improvement of existing ones."
      }
    ]
  },
  {
    "tid":"347",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Lazy evaluation methods for detecting complex events' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The goal of Complex Event Processing (CEP) systems is to efficiently detect complex patterns over a stream of primitive events. A pattern of particular significance is a sequence, where we are interested in identifying that a number of primitive events have arrived on the stream in a predefined order. Many popular CEP systems employ Non-deterministic Finite Automata (NFA) arranged in a chain topology to detect such sequences. Existing NFA-based mechanisms incrementally extend previously observed prefixes of a sequence until a match is found. Consequently, each newly arriving event needs to be processed to determine whether a new prefix is to be initiated or an existing one extended. This approach may be very inefficient when events at the beginning of the sequence are very frequent. We address the problem by introducing a lazy evaluation mechanism that is able to process events in descending order of selectivity. We employ this mechanism in a chain topology NFA, which waits until the most selective event in the sequence arrives and then adds events to partial matches according to a predetermined order of selectivity. In addition, we propose a tree topology NFA that does not require the selectivity order to be defined in advance. Finally, we experimentally evaluate our mechanism on real-world stock trading data, demonstrating a performance gain of two orders of magnitude, with significantly reduced memory resource requirements."
      }
    ]
  },
  {
    "tid":"348",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Service Chain and Virtual Network Embeddings: Approximations using Randomized Rounding.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The SDN and NFV paradigms enable novel network services which can be realized and embedded in a flexible and rapid manner. For example, SDN can be used to flexibly steer traffic from a source to a destination through a sequence of virtualized middleboxes, in order to realize so-called service chains. The service chain embedding problem consists of three tasks: admission control, finding suitable locations to allocate the virtualized middleboxes and computing corresponding routing paths. This paper considers the offline batch embedding of multiple service chains. Concretely, we consider the objectives of maximizing the profit by embedding an optimal subset of requests or minimizing the costs when all requests need to be embedded. Interestingly, while the service chain embedding problem has recently received much attention, so far, only non- polynomial time algorithms (based on integer programming) as well as heuristics (which do not provide any formal guarantees) are known. This paper presents the first polynomial time service chain approximation algorithms both for the case with admission and without admission control. Our algorithm is based on a novel extension of the classic linear programming and randomized rounding technique, which may be of independent interest. In particular, we show that our approach can also be extended to more complex service graphs, containing cycles or sub-chains, hence also providing new insights into the classic virtual network embedding problem."
      }
    ]
  },
  {
    "tid":"349",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Automatic composition of transition-based semantic web services with messaging' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we present Colombo, a framework in which web services are characterized in terms of (i) the atomic processes (i.e., operations) they can perform; (ii) their impact on the \"real world\" (modeled as a relational database); (iii) their transition-based behavior; and (iv) the messages they can send and receive (from/to other web services and \"human\" clients). As such, Colombo combines key elements from the standards and research literature on (semantic) web services. Using Colombo, we study the problem of automatic service composition (synthesis) and devise a sound, complete and terminating algorithm for building a composite service. Specifically, the paper develops (i) a technique for handling the data, which ranges over an infinite domain, in a finite, symbolic way, and (ii) a technique to automatically synthesize composite web services, based on Propositional Dynamic Logic."
      }
    ]
  },
  {
    "tid":"350",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Provisioning in Elastic Optical Networks with Non-Disruptive Defragmentation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"How serious is spectrum fragmentation in an elastic optical network? It is addressed by the tremendous provisioning gain achieved by eliminating a particular kind of fragmentation, namely, one that could be removed by Hitless Optical Path Shift (HOPS), a non-disruptive defragmentation technology. We devised a series of HOPS-based defragmentation algorithms, ranging from proactive to reactive strategies. Analysis and simulation results show that for typical backbone networks operated with a light load, using our optimal reactive defragmentation scheme, 98% of the requested spectrum bandwidth, that is otherwise blocked, can be provisioned. Moreover, the corresponding defragmentation costs (in terms of impact on existing connections, and defragmentation delay) are kept at minimum."
      }
    ]
  },
  {
    "tid":"351",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Quantification of margins and uncertainties of complex systems in the presence of aleatoric and epistemic uncertainty' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Performance assessment of complex systems is ideally done through full system-level testing which is seldom available for high consequence systems. Further, a reality of engineering practice is that some features of system behavior are not known from experimental data, but from expert assessment, only. On the other hand, individual component data, which are part of the full system are more readily available. The lack of system level data and the complexity of the system lead to a need to build computational models of a system in a hierarchical or building block approach (from simple components to the full system). The models are then used for performance prediction in lieu of experiments, to estimate the confidence in the performance of these systems. Central to this are the need to quantify the uncertainties present in the system and to compare the system response to an expected performance measure. This is the basic idea behind Quantification of Margins and Uncertainties (QMU). QMU is applied in decision making\u2014there are many uncertainties caused by inherent variability (aleatoric) in materials, configurations, environments, etc., and lack of information (epistemic) in models for deterministic and random variables that influence system behavior and performance. This paper proposes a methodology to quantify margins and uncertainty in the presence of both aleatoric and epistemic uncertainty. It presents a framework based on Bayes networks to use available data at multiple levels of complexity (i.e. components, subsystem, etc.) and demonstrates a method to incorporate epistemic uncertainty given in terms of intervals on a model parameter."
      }
    ]
  },
  {
    "tid":"352",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'DACS scheme as next generation policy-based network management scheme' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"As the work for managing a whole network effectively without a limited purpose, there is the work of PBNM (Policy-based network management). PBNM has two structural problems such as communication concentration from many clients to a communication control mechanism called PEP (Policy Enhancement Point) and the necessity of the network system updating at the time of introducing PBNM into LAN. Moreover, user support problems in campus-like computer networks such as trouble user support at updating a client's setups and coping with annoying communication can not be improved by PBNM. To improve these problems, we show a next generation PBNM which overcomes theses problems and has the function which does not exist in existing PBNM, and called it DACS (Destination Addressing Control System) Scheme. By DACS Scheme, communication concentration from many clients to PEP is solved, and system updating becomes unnecessary. Moreover, user support at updating the client's setups and coping with annoying communication by DACS Scheme becomes very effective."
      }
    ]
  },
  {
    "tid":"353",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Contribution-Guided Peer Selection for Reliable Peer-to-Peer Video Streaming Over Mesh Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes a sender-driven peer selection scheme, including estimation of packet loss propagation, evaluation of peers' contributions, and peer selection based on child-peers' contributions, for mesh-based peer-to-peer (P2P) video streaming systems. The proposed packet loss propagation model takes into account the link packet drop rate, peer dynamics, and forward error correction protection to capture the heterogeneous packet loss behavior of individual substreams transmitted over a mesh network. The evaluation of candidate peers' contributions is modeled through Markov random fields to significantly reduce complexity. Simulation results demonstrate that our peer selection scheme significantly mitigates packet loss in a mesh-based P2P network, compared to other state-of-the-art schemes."
      }
    ]
  },
  {
    "tid":"354",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Convergence of Time-Stepping Schemes for Passive and Extended Linear Complementarity Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Generalizing recent results in [M. K. Camlibel, Complementarity Methods in the Analysis of Piecewise Linear Dynamical Systems, Ph.D. thesis, Center for Economic Research, Tilburg University, Tilburg, The Netherlands, 2001], [M. K. Camlibel, W. P. M. H. Heemels, and J. M. Schumacher, IEEE Trans. Circuits Systems I: Fund. Theory Appl., 49 (2002), pp. 349-357], and [J.-S. Pang and D. Stewart, Math. Program. Ser. A, 113 (2008), pp. 345-424], this paper provides an in-depth analysis of time-stepping methods for solving initial-value and boundary-value, non-Lipschitz linear complementarity systems (LCSs) under passivity and broader assumptions. The novelty of the methods and their analysis lies in the use of \u201cleast-norm solutions\u201d in the discrete-time linear complementarity subproblems arising from the numerical scheme; these subproblems are not necessarily monotone and are not guaranteed to have convex solution sets. Among the principal results, it is shown that, using such least-norm solutions of the discrete-time subproblems, an implicit Euler scheme is convergent for passive initial-value LCSs; generalizations under a strict copositivity assumption and for boundary-value LCSs are also established."
      }
    ]
  },
  {
    "tid":"355",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Transcending Hardware Limits with Software Out-of-Order Processing.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Building high-performance, next-generation processors require novel techniques to enable improved performance given today&#39;s power- and energy-efficiency requirements. Additionally, a widening gap between processor and memory performance makes it even more difficult to improve efficiency with conventional techniques. While out-of-order architectures attempt to hide this memory latency with dynamica..."
      }
    ]
  },
  {
    "tid":"356",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Juggling the Gadgets: Binary-level Code Randomization using Instruction Displacement.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Code diversification is an effective mitigation against return-oriented programming attacks, which breaks the assumptions of attackers about the location and structure of useful instruction sequences, known as \\\"gadgets\\\". Although a wide range of code diversification techniques of varying levels of granularity exist, most of them rely on the availability of source code, debug symbols, or the assumption of fully precise code disassembly, limiting their practical applicability for the protection of closed-source third-party applications. In-place code randomization has been proposed as an alternative binary-compatible diversification technique that is tolerant of partial disassembly coverage, in the expense though of leaving some gadgets intact, at the disposal of attackers. Consequently, the possibility of constructing robust ROP payloads using only the remaining non-randomized gadgets is still open. In this paper we present instruction displacement, a code diversification technique based on static binary instrumentation that does not rely on complete code disassembly coverage. Instruction displacement aims to improve the randomization coverage and entropy of existing binary-level code diversification techniques by displacing any remaining non-randomized gadgets to random locations. The results of our experimental evaluation demonstrate that instruction displacement reduces the number of non-randomized gadgets in the extracted code regions from 15.04% for standalone in-place code randomization, to 2.77% for the combination of both techniques. At the same time, the additional indirection introduced due to displacement incurs a negligible runtime overhead of 0.36% on average for the SPEC CPU2006 benchmarks."
      }
    ]
  }
]