[
  {
    "tid":"0",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A trajectory splitting model for efficient spatio-temporal indexing\nContent: This paper addresses the problem of splitting trajectories optimally for the purpose of efficiently supporting spatio-temporal range queries using index structures (e.g., R-trees) that use minimum bounding hyper-rectangles as trajectory approximations. We derive a formal cost model for estimating the number of I/Os required to evaluate a spatio-temporal range query with respect to a given query size and an arbitrary split of a trajectory. Based on the proposed model, we introduce a dynamic programming algorithm for splitting a set of trajectories that minimizes the number of expected disk I/Os with respect to an average query size. In addition, we develop a linear time, near optimal solution for this problem to be used in a dynamic case where trajectory points are continuously updated. Our experimental evaluation confirms the effectiveness and efficiency of our proposed splitting policies when embedded into an R-tree structure.\n\nTitle: Independent Quantization: An Index Compression Technique for High-Dimensional Data Spaces\nContent: Two major approaches have been proposed to efficiently process queries in databases: Speeding up the search by using index structures, and speeding up the search by operating on a compressed database, such as a signature file. Both approaches have their limitations: Indexing techniques are inefficient in extreme configurations, such as high-dimensional spaces, where even a simple scan may be cheaper than an index-based search. Compression techniques are not very efficient in all other situations. We propose to combine both techniques to search for nearest neighbors in a high-dimensional space.For this purpose, we develop a compressed index, called the IQ-tree, with a three-level structure: The first level is a regular (flat) directory consisting of minimum bounding boxes, the second level contains data points in a compressed representation, and the third level contains the actual data.We overcome several engineering challenges in constructing an effective index structure of this type. The most significant of these is to decide how much to compress at the second level. Too much compression will lead to many needless expensive accesses to the third level. Too little compression will increase both the storage and the access cost for the first two levels.We develop a cost model and an optimization algorithm based on this cost model that permits an independent determination of the degree of compression for each second level page to minimize expected query cost. In an experimental evaluation, we demonstrate that the IQ-tree shows a performance that is the \"best of both worlds\" for a wide range of data distributions and dimensionalities.\n\nTitle: Efficiently Supporting Multiple Similarity Queries for Mining in Metric Databases\nContent: Metric databases are databases where a metric distance function is defined for pairs of database objects. In such databases, similarity queries in the form of range queries or k-nearest neighbor queries are the most important queries. In traditional query processing, single queries are issued independently by different users. In many data mining applications, however, the database is typically explored by iteratively asking similarity queries for answers of previous similarity queries.In this paper, we introduce a generic scheme for such data mining algorithms and we investigate two orthogonal approaches, reducing I/O cost as well as CPU cost, to speed-up the processing of multiple similarity queries. The proposed techniques apply to any type of similarity query and to an implementation based on an index or using a sequential scan. Parallelization yields an additional impressive speed-up. An extensive performance evaluation confirms the efficiency of our approach.\n\nTitle: Incremental and effective data summarization for dynamic hierarchical clustering\nContent: Mining informative patterns from very large, dynamically changing databases poses numerous interesting challenges. Data summarizations (e.g., data bubbles) have been proposed to compress very large static databases into representative points suitable for subsequent effective hierarchical cluster analysis. In many real world applications, however, the databases dynamically change due to frequent insertions and deletions, possibly changing the data distribution and clustering structure over time. Completely reapplying both the data summarization and the clustering algorithm to detect the changes in the clustering structure and update the uncovered data patterns following such deletions and insertions is prohibitively expensive for large fast changing databases. In this paper, we propose a new scheme to maintain data bubbles incrementally. By using incremental data bubbles, a high-quality hierarchical clustering is quickly available at any point in time. In our scheme, a quality measure for incremental data bubbles is used to identify data bubbles that do not compress well their underlying data points after certain insertions and deletions. Only these data bubbles are re-built using efficient split and merge operations. An extensive experimental evaluation shows that the incremental data bubbles provide significantly faster data summarization than completely re-building the data bubbles after a certain number of insertions and deletions, and are effective in preserving (and in some cases even improving) the quality of the data summarization.\n\nTitle: Multiple Similarity Queries: A Basic DBMS Operation for Mining in Metric Databases\nContent: Metric databases are databases where a metric distance function is defined for pairs of database objects. In such databases, similarity queries in the form of range queries or k-nearest-neighbor queries are the most important query types. In traditional query processing, single queries are issued independently by different users. In many data mining applications, however, the database is typically explored by iteratively asking similarity queries for answers of previous similarity queries. In this paper, we introduce a generic scheme for such data mining algorithms and we investigate two orthogonal approaches, reducing I/O cost as well as CPU cost, to speed-up the processing of multiple similarity queries. The proposed techniques apply to any type of similarity query and to an implementation based on an index or using a sequential scan. Parallelization yields an additional impressive speed-up. An extensive performance evaluation confirms the efficiency of our approach.\n\n## Task\nCompose an abstract for the title 'PIST: An Efficient and Practical Indexing Technique for Historical Spatio-Temporal Point Data' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Despite pressing need, current relational database management systems (RDBMS) support for spatio-temporal data is limited and inadequate, and most existing spatio-temporal indices cannot be readily integrated into existing RDBMSs. This paper proposes a practical index for spatio-temporal (PIST) data, an indexing technique, rather than a new indexing structure, for historical spatio-temporal data points that can be fully integrated within existing RDBMSs. PIST separates the spatial and temporal components of the data. For the spatial component, we develop a formal cost model and a partitioning strategy that leads to an optimal space partitioning for uniformly distributed data and an efficient heuristic partitioning for arbitrary data distributions. For the temporal component of the data a B\u9a74+\u9a74-tree is used. We show that this layer's performance can be maximized if an optimal maximal temporal range is enforced, and we present a procedure to determine such an optimal value. Being fully mapped onto a RDBMS, desirable and important properties, such as concurrency control, are immediately inherited by PIST. Using ORACLE as our implementation platform we perform extensive experiments with both real and synthetic datasets comparing its performance against other RDBMS-based options, as well as the MV3R-tree. PIST outperforms the former by at least one order of magnitude, and is competitive or better with respect to the latter, with the unarguable advantage that it can readily used on top of virtually any existing RDBMS."
      }
    ]
  },
  {
    "tid":"1",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Adaptation Regularization: A General Framework for Transfer Learning\nContent: Domain transfer learning, which learns a target classifier using labeled data from a different distribution, has shown promising value in knowledge discovery yet still been a challenging problem. Most previous works designed adaptive classifiers by exploring two learning strategies independently: distribution adaptation and label propagation. In this paper, we propose a novel transfer learning framework, referred to as Adaptation Regularization based Transfer Learning (ARTL), to model them in a unified way based on the structural risk minimization principle and the regularization theory. Specifically, ARTL learns the adaptive classifier by simultaneously optimizing the structural risk functional, the joint distribution matching between domains, and the manifold consistency underlying marginal distribution. Based on the framework, we propose two novel methods using Regularized Least Squares (RLS) and Support Vector Machines (SVMs), respectively, and use the Representer theorem in reproducing kernel Hilbert space to derive corresponding solutions. Comprehensive experiments verify that ARTL can significantly outperform state-of-the-art learning methods on several public text and image datasets.\n\nTitle: Transferring multi-device localization models using latent multi-task learning\nContent: In this paper, we propose a latent multi-task learning algorithm to solve the multi-device indoor localization problem. Traditional indoor localization systems often assume that the collected signal data distributions are fixed, and thus the localization model learned on one device can be used on other devices without adaptation. However, by empirically studying the signal variation over different devices, we found this assumption to be invalid in practice. To solve this problem, we treat multiple devices as multiple learning tasks, and propose a multi-task learning algorithm. Different from algorithms assuming that the hypotheses learned from the original data space for related tasks can be similar, we only require the hypotheses learned in a latent feature space are similar. To establish our algorithm, we employ an alternating optimization approach to iteratively learn feature mappings and multi-task regression models for the devices. We apply our latent multi-task learning algorithm to real-world indoor localization data and demonstrate its effectiveness.\n\nTitle: Domain Adaptation via Transfer Component Analysis\nContent: Domain adaptation allows knowledge from a source domain to be transferred to a different but related target domain. Intuitively, discovering a good feature representation across domains is crucial. In this paper, we first propose to find such a representation through a new learning method, transfer component analysis (TCA), for domain adaptation. TCA tries to learn some transfer components across domains in a reproducing kernel Hilbert space using maximum mean miscrepancy. In the subspace spanned by these transfer components, data properties are preserved and data distributions in different domains are close to each other. As a result, with the new representations in this subspace, we can apply standard machine learning methods to train classifiers or regression models in the source domain for use in the target domain. Furthermore, in order to uncover the knowledge hidden in the relations between the data labels from the source and target domains, we extend TCA in a semisupervised learning setting, which encodes label information into transfer components learning. We call this extension semisupervised TCA. The main contribution of our work is that we propose a novel dimensionality reduction framework for reducing the distance between domains in a latent space for domain adaptation. We propose both unsupervised and semisupervised feature extraction approaches, which can dramatically reduce the distance between domain distributions by projecting data onto the learned transfer components. Finally, our approach can handle large datasets and naturally lead to out-of-sample generalization. The effectiveness and efficiency of our approach are verified by experiments on five toy datasets and two real-world applications: cross-domain indoor WiFi localization and cross-domain text classification.\n\nTitle: Supervised representation learning: transfer learning with deep autoencoders\nContent: Transfer learning has attracted a lot of attention in the past decade. One crucial research issue in transfer learning is how to find a good representation for instances of different domains such that the divergence between domains can be reduced with the new representation. Recently, deep learning has been proposed to learn more robust or higherlevel features for transfer learning. However, to the best of our knowledge, most of the previous approaches neither minimize the difference between domains explicitly nor encode label information in learning the representation. In this paper, we propose a supervised representation learning method based on deep autoencoders for transfer learning. The proposed deep autoencoder consists of two encoding layers: an embedding layer and a label encoding layer. In the embedding layer, the distance in distributions of the embedded instances between the source and target domains is minimized in terms of KL-Divergence. In the label encoding layer, label information of the source domain is encoded using a softmax regression model. Extensive experiments conducted on three real-world image datasets demonstrate the effectiveness of our proposed method compared with several state-of-the-art baseline methods.\n\nTitle: Supervised Representation Learning with Double Encoding-Layer Autoencoder for Transfer Learning.\nContent: Transfer learning has gained a lot of attention and interest in the past decade. One crucial research issue in transfer learning is how to find a good representation for instances of different domains such that the divergence between domains can be reduced with the new representation. Recently, deep learning has been proposed to learn more robust or higher-level features for transfer learning. In this article, we adapt the autoencoder technique to transfer learning and propose a supervised representation learning method based on double encoding-layer autoencoder. The proposed framework consists of two encoding layers: one for embedding and the other one for label encoding. In the embedding layer, the distribution distance of the embedded instances between the source and target domains is minimized in terms of KL-Divergence. In the label encoding layer, label information of the source domain is encoded using a softmax regression model. Moreover, to empirically explore why the proposed framework can work well for transfer learning, we propose a new effective measure based on autoencoder to compute the distribution distance between different domains. Experimental results show that the proposed new measure can better reflect the degree of transfer difficulty and has stronger correlation with the performance from supervised learning algorithms (e.g., Logistic Regression), compared with previous ones, such as KL-Divergence and Maximum Mean Discrepancy. Therefore, in our model, we have incorporated two distribution distance measures to minimize the difference between source and target domains in the embedding representations. Extensive experiments conducted on three real-world image datasets and one text data demonstrate the effectiveness of our proposed method compared with several state-of-the-art baseline methods.\n\n\n\n## Task\nCompose an abstract for the title 'Ensemble of Anchor Adapters for Transfer Learning' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In the past decade, there have been a large number of transfer learning algorithms proposed for various real-world applications. However, most of them are vulnerable to \\emph{negative transfer}\\footnote{Here we say the ``negative transfer\" occurs when the accuracy from transfer learning algorithm is lower than the one of supervised learning algorithm, i.e., Logistic Regression (LR) in this paper.} when their performances are even worse than traditional supervised models. Aiming at more robust transfer learning models, we propose an ENsemble framework of \\emph{anCHOR adapters} (ENCHOR for short), in which an \\emph{anchor adapter} adapts the features of instances (from both source and target domains) based on their similarities to a specific \\emph{anchor} (an instance selected from source or target domains). Specifically, the more similar to the anchor instance, the higher degree of the original feature of an instance remains unchanged in the adapted representation, and vice versa. This adapted representation for the data actually expresses the local structure around the corresponding anchor, and then any transfer learning method can be applied to this adapted representation for a prediction model, which focuses more on the neighborhood of the anchor. Next, based on multiple anchors, multiple anchor adapters can be built and combined into an ensemble for final output. Additionally, we develop an effective measure to select the anchors for ensemble building to achieve further performance improvement. Extensive experiments on hundreds of text classification tasks are conducted to demonstrate the effectiveness of ENCHOR. The results show that: when traditional supervised models perform poorly, ENCHOR (based on only 8 selected anchors) achieves $6\\%-13\\%$ increase in terms of average accuracy compared with the state-of-the-art methods, and it greatly alleviates negative transfer."
      }
    ]
  },
  {
    "tid":"2",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Optimal Selection of Adaptive Streaming Representations\nContent: Adaptive streaming addresses the increasing and heterogeneous demand of multimedia content over the Internet by offering several encoded versions for each video sequence. Each version (or representation) is characterized by a resolution and a bit rate, and it is aimed at a specific set of users, like TV or mobile phone clients. While most existing works on adaptive streaming deal with effective playout-buffer control strategies on the client side, in this article we take a providers' perspective and propose solutions to improve user satisfaction by optimizing the set of available representations. We formulate an integer linear program that maximizes users' average satisfaction, taking into account network dynamics, type of video content, and user population characteristics. The solution of the optimization is a set of encoding parameters corresponding to the representations set that maximizes user satisfaction. We evaluate this solution by simulating multiple adaptive streaming sessions characterized by realistic network statistics, showing that the proposed solution outperforms commonly used vendor recommendations, in terms of user satisfaction but also in terms of fairness and outage probability. The simulation results show that video content information as well as network constraints and users' statistics play a crucial role in selecting proper encoding parameters to provide fairness among users and to reduce network resource usage. We finally propose a few theoretical guidelines that can be used, in realistic settings, to choose the encoding parameters based on the user characteristics, the network capacity and the type of video content.\n\nTitle: Optimal Set of 360-Degree Videos for Viewport-Adaptive Streaming.\nContent: With the decreasing price of Head-Mounted Displays (HMDs), 360-degree videos are becoming popular. The streaming of such videos through the Internet with state of the art streaming architectures requires, to provide high immersion feeling, much more bandwidth than the median user's access bandwidth. To decrease the need for bandwidth consumption while providing high immersion to users, scientists and specialists proposed to prepare and encode 360-degree videos into quality-variable video versions and to implement viewport-adaptive streaming. Quality-variable versions are different versions of the same video with non-uniformly spread quality: there exists some so-called Quality Emphasized Regions (QERs). With viewport-adaptive streaming the client, based on head movement prediction, downloads the video version with the high quality region closer to where the user will watch. In this paper we propose a generic theoretical model to find out the optimal set of quality-variable video versions based on traces of head positions of users watching a 360-degree video. We propose extensions to adapt the model to popular quality-variable version implementations such as tiling and offset projection. We then solve a simplified version of the model with two quality levels and restricted shapes for the QER. With this simplified model, we show that an optimal set of four quality-variable video versions prepared by a streaming server, together with a perfect head movement prediction, allow for 45% bandwidth savings to display video with the same average quality as state of the art solutions or allows an increase of 102% of the displayed quality for the same bandwidth budget.\n\n\n\nTitle: Optimal Delivery of Rate-Adaptive Streams in Underprovisioned Networks\nContent: The growth of Internet video traffic imposes a severe capacity problem in today's Content Delivery Network (CDN). Rate-adaptive streaming technologies, such as the Dynamic Adaptive Streaming over HTTP (DASH) standard, reinforces this problem in the core CDN infrastructure since delivering one video means delivering multiple representations for an aggregated bit-rate that is commonly over 10 Mbps. In this paper, we explore better trade-offs between CDN infrastructure cost and Quality of Experience (QoE) of the end-users for live broadcast video streaming applications. We consider in particular underprovisioned CDN networks, our goal being to maximize the QoE for the population of heterogeneous end-users despite the lack of resources in the intermediate CDN equipments. We show that previous theoretical models based on elastic bit-rates do not fit for this context. We propose a user-centric discretized streaming model where the satisfaction of end-users is related to the context and where a stream has to be either delivered in its entirety, or not delivered at all. We first formulate an Integer Linear Program (ILP) that achieves the optimal delivery through a multi-tree delivery overlay. The evaluation of the ILP shows the benefits of this model. We then design a practical system by revisiting the three main algorithms implemented in CDN: user-to-server assignment, content placement and content delivery. At last, we use a realistic trace-driven large-scale simulator to study the performances of our system. In particular, we show that the population of users is reasonably well served (three quarters of the population do not experience degradation) even when the CDN infrastructure experiences a severe underprovisioning (less than half of the required infrastructure).\n\nTitle: Optimal tree packing for discretized live rate-adaptive streaming in CDN\nContent: Rate-adaptive streaming technologies, such as the Dynamic Adaptive Streaming over HTTP (DASH) standard, provides an efficient and easy solution to stream multimedia in a heterogenous context. However, it reinforces the streaming capacity problem in the core Content Delivery Network (CDN) infrastructure since delivering one video means delivering an aggregation of multiple representations. In particular, for live rate-adaptive streaming, a large set of non-divisible data streams need to be either delivered in whole, or not delivered at all. Previous theoretical models that deal with streaming capacity problems are based on elastic bit rates, and do not capture these emerging features faced by today's CDNs. In this paper, we identify a new, discretized streaming model, for live rate-adaptive video delivery in CDNs. For this model we formulate a general optimization problem and show that it is NP-complete. Then we study two fundamental scenarios that occur in real CDNs. For each of these scenarios, we present a fast, easy to implement, and near-optimal algorithm with performance approximation ratios that are negligible for large networks. These are the first sets of results for the discretized streaming model, and have both practical and theoretical importance in a topic that has become critical.\n\nTitle: Dynamic adaptive streaming for multi-viewpoint omnidirectional videos.\nContent: Full immersion inside a Virtual Reality (VR) scene requires six Degrees of Freedom (6DoF) applications where the user is allowed to perform translational and rotational movements within the virtual space. The implementation of 6DoF applications is however still an open question. In this paper we study a multi-viewpoint (MVP) 360-degree video streaming system, where a scene is simultaneously captured by multiple omnidirectional video cameras. The user can only switch positions to predefined viewpoints (VPs). We focus on the new challenges that are introduced by adaptive MVP 360-degree video streaming. We introduce several options for video encoding with existing technologies, such as High Efficiency Video Coding (HEVC) and for the implementation of VP switching. We model three video-segment download strategies for an adaptive streaming client into Mixed Integer Linear Programming (MILP) problems: an omniscient download scheduler; one where the client proactively downloads all VPs to guarantee fast VP switch; one where the client reacts to the user's navigation pattern. We recorded a one MVP 360-degree video with three VPs, implemented a mobile MVP 360-degree video player, and recorded the viewing patterns of multiple users navigating the content. We solved the adaptive streaming optimization problems on this video considering the collected navigation traces. The results emphasize the gains obtained by using tiles in terms of objective quality of the delivered content. They also emphasize the importance of performing further study on VP switching prediction to reduce the bandwidth consumption and to measure the impact of VP switching delay on the subjective Quality of Experience (QoE).\n\n\n\n## Task\nCompose an abstract for the title 'Optimal set of video representations in adaptive streaming' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Adaptive streaming addresses the increasing and heterogenous demand of multimedia content over the Internet by offering several streams for each video. Each stream has a different resolution and bit rate, aimed at a specific set of users, e.g., TV, mobile phone. While most existing works on adaptive streaming deal with optimal playout-control strategies at the client side, in this paper we concentrate on the providers' side, showing how to improve user satisfaction by optimizing the encoding parameters. We formulate an integer linear program that maximizes users' average satisfaction, taking into account the network characteristics, the type of video content, and the user population. The solution of the optimization is a set of encoding parameters that outperforms commonly used vendor recommendations, in terms of user satisfaction and total delivery cost. Results show that video content information as well as network constraints and users' statistics play a crucial role in selecting proper encoding parameters to provide fairness among users and reduce network usage. By combining patterns common to several representative cases, we propose a few practical guidelines that can be used to choose the encoding parameters based on the user base characteristics, the network capacity and the type of video content."
      }
    ]
  },
  {
    "tid":"3",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Compile-time memory reuse in logic programming languages through update in place\nContent: Standard implementation techniques for single-assignment languages modify a data structure without destroying the original, which may subsequently be accessed. Instead a variant structure is created by using newly allocated cells to represent the changed portion and to replace any cell that references a newly allocated cell. The rest of the original structure is shared by the variant. The effort required to leave the original uncorrupted is unnecessary when the program will never reference the original again. This effort includes allocating and initializing new cells, as well as garbage collecting replaced cells. This article specifies a transformation system that introduces update-in-place operations, making Prolog programs update recursive data structures much as an imperative program would. The article introduces the notion of a reuse map, which formalizes reallocation decisions. Because optimal memory reuse is intractable, a heuristical method is presented that performs well in practice. Small Prolog programs that manipulate recursive data structures have their speed increased up to about five times (naive recurse), not counting any speedup obtained by avoiding garbage collection. Quicksort is about three times as fast, merge-sort about one and a half, matrix transposition about twice, and Gaussian elimination is about 1.2 times as fast.\n\nTitle: A confluent semantic basis for the analysis of concurrent constraint logic programs\nContent: The standard operational semantics of concurrent constraint logic languages is not confluent in the sense that different schedulings of processes may result in different program behaviors. While implementations are free to choose specific scheduling policies, analyses should be correct for all implementations. Moreover, in the presence of parallelism, it is usually not possible to determine how processes will actually be scheduled. Efficient program analysis is therefore difficult as all process schedulings must be considered. To overcome this problem, we introduce a confluent semantics which closely approximates the standard (nonconfluent) semantics. This semantics provides a basis for efficient and accurate program analysis for these languages. To illustrate the usefulness of this approach, we sketch analyses based on abstract interpretations of the confluent semantics which determine if a program is suspension- and local suspension-free.\n\nTitle: Demand transformation analysis for concurrent constraint programs\nContent: This paper presents a demand transformation analysis that maps a predicate's output demands to its input demands. This backward dataflow analysis for concurrent constraint programs is constructed in the framework of abstract interpretation. In the context of stream parallelism, this analysis identifies an amount of input data for which predicate execution can safely wait without danger of introducing deadlock. We assume that programs are well-moded and prove that our analysis is safe. We have constructed an implementation of this analysis and tested it on some small, illustrative programs and have determined that it gives useful results in practice. We identify several applications of the analysis results to distributed implementations of concurrent constraint languages, including thread construction and communication granularity control. This analysis will enable existing computational cost estimation analyses to be applied to stream-parallel logic languages.\n\nTitle: Optimal groundness analysis using propositional logic\nContent: It is well known that propositional formulas form a useful and computationally efficient abstract interpretation for different data-flow analyses of logic programs and, in particular, for groundness analysis. This article gives a complete and precise description of an abstract interpretation, called Prop, composed of a domain of positive, propositional formulas and three operations: abstract unification, least upper bound, and abstract projection. All three abstract operations are known to be correct. They are shown to be optimal in the classical sense. Two alternative stronger notions of optimality of abstract operations are introduced, which characterize very precise analyses. We determine whether the operations of Prop also satisfy these stronger forms of optimality.\n\nTitle: Prop revisited: propositional formula as abstract domain for groundness analysis\nContent: The abstract domain Prop for analyzing variable groundness in logic programs is considered. This domain consists of (equivalence classes of) propositional formulas whose propositional variables correspond to program variables with truth assignments indicating which program variables are ground. Some ambiguity remains about precisely which formula should be included in Prop so that all interesting sets of program execution states (substitutions) have a unique representation. This ambiguity is clarified by characterizing, both semantically and syntactically, the appropriate definition of Prop. The use of propositional formulas for representing properties of substitutions of a different type than groundness, such as freeness and independence of variables, is discussed\n\n## Task\nCompose an abstract for the title 'Live-structure dataflow analysis for Prolog' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"For the class of applicative programming languages, efficient methods for reclaiming the memory occupied by released data structures constitute an important aspect of current implementations. The present article addresses the problem of memory reuse for logic programs through program analysis rather than by run-time garbage collection. The aim is to derive run-time properties that can be used at compile time to specialize the target code for a program according to a given set of queries and to automatically introduce destructive assignments in a safe and transparent way so that fewer garbage cells are created.The dataflow analysis is constructed as an application of abstract interpretation for logic programs. An abstract domain for describing structure-sharing and liveness properties is developed as are primitive operations that guarantee a sound and terminating global analysis. We explain our motivation for the design of the abstract domain, make explicit the underlying implementation assumptions, and discuss the precision of the results obtained by a prototype analyzer."
      }
    ]
  },
  {
    "tid":"4",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A simple feature generation method based on fisher linear discriminant analysis\nContent: This paper presents a new iterative algorithm for feature generation, which is approximately derived based on geometrical interpretation of the Fisher linear discriminant (FLD) analysis. In a field of pattern recognition or signal processing, the principal component analysis (PCA) is popular for data compression and feature extraction. Furthermore, iterative learning algorithms for obtaining eigenvectors in PCA have been presented in a field of pattern recognition, image analysis, and neural networks. Their effectiveness has been demonstrated in many applications. However, recently the FLD analysis has been used in many fields, especially face image analysis. The drawback of FLD is a long computational time in compression of a large-sized between-class covariance and within-class covariance matrices. Usually FLD has to carry out minimization of a within-class variance. However in this case the inverse matrix of the within-class covariance matrix cannot be obtained, since data dimension is higher than the number of data and then it includes many zero eigenvalues. In order to overcome this difficulty, a new iterative feature generation method, a simple FLD is introduced and its effectiveness is demonstrated.\n\nTitle: Feature generation by simple FLD\nContent: This paper presents a new algorithm for feature generation, which is approximately derived based on geometrical interpretation of the Fisher linear discriminant analysis. In a field of pattern recognition or signal processing, the principal component analysis (PCA) is often used for data compression and feature extraction. Furthermore, iterative learning algorithms for obtaining eigenvectors have been presented in pattern recognition and image analysis. Their effectiveness has been demonstrated on computational time and pattern recognition accuracy in many applications. However, recently the Fisher linear discriminant (FLD) analysis has been used in such a field, especially face image analysis. The drawback of FLD is a long computational time in compression of large-sized between-class and within-class covariance matrices. Usually FLD has to carry out minimization of a within-class variance. However in this case the inverse matrix of the within-class covariance matrix cannot be obtained, since data dimension is higher than the number of data and then it includes many zero eigenvalues. In order to overcome this difficulty, a new iterative feature generation method, a simple FLD is introduced and its effectiveness is demonstrated.\n\nTitle: Fast Statistical Learning with Kernel-Based Simple-FDA\nContent: In this paper, new statistical learning algorithms with kernel function are presented. Recently, iterative learning algorithms for obtaining eigenvectors in the principal component analysis (PCA) have been presented in the field of pattern recognition and neural network. However, the Fisher linear discriminant analysis (FLDA) has been used in many fields, especially face image analysis. The drawback of FLDA is a long computational time based on a large-sized covariance matrix and the issue that the within-class covariance matrix is usually singular. In order to overcome this difficulty, we proposed the feature generation method Simple-FLDA which is approximately derived from geometrical interpretation of FLDA. This algorithm is similar to Simple-PCA and does not need matrix operation. In this paper, new statistical kernel based learning algorithms are presented. They are extended versions of Simple-PCA and Simple-FLDA to nonlinear space using the kernel function. Their preliminary simulation results are given for a simple face recognition problem.\n\nTitle: A Feature Extraction Method for Personal Identification System\nContent: Recently in the world, many researches for individual identification method using biometrics axe widely done. Especially, personal identification using faces axe used because of needless,for physical contact. However, when the number of registrant of a system increase, the recognition accuracy of system will get worse certainly. Therefore, in order to improve the recognition accuracy, it is necessary,to extract the feature area effectively for getting the high recognition are accuracy. In this paper, we analyze and examine about the individual feature in a face using the GA and the SPCA. Thus, by removing the area which is not valuable, we think that recognition accuracy becomes high. Then, in order to show the effectiveness of the proposed method, we,show computer simulations by using real image.\n\nTitle: Fast statistical learning algorithm for feature generation\nContent: This paper presents an improved statistical learning algorithm for feature generation in pattern recognition and signal processing. It is approximately derived from geometrical interpretation of the Fisher linear discriminant analysis (FLDA). The principal component analysis (PCA) is popular for data compression and feature extraction. Furthermore, iterative learning algorithms for obtaining eigenvectors in PCA have been presented in such fields. Their effectiveness has been demonstrated in many applications. However, recently FLDA has been often used in many fields, especially face image recognition. The drawback of FLDA is a long computational time based on a large-sized covariance matrix and the issue that the within-class covariance matrix is usually singular. Generally, in FLDA, the inverse matrix of the within-class covariance matrix cannot be obtained, since data dimension is generally higher than the number of data and then it includes many zero eigenvalues. In order to overcome this difficulty, a new iterative feature generation method, the simple-FLDA was proposed by authors. In this paper, further improvement is introduced into the simple-FLDA and its effectiveness is demonstrated for preliminary personal identification problem.\n\n## Task\nCompose an abstract for the title 'Feature Generation by Simple-FLDA for Pattern Recognition' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, a new feature generation method for pattern recognition is proposed, which is approximately derived from geometrical interpretation of the Fisher linear discriminant analysis (FLDA). In a field of pattern recognition or signal processing, the principal component analysis (PCA) is popular for data compression and feature extraction. Furthermore, iterative learning algorithms for obtaining eigenvectors in PCA have been presented in such fields, including neural networks. Their effectiveness has been demonstrated in many applications. However, recently the FLDA has been used in many fields, especially face image analysis. The drawback of FLDA is a long computational time based on a large-sized covariance matrix and the issue that the within-class covariance matrix is usually singular. Generally FLDA has to carry out minimization of a within-class variance. However in this case the inverse matrix of the within-class covariance matrix cannot be obtained, since data dimension is generally higher than the number of data and then it includes many zero eigenvalues. In order to overcome this difficulty, a new iterative feature generation method, a simple FLDA is introduced and its effectiveness is demonstrated for pattern recognition problems."
      }
    ]
  },
  {
    "tid":"5",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Variability in software architecture: views and beyond\nContent: Variability (the ability of a software system or software artifact to be adapted for use in a specific context) is reflected in and facilitated through the software architecture. The Second International Workshop on Variability in Software Architecture (VARSA) was held in conjunction with the Joint 10th Working IEEE/IFIP Conference on Software Architecture & 6th European Conference on Software Architecture 2012 in Helsinki, Finland. The workshop aimed at exploring current and emerging methods, languages, notations technologies and tools to model, implement, and manage variability in the software architecture. It featured one industrial talk, five research paper presentations, and three working group discussions. Working groups discussed topics that emerged during the workshop. This report summarizes the themes of the workshop and presents the results of the working group discussions.\n\nTitle: Variability in software architecture: current practice and challenges\nContent: Variability in software-intensive systems is usually understood as the ability of a software artifact to be changed in order to fit different contexts, environments, or purposes. Software architecture on the other hand determines the structure of a software system, and is described in an architecture description. This description includes the major stakeholders of a software system and their concerns. Variability is reflected in and facilitated through the software architecture. The First International Workshop on Variability in Software Architecture (VARSA) was held jointly with WICSA 2011 in Boulder, Colorado. The goal of the workshop was to explore and advance the state-of-the art in variability in software architecture. It featured four research paper presentations, two invited talks, and three working groups that discussed specific topics. This report summarizes the themes of the workshop, presents the results of the working group discussions, and suggests topics for further research.\n\nTitle: Handling Variability in Software Architecture: Problems and Implications\nContent: Variability helps manage differences and commonalities within and across software systems. As variability is reflected in and facilitated through the software architecture, it is important to understand the problems that architects face when carrying out their tasks. This would help us improve methods for architecting variability-intensive software systems. In this paper, we therefore present an exploratory study to identify problems that occur when performing variability-related tasks during software architecting. Our subjects were 27 graduate students. We identified eleven problems as experienced by the subjects of our study. The paper also presents implications of the findings for the software architecture field. In particular, we discuss implications for architecture description, methods and tools, and the training of architects.\n\nTitle: The notion of variability in software architecture: results from a preliminary exploratory study\nContent: Context: In the software product line domain, the concept of variability is well recognized. However, variability in the context of software architecture still seems to be poorly understood. Objective: In this paper, we aim at contributing to the development of a basic understanding of the notion of variability in the software architecture domain, beyond the idea of product lines. Method: We perform a preliminary exploratory study which consists of two parts: an expert survey among 11 subjects, and a mini focus group with 4 participants. For both parts, we collect and analyze mostly qualitative data. Results: Our observations indicate that there seems to be no common understanding of \"variability\" in the context of software architecture. On the other hand, some challenges related to variability in software architecture are similar to challenges identified in the product line domain. Conclusions: Variability in software architecture might require more theoretical foundations in order to establish \"variability\" as an architectural key concept and first-class quality attribute.\n\nTitle: Variability and Complexity in Software Design: Towards a Research Agenda.\nContent: Many of today\u00e2\u0080\u0099s software systems accommodate different usage and deployment scenarios. Intentional and unintentional variability in functionality or quality attributes (e.g., performance) of software significantly increases the complexity of the problem and design space of those systems. The complexity caused by variability becomes increasingly difficult to handle due to the increasing size of software systems, new and emerging application domains, dynamic operating conditions under which software systems have to operate, fast moving and highly competitive markets, and more powerful and versatile hardware. This paper reports results of the first International Workshop on Variability and Complexity in Software Design that brought together researchers and engineers interested in the topic of complexity and variability. It also outlines directions the field might move in the future\n\n## Task\nCompose an abstract for the title 'Variability in software architecture: the road ahead' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Variability in a software system is reflected in and facilitated through the architecture of that system. The Third International Workshop on Variability in Software Architecture (VARSA) was held in conjunction with the 11th Working IEEE/IFIP Conference on Software Architecture 2014 in Sydney, Australia. Based on the findings from previous editions of VARSA, this edition aimed at exploring methods, technologies and tools to handle variability at the software architecture level. The workshop featured one industrial keynote talk, one academic keynote talk and five research paper presentations."
      }
    ]
  },
  {
    "tid":"6",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: An implementation and evaluation of the MPI 3.0 one-sided communication interface.\nContent: The Message Passing Interface MPI 3.0 standard includes a significant revision to MPI's remote memory access RMA interface, which provides support for one-sided communication. MPI-3 RMA is expected to greatly enhance the usability and performance of MPI RMA. We present the first complete implementation of MPI-3 RMA and document implementation techniques and performance optimization opportunities enabled by the new interface. Our implementation targets messaging-based networks and is publicly available in the latest release of the MPICH MPI implementation. Using this implementation, we explore the performance impact of new MPI-3 functionality and semantics. Results indicate that the MPI-3 RMA interface provides significant advantages over the MPI-2 interface by enabling increased communication concurrency through relaxed semantics in the interface and additional routines that provide new window types, synchronization modes, and atomic operations. Copyright \u00a9 2016 John Wiley & Sons, Ltd.\n\nTitle: Implementing MPI-IO Atomic Mode and Shared File Pointers Using MPI One-Sided Communication\nContent: The ROMIO implementation of the MPI-IO standard provides a portable infrastructure for use on top of a variety of underlying storage targets. These targets vary widely in their capabilities, and in some cases additional effort is needed within ROMIO to support all MPI-IO semantics. Two aspects of the interface that can be problematic to implement are MPI-IO atomic mode and the shared file pointer access routines. Atomic mode requires enforcing strict consistency semantics, and shared file pointer routines require communication and coordination in order to atomically update a shared resource. For some file systems, native locks may be used to implement these features, but not all file systems have lock support. In this work, we describe algorithms for implementing efficient mutex locks using MPI-1 and the one-sided capabilities from MPI-2. We then show how these algorithms may be used to implement both MPI-IO atomic mode and shared file pointer methods for ROMIO without requiring any features from the underlying file system. We show that these algorithms can outperform traditional file system lock approaches. Because of the portable nature of these algorithms, they are likely useful in a variety of situations where distributed locking or coordination is needed in the MPI-2 environment.\n\nTitle: Minimizing Synchronization Overhead in the Implementation of MPI One-Sided Communication\nContent: The one-sided communication operations in MPI axe intended to provide the convenience of directly accessing remote memory and the potential for higher performance than regular point-to-point communication. Our performance measurements with three MPI implementations (IBM MPI, Sun MPI, and LAM) indicate, however, that one-sided communication can perform much worse than point-to-point communication if the associated synchronization calls are not implemented efficiently. In this paper, we describe our efforts to minimize the overhead of synchronization in our implementation of one-sided communication in MPICH-2. We describe our optimizations for all three synchronization mechanisms defined in MPI: fence, post-start-complete-wait, and lock-unlock. Our performance results demonstrate that, for short messages, MPICH-2 performs six times faster than LAM for fence synchronization and 50% faster for post-start-complete-wait synchronization, and it performs more than twice as fast as Sun MPI for all three synchronization methods.\n\nTitle: Implementing byte-range locks using MPI one-sided communication\nContent: We present an algorithm for implementing byte-range locks using MPI passive-target one-sided communication. This algorithm is useful in any scenario in which multiple processes of a parallel program need to acquire exclusive access to a range of bytes. One application of this algorithm is for implementing MPI-IO\u2019s atomic-access mode in the absence of atomicity guarantees from the underlying file system. Another application is for implementing data sieving, a technique for optimizing noncontiguous writes by doing an atomic read-modify-write of a large, contiguous block of data. This byte-range locking algorithm can be used instead of POSIX fcntl file locks on file systems that do not support fcntl locks, on file systems where fcntl locks are unreliable, and on file systems where fcntl locks perform poorly. Our performance results demonstrate that the algorithm has low overhead and significantly outperforms fcntl locks on NFS file systems on a Linux cluster and on a Sun SMP.\n\nTitle: An evaluation of implementation options for MPI one-sided communication\nContent: MPI defines one-sided communication operations\u2014put, get, and accumulate\u2014together with three different synchronization mechanisms that define the semantics associated with the initiation and completion of these operations. In this paper, we analyze the requirements imposed by the MPI Standard on any implementation of one-sided communication. We discuss options for implementing the synchronization mechanisms and analyze the cost associated with each. An MPI implementer can use this information to select the implementation method that is best suited (has the lowest cost) for a particular machine environment. We also report on experiments we ran on a Linux cluster and a Sun SMP to determine the gap between the performance that could be achievable and what is actually achieved with MPI.\n\n## Task\nCompose an abstract for the title 'Leveraging MPI's one-sided communication interface for shared-memory programming' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Hybrid parallel programming with MPI for internode communication in conjunction with a shared-memory programming model to manage intranode parallelism has become a dominant approach to scalable parallel programming. While this model provides a great deal of flexibility and performance potential, it saddles programmers with the complexity of utilizing two parallel programming systems in the same application. We introduce an MPI-integrated shared-memory programming model that is incorporated into MPI through a small extension to the one-sided communication interface. We discuss the integration of this interface with the upcoming MPI 3.0 one-sided semantics and describe solutions for providing portable and efficient data sharing, atomic operations, and memory consistency. We describe an implementation of the new interface in the MPICH2 and Open MPI implementations and demonstrate an average performance improvement of 40% to the communication component of a five-point stencil solver."
      }
    ]
  },
  {
    "tid":"7",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Novel Low Power Interface Circuit Design Technique For Multiple Voltage Islands Scheme\nContent: Power has become an important concern for nanometer circuit design as well as timing characteristic. In this paper, we propose a novel low power interface circuit design technique for multiple voltage islands scheme by using output feedback, conditional switch, and pulsed dock technique. We apply our method to new types of flip-flops and combinational logics to eliminate level converters and remove redundant switching activities. Combined with multiple VTH technique, a low clock swing flip-flop is designed to verify our new method. Experimental results show that the leakage power of the new flip-flop can be reduced by an average of 58.14% in standby mode and the total power consumption can be reduced by an average of 55.76% in active mode, while the delay time stays the same.(1)\n\nTitle: A Novel Redundant Pipelined Successive Approximation Register Adc\nContent: This paper presents a 12-bit 100 MS/s time-interleaved successive approximation register (SAR) ADC designed for intermediate frequency 3G receivers. The 12-bit 100 MS/s 2-channel ADC with voltage-controlled delay lines (VCDLs) based time-domain comparator is designed in 65 nm CMOS, single-channel 12-bit 50 MS/s pipelined SAR ADC consists of a 6-bit MDAC first stage and a 7-bit SAR ADC second stage, the operational amplifier is shared between the two channels for low power dissipation. A novel redundant SAR ADC used in the first stage is proposed to avoid comparator offset issue. Moreover, tri-level flash-SAR ADC is proposed and used in the second stage, the total unit capacitors to realize a 7-bit tri-level flash-SAR ADC are 50% of the conventional two's complement flash-SAR architecture. Simulation results in 65nm CMOS process show that, the 12-bit 100 MS/s ADC achieves a signal-to-noise-and-distortion ratio (SNDR) of 73.32 dB (11.89 ENOB), a spurious free dynamic range (SFDR) of 90.65 dB with a 2.7 MHz input tone, while dissipating 10 mW from a 1.2 V supply.\n\nTitle: A 250 Ks/S, 0.8 V Ultra Low Power Successive Approximation Register Adc Using A Dynamic Rail-To-Rail Comparator\nContent: A low voltage low power successive approximation register (SAR) analog-to-digital converter (ADC) based on a novel rail-to-rail comparator is proposed in this paper. The power consumption of the comparator is significantly reduced through dynamic operation while the speed is augmented by using an efficient regenerative latch. No biasing circuits are needed and there are no floating nodes in the comparator throughout the conversion process. The digital-to-analog converter (DAC) is formed from a binary array of MIM capacitors. The 250 KS/s ADC implemented in a 0.18 mu m process consumes only 1.35 mu W of power at a supply voltage of 0.8 V.\n\nTitle: A Low-Power Fast-Settling Bond-Wire Frequency Synthesizer With a Dynamic-Bandwidth Scheme\nContent: This paper presents a low-power fast-settling phase-locked loop (PLL) frequency synthesizer working at 1.72\u20131.74 GHz for a 100 kb/s Gaussian frequency shift keying (GFSK) based transceiver suitable for wireless sensor network (WSN) applications. We propose several new techniques for lowering the power consumption in the frequency synthesizer. Resistor-based electro-static discharge protection (ESD-P) and automatic frequency calibration (AFC) are introduced to the bond-wire voltage-controlled oscillator (VCO) to deal with the inaccuracy of bond-wire inductors and uncertain parasitics. A multi-stage power-scaling scheme is developed for classical phase-switching prescaler. A dynamic-bandwidth scheme is proposed to speed up the settling time and improve the energy efficiency in a time-division WSN system. Implemented in 0.18 CMOS technology, the measurement results show that the PLL consumes 10.6 mW under 1.8 V supply voltage and settles within 18 including the AFC process. The phase noise is 89.8 dBc/Hz@10 kHz and 119.3 dBc/Hz@1 MHz under receiving (Rx) mode; from receiving (Rx) state to transmitting (Tx) state, the out-band fractional- phase noise is increased by 2.7 dB, whereas the in-band noise is lowered by 2.1 dB.\n\nTitle: An energy efficient backup scheme with low inrush current for nonvolatile SRAM in energy harvesting sensor nodes\nContent: In modern energy harvesting sensor nodes, nonvolatile SRAM (nvSRAM) has been widely investigated as a promising on-chip memory architecture because of its zero standby power, resilience to power failures, and fast read/write operations. However, conventional approaches transfer all data from SRAM into NVM during the backup process. Thus, large on-chip energy storage capacitors are normally required. In addition, high peak inrush current is generated instantaneously, which has a negative impact on energy efficiency and circuit reliability. To mitigate these problems, we propose a novel holistic backup flow, which consists of a partial backup process and a run-time pre-writeback scheme for nvSRAM based caches. A statistics based dead-block predictor is employed to achieve a fast and low power partial backup process. We also present an adaptive pre-writeback point allocation strategy to further reduce the backup load. Simulation results show that, with our proposed backup scheme, energy storage capacitance is reduced by 34% and inrush current is reduced by 54% on average compared to the conventional full backup scheme.\n\n## Task\nCompose an abstract for the title 'A Novel Low Power Time-Mode Comparator For Successive Approximation Register Adc' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"A novel low power time-mode comparator with enhanced resolution and speed is proposed in this paper. The comparator incorporates a symmetrical input time-to-digital converter (TDC) and a highly dynamic voltage-to-time converter (VTC). Energy reduction is achieved mainly through the use of capacitor discharge automatic switch-off and inverter clocking. The combined effect of the low timing requirement and capacitor voltage presetting enables significant precision and speed improvements. Simulations in a 0.18 um process show that the comparator can be clocked at 38 MHz, draws less than 0.4 pJ energy from supply and can resolve voltages as low as 10 mu V."
      }
    ]
  },
  {
    "tid":"8",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Behavioral Biometrics Based and Machine Learning Aided Framework for Academic Integrity in E-Assessment\nContent: The common approaches to academic integrity in the e-learning environment are resource-intensive and require technology and/or dedicated invigilation staff to monitor assessment activities. These approaches are observational and often external to the learning spaces where the majority of the instructional content resides. Authentic assessments such as discussions, projects and portfolios may not always undergo the same scrutiny as high-stakes examinations and therefore differ in the level of identity and authorship assurance they provide. In this paper, we propose an integrated approach to enhancing academic integrity of e-assessments. The approach is based on behavioral biometrics and aided by machine-learning techniques. It provides continuous identity and authorship assurance throughout the learning activities within the existing learning space. It can be applied to measure the degree of learner collaboration with peers and interaction with the course content, concurrently verifying learner identity and validating authorship of the academic artifacts. We present the preliminary results and discuss future directions.\n\nTitle: Prototyping a Cognitive Assessment System to Enrich the Virtualization of Collaborative Learning\nContent: Cognitive assessment in collaborative and social learning requires a broad perspective about learning and the involved processes. Assessment processes have a significant effect on collaborative learning because they engage learners through accountability and constructive feedback. However, in order to design a coherent and efficient assessment system for collaborative and social learning it is necessary to design an enriched learning experience that predisposes the feedback and awareness in the group. In this paper, collaborative learning is developed into a special environment: a Virtualized Collaborative Sessions. Two levels of assessment are incorporated, deferred and immediate time, in order to produce an efficient and personalized assessment feedback to ultimately enhance and improve the collaborative learning experience. The research reported in this paper was undertaken within the FP7 European project ALICE.\n\nTitle: Improving e-Assessment in Collaborative and Social Learning Settings\nContent: Cognitive assessment in collaborative and social learning requires assessment processes that achieve significant effect on collaborative learning and engage learners through accountability and constructive feedback. In order to design a coherent and efficient assessment system for collaborative and social learning it is necessary to design an enriched learning experience that predisposes the feedback and awareness in the group. This research focuses on e-assessment of collaborative learning and extends it with Social Network Analysis (SNA) techniques that are able to analyze and represent social network interaction during the live collaborative sessions. The interaction data extracted from social and collaborative networking must be integrated into a general assessment system to produce an efficient and personalized awareness and feedback about the collaborative activity and the social behavior of the participants. In previous work we provided a conceptual and methodological research approach of e-assessment applications and tools that meet the mentioned requirements and goals. In this paper we provide empirical data and interpretation to validate the approach.\n\nTitle: A Methodology to Evaluate Complex Learning Resources to Improve e-Assessment from Collaborative and Networking Settings\nContent: Cognitive assessment in collaborative and social learning requires a broad perspective about learning and the involved processes. Assessment processes have a significant effect on collaborative learning because they engage learners through accountability and constructive feedback. However, in order to design a coherent and efficient assessment system for collaborative and social learning it is necessary to design an enriched learning experience that predisposes the feedback and awareness in the group. This paper focuses on e-assessment of collaborative learning and extends it with Social Network Analysis (SNA) techniques that are able to analyze and represent social network interaction during the live sessions of collaboration by using collaborative complex learning resources (CC-LR). The knowledge extracted from analyzing the large amounts of interaction data related to social and collaborative networking is then integrated into a general assessment system to produce an efficient and personalized awareness and feedback about the collaborative activity and the social behavior of the participants to ultimately enhance and improve the collaborative learning experience with CC-LRs. The conceptual and methodological research approach is provided along with the evaluation design of e-assessment applications and tools that meet the mentioned requirements and goals.\n\nTitle: Developing an Information System for Monitoring Student's Activity in Online Collaborative Learning\nContent: In this paper we address the issue of monitoring students\u2019 and groups\u2019 activity in online collaborative learning environments. This issue is especially important in the collaborative e-learning context, since an efficient monitoring process can provide valuable information to online instructors who may guide and support the development of collaborative learning projects. We have developed and tested an information system model which facilitates the automatic generation of weekly monitoring reports derived from data contained in server log files. These reports provide online instructors with visual information regarding students\u2019 and groups\u2019 activity, thus allowing for a quick and easy classification of students and groups according to their activity level. Therefore, entities with a low activity level are identified as soon as possible and just-in-time assistance can be established for them. Furthermore, instructors can use these monitoring reports to forecast potential problems \u2013such as students\u2019 dropouts or possible conflicts inside the groups due to unbalanced distribution of tasks\u2013 and take operational and tactical decisions oriented to avoid them.\n\n## Task\nCompose an abstract for the title 'Open Proctor: An Academic Integrity Tool for the Open Learning Environment.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Managing integrity of continuous and authentic assessments in an open and distance environments is a complex process. The core challenge is to map student identities with their academic work in effective and efficient manners, while preserving privacy, ensuring minimal disruption, minimizing impacts on accessibility and convenience. To address this issue, we have developed a prototype cloud-based application entitled OpenProctor that employs machine-learning techniques to analyze patterns in the learner-produced textual content in order to provide identity and authorship assurance. In contrast to the traditional academic integrity approaches that seek to control the remote learning environment, our method takes advantage of the readily available learner-generated data to analyze how the students go about doing their academic work. We present the framework, the architecture and main functions of the OpenProctor system and discuss the future research directions and plans for the system's future development."
      }
    ]
  },
  {
    "tid":"9",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Automatic bone age assessment based on intelligent algorithms and comparison with TW3 method.\nContent: New algorithms are proposed to improve the validity, accuracy and practicality of automatic bone age assessment (ABAA).The concept of object-based region of interest (ROI) was proposed. Thirteen RUS (including radius, ulna and short finger bones) ROIs and seven carpal ROIs were appointed respectively according to Tanner-Whitehouse (TW3) method. Five features including size, morphologic features and fusional/adjacent stage of each ROI were extracted based on particle swarm optimization (PSO) and input into ANN classifiers. ANNs were built upon feed-forward multilayer networks and trained with back-propagation algorithm rules to process RUS and carpal features respectively. About 1046 digital left hand-wrist radiographs were randomly utilized half for training ANNs and the rest for ABAA after manual reading by TW3 method.BA comparison between observers indicated that the S.D. of RUS BA was larger than that of carpal BA (S.D.=4.40, 2.42 respectively), but interestingly, both CVs were 4.0, and both concordance rates were very high (95.5% and 94.2%), and both differences between observers were not significant (both P>0.05). We found by comparison between results of ABAA and manual readings that RUS BA had larger S.D.s than carpal BA between two methods, but the CVs were very similar in the case of carpal BA<9 years and RUS BA>or=9 years (CV=3.0, 3.1 respectively), apart from a comparatively larger CV for RUS BA<9 years (CV=3.5). Both parts of ABAA system, RUS and carpal, had very high concordance rates (97%, 93.8% and 96.5%) and no significant difference compared with manual method (all P>0.05).PSO method made image segmentation and feature extraction more valid and accurate, and the ANN models were sophisticated in processing image information. ABAA system based on intelligent algorithms had been successfully applied to all cases from 0 to 18 years of bone age.\n\nTitle: Graph Regularized Non-negative Matrix with L0-Constraints for Selecting Characteristic Genes.\nContent: Non-negative Matrix Factorization (NMF) has been widely concerned in computer vision and data representation. However, the penalized and restriction L0-norm measure are imposed on the NMF model in traditional NMF methods. In this paper, we propose a novel graph regularized non-negative matrix with L0-constraints (GL0NMF) method which comprises the geometrical structure and a more interpretation sparseness measure. In order to extract the characteristic gene effectively, the steps are shown as follows. Firstly, the original data ( {mathbf{Q}} ) is decomposed into two non-negative matrices ( {mathbf{F}} ) and ( {mathbf{P}} ) by utilizing GL0NMF method. Secondly, characteristic genes are extracted by the sparse matrix ( {mathbf{F}} ). Finally, the extracted characteristic genes are validated by using Gene Ontology. In conclusion, the results demonstrate that our method can extract more genes than other conventional gene selection methods.\n\nTitle: Rural Road Extraction from High-Resolution Remote Sensing Images Based on Geometric Feature Inference.\nContent: Road information as a type of basic geographic information is very important for services such as city planning and traffic navigation, as such there is an urgent need for updating road information in a timely manner. Scholars have proposed various methods of extracting roads from remote sensing images, but most of them are not applicable to rural roads with diverse materials, large curvature changes, and a severe shelter problem. In view of these problems, we propose a road extraction method based on geometric feature inference. In this method, we make full use of the linear characteristics of roads, and construct a geometric knowledge base of rural roads using information on selected sample road segments. Based on the knowledge base, we identify the parallel line pairs in images, and further conduct grouping and connection instructed by knowledge reasoning, and finally obtain complete rural roads. The case study in Xiangtan City of China's Hunan Province validates the performance of the proposed method.\n\nTitle: Deductive glue code synthesis for embedded software systems based on code patterns\nContent: Automated code synthesis is a constructive process that can be used to generate programs from specifications. It can, thus, greatly reduce the software development cost and time. The use of formal code synthesis approach for software generation further increases the dependability of the system. Though code synthesis has many potential benefits, the synthesis techniques are still limited. Meanwhile, components are widely used in embedded system development. Applying code synthesis to component based software development (CBSD) process can greatly enhance the capability of code synthesis while reducing the component composition efforts. In this paper, we discuss the issues and techniques for applying deductive code synthesis techniques to CBSD. For deductive synthesis in CBSD, a rule base is the key for inferring appropriate component composition. We use the code patterns to guide the development of rules. Code patterns have been proposed to capture the typical usages of the components. Several general composition operations have been identified to facilitate systematic composition. We present the technique for rule development and automated generation of new patterns from existing code patterns. A case study of using this method in building a real-time control system is also presented\n\nTitle: SEDA4SC: A staged event-driven architecture for adaptive service computing runtime\nContent: The rapid development of web service technology brings up a number of crucial requirements for designing service computing runtime, such as supporting multiple message exchange patterns, switching among different transports, integrating various extended web service protocols and achieving robust performance under high concurrency. Based on staged event-driven architecture, we propose a novel architecture for an adaptive web-service-centric service computing runtime, named SEDA4SC. In SEDA4SC, the process of basic and extended web service protocols is divided into four primary event-driven stages to enable system independence and module isolation. Moreover, this architecture allows messages to be handled in two independent pipelines: the input pipeline and the output pipeline. Arbitrary message exchange patterns can be supported through a combination of the two pipelines. With SEDA4SC, we design and implement a service computing runtime system. The performance evaluation results show that our system exhibits robust performance under high concurrency.\n\n## Task\nCompose an abstract for the title 'Extraction of prostatic lumina and automated recognition for prostatic calculus image using PCA-SVM.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Identification of prostatic calculi is an important basis for determining the tissue origin. Computation-assistant diagnosis of prostatic calculi may have promising potential but is currently still less studied. We studied the extraction of prostatic lumina and automated recognition for calculus images. Extraction of lumina from prostate histology images was based on local entropy and Otsu threshold recognition using PCA-SVM and based on the texture features of prostatic calculus. The SVM classifier showed an average time 0.1432 second, an average training accuracy of 100%, an average test accuracy of 93.12%, a sensitivity of 87.74%, and a specificity of 94.82%. We concluded that the algorithm, based on texture features and PCA-SVM, can recognize the concentric structure and visualized features easily. Therefore, this method is effective for the automated recognition of prostatic calculi."
      }
    ]
  },
  {
    "tid":"10",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Soft-Split Random Forest for Anatomy Labeling\nContent: Random Forest RF has been widely used in the learning-based labeling. In RF, each sample is directed from the root to each leaf based on the decisions made in the interior nodes, also called splitting nodes. The splitting nodes assign a testing sample to either left or right child based on the learned splitting function. The final prediction is determined as the average of label probability distributions stored in all arrived leaf nodes. For ambiguous testing samples, which often lie near the splitting boundaries, the conventional splitting function, also referred to as hard split function, tends to make wrong assignments, hence leading to wrong predictions. To overcome this limitation, we propose a novel soft-split random forest SSRF framework to improve the reliability of node splitting and finally the accuracy of classification. Specifically, a soft split function is employed to assign a testing sample into both left and right child nodes with their certain probabilities, which can effectively reduce influence of the wrong node assignment on the prediction accuracy. As a result, each testing sample can arrive at multiple leaf nodes, and their respective results can be fused to obtain the final prediction according to the weights accumulated along the path from the root node to each leaf node. Besides, considering the importance of context information, we also adopt a Haar-features based context model to iteratively refine the classification map. We have comprehensively evaluated our method on two public datasets, respectively, for labeling hippocampus in MR images and also labeling three organs in Head & Neck CT images. Compared with the hard-split RF HSRF, our method achieved a notable improvement in labeling accuracy.\n\nTitle: Multi-Task SE-Network for Image Splicing Localization\nContent: Image splicing can be easily used for illegal activities such as falsifying propaganda for political purposes and reporting false news, which may result in negative impacts on society. Hence, it is highly required to detect spliced images and localize the spliced regions. In this work, we propose a multi-task squeeze and excitation network (SE-Network) for splicing localization. The proposed network consists of two streams, namely label mask stream and edge-guided stream, both of which adopt convolutional encoder-decoder architecture. The information from the edge-guided stream is transmitted to the label mask stream for enhancing the discrimination of features between the spliced and host regions. This work has three main contributions. First, image edges, along with label masks and mask edges, are exploited to supply more comprehensive supervision for the localization of spliced regions. Second, the low-level feature maps extracted from shallow layers are fused with the high-level feature maps from deep layers to provide more reliable feature for splicing localization. Finally, several squeeze and excitation attention modules are incorporated into the network to recalibrate the fused features to enhance the feature expression. Extensive experiments show that the proposed multi-task SE-Network outperforms existing splicing localization methods evidently on two synthetic splicing datasets and four benchmark splicing datasets.\n\nTitle: A novel algorithm for wavelet neural networks with application to enhanced PID controller design\nContent: This paper presents a variable step-size updating algorithm for wavelet neural network (WNN) in setting the enhanced PID controller parameters. Compared to the iterative method with constant step-size, the most innovative character of the algorithm proposed is its capability of shortening tracking time and improving the convergence in weights updating process for complex systems or large-scale networks. By combining the relationship among WNN, the Kalman filter and the normalized least mean square (NLMS), we introduce the T-S fuzzy inference mechanism for activation derived functions. Furthermore, a once-through steam generator (OTSG) model is established for validating the practicability and reliability in a real complicated system. Finally, simulation results are presented to exhibit the effectiveness of the proposed variable step-size algorithm.\n\nTitle: Subspace ensemble learning via totally-corrective boosting for gait recognition.\nContent: Human identification at a distance has recently become a hot research topic in the fields of computer vision and pattern recognition. Since gait patterns can operate from a distance without subject cooperation, gait recognition has most widely been studied to address this problem. In this paper, a subspace ensemble learning using totally-corrective boosting (SEL_TCB) framework and its kernel extension are proposed for gait recognition. In this framework, multiple subspaces are iteratively learned with different weight distributions on the triplet set using totally-corrective technology, in order to preserve the proximity relationships among instance triplets. Further, we extend the SEL_TCB framework to the kernel SEL_TCB (KSEL_TCB) framework which can deal with the nonlinear manifold of data. We compare our method with the recently published gait recognition approaches on USF HumanID Database. Experimental results indicate that the proposed method achieves highly competitive performance against the state-of-the-art gait recognition approaches.\n\nTitle: Navigability analysis of magnetic map with projecting pursuit-based selection method by using firefly algorithm\nContent: The performance of geomagnetic aided navigation is closely related to the selection of geomagnetic matching area. This paper mainly studies the selection method of multi-parameter geomagnetic matching area based on the projection pursuit model, and then adopts the firefly algorithm, particle swarm optimization algorithm and differential evolution algorithm to obtain the optimal projection direction. After that, we compare the optimizing results comprehensively and give the evaluation of navigation performance in geomagnetic matching area to provide the basis for the selection of matching area. The implementation results show that when there are large differences between the characteristic parameters, the computation efficiency of firefly algorithm performs significantly better than the other two optimization algorithms. And under the same experimental conditions, the optimal geomagnetic matching area obtained by the method presented in this paper has the minimum matching position error and optimal navigation performance.\n\n## Task\nCompose an abstract for the title 'Atlas-Guided Multi-channel Forest Learning for Human Brain Labeling.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Labeling MR brain images into anatomically meaningful regions is important in quantitative brain researches. Previous works can be roughly categorized into two classes: multi-atlas and learning based labeling methods. These methods all suffer from their own limitations. For multi-atlas based methods, the label fusion step is often handcrafted based on the predefined similarity metrics between voxels in the target and atlas images. For learning based methods, the spatial correspondence information encoded in the atlases is lost since they often use only the target image appearance for classification. In this paper, we propose a novel atlas-guided multi-channel forest learning, which could effectively address the aforementioned limitations. Instead of handcrafting the label fusion step, we learn a non-linear classification forest for automatically fusing both image appearance and label information of the atlas with the image appearance of the target image. Validated on LONI-LBPA40 dataset, our method outperforms several traditional labeling approaches."
      }
    ]
  },
  {
    "tid":"11",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Hybridizing Harmony Search Algorithm with Multi-Parent Crossover to Solve Real World Optimization Problems\nContent: Harmony search algorithm HSA is a recent evolutionary algorithm used to solve several optimization problems. The algorithm mimics the improvisation behaviour of a group of musicians to find a good harmony. Several variations of HSA have been proposed to enhance its performance. In this paper, a new variation of HSA that uses multi-parent crossover is proposed HSA-MPC. In this technique three harmonies are used to generate three new harmonies that will replace the worst three solution vectors in the harmony memory HM. The algorithm has been applied to solve a set of eight real world numerical optimization problems 1-8 introduced for IEEE-CEC2011 evolutionary algorithm competition. The experimental results of the proposed algorithm are compared with the original HSA, and two variations of HSA: global best HSA and tournament HSA. The HSA-MPC almost always shows superiority on all test problems.\n\nTitle: Adaptive genetic algorithm using harmony search\nContent: Evolutionary algorithm is one of the major classes of stochastic search methods. This algorithm searches the problem space by exploring and exploiting the search space. The balance between exploration and exploitation will change throughout the search process. Maintaining the right balance between the exploration and exploitation in the search process is crucial for the success of the search process. The parameter values of the algorithm play a crucial role in determining the nature of the search, whether explorative or exploitative. In this paper, we propose an adaptive parameter controlling approach using harmony search. During the search process, harmony search directs the search from the current state to a desired state by determining suitable parameter values such that the balance between exploration and exploitation is suitable for that state transition. The preliminary results of the proposed method is comparable with those from the literature.\n\nTitle: An ensemble of intelligent water drop algorithms and its application to optimization problems\nContent: The Intelligent Water Drop (IWD) algorithm is a recent stochastic swarm-based method that is useful for solving combinatorial and function optimization problems. In this paper, we propose an IWD ensemble known as the Master-River, Multiple-Creek IWD (MRMC-IWD) model, which serves as an extension of the modified IWD algorithm. The MRMC-IWD model aims to improve the exploration capability of the modified IWD algorithm. It comprises a master river which cooperates with multiple independent creeks to undertake optimization problems based on the divide-and-conquer strategy. A technique to decompose the original problem into a number of sub-problems is first devised. Each sub-problem is then assigned to a creek, while the overall solution is handled by the master river. To empower the exploitation capability, a hybrid MRMC-IWD model is introduced. It integrates the iterative improvement local search method with the MRMC-IWD model to allow a local search to be conducted, therefore enhancing the quality of solutions provided by the master river. To evaluate the effectiveness of the proposed models, a series of experiments pertaining to two combinatorial problems, i.e., the travelling salesman problem (TSP) and rough set feature subset selection (RSFS), are conducted. The results indicate that the MRMC-IWD model can satisfactorily solve optimization problems using the divide-and-conquer strategy. By incorporating a local search method, the resulting hybrid MRMC-IWD model not only is able to balance exploration and exploitation, but also to enable convergence towards the optimal solutions, by employing a local search method. In all seven selected TSPLIB problems, the hybrid MRMC-IWD model achieves good results, with an average deviation of 0.021% from the best known optimal tour lengths. Compared with other state-of-the-art methods, the hybrid MRMC-IWD model produces the best results (i.e. the shortest and uniform reducts of 20 runs) for all13 selected RSFS problems.\n\nTitle: Global best Harmony Search with a new pitch adjustment designed for Nurse Rostering\nContent: In this paper, the Harmony Search Algorithm (HSA) is proposed to tackle the Nurse Rostering Problem (NRP) using a dataset introduced in the First International Nurse Rostering Competition (INRC2010). NRP is a combinatorial optimization problem that is tackled by assigning a set of nurses with different skills and contracts to different types of shifts, over a predefined scheduling period. HSA is an approximation method which mimics the improvisation process that has been successfully applied for a wide range of optimization problems. It improvises the new harmony iteratively using three operators: memory consideration, random consideration, and pitch adjustment. Recently, HSA has been used for NRP, with promising results. This paper has made two major improvements to HSA for NRP: (i) replacing random selection with the Global-best selection of Particle Swarm Optimization in memory consideration operator to improve convergence speed. (ii) Establishing multi-pitch adjustment procedures to improve local exploitation. The result obtained by HSA is comparable with those produced by the five INRC2010 winners' methods. (C) 2013 Production and hosting by Elsevier B.V. on behalf of King Saud University.\n\nTitle: Tournament-based harmony search algorithm for non-convex economic load dispatch problem.\nContent: Graphical abstractDisplay Omitted HighlightsThe tournament-based harmony search (THS) algorithm is proposed for economic load dispatch (ELD) problem.In THS, the random selection in the memory consideration is replaced by the tournament selection to observe the natural selection strategy.The proposed THS is tested using three different tournament size values for four test ELD systems.The results suggest that the THS with higher tournament size is efficient for ELD.New results appeared using THS for ELD when THS was compared with 43 methods published in 33 articles. This paper proposes a tournament-based harmony search (THS) algorithm for economic load dispatch (ELD) problem. The THS is an efficient modified version of the harmony search (HS) algorithm where the random selection process in the memory consideration operator is replaced by the tournament selection process to activate the natural selection of the survival-of-the-fittest principle and thus improve the convergence properties of HS. The performance THS is evaluated with ELD problem using five different test systems: 3-units generator system; two versions of 13-units generator system; 40-units generator system; and large-scaled 80-units generator system. The effect of tournament size (t) on the performance of THS is studied. A comparative evaluation between THS and other existing methods reported in the literature are carried out. The simulation results show that the THS algorithm is capable of achieving better quality solutions than many of the well-popular optimization methods.\n\n## Task\nCompose an abstract for the title 'Incorporating Great Deluge with Harmony Search for Global Optimization Problems.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Harmony search (HS) algorithm is relatively a recent metaheuristic optimization method inspired by natural phenomenon of musical improvisation process. despite its success, the main drawback of harmony search are contained in its tendency to converge prematurely due to its greedy selection method. This probably leads the harmony search algorithm to get stuck in local optima and unsought solutions owing to the limited exploration of the search space. The great deluge algorithm is a local search-based approach that has an efficient capability of increasing diversity and avoiding the local optima. This capability comes from its flexible method of accepting the new constructed solution. The aim of this research is to propose and evaluate a new variant of HS. To do so, the acceptance method of the great deluge algorithm is incorporated in the harmony search to enhance its convergence properties by maintaining a higher rate of diversification at the initial stage of the search process. The proposed method is called Harmony Search Great Deluge (HS-GD) algorithm The performance of HS-GD and the classical harmony search algorithm was evaluated using a set of ten benchmark global optimization functions. In addition, five benchmark functions of the former set were employed to compare the results of the proposed method with three previous harmony search variations including the classical harmony search. The results show that HS-GD often outperforms the other comparative approaches."
      }
    ]
  },
  {
    "tid":"12",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: TWO-STAGE BLIND SEPARATION OF MOVING SOUND SOURCES WITH POCKET-SIZE REAL-TIME DSP MODULE\nContent: A new real-time two-stage blind source separation (BSS) method for convolutive mixtures of speech is proposed, in which a single- input multiple-output (SIMO)-model-based independent compo- nent analysis (ICA) and a new SIMO-model-based binary masking are combined. SIMO-model-based ICA can separate the mixed sig- nals, not into monaural source signals but into SIMO-model-based signals from independent sources in their original form at the micro- phones. Thus, the separated signals of SIMO-model-based ICA can maintain the spatial qualities of each sound source. Owing to this attractive property, novel SIMO-model-based binary masking can be applied to efficiently remove the residual interference compo- nents after SIMO-model-based ICA. In addition, the performance deterioration due to the latency problem in ICA can be mitigated by introducing real-time binary masking. We develop a pocket-size real-time DSP module implementing the new BSS method, and re- port the experimental evaluation of the proposed method's supe- riority to the conventional BSS methods, regarding moving-sound separation.\n\nTitle: Interface for barge-in free spoken dialogue system based on sound field reproduction and microphone array\nContent: A barge-in free spoken dialogue interface using sound field control and microphone array is proposed. In the conventional spoken dialogue system using an acoustic echo canceller, it is indispensable to estimate a room transfer function, especially when the transfer function is changed by various interferences. However, the estimation is difficult when the user and the system speak simultaneously. To resolve the problem, we propose a sound field control technique to prevent the response sound from being observed. Combined with a microphone array, the proposed method can achieve high elimination performance with no adaptive process. The efficacy of the proposed interface is ascertained in the experiments on the basis of sound elimination and speech recognition.\n\nTitle: Rapid Compensation of Temperature Fluctuation Effect for Multichannel Sound Field Reproduction System\nContent: In this paper, we propose a computationally efficient method of compensating temperature for the transaural stereo. The conventional method can be used to estimate the change in impulse responses caused by the fluctuation of temperature with high accuracy. However, the large amount of computation required makes real-time implementation difficult. Focusing on the fact that the amount of compensation depends on the length of the impulse response, we reduce the computation required by segmenting the impulse response. We segment the impulse responses in the time domain and estimate the effect of temperature fluctuation for each of the segments. By joining the processed segments, we obtain the compensated impulse response of the whole length. Experimental results show that the proposed method can reduce the computation required by a factor of nine without degradation of the accuracy.\n\nTitle: Source adaptive blind signal extraction using closed-form ICA for hands-free robot spoken dialogue system\nContent: In this paper, we propose a new ICA-based BSS algorithm including estimation of sources' probability density functions (PDFs) to adapt the nonlinear activation function to various noise conditions. In the proposed method, closed-form second-order ICA is introduced as a computational-cost-efficient preprocessing to extract sources' PDFs, which is beneficial for real-time application. Compared with various type of conventional ICAs, e.g., fixed activation-function type and ML-based type, our proposed algorithm can give a faster and higher convergence. Based on the proposed source-adaptive ICA, we show a real-time noise reduction results under diffuse noise environment. Also we can demonstrate our recently developed hands-free robot spoken dialogue system via real-time ICA.\n\nTitle: Sound field decomposition in reverberant environment using sparse and low-rank signal models.\nContent: A sound field decomposition method for a reverberant environment is proposed. Sound field decomposition is the foundation of various acoustic signal processing applications and enables the estimation of the entire sound field from pressure measurements. Although spatial Fourier analysis of the sound field has been widely used, sparse decomposition of the sound field has recently been proved to be effective in several applications. However, in current methods, no constraints are imposed on ambiance components, whereas source components are assumed to be sparsely distributed in the space. This results in inaccurate decomposition in a reverberant environment. The proposed method is based on sparse and low-rank signal models, which are used for simultaneous decomposition of the observed signals into source and ambiance components. Numerical simulation results indicated that the decomposition accuracy is superior to that of current methods.\n\n## Task\nCompose an abstract for the title 'Development, Long-Term Operation and Portability of a Real-Environment Speech-Oriented Guidance System' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, the development, long-term operation and portability of a practical ASR application in a real environment is investigated. The target application is a speech-oriented guidance system installed at the local community center. The system has been exposed to ordinary people since November 2002. More than 300 hours or more than 700,000 inputs have been collected during four years. The outcome is a rare example of a large scale real-environment speech database. A simulation experiment is carried out with this database to investigate how the system's performance improves during the first two years of operation. The purpose is to determine empirically the amount of real-environment data which has to be prepared to build a system with reasonable speech recognition performance and response accuracy. Furthermore, the relative importance of developing the main system components, i.e. speech recognizer and the response generation module, is assessed. Although depending on the system's modeling capacities and domain complexity, experimental results show that overall performance stagnates after employing about 10-15 k utterances for training the acoustic model, 40\u201350 k utterances for training the language model and 40 k\u201350 k utterances for compiling the question and answer database. The Q&A database was most important for improving the system's response accuracy. Finally, the portability of the well-trained first system prototype for a different environment, a local subway station, is investigated. Since collection and preparation of large amounts of real data is impractical in general, only one month of data from the new environment is employed for system adaptation. While the speech recognition component of the first prototype has a high degree of portability, the response accuracy is lower than in the first environment. The main reason is a domain difference between the two systems, since they are installed in different environments. This implicates that it is imperative to take the behavior of users under real conditions into account to build a system with high user satisfaction."
      }
    ]
  },
  {
    "tid":"13",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Multi-Expert System to Classify Fluorescent Intensity in Antinuclear Autoantibodies Testing\nContent: Indirect Immunofluorescence is the recommended method for antinuclear autoantibodies (ANA) detection. IIF diagnosis requires estimating fluorescent intensity and pattern description, but resources and adequately trained personnel are not always available for these tasks. In this respect, an evident medical demand is the development of computer aided diagnosis tools that can offer a support to physician decision. In this paper we propose a system to classify the fluorescent intensity: initially we discuss two classifiers based on Artificial Neural Networks that can recognize intrinsically dubious samples and whose error tolerance can be flexibly set according to a given rule. Since such classifiers complement one other, we adopt a Multiple Expert System that aggregates the two experts. The final decision of the system results from the combination of the outputs of the single experts. Measured performance shows error rates less than 1%, which candidates the method to be used in daily medical practice.\n\nTitle: Automatic Acquisition of Immunofluorescence Images: Algorithms and Evaluation\nContent: In this paper, we report our experience in the development of a system for automatic acquisition of Immuno-Fluorescence Assay (IFA) images. We focus on two basic issues. Firstly, we determine an autofocus function that can deal with photobleaching, a physical phenomenon affecting automatic acquisition of IFA images, and present a set of experiments on real images that confirm its effectiveness. Secondly, we discuss if the physicians may reliably use digital IFA images in place of direct microscope observations to carry out the diagnosis. In this respect, we present the results of a preliminary experiment where physicians perform the diagnosis on a set of images both by looking directly to them at the fluorescence microscope and by looking at digital images on the screen of a workstation.\n\nTitle: A multiple expert system for classifying fluorescent intensity in antinuclear autoantibodies analysis\nContent: At the present, Indirect Immunofluorescence (IIF) is the recommended method for the detection of antinuclear autoantibodies (ANA). IIF diagnosis requires both the estimation of the fluorescent intensity and the description of the staining pattern, but resources and adequately trained personnel are not always available for these tasks. In this respect, an evident medical demand is the development of computer-aided diagnosis (CAD) tools that can offer a support to physician decision. In this paper we first propose a strategy to reliably label the image data set by using the diagnoses performed by different physicians, and then we present a system to classify the fluorescent intensity. Such a system adopts a multiple expert system architecture (MES), based on the classifier selection paradigm. Two different selection rules are presented and, given the application domain, the convenience of using one of them is analyzed. Different sets of operating points are determined, making the recognition system suited to application in daily practice and in a wide spectrum of scenarios. The measured performance on an annotated database of IIF images shows a low overall miss rate (\n\nTitle: Aggregation of classifiers for staining pattern recognition in antinuclear autoantibodies analysis.\nContent: Indirect immunofluorescence is currently the recommended method for the detection of antinuclear autoantibodies (ANA). The diagnosis consists of both estimating the fluorescence intensity and reporting the staining pattern for positive wells only. Since resources and adequately trained personnel are not always available for these tasks, an evident medical demand is the development of computer-aided diagnosis (CAD) tools that can support the physician decisions. In this paper, we present a system that classifies the staining pattern of positive wells on the strength of the recognition of their cells. The core of the CAD is a multiple expert system (MES) based on the one-per-class approach devised to label the pattern of single cells. It employs a hybrid approach since each composing binary module is constituted by an ensemble of classifiers combined by a fusion rule. Each expert uses a set of stable and effective features selected from a wide pool of statistical and spectral measurements. In this framework, we present a novel parameter that measures the reliability of the final classification provided by the MES. This feature is used to introduce a reject option that allows to reduce the error rate in the recognition of the staining pattern of the whole well. The approach has been evaluated on 37 wells, for a total of 573 cells. The measured performance shows a low overall error rate ( 2.7%-5.8%), which is below the observed intralaboratory variability.\n\nTitle: A Hybrid Multi-Expert Systems for HEp-2 Staining Pattern Classification\nContent: In autoimmune diseases, HEp-2 cells are used to detect antinuclear autoantibodies through Indirect Immunofluorescence (IIF) method. These cells can reveal different staining patterns that are relevant to diagnostic purposes. To classify them highly specialized personnel are required, who are not always available. In this respect, a medical demand is the development of a recognition system supporting such an activity. In this paper we present a hybrid Multi-Expert Systems (MES) based on the reduction of the multiclass learning task to several binary problems. The combination scheme, based on both classifier fusion and selection, employs reliability estimators that aim at improving the accuracy of final classification. The performance of such a hybrid system has been compared with those of a MES based only on classifier selection , showing that the hybrid approach benefits of advantages of both combination rules.\n\n## Task\nCompose an abstract for the title 'Advances in Computer-Based Autoantibodies Analysis' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Indirect Immunofluorescence (BF) imaging is the recommended method to detect autoantibodies in patient serum, whose common markers are antinuclear autoantibodies (ANA) and autoantibodies directed against double strand DNA (anti-dsDNA). Since the availability of accurately performed and correctly reported laboratory determinations is crucial for the clinicians, an evident medical demand is the development of Computer Aided Diagnosis (CAD) tools supporting physicians' decisions. In this paper we present a comprehensive system that helps in recognising the presence of ANA and anti-dsDNA autoantibodies. The analysis of CAD performance shows its potential in lowering the method variability, in increasing the level of standardization and in serving as a second reader reducing the physicians' workload. The system has been successfully tested on annotated datasets."
      }
    ]
  },
  {
    "tid":"14",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Regularized Non-Negative Matrix Factorization Using Alternating Direction Method Of Multipliers And Its Application To Source Separation\nContent: Non-negative matrix factorization (NMF) aims at finding non negative representations of nonnegative data. Among different NMF algorithms, alternating direction method of multipliers (ADMM) is a popular one with superior performance. However, we find that ADMM shows instability and inferior performance on real-world data like speech signals. In this paper, to solve this problem, we develop a class of advanced regularized ADMM algorithms for NMF. Efficient and robust learning rules are achieved by incorporating 11-norm and the Frobenius norm regularization. The prior information of Laplacian distribution of data is used to solve the problem with a unique solution. We evaluate this class of ADMM algorithms using both synthetic and real speech signals for a source separation task at different cost functions, i.e., Euclidean distance (EUD), Kullback-Leibler (KL) divergence and Itakura-Saito (IS) divergence. Results demonstrate that the proposed algorithms converge faster and yield more stable and accurate results than the original ADMM algorithm.\n\nTitle: Non-negative matrix factorization using stable alternating direction method of multipliers for source separation.\nContent: Nonnegative matrix factorization (NMF) is a popular method for source separation. In this paper, an alternating direction method of multipliers (ADMM) for NMF is studied, which deals with the NMF problem using the cost function of beta-divergence. Our study shows that this algorithm outperforms state-of-the-art algorithms on synthetic data sets, but it presents unstable behavior and low accuracy on real data sets. Therefore, we propose two different stable ADMM algorithms for NMF to solve this problem. They differ slightly in the multiplicative factor utilized in the update rules. One algorithm is to adapt the step size to guarantee the convergence while the other minimizes the beta-divergence with a pivot element weighting iterative method (PEWI). Experimental results demonstrate that the proposed algorithms are more stable and accurate. Particularly, PEWI based ADMM shows superior performance in the source separation task.\n\nTitle: Real-time tracking-by-learning with high-order regularization fusion for big video abstraction\nContent: Visual tracking is a key technique used by video abstraction to achieve efficient post-analysis for big video surveillance. In order to tackle the problem of constantly changing scenarios during online tracking, additional factors such as motion can be incorporated by utilizing a fusion strategy to improve the final performance. Unfortunately, straightforward output fusion is difficult to be synchronized due to the diversity in model regression. Therefore, a widely cited problem for learning based fusion is to incorporate regularizers for label assignment of unlabeled samples, which is one of the major research focuses on semi-supervised learning. In this paper, a novel tracking strategy based on semi-supervised learning with high order regularization fusion has been proposed. It employs two different types of regularizers to achieve more accurate label assignment based on kernelized confidence prediction and graph-based bi-directional trace from motion. The computation of the proposed tracker takes advantage of the unique feature of circulant matrix in Fourier domain and integral patterns, and thus can be readily implemented for real-time processing, even without any code optimization. Via a dynamic budget maintenance for model updating, the proposed tracking method demonstrated to outperform most state-of-art trackers on challenging benchmark videos with a fixed parameter configuration. HighlightsBy considering the object motion is an essential factor that not only needs to be taken care of while sampling during learning, we incorporate the motion information into an online tracking framework with an effective optimization process of the collaborative learning.we model the object motion as a high order regularization, which is composed of a graph-based bidirectional trace regularizer and a kernelized confidence prediction regularizer.For implementing with the abruption motion estimation and budget maintenance, the proposed tracking has demonstrated its advantages in both accuracy and efficiency, which also showed its practicability for big video abstraction applications.\n\nTitle: Parallel Inference Of Dirichlet Process Gaussian Mixture Models For Unsupervised Acoustic Modeling: A Feasibility Study\nContent: We adopt a Dirichlet process Gaussian mixture model (DPGMM) for unsupervised acoustic modeling and represent speech frames with Gaussian posteriorgrams. The model performs unsupervised clustering on untranscribed data, and each Gaussian component can be considered as a cluster of sounds from various speakers. The model infers its model complexity (i.e. the number of Gaussian components) from the data. For computation efficiency, we use a parallel sampler for the model inference. Our experiments are conducted on the corpus provided by the zero resource speech challenge. Experimental results show that the unsupervised DPGMM posteriorgrams obviously outperform MFCC, and perform comparably to the posteriorgrams derived from language-mismatched phoneme recognizers in terms of the error rate of ABX discrimination test. The error rates can be further reduced by the fusion of these two kinds of posteriorgrams.\n\nTitle: A hybrid virtual bass system with improved phase vocoder and high efficiency\nContent: Virtual bass has drawn wide attention from consumer electronics manufacturers who demand good bass quality from small loundspeakers that have no low-frequency capability. Based on the missing fundamental phenomena, virtual bass aims to extend the low frequency bandwidth psychoacoustically by generating a series of harmonics in the mid-frequency range where loudspeakers can reproduce well. State-of-the-art virtual bass systems usually combine the phase vocoder (PV) approach and the non-linear device (NLD) approach, which aims to overcome their individual drawbacks and achieve good bass performance through a hybrid approach. However, these bass enhancement approaches still lack in audio quality and computation efficiency. In this paper, we propose a new hybrid system with an improved phase vocoder and high efficiency. Previous studies show that artificially-added harmonics may change the timbre of the sound tracks and damage audio quality. We solve this problem by a subjective preference based harmonic magnitude control (HMC) in PV, which is motivated by a recent study on the relationship between the audio timbre and the relative intensity of harmonics. We also significantly improve the computation efficiency through a fast transient content detector based on constant-Q transform (CQT), a direct FFT/IFFT implementation of PV and a more efficient mechanism of HMC. Experiments show that the proposed approach is five times faster than a recent hybrid approach [1] and achieves the best bass effect and audio quality.\n\n## Task\nCompose an abstract for the title 'An Alternating Optimization Approach For Phase Retrieval' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, we address the problem of phase retrieval to recover a signal from the magnitude of its Fourier transform. In many applications of phase retrieval, the signals encountered are naturally sparse. In this work, we consider the case where the signal is sparse under the assumption that few components are nonzero. We exploit further the sparse nature of the signals and propose a two stage sparse phase retrieval algorithm. A simple iterative minimization algorithm recovers a sparse signal from measurements of its Fourier transform (or other linear transform) magnitude based on the minimization of a block l(1) norm. We show in the experiments that the proposed algorithm achieves a competitive performance. It is robust to noise and scalable in practical implementation. The proposed method converges to a more accurate and stable solution than other existing techniques for synthetic signals. For speech signals, experiments show that the voice quality of reconstructed speech signals is almost as good as the original signals."
      }
    ]
  },
  {
    "tid":"15",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Infant Brain Extraction in T1-weighted MR Images using BET and Refinement using LCDG and MGRF Models.\nContent: In this paper, we propose a novel framework for the automated extraction of the brain from T1-weighted MR images. The proposed approach is primarily based on the integration of a stochastic model (a two-level Markov-Gibbs random field (MGRF)) that serves to learn the visual appearance of the brain texture, and a geometric model (the brain iso-surfaces) that preserves the brain geometry during the extraction process. The proposed framework consists of three main steps: (i) Following bias correction of the brain, a new 3D MGRF having a 26- pairwise interaction model is applied to enhance the homogeneity of MR images and preserve the 3D edges between different brain tissues; (ii) The non-brain tissue found in the MR images is initially removed using the brain extraction tool (BET), and then the brain is parceled to nested iso-surfaces using a fast marching level set method; (iii) Finally, a classification step is applied in order to accurately remove the remaining parts of the skull without distorting the brain geometry. The classification of each voxel found on the iso-surfaces is made based on the first- and second-order visual appearance features. The first-order visual appearance is estimated using a linear combination of discrete Gaussians (LCDG) to model the intensity distribution of the brain signals. The second-order visual appearance is constructed using an MGRF model with analytically estimated parameters. The fusion of the LCDG and MGRF, along with their analytical estimation, allows the approach to be fast and accurate for use in clinical applications. The proposed approach was tested on in-vivo data using 300 infant 3D MR brain scans, which were qualitatively validated by an MR expert. In addition, it was quantitatively validated using 30 data sets based on three metrics: the Dice coefficient, the 95-percentile modified Hausdorff distance, and absolute brain volume difference. Results showed the capability of the proposed approach, outperforming four widely used brain extraction tools: BET, BET2, brain surface extractor (BSE), and infant brain extraction and analysis toolbox (iBEAT). Experiments conducted also proved that the proposed framework can be generalized to adult brain extraction as well.\n\nTitle: A novel framework for the segmentationof mrinfant brain images\nContent: This paper introduces a novel adaptive atlas-based framework for the automated segmentation of different brain structures from infant magnetic resonance (MR) brain images. The proposed framework provides a more accurate segmentation of different infant brain structures in the isointense age stage (6\u201312 months) by integrating diffusion tensor imaging (DTI) image features (e.g., fractional anisotropy (FA)) in the segmentation procedure. The input to the proposed system is the medical scans of the infant brain, i.e., 4D diffusion weighted images (DWI). The input brain first undergoes a quality control procedure to remove scan artifacts, and correct motion and eddy current distortions, followed by brain extraction, in which any non-brain tissues are removed. Then, specific DTI features are extracted, and fused to guide the segmentation process to produce the final segmented brain tissues. The high accuracy of the proposed segmentation approach was confirmed by testing it on 10 in-vivo diffusion weighted infant MR brain data sets using three metrics: the Dice coefficient, the 95-percentile modified Hausdorff distance, and the absolute volume difference.\n\nTitle: Atlas-based approach for the segmentation of infant DTI MR brain images\nContent: In this paper, we propose a new adaptive atlas-based technique for the automated segmentation of brain tissues (white matter and grey matter) from infant diffusion tensor images (DTI). Brain images and desired region maps (brain, Cerebrospinal fluid, etc.) are modeled by a joint Markov-Gibbs random field (MGRF) model of independent image signals and interdependent region labels. The proposed joint MGRF model accounts for the following three descriptors: (i) a 1st-order visual appearance to describe the empirical distribution of six features that has been estimated from the DTI in addition to the non-diffusion (b0) scans, (ii) 3D probabilistic atlases, and (iii) a 3D spatially invariant 2nd-order homogeneity descriptor. The 1st-order visual appearance descriptor, assuming each of the estimated DTI parameters are independent, is precisely approximated using our previously developed linear combination of discrete Gaussians (LCDG) intensity model that includes positive and negative Gaussian components. The 3D probabilistic atlases are learned using a subset of the 3D co-aligned training DTI brain images. The 2nd-order homogeneity descriptor is modeled by a 2nd-order translation and rotation invariant MGRF of region labels, with analytically estimated potentials. We tested our approach on 25 DTI brain images, and evaluated the performance on 5 manually segmented 3D DTI brain images to confirm the high accuracy of the proposed approach, as evidenced by the Dice similarity, Hausdorff distance, and absolute volume difference metrics.\n\nTitle: Cerebral White Matter Segmentation From Mri Using Probabilistic Graph Cuts And Geometric Shape Priors\nContent: Study of cerebral white matter in the brain is an important medical problem which helps in better understanding of brain disorders like autism. The goal of this research is to segment the cerebral white matter from the input Magnetic Resonance Imaging (MRI) data. The present segmentation problem becomes extremely difficult due to i) the complex shape of the cerebral white matter and ii) the very low contrast between the white matter and the surrounding structures in the MRI data. We employ a novel probabilistic graph cut algorithm, where the edge capacity functions of the classical graph cut algorithm are modified according to the probabilities of pixels to belong to different segmentation classes. In order to separate the surrounding structures from the white matter, two appropriate geometric shape priors are introduced. Experimentation in 2D with 20 different datasets has yielded an average segmentation accuracy of 94.78%.\n\nTitle: Iterative approximation of empirical grey-level distributions for precise segmentation of multimodal images\nContent: A new algorithm for segmenting a multimodal grey-scale image is proposed. The image is described as a sample of a joint Gibbs random field of region labels and grey levels. To initialize the model, a mixed multimodal empirical grey-level distribution is approximated with linear combinations of Gaussians, one combination per region. Bayesian decisions involving expectation maximization and genetic optimization techniques are used to sequentially estimate and refine parameters of the model, including the number of Gaussians for each region. The final estimates are more accurate than with conventional normal mixture models and result in more adequate region borders in the image. Experiments show that the proposed technique segments complex multimodal medical images of different types more accurately than several other known algorithms.\n\n## Task\nCompose an abstract for the title 'An integrated geometrical and stochastic approach for accurate infant brain extraction' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper presents a novel approach for extracting the brain from 3D T1-weighted MR images. The proposed approach combines a stochastic two-level Markov-Gibbs random field (MGRF) image model with a geometric model that parcels the brain into a set of nested iso-surfaces using a fast marching level setmethod. The classification of each brain voxel found on the iso-surfaces is performed based on the first-order (a linear combination of discrete gaussian (LCDG) model) and second-order (an MGRF model with analytically estimated parameters) visual appearance features of the brain structures. Our approach is tested on 280 infant 3D MR brain scans and evaluated on 9 data sets using the Dice coefficient, the 95-percentile modified Hausdorff distance, and absolute brain volume difference. Experimental results showed that the fusion of the stochastic and geometric models of brain MRI data has led to more accurate brain extraction, when compared with other widely-used brain extraction tools, such as BET, BET2, and brain surface extractor (BSE)."
      }
    ]
  },
  {
    "tid":"16",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Fuzzy Logic-Based Event Notification in Sparse MANETs\nContent: In the Ad-Hoc InfoWare project, we develop a delay tolerant event notification service for sparse Mobile Ad-Hoc Networks for emergency and rescue operations. In most event notification solutions, subscriptions are formed with crisp values or crisp value ranges. Filtering mechanisms do not take into account more expressive subscriptions in terms of approximate predicates and complex aggregating relations among them. However, in emergency scenarios subscribers' interests often have gradual nature and subjective measure. Therefore, we design an intelligent event notification system allowing uncertainties to be modeled and complex matching semantics to be processed by fuzzy reasoning. Requiring more computational efforts, fuzzy logic introduces performance penalties in the whole network. We have developed a new subscription data structure and filtering algorithms, and evaluated and optimized it for runtime and space efficiency.\n\nTitle: Supporting multiple subscription languages by a single event notification overlay in sparse MANETs\nContent: The subscription language is an important design decision for distributed event notification services (DENS). In order to minimize resource consumption and enable applications to use rich and complex subscription languages only when they are really needed, we have developed a DENS that separates the concerns of delivering subscriptions and notifications from the subscription specification and event filtering, i.e., the subscription language. To resolve the conflict between subscription language independence in DENS and a strict decoupling of publishers and subscribers through the DENS, we request that for each new subscription language three language specific plug-ins are provided. In this paper, we present the technical details of this solution and describe our proof-of-concept implementation that supports a simple attribute-value based subscription language and a fuzzy concept-based language.\n\nTitle: Towards Quality And Energy Aware Complex Event Processing\nContent: There is a rising interest to use sensors of various kinds for human activity recognition. Complex human activities are composed of temporally related simple activities and the concept of complex events is well suited to capture this relationship and to detect complex activities, i.e., complex events. This paper addresses the need for high accuracy of complex activity detection and low battery consumption of the sensors. To achieve this goal, this paper presents a framework to obtain the required accuracy of complex activity detection and minimize the cost of delivering samples from the sensors to the sink. We formalize the problem as an integer-programming problem and solve it by an approximation approach. The goal is to satisfy the user specified accuracy at a minimum cost. We compare the performance of our approximation approach with exact method for complex human activity determination. Evaluation results with a publicly available dataset demonstrate the potential impact of our approach.\n\nTitle: A Practical Evaluation of Load Shedding in Data Stream Management Systems for Network Monitoring\nContent: In network monitoring, an important issue is the number of tuples the data stream management system (DSMS) can handle for different network loads. In order to gracefully handle overload situations, some DSMSs are equipped with a tuple dropping functionality, also known as load shedding. These DSMSs register and relate the number of received and dropped tuples, i.e., the relative throughput, and perform different kinds of calculations on them. Over the past few years, several solutions and methods have been suggested to efficiently perform load shedding. The simplestapproach is to keep a count of all the dropped tuples, and to report this to the end user. In our experiments, we study two DSMSs, i.e., TelegraphCQ with support for load shedding, and STREAM without this support. We use three particular network monitoring tasks to evaluate the two DSMS with respect to their ability of load shedding and performance. We demonstrate that it is impor- tant to investigate the correctness of load shedding by showing that the reported number of dropped tuples is not always correct.\n\nTitle: To happen or not to happen: towards an open distributed complex event processing system\nContent: In order to facilitate development of applications that are real-world aware through arbitrary sensing devices, we develop a complex event processing system (CEP). This CEP is open to any kind of sensor, distributes event processing to optimize resource utilization, supports complex temporal and spatial events, and provides the possibility to query both the complex events as well as deviations from these events. The latter is especially useful for application domains with regular patterns in which a deviation from such a pattern represents an important event. The system reads queries that describe what is expected as normal behavior. This query is distributed to the data sources, like sensors, which are interesting for the query. In this paper, we define the event model and a finite automaton that models the queries. We discuss possibilities of distributing this finite automaton, and propose two causality operators and a syntax for these operators. We also show preliminary results from our first prototype.\n\n## Task\nCompose an abstract for the title 'Using Continuous Queries for Event Filtering and Routing in Sparse MANETs' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In our project, we design middleware services for emergency and rescue scenarios in sparse Mobile Ad- Hoc Networks (SMANETs). One of these services is a distributed event notification service (DENS) for asynchronous communication. The DENS architecture allows several subscription languages for different kinds of subscriptions. To support complex subscriptions, we use a Data Stream Management System (DSMS) for filtering of the data streams and matching of filtered events and subscriptions. We have designed a simple rescue scenario to investigate the possibilities of using a DSMS together with the DENS middleware layer. In this paper we discuss the design of our implementation and first experimental results of our ongoing work."
      }
    ]
  },
  {
    "tid":"17",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Simultaneous Localization And Mapping In Sensor Networks: A Ges Sensor-Based Filter With Moving Object Tracking\nContent: This paper presents the design, analysis, and validation of a globally exponentially stable (GES) filter for tridimensional (3-D) range-only simultaneous localization and mapping in sensor networks with moving object tracking. For observability analysis purposes, two nonlinear sensor-based dynamic systems are formulated resorting only to exact linear and angular kinematics and a motion model for the target. A state augmentation is exploited that allows the proposed formulation to be considered as linear time-varying without linearizing the original nonlinear systems. Constructive observability results can then be established, leading naturally to the design of a Kalman Filter with GES error dynamics. These results also provide valuable insight on the motion planning of the vehicle. Simulation results demonstrate the good performance of the algorithm and help validate the theoretical results, as well as illustrate the necessity of a proper trajectory.\n\nTitle: Experimental evaluation of simultaneous 3D localization of sensor nodes and tracking moving targets.\nContent: In this work we carry out an experimental performance characterization of a simultaneous localization and tracking (SLAT) algorithm for sensor networks, whose aim is to determine the positions of sensor nodes and a moving target in a network, given incomplete and inaccurate range measurements between the target and each of the sensors. To achieve this, we propose to iteratively maximize a likelihood function (ML) of positions given the observed ranges, which requires initialization with an approximate solution to avoid convergence towards local extrema. A modified Euclidean Distance Matrix (EDM) completion problem is solved for a block of target range measurements to approximately set up initial sensor/target positions, and the likelihood function is then iteratively refined through Majorization-Minimization (MM). To reduce the computational load, an incremental scheme is used whereby each new target or sensor position is estimated from range measurements, providing additional initialization for ML without the need for solving an expanded EDM completion problem. The proposed algorithms are experimentally evaluated with a series of 3D indoor tests for a range of operation of up to ten meters using a Crossbow Cricket location system and a robotic or human target. Centimetric accuracy is obtained under realistic conditions.\n\nTitle: TW-TOA based cooperative sensor network localization with unknown turn-around time.\nContent: This work aims to estimate multiple node positions in the presence of unknown turn-around times within the context of cooperative sensor network localization. In the adopted scheme, each target can communicate with a set of anchors (probably not in sufficient numbers) and a set of other targets. Two-Way Times-of-Arrival between them are measured, which includes unknown processing delays at both channel endpoints. Since finding the Maximum Likelihood Estimates (MLE) of the positions and turn-around times given those measurements poses a difficult nonconvex optimization problem, it is approximated by a Nonlinear Least Squares problem. Then, the positions and turn-around times of multiple targets are estimated jointly by solving an Euclidean Distance Matrix completion problem. Simulations show that the localization accuracy of the proposed method is good, providing an initial point that subsequently enables MLE to attain the Cramer-Rao Lower Bound for all considered scenarios.\n\nTitle: Rss based cooperative sensor network localization with unknown transmit power.\nContent: This work aims to estimate multiple node positions in the presence of unknown transmit powers within the context of cooperative sensor network localization. In the adopted scheme, each source can communicate with a set of anchors (probably not in sufficient numbers) and a set of other sources. Received Signal Strength (RSS) between them are measured. Since finding the Maximum Likelihood Estimates (MLE) of the positions and transmit powers given those measurements poses a difficult nonconvex optimization problem, it is approximated by a Nonlinear Least Squares (NLS) problem. Then, the position and transmit power of multiple sources are estimated jointly by solving Euclidean Distance Matrix (EDM) completion problem. Simulations show that the localization accuracy and the running time of the proposed method is better than the state of the art method and close to the Cramer-Rao Lower Bound (CRLB) for some scenarios.\n\nTitle: Sensor-based simultaneous localization and mapping \u2014 Part II: Online inertial map and trajectory estimation\nContent: A novel sensor-based filter for simultaneous localization and mapping (SLAM), featuring globally asymptotically stable error dynamics, is proposed in a companion paper, with application to uninhabited aerial vehicles (UAVs). This paper presents the second part of the algorithm, detailing a computationally efficient and numerically robust method for online inertial map and trajectory estimation based on the estimates provided by the SLAM filter previously derived. Central to the solution is the formulation of an optimization problem, that of finding the translation and the rotation that best explain the transformation between two sets of landmarks, with known associations, for consecutive time instants. The validation, performance, and consistency assessment of the proposed SLAM algorithm is successfully performed with real data, which was acquired by an instrumented quadrotor.\n\n## Task\nCompose an abstract for the title 'ML-based sensor network localization and tracking: Batch and time-recursive approaches' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Simultaneous localization and tracking (SLAT) in sensor networks aims to determine the positions of sensor nodes and a moving target in a network, given incomplete and inaccurate range measurements. One of the established methods for achieving this goal is to maximize a likelihood function (ML), which requires initialization with an ap- proximate solution to avoid convergence towards local ex- trema. In this paper a Euclidean Distance Matrix (EDM) completion problem is solved to obtain initial sensor/target positions. The likelihood function is then iteratively opti- mized through either a Majorization-Minimization (MM) or Newton method. To reduce the computational load, an in- cremental scheme is proposed whereby each new target po- sition is estimated from range measurements, providing ad- ditional initialization for ML without the need for solving an expanded EDM completion problem. The performance of these methods is assessed through simulation."
      }
    ]
  },
  {
    "tid":"18",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Resource orchestration for multi-task application in home-to-home cloud.\nContent: Along with the increasing capability of consumer electronic devices, it becomes the new trend that the home cloud computing takes household consumer electronic devices as a sharing resource pool, which can bring great convenience to consumers. However, how to find the most suitable resources or services in the home cloud is a challenge for the consumers. When offloading a complicated application to the home cloud, each task of the application can be executed individually with its own computation, storage and bandwidth requirements. The heterogeneous consumer electronic devices that contain different performance metrics affect the destination choice. This paper proposes a distributed home-to-home cloud infrastructure where volunteer users allow the resources of their consumer electronic devices to be shared with the neighboring houses. By extending the home gateways, a decentralized controlling is responsible for resource monitoring, orchestration, task offload and migration. With consideration of load balance and volunteers' opinion, the resource orchestration is formulated as an optimization problem with one objective and multi-constraints. Finally, a Particle Swarm Optimization (PSO)-based algorithm is used to obtain the approximate optimal solution. Simulation results show that the solution for all cases studied almost achieves 90% objective value in acceptable time1.\n\nTitle: IARA: An Intelligent Application-Aware VNF for Network Resource Allocation with Deep Learning\nContent: Application awareness is essential for traffic engineering and Quality of Service (QoS) guarantee, especially in Internet of Things (IoT). Software Defined Network (SDN) with centralized controlling of network resources provides opportunities for fine- grained resource allocation. However, the controller cannot autonomously identify applications effectively, because sampling and recognizing traffic data consumes a lot of IO and computing resources. In this demonstration, we provide an intelligent application-aware Virtualized Network Function (VNF) with deep learning technology to identify the network traffic. The traffic type information will be mapped to specific network requirements and utilized to search appropriate route paths for different applications. The intelligent VNF is deployed on a GPU-equipped standalone server and works on the data plane of SDN. It identifies the traffic and sends the type information to the controller through OpenFlow protocol. The experiments show that by introducing the type information, SDN controller can assign more appropriate route paths for different types of traffic and highly improve the network QoS.\n\nTitle: Hybrid virtual network embedding with K-core decomposition and time-oriented priority\nContent: Network virtualization provides a powerful tool to allow multiple networks, each customized to a specific purpose, to run on a shared substrate. However, a big challenge is how to map multiple virtual networks onto specific nodes and links in the shared substrate network, known as virtual network embedding problem. Previous works in virtual network embedding can be decomposed to two classes: two-stage virtual network embedding and one-stage virtual network embedding. In this paper, by pruning the topology of virtual network using k-core decomposition, a hybrid virtual network embedding algorithm is proposed to leverage the respective advantage of the two kinds of algorithm simultaneously in the mapping process. In addition, a time-oriented scheduling policy is introduced to improve the mapping performance. We conduct extensive simulations and the results show that the proposed algorithm obtains more revenue in the long-term run.\n\nTitle: Density cluster based approach for controller placement problem in large-scale software defined networkings.\nContent: Software Defined Networking (SDN) decouples control and data planes. The separation arises a problem known as the controller placement, i.e., how many and where controllers should be deployed. Currently, most works defined this problem as the multi-objective combinatorial optimization problem and used heuristic algorithms to search the optimal solution. However, these heuristic algorithms have the drawback of being easily trapped in local optimal solutions and consuming high time. In this paper, we propose an approach named as Density Based Controller Placement (DBCP), which uses a density-based switch clustering algorithm to split the network into several sub-networks. As switches are tightly connected within the same sub-network and less connected from the switches in other sub-networks, we deploy one controller in each sub-network. In DBCP, the size of each sub-network can be decided by the capacity of the controller deployed. Moreover, the optimal number of controllers is obtained according to the density-based clustering. We evaluate DBCPs performance on a set of 262 publicly available network topologies. The experimental results show that DBCP provides better performance than the state-of-the-art approaches in terms of time consumption, propagation latency, and fault tolerance.\n\nTitle: A Self-Adaption Handoff Mechanism for Multimedia Services in Mobile Cloud Computing\nContent: Developing mobile multimedia cloud services over heterogeneous wireless networks poses a challenge for service continuity. The degraded link quality and connection losses are likely to happen and these may affect service execution times and service availability in mobile cloud computing scenarios. To improve handoff quality and minimize utilized bandwidth, we propose a self-adaption handoff scheme for multimedia services in mobile cloud computing. The proposed scheme uses multipath transmission for media flows, and consists of the duplicate mode and the effective mode, which are changed according to the network condition. Analytic model and simulation are developed to investigate our new scheme. The results demonstrate that the new mechanism can realize seamless handoff for multimedia services in cloud, reduce the packet loss rate, as well as obtain a more efficient use of the scarce wireless bandwidth and the power of mobile devices.\n\n## Task\nCompose an abstract for the title 'Cluster-PSO Based Resource Orchestration for Multi-task Applications in Vehicular Cloud.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Vehicular cloud computing (VCC) provides a vehicular user attaching several resources with different types at the same time. Additionally, the vehicular applications especially for big data processing are always complicated and may be decomposed into several fine-grained tasks. When offloading the complicated multi-task application to the vehicular clouds, the task executes individually in terms of its own computation, storage and bandwidth requirement. Different from the task offloading in mobile cloud computing which aims to optimize the energy consumption, the important metric for vehicular users is the application delay. Moreover, the moving vehicles always have the similar resource properties and may form the solution clusters when finding the resource orchestration policy, which brings an opportunity of improving resource orchestration performance. In this paper, we formulate the VCC resource orchestration as an optimization problem, and propose a cluster-particle swarm optimization (PSO) algorithm to obtain the resource orchestration policy. A fast cluster algorithm is used to divide the solution space and generate sub-swarms for better exploring the orchestration solutions. The experiment results show that the cluster-PSO algorithm can achieve a higher resource orchestration accuracy in an acceptable time comparing to the other PSO algorithms. Especially, when there are more tasks in an application and the vehicle has more optional VCC resources, the performance of the cluster-PSO based resource orchestration is outstanding."
      }
    ]
  },
  {
    "tid":"19",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Image fusion techniques for remote sensing applications\nContent: Image fusion refers to the acquisition, processing and synergistic combination of information provided by various sensors or by the same sensor in many measuring contexts. The aim of this survey paper is to describe three typical applications of data fusion in remote sensing. The first study case considers the problem of the synthetic aperture radar (SAR) interferometry, where a pair of antennas are used to obtain an elevation map of the observed scene; the second one refers to the fusion of multisensor and multitemporal (Landsat Thematic Mapper and SAR) images of the same site acquired at different times, by using neural networks; the third one presents a processor to fuse multifrequency, multipolarization and mutiresolution SAR images, based on wavelet transform and multiscale Kalman filter (MKF). Each study case presents also the results achieved by the proposed techniques applied to real data.\n\nTitle: Wavelet and Unsupervised Learning Techniques for Experimental Biomedical Data Processing\nContent: Learning theories and algorithms for both supervised and unsupervised Neural Networks NNs have already been accepted as relevant tools to cope with difficult problems based on the processing of experimental electromagnetic data. These kinds of problems are typically formulated as inverse problems. In this paper, in particular, the electrical signals under investigations derive from experimental electromyogram interference patterns measured on human subjects by means of non-invasive sensors ElectroMyoGraphic, sEMS surface data. The monitoring and the analysis of dynamic sEMG data reveal important information on muscles activity and can be used by clinicians for both preventing dramatic illness evolution and improving athletes performances. The paper proposes the use of the Independent Component Analysis ICA, an unsupervised learning technique, in order to process raw sEMG data by reducing the typical \"cross-talk\" effect on the electric interference pattern measured by the surface sensors. The ICA is implemented by means of a multi-layer NN scheme. Since the IC extraction is based on the assumption of stationarity of the involved sEMG recording, which is often inappropriate in the case of biomedical data, we also propose a technique for dealing with non-stationary recordings. The basic tool is the wavelet time-frequency decomposition, that allows us to detect and analyze time-varying signals. An auto-associative NN that exploits wavelet coefficients as an input vector is also used as simple detector of non-stationarity based on a measure of reconstruction error. The proposed approach not only yields encouraging results to the problem at hand, but suggests a general approach to solve similar relevant problems in some other experimental electromagnetics applications.\n\nTitle: Advanced Integration of Neural Networks for Characterizing Voids in Welded Strips\nContent: Within the framework of aging materials inspection, one of the most important aspects regards defects detection in metal welded strips. In this context, it is important to plan a method able to distinguish the presence or absence of defects within welds as well as a robust procedure able to characterize the defect itself. In this paper an innovative solution that exploits a rotating magnetic field is presented. This approach has been carried out by a Finite Element Model. Within this framework, it is necessary to consider techniques able to offer advantages in terms of sensibility of analysis, strong reliability, speed of carrying out, low costs: its implementation can be a useful support for inspectors. To this aim, it is necessary to solve inverse problems which are mostly ill-posed: in this case, the main problems consist on both the accurate formulation of the direct problem and the correct regularization of the inverse electromagnetic problem. In the last decades, a useful and very performing way to regularize ill-posed inverse electromagnetic problems is based on the use of a Neural Network approach, the so called \"learning by sample techniques\".\n\nTitle: Elman neural networks for characterizing voids in welded strips: a study\nContent: Within the framework of aging materials inspection, one of the most important aspects regarding defects detection in metal welded strips. In this context, it is important to plan a method able to distinguish the presence or absence of defects within welds as well as a robust procedure able to characterize the defect itself. In this paper, an innovative solution that exploits a rotating magnetic field is presented. This approach has been carried out by a finite element model. Within this framework, it is necessary to consider techniques able to offer advantages in terms of sensibility of analysis, strong reliability, speed of carrying out, low costs: its implementation can be a useful support for inspectors. To this aim, it is necessary to solve inverse problems which are mostly ill-posed; in this case, the main problems consist on both the accurate formulation of the direct problem and the correct regularization of the inverse electromagnetic problem. We propose a heuristic inversion, regularizing the problem by the use of an Elman network. Experimental results are obtained using a database created through numerical modeling, confirming the effectiveness of the proposed methodology.\n\nTitle: Adaptive Image Contrast Enhancement by Computing Distances into a 4-Dimensional Fuzzy Unit Hypercube.\nContent: A new fuzzy procedure for adaptive gray-level image contrast enhancement (CE) is presented in this paper. Starting from the pixels belonging to a normalized gray-level image, an appropriate smooth S-shaped fuzzy membership function (MF) is considered for gray-scale transformation and is adaptively developed through noise reduction and information loss minimization. Then, a set of fuzzy patches is extracted from the MF, and for each support of each patch, we compute four ascending-order statistics that become points inside a 4-D fuzzy unit hypercube after a suitable fuzzification step. CE is performed by computing the distances among the above points and the points of maximum darkness and maximum brightness (special vertexes in the hypercube), and by determining the rotation of the tangent line to the MF around a crucial point where fuzzy patches and the MF coexist. The proposed procedure enables high CE in all the treated images with performance that is fully comparable with that obtained by three more sophisticated fuzzy techniques and by standard histogram equalization.\n\n## Task\nCompose an abstract for the title 'Remote Sensing Imagery for Soil Characterization: a Wavelet Neural Data Fusion Approach' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Technological advances in remote sensing imagery allows to obtain high resolution images, helpful in soil characterizing, monitoring and predicting natural hazards. On the other hand, the different kind of in-service sensors allows inferences on a large frequency band. In spite of this, due to the increasing requirements of industrial and civil entities, the academic research is actually involved in improving the quality of imageries. The aim is to implement automatic tools able to work in real-time applications, above all in order to solve pattern identification problem in remote sensing. Within this framework, our work proposes a data fusion methodology, based on the Multiscale Kalman Filter, in order to improve soil characterization in Ikonos surveys."
      }
    ]
  },
  {
    "tid":"20",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Intellwheels - A Development Platform For Intelligent Wheelchairs For Disabled People\nContent: Many people with disabilities find it difficult or even impossible to use traditional powered wheelchairs independently by manually controlling the devices. Intelligent wheelchairs are a very good solution to assist severely handicapped people who are unable to operate classical electrical wheelchair by themselves in their daily activities. This paper describes a development platform for intelligent wheelchairs called IntellWheels. The intelligent system developed may be added to commercial powered wheelchairs with minimal modifications in a very straightforward manner. The paper describes the concept and design of the platform and also the intelligent wheelchair prototype developed to validate the approach. Preliminary results concerning automatic movement of the IntellWheels prototype are also presented.\n\nTitle: Multi-Level Control Of An Intelligent Wheelchair In A Hospital Environment Using A Cyber-Mouse Simulation System\nContent: The development of intelligent wheelchairs is a very good solution to assist severely handicapped people who are unable to operate classical electrical wheelchair by themselves in their daily activities. This paper describes the integration of a robotic simulator with our intelligent wheelchair shared control and planning modules. An adapted version of the free Cyber-Mouse robotic simulator was used to simulate the movement of the intelligent wheelchair in a hospital environment. Adaptations of the subsumption architecture, Strips Planning and A* Algorithms were employed and integrated to allow wheelchair intelligent behavior. The experimental results have demonstrated the success of the integration of these algorithms in our simulator allowing very safe motion of the intelligent wheelchair in the simulated hospital environment. Also, the adapted Cyber-Mouse simulator proved its capability and robustness in simulating the hospital environment and wheelchair physic characteristics.\n\nTitle: Adapted Control Methods for Cerebral Palsy Users of an Intelligent Wheelchair\nContent: The development of an intelligent wheel chair (IW) platform that may be easily adapted to any commercial electric powered wheelchair and aid any person with special mobility needs is the main objective of the IntellWheels project. To be able to achieve this main objective, three distinct control methods were implemented in the IW: manual, shared and automatic. Several algorithms were developed for each of these control methods. This paper presents three of the most significant of those algorithms with emphasis on the shared control method. Experiments were performed by users suffering from cerebral palsy, using a realistic simulator, in order to validate the approach. The experiments revealed the importance of using shared (aided) controls for users with severe disabilities. The patients still felt having complete control over the wheelchair movement when using a shared control at a 50 % level and thus this control type was very well accepted. Thus it may be used in intelligent wheelchairs since it is able to correct the direction in case of involuntary movements of the user but still gives him a sense of complete control over the IW movement.\n\nTitle: Intelligent Wheelchair Driving: A Comparative Study of Cerebral Palsy Adults with Distinct Boccia Experience.\nContent: An electronic wheelchair facilitates the autonomy and independence of a person, however specific cognitive, sensorial and perceptual skills are needed to conduct the assistive technology. These skills are also inherent to the sport boccia. Thus, the aim of this study is to understand the relationship between the experience of the participant in driving a wheelchair in relation to their autonomy and independence and also examine the practice of boccia in relation to the cognitive skills and performance in driving an intelligent wheelchair using a simulator. It was performed an evaluation of 28 participants, 6 of whom had no experience driving an electronic wheelchair and 22 had experience, 15 practice boccia and 13 did not practice this type of adapted sports. In the collection of data was tested three interfaces command of a smart wheelchair in a simulator. It was showed a good performance of the participants with experience in using electronic wheelchair and practitioners of boccia. It was also possible to observe that the autonomous and independent participants showed good results.\n\nTitle: A Methodology for Creating an Adapted Command Language for Driving an Intelligent Wheelchair.\nContent: Intelligent wheelchairs (IW) are technologies that can increase the autonomy and independence of elderly people and patients suffering from some kind of disability. Nowadays the intelligent wheelchairs and the human-machine studies are very active research areas. This paper presents a methodology and a Data Analysis System (DAS) that provides an adapted command language to an user of the IW. This command language is a set of input sequences that can be created using inputs from an input device or a combination of the inputs available in a multimodal interface. The results show that there are statistical evidences to affirm that the mean of the evaluation of the DAS generated command language is higher than the mean of the evaluation of the command language recommended by the health specialist (p value = 0.002) with a sample of 11 cerebral palsy users. This work demonstrates that it is possible to adapt an intelligent wheelchair interface to the user even when the users present heterogeneous and severe physical constraints.\n\n## Task\nCompose an abstract for the title 'A Survey on Intelligent Wheelchair Prototypes and Simulators.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Nowadays more than 700 million persons around the world have some kind of disability or handicap. During the last decades the elderly population in most of the European countries and across all the most civilized countries is also growing at an increasing pace. This phenomenon is receiving increasing attention from the scientific community, during the last years, and several solutions are being proposed in order to allow a more independent life to the people belonging to those groups. In this context Intelligent Wheelchairs (IW) are instruments that are a natural development of the scientific work that has been conducted to improve the traditional Wheelchair characteristics using health informatics, assistive robotics and human computer interface technologies. Some of the most important features of the IW are their navigation capabilities and automatic adaptation of their interface to the user. This paper presents the evolution and state of art concerning IWs prototypes and simulators and intelligent human-computer interfaces in the context of this devices. Our study enabled us to conclude that although several Intelligent Wheelchair prototypes are being developed in a large number of research projects, around the world, the adaptation of their user interface to the patient is an often neglected research topic. Thus, projects aiming at developing new concepts of Intelligent Wheelchairs are needed mainly using multimodal interfaces and wheelchair interfaces adapted to the user characteristics."
      }
    ]
  },
  {
    "tid":"21",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Bayesian Multivariate Probit Analysis Of Korean Firms' Information System Adoption\nContent: Purpose - The purpose of this paper is to reveal the core determinants and adoption patterns of the major enterprise information systems.Design/methodology/approach - This study incorporated the core representative and meaningful explanatory variables in the major previous literatures and analyzes the core determinants of businesses' adoption of the essential information systems and the substitutionary patterns among them, using a Bayesian multivariate probit model, which is based on McFadden's random utility model and capable of handling multiple response data.Findings - It was found that not only factors from the classical technological diffusion viewpoint but also factors such as organizational tools and strategic behaviors play an important role in firms' adoption of information systems. Specifically, epidemic effect generally outweighs size effect, and putting more effort into the intensity of information strategy planning is more influential than the hiring of a professional chief information officer. On the other hand, such variables as age of the firm, labor intensity, and number of PCs per person generally have no significant impacts. Finally, a relatively strong complementary relationship exists between enterprise resource planning and customer relationship management adoption, and between e-buy and groupware adoption.Originality/value - The results presented in this paper have important implications for firms on a minimal budget that want to maximize their productivity through the adoption of information systems. They also provide important information for government policymakers whose job it is to design strategies for the successful deployment of information systems.\n\nTitle: A Secure, Intelligent, and Smart-Sensing Approach for Industrial System Automation and Transmission over Unsecured Wireless Networks.\nContent: In Industrial systems, Supervisory control and data acquisition (SCADA) system, the pseudo-transport layer of the distributed network protocol (DNP3) performs the functions of the transport layer and network layer of the open systems interconnection (OSI) model. This study used a simulation design of water pumping system, in-which the network nodes are directly and wirelessly connected with sensors, and are monitored by the main controller, as part of the wireless SCADA system. This study also intends to focus on the security issues inherent in the pseudo-transport layer of the DNP3 protocol. During disassembly and reassembling processes, the pseudo-transport layer keeps track of the bytes sequence. However, no mechanism is available that can verify the message or maintain the integrity of the bytes in the bytes received/transmitted from/to the data link layer or in the send/respond from the main controller/sensors. To properly and sequentially keep track of the bytes, a mechanism is required that can perform verification while bytes are received/transmitted from/to the lower layer of the DNP3 protocol or the send/respond to/from field sensors. For security and byte verification purposes, a mechanism needs to be proposed for the pseudo-transport layer, by employing cryptography algorithm. A dynamic choice security buffer (SB) is designed and employed during the security development. To achieve the desired goals of the proposed study, a pseudo-transport layer stack model is designed using the DNP3 protocol open library and the security is deployed and tested, without changing the original design.\n\nTitle: Real Time MODBUS Transmissions and Cryptography Security Designs and Enhancements of Protocol Sensitive Information\nContent: Information technology (IT) security has become a major concern due to the growing demand for information and massive development of client/server applications for various types of applications running on modern IT infrastructure. How has security been taken into account and which paradigms are necessary to minimize security issues while increasing efficiency, reducing the influence on transmissions, ensuring protocol independency and achieving substantial performance? We have found cryptography to be an absolute security mechanism for client/server architectures, and in this study, a new security design was developed with the MODBUS protocol, which is considered to offer phenomenal performance for future development and enhancement of real IT infrastructure. This study is also considered to be a complete development because security is tested in almost all ways of MODBUS communication. The computed measurements are evaluated to validate the overall development, and the results indicate a substantial improvement in security that is differentiated from conventional methods.\n\nTitle: Evaluating Energy Efficiency Of Internet Of Things Software Architecture Based On Reusable Software Components\nContent: Nowadays, Internet of Things technology has garnered a great amount of interest because it can make our life much easier, convenient, and even safer. Internet of Things devices can be connected to the Internet or to each other whenever and wherever in order to collect, process, and share information to support various services. In order to provide useful support, important issues related to security, performance, and energy consumption have to be considered. For example, important personal information can be easily exposed to others because Internet of Things can be easily hacked; low performance and high energy consumption can limit the effectiveness of devices. These issues can be considered as quality factors that need to be met in order to develop software applications in the Internet of Things domain. Energy consumption is critical to provide sustained service within mobile and wireless environments. To this end, this article focuses on how to develop Internet of Things software that takes low energy consumption into account. In particular, we propose energy evaluation techniques that are based on a software architecture that is designed to use reusable components. By performing an experiment, we could verify that our proposing method shows maximum 6.83% of error rate against code-based energy simulation. Our technique can help software engineers to judge whether or not software is developed to satisfy the particular requirements related with energy consumption.\n\nTitle: Centralized Connectivity for Multiwireless Edge Computing and Cellular Platform: A Smart Vehicle Parking System.\nContent: This study takes an intuitive step to develop the user-convenient smart vehicle parking system (SVPS), a smart system able to manage the massive crowd of vehicles during parking searching and do the better jobs of parking reservation and management, with the shorter-path processing tactics. For that, this study inclusively employed the mapping strategy, where the system parking points are prevalent, to assist the users to get the parking information fast and conveniently. This study is comprised of the several parking points systematically spread over the several locations and traceable over the available graphical map, and the overall information is easily accessible using smart devices. For parking information, a smart web application which is another important module of this study is designed, with which the SVPS system's registered users are able to access all the services provided for smart vehicle parking searching and reservation in efficient and reliable ways. An integrated network approach, RFID (radio frequency identification) and wireless sensors network (WSN), called RF-WSN, is employed to retrieve the real-time information from the installed and configured sensor devices in RFID-WSN network.\n\n## Task\nCompose an abstract for the title 'Discrete Continuity of Information System, Knowledge System, and e-Business System' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Since information systems are pervasive in the busi-ness and non-business areas, the issue of extending researches on information systems to knowledge sys-tems and e-business systems is one of the most profit-able topics of researches. We propose a historical, discontinuous changes introducing ambiguity in explaining and interpreting innovative nature of three paradigms of systems: information systems, knowledge systems, and e-business systems. Resorting to the historical perspective in developing ideas into meaningful themes, we proposed a discrete continuity in interpreting changes of paradigms of sys-tems. Discrete continuity may be explained by ambigu-ously-shared meaningful perspectives applied to differ-ent paradigms of systems and interpretive elements of each system. The discrete continuity has been adopted to make ambiguity utilized may have instrumental contribution in researches. The engrafted ambiguity in systems design, development, and use could have enduring instrumental value in interpreting the types or variants of systems in each paradigm of systems."
      }
    ]
  },
  {
    "tid":"22",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Fast Transcoding from H.264 AVC to High Efficiency Video Coding\nContent: In this paper, we present several strategies for transcoding from H.264/AVC bit streams to High Efficiency Video Coding (HEVC) bit streams. Because HEVC and AVC share the similar coding architecture, we try to exploit the information in AVC bit streams as much as possible. For inter picture, we utilize the power spectrum based rate-distortion optimization (PS-RDO) model as well as the input residual, modes and motion vectors to estimate the best coding unit (CU) split quad tree, the best prediction unit (PU) mode and the best motion vector of each PU partition For intra picture, we propose to reduce the CU and PU partition candidates. The proposed strategies can significantly reduce the transcoding complexity in terms of reduced processing for RDO evaluations, motion estimation, motion compensation as well as fractional pixel interpolation operations. Experiment results show that the proposed transcoding methods can achieve a good tradeoff between coding efficiency and transcoding complexity.\n\nTitle: Parsing robustness in High Efficiency Video Coding - analysis and improvement.\nContent: One problem in the current High Efficiency Video Coding design is that the entropy decoding, or the parsing process of one frame may depend on syntax elements on other frames. Thus, a small error may lead to immediate failure of decoding of the current frame and all the following inter frames. This paper analyzes the cause of this vulnerability of HEVC bitstreams. Based on the analysis, we further present two solutions to make a tradeoff between the coding efficiency and the error robustness by cutting down the error propagation caused by temporal motion vector prediction. One solution is to redesign some code tables so that the dependency on other frames' syntax elements can be broken. The other solution is to insert recovery points for the coded bitstream, so that the decoding process can recover from errors. The pros and cons of these two solutions are discussed. Experimental results show that the proposed methods can solve the parsing problem in HEVC with a marginal influence on the coding performance and provide significant improvements for the decoded video when there are errors. \u00a9 2011 IEEE.\n\nTitle: Rate-distortion optimization with adaptive weighted distortion in high Efficiency Video Coding\nContent: This paper presents an adaptive weighted distortion optimization algorithm used in the Rate-Distortion Optimization (RDO) process of the High Efficiency Video Coding (HEVC). RDO is an important tool to improve the coding efficiency. Usually the distortion weights of different color components are equal or predetermined. In this paper, an adaptive weighted distortion optimization algorithm is introduced to improve the coding efficiency. The distortion weight is estimated according to the previous coded pictures belonging to the same temporal level, such that encoding complexity is almost unchanged. With the proposed adaptive weighted distortion optimization method, on average about 3.3% and up to 10.6% bit-saving are obtained based on the latest HEVC reference software, HM-8.0 and the corresponding common test conditions. The proposed algorithm can also be applied to other coding schemes such as H.264/MPEG-4 AVC.\n\nTitle: Rate-Distortion Optimized Reference Picture Management for High Efficiency Video Coding\nContent: Motion compensation with multiple reference pictures has been widely used during the development of the emerging High Efficiency Video Coding (HEVC) standard, which greatly helps to improve the coding efficiency. Usually, a heuristic strategy is exploited to use the nearest reconstructed pictures as references. However, such a strategy may not be efficient on all occasions, especially when different content characteristics and coding settings are considered. In this paper, we investigate how to manage reference pictures so as to achieve better rate-distortion performance under the memory constraint of the decoded picture buffer at the decoder. We formulate the reference picture management as an optimization problem and approximate its optimal solution. Moreover, we explore how to adjust quality for each picture according to the reference structure to further improve coding efficiency. For some coding cases, where a complicated encoder optimization is unaffordable, we also develop fast algorithms to get the most benefit from reference picture selection. Among them, one strategy has been adopted by the HEVC software and common test conditions to generate the anchor. Experimental results show that the proposed full search algorithm and fast search algorithms achieve significant bitrate reduction.\n\nTitle: Counter Based Adaptation For Cavlc In Hevc\nContent: Compared to the CAVLC in H.264/AVC, CAVLC in HEVC has been improved much. One mechanism that contributes to the improvement is adaptive code word swapping based on previous coded symbol in the early design of HEVC. However, such a instantly swapping strategy often makes a wrong estimation of the real statistics and may lead to inefficient coding that may even worse than without adaptation. In this paper, we analyze potential loss caused by instantly swapping and propose a counter-based adaptation scheme to further improve the entropy coding efficiency. We discuss the bit-depth of those counters that achieves a trade-off between the adaptation capability and a good estimation of the statistics. The proposed scheme introduces a sum counter for each code table and a counter for each code. The sum counter determines when to shift all the counters to prevent overflow and make sure symbols far away influence less. The experimental results show that the counter based algorithm brings about 0.7% bits saving compared with the instantly swapping adaptation algorithm. Compared with the CAVLC without adaptation, the performance gain of the counter adaptation algorithm is about 4.0% bits saving. A variant of the proposed counter-based adaptation has been adopted by HEVC design.\n\n## Task\nCompose an abstract for the title 'Compression Performance Of High Efficiency Video Coding (Hevc) Working Draft 4' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper presents the results of compression comparison tests between the current state of the emerging High Efficiency Video Coding (HEVC) draft standard and the current dominant standard H.264/MPEG-4 AVC (High Profile) as an anchor reference. The conditions used for the comparison tests were designed to reflect relevant application scenarios and to enable a fair comparison to the maximum extent feasible, i.e. using comparable quantization settings, reference frame buffering, etc. The testing was generally configured in favour of using a relatively strong H.264/MPEG-4 AVC anchor reference. Several of the encoder optimizations currently found in the HEVC software are tested and shown to be helpful to improve the H.264/MPEG-4 AVC anchor performance. When compared to the improved anchor encoder configurations, the HEVC draft design currently provides a bit rate savings for equal PSNR of about 39% for random access applications, 44% for low-delay use, and 25% for all-intra use."
      }
    ]
  },
  {
    "tid":"23",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Requirements-Driven Self-Optimization of Composite Services Using Feedback Control\nContent: In an uncertain and changing environment, a composite service needs to continuously optimize its business process and service selection through runtime adaptation. To achieve the overall satisfaction of stakeholder requirements, quality tradeoffs are needed to adapt the composite service in response to the changing environments. Existing approaches on service selection and composition, however, are mostly based on quality preferences and business processes decisions made statically at the design time. In this paper, we propose a requirements-driven self-optimization approach for composite services. It measures the quality of services (QoS), estimates the earned business value, and tunes the preference ranks through a feedback loop. The detection of unexpected earned business value triggers the proposed self-optimization process systematically. At the process level, a preference-based reasoner configures a requirements goal model according to the tuned preference ranks of QoS requirements, reconfiguring the business process according to its mappings from the goal configurations. At the service level, selection decisions are optimized by utilizing the tuned weights of QoS criteria. We used an experimental study to evaluate the proposed approach. Results indicate that the new approach outperforms both fixed-weighted and floating-weighted service selection approaches with respect to earned business value and adaptation flexibility.\n\nTitle: Requirements-Driven Self-Repairing against Environmental Failures\nContent: Self-repairing approaches have been proposed to alleviate the runtime requirements satisfaction problem by switching to appropriate alternative solutions according to the feedback monitored. However, little has been done formally on analyzing the relations between specific environmental failures and corresponding repairing decisions, making it a challenge to derive a set of alternative solutions to withstand possible environmental failures at runtime. To address these challenges, we propose a requirements-driven self-repairing approach against environmental failures, which combines both development-time and runtime techniques. At the development phase, in a stepwise manner, we formally analyze the issue of self-repairing against environmental failures with the support of the model checking technique, and then design a sufficient and necessary set of alternative solutions to withstand possible environmental failures. The runtime part is a runtime self-repairing mechanism that monitors the operating environment for unsatisfiable situations, and makes self-repairing decisions among alternative solutions in response to the detected environmental failures.\n\nTitle: Quality-Driven Self-Adaptation: Bridging the Gap between Requirements and Runtime Architecture by Design Decision\nContent: Running with static requirements and design decisions, a software system cannot always perform optimally in a highly uncertain and rapidly changing environment. Quality-driven self-adaptation, which enables a software system to continually adapt its structure and behavior to improve the overall quality satisfaction, thus becomes a promising capability of software systems. Existing researches on self-adaptive systems, although having proposed effective methods and techniques on requirements-driven self-adaptation and reflective components, do not well address the gap between requirements and runtime architecture. In this paper, we propose a quality-driven self-adaptation approach, which incorporates both requirements- and architecture-level adaptations. At the requirements level, value-based quality tradeoff decisions are made with the aim of maximizing system-level value propositions. At the architecture level, component-based architecture adaptations are conducted. To bridge the gap between requirements and runtime architecture, design decisions capturing alternative design options and their rationales are introduced to help map requirements adaptations and context changes to adaptation operations on the runtime architecture. To validate the effectiveness, we implement the approach based on a reflective component model and conduct an experimental study on it. The results show that the approach leads to better performance compared with traditional software and the overall quality satisfaction is kept maintained. Furthermore, the development effort is affordable but the approach still has shortage in extensibility.\n\nTitle: Survivability-oriented self-tuning of web systems\nContent: Running in a highly uncertain and changing environment, Web systems cannot always provide full set of services with optim\u00acal quality, especially when the workload is high or failures in subsys-tems occur frequently. It is thus desirable to continuously maintain a high satisfaction level of the system value proposition, hereafter survivability assurance, while relaxing/sacrificing certain quality/functional requirements that are not crucial to the survival of the Web systems. In this paper, we propose a requirements-driven self-tuning method for survivability assurance of Web systems. Using a value-based feedback controller plus a requirements-oriented reasoner, our method makes both quality and functional requirements tradeoffs decisions at runtime.\n\nTitle: Software Product Line Engineering for Developing Self-Adaptive Systems: Towards the Domain Requirements\nContent: Self-adaptive systems are now facing the anticipation of mass customization. Therefore, the Software Product Line (SPL) engineering for developing Self-Adaptive systems (SPL4SA) can be an effective way. At the first sight, SPL4SA is the straightforward combination of the two methodologies of SPL engineering and self-adaptive systems. However, the direct and unsystematic combination will bring difficulty in the domain requirements analysis and in the customization process. In this paper, in order to give a solution to the practical problems, we propose a domain requirements meta-model in SPL4SA. It is described with different point of views and the variability binding constraints inside are emphasized. Based on it, a guidance is concluded to support the consistent customization towards the domain model. In addition, an experimental study about a web-based business product line involving self-adaptation capability is conducted to evaluate the model.\n\n## Task\nCompose an abstract for the title 'Are your sites down? Requirements-driven self-tuning for the survivability of Web systems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Running in a highly uncertain and greatly complex environment, Web systems cannot always provide full set of services with optimal quality, especially when work loads are high or subsystem failures are frequent. Hence, it is significant to continuously maintain a high satisfaction level of survivability, hereafter survivability assurance, while relaxing or sacrificing certain quality or functional requirements that are not crucial to the survival of the entire system. After giving a value-based interpretation to survivability assurance to facilitate a quantitative analysis, we propose a requirements-driven self-tuning method for the survivability assurance of Web systems. Maintaining an enriched and live goal model, our method adapts to runtime tradeoff decisions made by our PID (proportional-integral-derivative) controller and goal-oriented reasoner for both quality and functional requirements. The goal-based configuration plans produced by the reasoner is carried out on the live goal model, and then mapped into system architectural configurations. Experiments on an online shopping system are conducted to validate the effectiveness of the proposed method."
      }
    ]
  },
  {
    "tid":"24",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Distributed Transmission Power Control Protocol for Mobile Ad Hoc Networks\nContent: Abstract--In this paper, we propose a comprehensive solution for power control in mobile ad hoc networks (MANETs). Our solution emphasizes the interplay between the MAC and network layers, whereby the MAC layer indirectly influences the selection of the next-hop by properly adjusting the power of route request packets. This is done while maintaining network connectivity. Channel-gain information obtained mainly from overheard RTS and CTS packets is used to dynamically construct the network topology. Unlike the IEEE 802.11 approach and previously proposed schemes, ours does not use the RTS/CTS packets to silence the neighboring nodes. Instead, collision avoidance information is inserted in the CTS packets and sent over an out-of-band control channel. This information is used to dynamically bound the transmission power of potentially interfering nodes in the vicinity of a receiver. By properly estimating the required transmission power for data packets, our protocol allows for interference-limited simultaneous transmissions to take place in the neighborhood of a receiving node. Simulation results indicate that, compared to the IEEE 802.11 approach, the proposed protocol achieves a significant increase in the channel utilization and end-to-end network throughput and a significant decrease in the total energy consumption.\n\nTitle: Solving the near\u2013far problem in CDMA-based ad hoc networks\nContent: In this paper, we propose a distributed CDMA-based medium access protocol for mobile ad hoc networks (MANETs). Our approach accounts for multiple access interference (MAI) at the protocol level, thereby addressing the notorious near\u2013far problem that undermines the throughput performance in MANETs. Collision avoidance information is inserted in the clear-to-send (CTS) packets and broadcasted over an out-of-band control channel. This information is used to dynamically bound the transmission power of possible interfering nodes in the vicinity of a receiver. Data packets are transmitted at a power level such that interference-limited simultaneous transmissions can take place in the vicinity of a receiving terminal without disturbing its reception. Simulation results indicate that the proposed protocol achieves a significant increase in network throughput relative to the 802.11 approach, at no additional cost in energy consumption. Finally, we show that variable processing gain may be used to increase the capacity of the proposed protocol.\n\nTitle: MAC Protocol for Opportunistic Cognitive Radio Networks with Soft Guarantees\nContent: Cognitive radio (CR) is the key enabling technology for an efficient dynamic spectrum access. It aims at exploiting an underutilized licensed spectrum by enabling opportunistic communications for unlicensed users. In this work, we first develop a distributed cognitive radio MAC (COMAC) protocol that enables unlicensed users to dynamically utilize the spectrum while limiting the interference on primary (PR) users. The main novelty in COMAC lies in not assuming a predefined CR-to-PR power mask and not requiring active coordination with PR users. COMAC provides a statistical performance guarantee for PR users by limiting the fraction of the time during which the PR users' reception is negatively affected by CR transmissions. To provide such a guarantee, we develop probabilistic models for the PR-to-PR and the PR-to-CR interference under a Rayleigh fading channel model. From these models, we derive closed-form expressions for the mean and variance of interference. Empirical results show that the distribution of the interference is approximately lognormal. Based on the developed interference models, we derive a closed-form expression for the maximum allowable power for a CR transmission. We extend the min-hop routing to exploit the available channel information for improving the perceived throughput. Our simulation results indicate that COMAC satisfies its target soft guarantees under different traffic loads and arbitrary user deployment scenarios. Results also show that exploiting the available channel information for the routing decisions can improve the end-to-end throughput of the CR network (CRN).\n\nTitle: POWMAC: a single-channel power-control protocol for throughput enhancement in wireless ad hoc networks\nContent: Transmission power control (TPC) has great potential to increase the throughput of a mobile ad hoc network (MANET). Existing TPC schemes achieve this goal by using additional hardware (e.g., multiple transceivers), by compromising the collision avoidance property of the channel access scheme, by making impractical assumptions on the operation of the medium access control (MAC) protocol, or by overlooking the protection of link-layer acknowledgment packets. In this paper, we present a novel power controlled MAC protocol called POWMAC, which enjoys the same single-channel, single-transceiver design of the IEEE 802.11 ad hoc MAC protocol but which achieves a significant throughput improvement over the 802.11 protocol. Instead of alternating between the transmission of control (RTS/CTS) and data packets, as done in the 802.11 scheme, POWMAC uses an access window (AW) to allow for a series of request-to-send/clear-to-send (RTS/CTS) exchanges to take place before several concurrent data packet transmissions can commence. The length of the AW is dynamically adjusted based on localized information to allow for multiple interference-limited concurrent transmissions to take place in the same vicinity of a receiving terminal. Collision avoidance information is inserted into the CTS packet and is used to bound/ the transmission power of potentially interfering terminals in the vicinity of the receiver, rather than silencing such terminals. Simulation results are used to demonstrate the significant throughput and energy gains that can be obtained under the POWMAC protocol.\n\nTitle: Directional medium access protocol (DMAP) with power control for wireless ad hoc networks\nContent: Protocols for mobile ad hoc networks (MANETs) are often designed with the assumption that nodes are equipped with omnidirectional antennas. Spatial reuse in such networks can be significantly improved by using directional antennas, leading to higher system capacity. This gain is associated with a substantial energy saving that results for beamforming the transmitter and/or receiver antennas in the appropriate directions. However, several medium access problems (e.g., hidden terminal, deafness) resurface when directional antennas are integrated into existing MAC protocols. In this paper, we propose a power-controlled MAC protocol for directional antennas that ameliorates many these problems. Our protocol uses separate control and data channels to reduce collisions. It allows for dynamic adjustment of the data-packet transmission power, such that this power is just enough to overcome interference at the receiver. Simulation results demonstrate that the combined gain from using directional antennas and power control results in significant energy saving and improved throughput performance.\n\n## Task\nCompose an abstract for the title 'CDMA-based MAC protocol for wireless ad hoc networks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We propose a CDMA-based power controlled medium access protocol for mobile ad hoc networks (MANETs). Unlike previously proposed protocols, ours accounts for the multiple access interference (MAI), thereby addressing the notorious near-far problem that undermines the throughput performance in MANETs. Channel-gain information obtained from overheard RTS and CTS packets over an out-of-band control channel is used to dynamically bound the transmission power of mobile terminals in the vicinity of a receiver. By properly estimating the required transmission power for data packets, the proposed protocol allows for interference-limited simultaneous transmissions to take place in the neighborhood of a receiving terminal. Simulation results indicate that compared to the IEEE 802.11 approach, the proposed protocol achieves a significant increase in network throughput at no additional cost in energy consumption."
      }
    ]
  },
  {
    "tid":"25",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Self-supervised learning by information enhancement: Target-generating and spontaneous learning for competitive learning\nContent: In this paper, we propose a new self-supervised learning method for competitive learning as well as self-organizing maps. In this model, a network enhances its state by itself, and this enhanced state is to be imitated by another state of the network. We set up an enhanced and a relaxed state, and the relaxed state tries to imitate the enhanced state as much as possible by minimizing the free energy. To demonstrate the effectiveness of this method, we apply information enhancement learning to the SOM. For this purpose, we introduce collective-ness, in which all neurons collectively respond to input patterns, into an enhanced state. Then, this enhanced and collective state should be imitated by the other non-enhanced and relaxed state. We applied the method to an artificial data and three data from the well-known machine learning database. Experimental results showed that the U-matrices obtained were significantly similar to those produced by the conventional SOM. However, better performance could be obtained in terms of quantitative and topological errors. The experimental results suggest the possibility for self-supervised learning to be applied to many different neural network models.\n\nTitle: Self-enhancement learning: target-creating learning and its application to self-organizing maps.\nContent: In this article, we propose a new learning method called \"self-enhancement learning.\" In this method, targets for learning are not given from the outside, but they can be spontaneously created within a neural network. To realize the method, we consider a neural network with two different states, namely, an enhanced and a relaxed state. The enhanced state is one in which the network responds very selectively to input patterns, while in the relaxed state, the network responds almost equally to input patterns. The gap between the two states can be reduced by minimizing the Kullback-Leibler divergence between the two states with free energy. To demonstrate the effectiveness of this method, we applied self-enhancement learning to the self-organizing maps, or SOM, in which lateral interactions were added to an enhanced state. We applied the method to the well-known Iris, wine, housing and cancer machine learning database problems. In addition, we applied the method to real-life data, a student survey. Experimental results showed that the U-matrices obtained were similar to those produced by the conventional SOM. Class boundaries were made clearer in the housing and cancer data. For all the data, except for the cancer data, better performance could be obtained in terms of quantitative and topological errors. In addition, we could see that the trustworthiness and continuity, referring to the quality of neighborhood preservation, could be improved by the self-enhancement learning. Finally, we used modern dimensionality reduction methods and compared their results with those obtained by the self-enhancement learning. The results obtained by the self-enhancement were not superior to but comparable with those obtained by the modern dimensionality reduction methods.\n\nTitle: Self-enhancement learning: self-supervised and target-creating learning\nContent: In this paper, we propose a new learning method called \"self-enhancement learning.\" In this model, a network enhances its state by itself, and this enhanced state is to be imitated by another state of the network. The word \"target\" in our model means that a target is created spontaneously by a network, which must try to attain the target. Enhancement is realized by changing the Gaussian width or enhancement parameter. With different enhancement parameters, we can set up the different states of a network. In particular, we set up an enhanced and a relaxed state, and the relaxed state tries to imitate the enhanced state as much as possible. To demonstrate the effectiveness of this method, we apply the self-enhancement learning to the SOMe For this purpose, we introduce collectiveness into an enhanced state in which all neurons collectively respond to input patterns. Then, this enhanced and collective state should be imitated by the other non-enhanced and relaxed state. We applied the method to the Iris problem. Experimental results showed that the V-matrices obtained were significantly similar to those produced by the conventional SOMe However, much better performance could be obtained in terms of quantitative and topological errors. The experimental results suggest the possibility for the self-enhancement learning to be applied to many different neural network models.\n\nTitle: Selective information enhancement learning for creating interpretable representations in competitive learning.\nContent: In this paper, we propose a new information-theoretic method, called \"selective information enhancement learning,\" to explicitly interpret final representations created by learning. More specifically, we aim to make class boundaries obtained by learning as overt as possible by picking up the small number of important variables. The variable selection is performed by information enhancement in which mutual information between input patterns and competitive units is measured, while focusing upon a specific input variable. When this information is larger, the importance of the variable is higher. With selected and important variables, a network is retrained by free energy minimization. With this free energy minimization, we can obtain connection weights by considering the importance of specific variables. We applied the method to an artificial data problem, the Senate problem and the voting attitude problem, all of which are easily obtained for purposes of reproduction. Experimental results for all three problems showed that clear class boundaries could be obtained with a smaller number of variables. In addition, we could observe that a smaller number of input variables tended to have the majority of information on input patterns. This tendency became more explicit when the network size was large.\n\nTitle: Information-theoretic multi-layered supervised self-organizing maps for improved prediction performance and explicit internal representation\nContent: In this paper, we propose a new information-theoretic method to train multi-layered neural networks. The method is composed of unsupervised and supervised phase. In the unsupervised phase, the information-theoretic SOM is used to produce knowledge or SOM knowledge in terms of connection weights. In the supervised phase, connection weights obtained in the unsupervised phase are given as the initial connection weights. We applied the method to the segmentation data in machine learning database. The information-theoretic SOM produced connection weights with explicit class boundaries even for the higher layers. By these connection weights, networks reached their lower level of classification errors very rapidly. In addition, the classification error was lower by the choice of the appropriate number of layers.\n\n## Task\nCompose an abstract for the title 'Double enhancement learning for explicit internal representations: unifying self-enhancement and information enhancement to incorporate information on input variables' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a new type of information-theoretic method called \"double enhancement learning,\" in which two types of enhancement, namely, self-enhancement and information enhancement, are unified. Self-enhancement learning has been developed to create targets spontaneously within a network, and its performance has proven to be comparable with that of conventional competitive learning and self-organizing maps. To improve the performance of the self-enhancement learning, we try to include information on input variables in the framework of self-enhancement learning. The information on input variables is computed by information enhancement in which a specific input variable is used to enhance competitive unit outputs. This information is again used to train a network with the self-enhancement learning. We applied the method to three problems, namely, an artificial data, a student survey and the voting attitude problem. In all three problems, quantization errors were significantly decreased with the double enhancement learning. The topographic errors were relatively higher, but the smallest number of topographic errors was also obtained by the double enhancement learning. In addition, we saw that U-matrices for all problems showed explicit boundaries reflecting the importance of input variables."
      }
    ]
  },
  {
    "tid":"26",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Performance of CUDA Virtualized Remote GPUs in High Performance Clusters\nContent: In a previous work we presented the architecture of rCUDA, a middleware that enables CUDA remoting over a commodity network. That is, the middleware allows an application to use a CUDA-compatible Graphics Processor (GPU) installed in a remote computer as if it were installed in the computer where the application is being executed. This approach is based on the observation that GPUs in a cluster are not usually fully utilized, and it is intended to reduce the number of GPUs in the cluster, thus lowering the costs related with acquisition and maintenance while keeping performance close to that of the fully-equipped configuration. In this paper we model rCUDA over a series of high throughput networks in order to assess the influence of the performance of the underlying network on the performance of our virtualization technique. For this purpose, we analyze the traces of two different case studies over two different networks. Using this data, we calculate the expected performance for these same case studies over a series of high throughput networks, in order to characterize the expected behavior of our solution in high performance clusters. The estimations are validated using real 1 Gbps Ethernet and 40 Gbps InfiniBand networks, showing an error rate in the order of 1% for executions involving data transfers above 40 MB. In summary, although our virtualization technique noticeably increases execution time when using a 1 Gbps Ethernet network, it performs almost as efficiently as a local GPU when higher performance interconnects are used. Therefore, the small overhead incurred by our proposal because of the remote use of GPUs is worth the savings that a cluster configuration with less GPUs than nodes reports.\n\nTitle: Increasing the Performance of Data Centers by Combining Remote GPU Virtualization with Slurm\nContent: The use of Graphics Processing Units (GPUs) presents several side effects, such as increased acquisition costs as well as larger space requirements. Furthermore, GPUs require a non-negligible amount of energy even while idle. Additionally, GPU utilization is usually low for most applications. Using the virtual GPUs provided by the remote GPU virtualization mechanism may address the concerns associated with the use of these devices. However, in the same way as workload managers map GPU resources to applications, virtual GPUs should also be scheduled before job execution. Nevertheless, current workload managers are not able to deal with virtual GPUs. In this paper we analyze the performance attained by a cluster using the rCUDA remote GPU virtualization middleware and a modified version of the Slurm workload manager, which is now able to map remote virtual GPUs to jobs. Results show that cluster throughput is doubled at the same time that total energy consumption is reduced up to 40%. GPU utilization is also increased.\n\nTitle: A Performance Comparison of CUDA Remote GPU Virtualization Frameworks\nContent: Using GPUs reduces execution time of many applications but increases acquisition cost and power consumption. Furthermore, GPUs usually attain a relatively low utilization. In this context, remote GPU virtualization solutions were recently created to overcome the drawbacks of using GPUs. Currently, many different remote GPU virtualization frameworks exist, all of them presenting very different characteristics. These differences among them may lead to differences in performance. In this work we present a performance comparison among the only three CUDA remote GPU virtualization frameworks publicly available at no cost. Results show that performance greatly depends on the exact framework used, being the rCUDA virtualization solution the one that stands out among them. Furthermore, rCUDA doubles performance over CUDA for pageable memory copies.\n\nTitle: A Comparative Performance Analysis of Remote GPU Virtualization over Three Generations of GPUs\nContent: The use of Graphics Processing Units (GPUs) has become a very popular way to accelerate the execution of many applications. However, GPUs are not exempt from side effects. For instance, GPUs are expensive devices which additionally consume a non-negligible amount of energy even when they are not performing any computation. Furthermore, most applications present low GPU utilization. To address these concerns, the use of GPU virtualization has been proposed. In particular, remote GPU virtualization is a promising technology that allows applications to transparently leverage GPUs installed in any node of the cluster. In this paper the remote GPU virtualization mechanism is comparatively analyzed across three different generations of GPUs. The goal of this study is to analyze how the performance of the remote GPU virtualization technique is impacted by the underlying hardware. To that end, the Tesla K20, Tesla K40 and Tesla P100 GPUs along with FDR and EDR InfiniBand fabrics are used in the study. The analysis is performed in the context of the rCUDA middleware. It is clearly shown that the GPU virtualization middleware requires a comprehensive design of its communication layer, which should be perfectly adapted to every hardware generation in order to avoid a reduction in performance.\n\nTitle: InfiniBand Verbs Optimizations for Remote GPU Virtualization\nContent: The use of InfiniBand networks to interconnect high performance computing clusters has considerably increased during the last years. So much so that the majority of the supercomputers included in the TOP500 list either use Ethernet or InfiniBand interconnects. Regarding the latter, due to the complexity of the InfiniBand programming API (i.e., InfiniBand Verbs) and the lack of documentation, there are not enough recent available studies explaining how to optimize applications to get the maximum performance from this fabric. In this paper we expose two different optimizations to be used when developing applications using InfiniBand Verbs, each providing an average bandwidth improvement of 3.68% and 217.14%, respectively. In addition, we show that when combining both optimizations, the average bandwidth gain is 43.29%. This bandwidth increment is key for remote GPU virtualization frameworks. Actually, this noticeable gain translates into a reduction of up to 35% in execution time of applications using remote GPU virtualization frameworks.\n\n## Task\nCompose an abstract for the title 'An efficient implementation of GPU virtualization in high performance clusters' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Current high performance clusters are equipped with high bandwidth/low latency networks, lots of processors and nodes, very fast storage systems, etc. However, due to economical and/or power related constraints, in general it is not feasible to provide an accelerating coprocessor -such as a graphics processor (GPU)- per node. To overcome this, in this paper we present a GPU virtualization middleware, which makes remote CUDA-compatible GPUs available to all the cluster nodes. The software is implemented on top of the sockets application programming interface, ensuring portability over commodity networks, but it can also be easily adapted to high performance networks."
      }
    ]
  },
  {
    "tid":"27",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Multi-scale Fusion of Texture and Color for Background Modeling\nContent: Background modeling from a stationary camera is a crucial component in video surveillance. Traditional methods usually adopt single feature type to solve the problem, while the performance is usually unsatisfactory when handling complex scenes. In this paper, we propose a multi-scale strategy, which combines both texture and color features, to achieve a robust and accurate solution. Our contributions are two folds: one is that we propose a novel texture operator named Scale-invariant Center-symmetric Local Ternary Pattern, which is robust to noise and illumination variations, the other is that a multi-scale fusion strategy is proposed for the issue. Our method is verified on several complex real world videos with illumination variation, soft shadows and dynamic backgrounds. We compare our method with four state-of-the-art methods, and the experimental results clearly demonstrate that our method achieves the highest classification accuracy in complex real world videos.\n\nTitle: Graph-Based Background Suppression for Scene Text Detection\nContent: Detecting text in video or natural scene image is quite challenging due to the complex background, various fonts and illumination conditions. The preprocessing period, which suppresses the nontext areas so as to highlight the text areas, is the basis for further text detection. In this paper, a novel graph-based background suppression method for scene text detection is proposed. Considering each pixel as a node in the graph, our approach incorporates pixel-level and context-level features into a graph. Various factors contribute to the unary and pair wise cost function which is optimized via max-flow/min-cut algorithm [16] to get a binary image whose nontext pixels are suppressed so that text pixels are highlighted. Furthermore, the proposed background suppression method could be easily combined with other detection methods to improve the performance. Experimental results on ICDAR 2011 competition dataset show promising performance.\n\nTitle: Conditional random field for text segmentation from images with complex background\nContent: Text contained in images and video frames provide important clues for information indexing and retrieval. But it is difficult to segment text from images, especially those images with complex background. This paper presents a new conditional random field approach, in which contextual features are introduced into text segmentation. Local visual information and contextual label information are integrated into a conditional random field by several components. Some components focus on visual image information to predict the category within the image sites, while others focus on contextual label information to determine the patterns within the label field. Integrating contextual label information in conditional random field can effectively resolve local ambiguities and improve text segmentation performance in complex background. The comparing results demonstrate that the proposed method outperforms other methods for text segmentation from complex background.\n\nTitle: Adaptive Graph Cut Based Binarization of Video Text Images\nContent: Interactive image segmentation which needs the user to give certain hard constraints has shown promising performance for object segmentation. In this paper, we consider characters in text image as a special kind of object, and propose an adaptive graph cut based text binarization method to segment text from background. The main contributions of the paper lie in: 1) in order to make the binarization local adaptive with uneven background, the text region image is firstly roughly split into several sub-images on which graph cut is applied, and 2) considering the unique characteristics of the text, we propose to automatically classify some pixels as text or background with high confidence, severed as hard constraints seeds for graph cut to extract text from background by spreading the seeds into the whole sub-image. The experimental results show that our approach could get better performance in both character extraction accuracy and recognition accuracy.\n\nTitle: Scene text detection using graph model built upon maximally stable extremal regions\nContent: Scene text detection could be formulated as a bi-label (text and non-text regions) segmentation problem. However, due to the high degree of intraclass variation of scene characters as well as the limited number of training samples, single information source or classifier is not enough to segment text from non-text background. Thus, in this paper, we propose a novel scene text detection approach using graph model built upon Maximally Stable Extremal Regions (MSERs) to incorporate various information sources into one framework. Concretely, after detecting MSERs in the original image, an irregular graph whose nodes are MSERs, is constructed to label MSERs as text regions or non-text ones. Carefully designed features contribute to the unary potential to assess the individual penalties for labeling a MSER node as text or non-text, and color and geometric features are used to define the pairwise potential to punish the likely discontinuities. By minimizing the cost function via graph cut algorithm, different information carried by the cost function could be optimally balanced to get the final MSERs labeling result. The proposed method is naturally context-relevant and scale-insensitive. Experimental results on the ICDAR 2011 competition dataset show that the proposed approach outperforms state-of-the-art methods both in recall and precision.\n\n## Task\nCompose an abstract for the title 'Background modeling by exploring multi-scale fusion of texture and intensity in complex scenes' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Background modeling is a fundamental yet challenging issue in video surveillance. Traditional methods usually adopt single feature type to solve the problem, while the performance is usually unsatisfactory when handling complex scenes. In this paper, we propose a multi-scale framework, which combines both texture and intensity feature, to achieve a robust and accurate solution. Our contributions are three folds: first, we provide a multi-scale analysis for the issue; second, for texture feature we propose a novel texture operator named Scale-invariant Center-symmetric Local Ternary Pattern, and a corresponding Pattern Adaptive Kernel Density Estimation technique for its probability estimation; third, we design a Simplified Gaussian Mixture Models for intensity feature. Our method is tested on several complex real world videos with illumination variation, soft shadows and dynamic backgrounds. The experimental results clearly demonstrate that our method is superior to the previous methods."
      }
    ]
  },
  {
    "tid":"28",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Trust Vector Approach to Service-Oriented Applications\nContent: Trust is a critical issue in e-commerce and e-service environments. In some applications (such as eBay), the trust management mechanisms have been introduced to provide valuable information to buyers prior to placing orders and making payments. Meanwhile, the trust issue is also actively studied in the research community. However, most studies and applications focus on approaches that result in a single trust value to represent the trust level of sellers or service providers. Such a simple trust evaluation method may not be able to depict the trust history exactly and may leave misleading information to service customers. In this paper, we present a novel approach of trust vector consisting of three values to reflect the trust level with more indications.\n\nTitle: The study of trust vector based trust rating aggregation in service-oriented environments.\nContent: In most existing studies on trust evaluation, a single trust value is aggregated from the ratings given to previous services of a service provider, to indicate his/her current trust level. Such a mechanism is useful but may not be able to depict the trust features of a service provider well under certain circumstances. Alternatively, a complete set of trust ratings can be transferred to a service client for local trust evaluation. However, this incurs a big overhead in communication, since the rating dataset is usually in large scale covering a long service history. The third option is to generate a small set of data that should represent well the large set of trust ratings of a long time period. In the literature, a trust vector approach has been proposed, with which a trust vector of three values resulting from a computed regression line can represent a set of ratings distributed within a time interval (e.g., a week or a month, etc.). However, the computed trust vector can represent the set of ratings well only if these ratings imply consistent trust trend changes and are all very close to the obtained regression line. In a more general case with trust ratings for a long service history, multiple time intervals have to be determined, within each of which a trust vector can be obtained and can represent well all the corresponding ratings. Hence, a small set of data can represent well a large set of trust ratings with well preserved trust features. This is significant for large-scale trust rating transmission, trust evaluation and trust management. In this paper, we propose one greedy and two optimal multiple time interval (MTI) analysis algorithms. We also have studied the properties of our proposed algorithms analytically and empirically. These studies can illustrate that our algorithms can return a small set of MTI to represent a large set of trust ratings and preserve well the trust features.\n\nTitle: Context based trust normalization in service-oriented environments\nContent: With the development of information technology, the issue of trust becomes more and more important. In e-commerce or service-oriented environments, when there are a few sellers or service providers providing the same product/service, the buyer or service client would like to request the trust management authority to provide trust values of sellers or service providers, which are based on the ratings reflecting the quality of previous transactions. In addition, trust is context dependent, i.e. for different context of transactions, there are different factors influencing the trust result. In this paper, we propose a fuzzy comprehensive evaluation based method for building up a projection from the trust ratings in the transaction history of a service provider to an upcoming transaction depending on the similarity between previous transactions and the upcoming one, and the familiarity between each rater and the service client of the upcoming transaction. This process is termed as context based trust normalization. After trust normalization, normalized trust ratings are used for trust evaluation, the results of which would be closely bound to the upcoming transaction. Finally, we introduce the results of our conducted experiments to illustrate how our proposed method can detect some typical risks.\n\nTitle: Fuzzy Regression Based Trust Prediction in Service-Oriented Applications\nContent: Reputation-based trust evaluation is critical to e-commerce or e-service applications. In some applications (such as eBay), the trust management mechanisms have been introduced to provide valuable information to buyers prior to placing orders and making payments. Meanwhile, the trust issue is also actively studied in the research community. However, in most existing studies, a single trust value is computed based on ratings given for a seller or a service provider to indicate the current trust level. This is interesting but too simple to reflect the service quality history and trust features well under some circumstances. It may also be misleading for the decision making of buyers or service customers. In this paper, we present a novel fuzzy regression based trust vector approach to depict the trust level with more indications, and predict the trustworthiness of a forthcoming transaction with its advertised QoS values and transaction history.\n\nTitle: Price Trust Evaluation in E-service Oriented Applications\nContent: Trust is a critical issue in e-commerce and e-service environments. Some applications (such as eBay) have introduced various trust management mechanisms to provide valuable information to buyers prior to placing orders and making payments. While trust has been actively studied in the research community, some malicious sellers and fraudulent transactions still widely exist. A typical attack is to entice buyers into making payments for buying items at very cheap and attractive prices and not to deliver the items or deliver the fake item after payment. This paper proposes a new trust management system that provides information to indicate to some extent the trust level of a seller at an offered price. In this paper, we describe our proposed approach for price trust evaluation and an approach for market price calculation. We also evaluated our approaches empirically.\n\n## Task\nCompose an abstract for the title 'Trust management towards service-oriented applications' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In service-oriented computing (SOC) environments, service clients interact with service providers for services or transactions.\n From the point view of service clients, the trust status of a service provider is a critical issue to consider, particularly\n when the service provider is unknown to them. Typically, the trust evaluation is based on the feedback on the service quality\n provided by service clients. In this paper, we first present a trust management framework that is event-driven and rule-based.\n In this framework, trust computation is based on formulae. But rules are defined to determine which formula to use and what\n arguments to use, according to the event occurred during the transaction or service. In addition, we propose some trust evaluation\n metrics and a formula for trust computation. The formula is designed to be adaptable to different application domains by setting\n suitable arguments. Particularly, the proposed model addresses the incremental characteristics of trust establishment process.\n Furthermore, we propose a fuzzy logic based approach for determining reputation ranks that particularly differentiates new\n service providers and old (long-existing) ones. This is further incentive to new service providers and penalize poor quality\n services from service providers. Finally, a set of empirical studies has been conducted to study the properties of the proposed\n approaches, and the method to control the trust changes in both trust increment and decrement cases. The proposed framework\n is adaptable for different domains and complex trust evaluation systems."
      }
    ]
  },
  {
    "tid":"29",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Multi-Task Neural Network for Non-discrete Attribute Prediction in Knowledge Graphs.\nContent: Many popular knowledge graphs such as Freebase, YAGO or DBPedia maintain a list of non-discrete attributes for each entity. Intuitively, these attributes such as height, price or population count are able to richly characterize entities in knowledge graphs. This additional source of information may help to alleviate the inherent sparsity and incompleteness problem that are prevalent in knowledge graphs. Unfortunately, many state-of-the-art relational learning models ignore this information due to the challenging nature of dealing with non-discrete data types in the inherently binary-natured knowledge graphs. In this paper, we propose a novel multi-task neural network approach for both encoding and prediction of non-discrete attribute information in a relational setting. Specifically, we train a neural network for triplet prediction along with a separate network for attribute value regression. Via multi-task learning, we are able to learn representations of entities, relations and attributes that encode information about both tasks. Moreover, such attributes are not only central to many predictive tasks as an information source but also as a prediction target. Therefore, models that are able to encode, incorporate and predict such information in a relational learning context are highly attractive as well. We show that our approach outperforms many state-of-the-art methods for the tasks of relational triplet classification and attribute value prediction.\n\n\n\nTitle: Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering.\nContent: The dominant neural architectures in question answer retrieval are based on recurrent or convolutional encoders configured with complex word matching layers. Given that recent architectural innovations are mostly new word interaction layers or attention-based matching mechanisms, it seems to be a well-established fact that these components are mandatory for good performance. Unfortunately, the memory and computation cost incurred by these complex mechanisms are undesirable for practical applications. As such, this paper tackles the question of whether it is possible to achieve competitive performance with simple neural architectures. We propose a simple but novel deep learning architecture for fast and efficient question-answer ranking and retrieval. More specifically, our proposed model, HyperQA, is a parameter efficient neural network that outperforms other parameter intensive models such as Attentive Pooling BiLSTMs and Multi-Perspective CNNs on multiple QA benchmarks. The novelty behind HyperQA is a pairwise ranking objective that models the relationship between question and answer embeddings in Hyperbolic space instead of Euclidean space. This empowers our model with a self-organizing ability and enables automatic discovery of latent hierarchies while learning embeddings of questions and answers. Our model requires no feature engineering, no similarity matrix matching, no complicated attention mechanisms nor over-parameterized layers and yet outperforms and remains competitive to many models that have these functionalities on multiple benchmarks.\n\n\n\nTitle: Cross Temporal Recurrent Networks for Ranking Question Answer Pairs\nContent: Temporal gates play a significant role in modern recurrent based neural encoders, enabling fine-grained control over recursive compositional operations over time. In recurrent models such as the long short-term memory (LSTM), temporal gates control the amount of information retained or discarded over time, not only playing an important role in influencing the learned representations but also serving as a protection against vanishing gradients. This paper explores the idea of learning temporal gates for sequence pairs (question and answer), jointly influencing the learned representations in a pairwise manner. In our approach, temporal gates arc learned via it) convolutional layers and then subsequently cross applied across question and answer for joint learning. Empirically, we show that this conceptually simple sharing of temporal gates can lead to competitive performance across multiple benchmarks. Intuitively, what our network achieves can he interpreted as learning representations of question and answer pairs that are aware of what each other is remembering or forgetting, i.e., pairwise temporal gating. Via extensive experiments, we show that our proposed model achieves state-of-the-art performance on two community-based QA datasets and competitive performance on one factoid-based QA dataset.\n\nTitle: Learning to Rank Question Answer Pairs with Holographic Dual LSTM Architecture\nContent: We describe a new deep learning architecture for learning to rank question answer pairs. Our approach extends the long short-term memory (LSTM) network with holographic composition to model the relationship between question and answer representations. As opposed to the neural tensor layer that has been adopted recently, the holographic composition provides the benefits of scalable and rich representational learning approach without incurring huge parameter costs. Overall, we present Holographic Dual LSTM (HD-LSTM), a unified architecture for both deep sentence modeling and semantic matching. Essentially, our model is trained end-to-end whereby the parameters of the LSTM are optimized in a way that best explains the correlation between question and answer representations. In addition, our proposed deep learning architecture requires no extensive feature engineering. Via extensive experiments, we show that HD-LSTM outperforms many other neural architectures on two popular benchmark QA datasets. Empirical studies confirm the effectiveness of holographic composition over the neural tensor layer.\n\nTitle: Dyadic Memory Networks for Aspect-based Sentiment Analysis.\nContent: This paper proposes Dyadic Memory Networks (DyMemNN), a novel extension of end-to-end memory networks (memNN) for aspect-based sentiment analysis (ABSA). Originally designed for question answering tasks, memNN operates via a memory selection operation in which relevant memory pieces are adaptively selected based on the input query. In the problem of ABSA, this is analogous to aspects and documents in which the relationship between each word in the document is compared with the aspect vector. In the standard memory networks, simple dot products or feed forward neural networks are used to model the relationship between aspect and words which lacks representation learning capability. As such, our dyadic memory networks ameliorates this weakness by enabling rich dyadic interactions between aspect and word embeddings by integrating either parameterized neural tensor compositions or holographic compositions into the memory selection operation. To this end, we propose two variations of our dyadic memory networks, namely the Tensor DyMemNN and Holo DyMemNN. Overall, our two models are end-to-end neural architectures that enable rich dyadic interaction between aspect and document which intuitively leads to better performance. Via extensive experiments, we show that our proposed models achieve the state-of-the-art performance and outperform many neural architectures across six benchmark datasets.\n\n\n\n## Task\nCompose an abstract for the title 'SKIPFLOW: Incorporating Neural Coherence Features for End-to-End Automatic Text Scoring' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Deep learning has demonstrated tremendous potential for Automatic Text Scoring (ATS) tasks. In this paper, we describe a new neural architecture that enhances vanilla neural network models with auxiliary neural coherence features. Our new method proposes a new SKIPFLOW mechanism that models relationships between snapshots of the hidden representations of a long short-term memory (LSTM) network as it reads. Subsequently, the semantic relationships between multiple snapshots are used as auxiliary features for prediction. This has two main benefits. Firstly, essays are typically long sequences and therefore the memorization capability of the LSTM network may be insufficient. Implicit access to multiple snapshots can alleviate this problem by acting as a protection against vanishing gradients. The parameters of the SKIPFLOW mechanism also acts as an auxiliary memory. Secondly, modeling relationships between multiple positions allows our model to learn features that represent and approximate textual coherence. In our model, we call this neural coherence features. Overall, we present a unified deep learning architecture that generates neural coherence features as it reads in an end-to-end fashion. Our approach demonstrates state-of-the-art performance on the benchmark ASAP dataset, outperforming not only feature engineering baselines but also other deep learning models."
      }
    ]
  },
  {
    "tid":"30",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Performance analysis of classical, model-based and eigenvector methods: Ophthalmic arterial disorders detection case\nContent: In this study, ophthalmic arterial Doppler signals recorded from 214 subjects were processed using classical, model-based, and eigenvector methods. The classical method (fast Fourier transform), two model-based methods (Burg autoregressive, least squares modified Yule-Walker autoregressive moving average methods), and three eigenvector methods (Pisarenko, multiple signal classification, and Minimum-Norm methods) were selected for performing spectral analysis of the ophthalmic arterial Doppler signals. Doppler power spectral density estimates of the ophthalmic arterial Doppler signals were obtained using these spectrum analysis techniques. The variations in the shape of the Doppler power spectra were examined in order to detect variabilities such as stenosis, ocular Behcet disease, and uveitis disease in the physical state of ophthalmic arterial Doppler signals. These power spectra were then used to compare the applied methods in terms of their frequency resolution and the effects in detecting stenosis, Behcet disease and uveitis disease in ophthalmic arteries.\n\nTitle: Expert systems for time-varying biomedical signals using eigenvector methods\nContent: In this paper, we present the expert systems for time-varying biomedical signals classification and determine their accuracies. The combined neural network (CNN), mixture of experts (ME), and modified mixture of experts (MME) were tested and benchmarked for their performance on the classification of the studied time-varying biomedical signals (ophthalmic arterial Doppler signals, internal carotid arterial Doppler signals and electroencephalogram signals). Decision making was performed in two stages: feature extraction by eigenvector methods and classification using the classifiers trained on the extracted features. The inputs of these expert systems composed of diverse or composite features were chosen according to the network structures. The present study was conducted with the purpose of answering the question of whether the expert system with diverse features (MME) or composite feature (CNN, ME) improve the capability of classification of the time-varying biomedical signals. The purpose was to determine an optimum classification scheme for the problem and also to infer clues about the extracted features. Our research demonstrated that the power levels of power spectral density (PSD) estimations obtained by the eigenvector methods are the valuable features which are representing the time-varying biomedical signals and the CNN, ME, and MME trained on these features achieved high classification accuracies.\n\nTitle: Features extracted by eigenvector methods for detecting variability of EEG signals\nContent: In this paper, we present the expert systems for detecting variability of electroencephalogram (EEG) signals. In practical applications of pattern recognition, there are often diverse features extracted from raw data which needs recognizing. Methods of combining multiple classifiers with diverse features are viewed as a general problem in various application areas of pattern recognition. Because of the importance of making the right decision, we are looking for better classification procedures for EEG signals. The mixture of experts (ME) and modified mixture of experts (MME) were tested and benchmarked for their performance on the classification of the studied EEG signals. Decision making was performed in two stages: feature extraction by eigenvector methods and classification using the classifiers trained on the extracted features. The inputs of these expert systems composed of diverse or composite features were chosen according to the network structures. The present study was conducted with the purpose of answering the question of whether the expert system with diverse features (MME) or composite feature (ME) improve the capability of classification of the EEG signals. Our research demonstrated that the MME trained on diverse features achieved accuracy rates which were higher than that of the ME.\n\nTitle: Comparison of FFT and adaptive ARMA methods in transcranial Doppler signals recorded from the cerebral vessels.\nContent: In this work, transcranial Doppler signals recorded from the temporal region of the brain on 35 patients were transferred to a personal computer by using a 16-bit sound card. Fast Fourier transform and adaptive auto regressive-moving average (A-ARMA) methods were applied to transcranial Doppler frequencies obtained from the middle cerebral artery in the temporal region. Spectral analyses were obtained to compare both methods for medical diagnoses. The sonograms obtained using A-ARMA method give better results for spectral resolution than the FFT method. The sonograms of A-ARMA method offer net envelope and better imaging, so that the determination of blood flow and brain pressure can be calculated more accurately. All diseases show higher resistance to flow than controls with no difference between males and females. Whereas values between disease classes differed, resistance within each class was remarkably constant.\n\nTitle: Comparison of STFT and wavelet transform methods in determining epileptic seizure activity in EEG signals for real-time application\nContent: Electroencephalography (EEG) is widely used in clinical settings to investigate neuropathology. Since EEG signals contain a wealth of information about brain functions, there are many approaches to analyzing EEG signals with spectral techniques. In this study, the short-time Fourier transform (STFT) and wavelet transform (WT) were applied to EEG signals obtained from a normal child and from a child having an epileptic seizure. For this purpose, we developed a program using Labview software. Labview is an application development environment that uses a graphical language G, usable with an online applicable National Instruments data acquisition card. In order to obtain clinically interpretable results, frequency band activities of \u03b4, \u03b8, \u03b1 and \u03b2 signals were mapped onto frequency\u2013time axes using the STFT, and 3D WT representations were obtained using the continuous wavelet transform (CWT). Both results were compared, and it was determined that the STFT was more applicable for real-time processing of EEG signals, due to its short process time. However, the CWT still had good resolution and performance high enough for use in clinical and research settings.\n\n## Task\nCompose an abstract for the title 'Comparison of eigenvector methods with classical and model-based methods in analysis of internal carotid arterial Doppler signals' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Doppler ultrasound is known as a reliable technique, which demonstrates the flow characteristics and resistance of arteries in various vascular disease. In this study, internal carotid arterial Doppler signals recorded from 105 subjects were processed by PC-computer using classical, model-based, and eigenvector methods. The classical method (fast Fourier transform), two model-based methods (Burg autoregressive, least-squares modified Yule-Walker autoregressive moving average methods), and three eigenvector methods (Pisarenko, multiple signal classification, and Minimum-Norm methods) were selected for processing internal carotid arterial Doppler signals. Doppler power spectra of internal carotid arterial Doppler signals were obtained using these spectrum analysis techniques. The variations in the shape of the Doppler power spectra were examined in order to obtain medical information. These power spectra were then used to compare the applied methods in terms of their frequency resolution and the effects in determination of stenosis and occlusion in internal carotid arteries."
      }
    ]
  },
  {
    "tid":"31",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Collaborative Filtering Process in a Whole New Light\nContent: Collaborative filtering (CF) systems are gaining widespread acceptance in recommender systems and e-commerce applications. These systems combine information retrieval and data mining techniques to provide recommendations for products, based on suggestions of users with similar preferences. Nearest-neighbor CF process is influenced by several factors, which were not examined carefully in past work. In this paper, we bring to surface these factors in order to identify existing false beliefs. Moreover, by being able to view the \"big picture\" from the CF process, we propose new approaches that substantially improve the performance of CF algorithms. For instance, we obtain more than 40% percent increase in precision in comparison to widely-used CF algorithms. We perform an extensive experimental evaluation, with several real data sets, and produce results that invalidate some existing beliefs and illustrate the superiority of the proposed extensions\n\nTitle: Collaborative recommender systems: Combining effectiveness and efficiency\nContent: Recommender systems base their operation on past user ratings over a collection of items, for instance, books, CDs, etc. Collaborative filtering (CF) is a successful recommendation technique that confronts the ''information overload'' problem. Memory-based algorithms recommend according to the preferences of nearest neighbors, and model-based algorithms recommend by first developing a model of user ratings. In this paper, we bring to surface factors that affect CF process in order to identify existing false beliefs. In terms of accuracy, by being able to view the ''big picture'', we propose new approaches that substantially improve the performance of CF algorithms. For instance, we obtain more than 40% increase in precision in comparison to widely-used CF algorithms. In terms of efficiency, we propose a model-based approach based on latent semantic indexing (LSI), that reduces execution times at least 50% than the classic CF algorithms.\n\nTitle: Continuous Trend-Based Clustering in Data Streams\nContent: Trend analysis of time series is an important problem since trend identification enables the prediction of the near future. In streaming time series the problem is more challenging due to the dynamic nature of the data. In this paper, we propose a method to continuously clustering a number of streaming time series based on their trend characteristics. Each streaming time series is transformed to a vector by means of the Piecewise Linear Approximation (PLA) technique. The PLA vector comprises pairs of values (timestamp, trend) denoting the starting time of the trend and the type of the trend (either UP or DOWN) respectively. A distance metric for PLA vectors is introduced. We propose split and merge criteria to continuously update the clustering information. Moreover, the proposed method handles outliers. Performance evaluation results, based on real-life and synthetic data sets, show the efficiency and scalability of the proposed scheme.\n\nTitle: Nearest-biclusters collaborative filtering based on constant and coherent values\nContent: Collaborative Filtering (CF) Systems have been studied extensively for more than a decade to confront the \"information overload\" problem. Nearest-neighbor CF is based either on similarities between users or between items, to form a neighborhood of users or items, respectively. Recent research has tried to combine the two aforementioned approaches to improve effectiveness. Traditional clustering approaches (k-means or hierarchical clustering) has been also used to speed up the recommendation process. In this paper, we use biclustering to disclose this duality between users and items, by grouping them in both dimensions simultaneously. We propose a novel nearest-biclusters algorithm, which uses a new similarity measure that achieves partial matching of users' preferences. We apply nearest-biclusters in combination with two different types of biclustering algorithms--Bimax and xMotif--for constant and coherent biclustering, respectively. Extensive performance evaluation results in three real-life data sets are provided, which show that the proposed method improves substantially the performance of the CF process.\n\nTitle: Continuous trend-based classification of streaming time series\nContent: Trend analysis of time series data is an important research direction. In streaming time series the problem is more challenging, taking into account the fact that new values arrive for the series, probably in very high rates. Therefore, effective and efficient methods are required in order to classify a streaming time series based on its trend. Since new values are continuously arrive for each stream, the classification is performed by means of a sliding window which focuses on the last values of each stream. Each streaming time series is transformed to a vector by means of a Piecewise Linear Approximation (PLA) technique. The PLA vector is a sequence of symbols denoting the trend of the series (either UP or DOWN), and it is constructed incrementally. Efficient in-memory methods are used in order to: 1) determine the class of each streaming time series and 2) determine the streaming time series that comprise a specific trend class. Performance evaluation based on real-life datasets is performed, which shows the efficiency of the proposed approach both with respect to classification time and storage requirements. The proposed method can be used in order to continuously classify a set of streaming time series according to their trends, to monitor the behavior of a set of streams and to monitor the contents of a set of trend classes.\n\n## Task\nCompose an abstract for the title 'Collaborative Filtering Based on User Trends' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Recommender systems base their operation on past user ratings over a collection of items, for instance, books, CDs, etc. Collaborative Filtering (CF) is a succesful recommendation technique. User ratings are not expected to be independent, as users follow trends of similar rating behavior. In terms of Text Mining, this is analogous to the formation of higher-level concepts from plain terms. In this paper, we propose a novel CF algorithm which uses Latent Semantic Indexing (LSI) to detect rating trends and performs recommendations according to them. Our results indicate its superiority over existing CF algorithms."
      }
    ]
  },
  {
    "tid":"32",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Multilevel Active Storage for big data applications in high performance computing\nContent: Given the growing importance of supporting dataintensive sciences and big data applications, an effective HPC I/O solution has become a key issue and has attracted intensive attention in recent years. Active storage has been shown effective in reducing data movement and network traffic as a potential new I/O solution. Existing prototypes and systems, however, are primarily designed for read-intensive applications. In addition, they generally assume that offloaded processing kernels have small computational demands, which makes this solution a poor fit for data-intensive operations that have significant computational demands, including write-intensive operations. In this research, we propose a new Multilevel Active Storage (MAS) solution. The new MAS design can support and handle both read- and write-intensive operations, as well as complex operations that have considerable computational demands. Experimental tests have been carried out and confirmed that the MAS approach is feasible and outperformed existing approaches. The new multilevel active storage design has a potential to deliver a high performance I/O solution for big data applications in HPC.\n\nTitle: Using pattern-models to guide SSD deployment for Big Data applications in HPC systems\nContent: Flash-memory based Solid State Drives (SSDs) embrace higher performance and lower power consumption compared to traditional storage devices (HDDs). These benefits are needed in HPC systems, especially with the growing demand of supporting Big Data applications. In this paper, we study placement and deployment strategies of SSDs in HPC systems to maximize the performance improvement, given a practical fixed hardware budget constraint. We propose a pattern-model approach to guide SSD deployment for HPC systems through two steps; characterizing workload and mapping deployment strategy. The first step is responsible for characterizing the access patterns of the workload and the second step contributes the actual deployment recommendation for Parallel File System (PFS) configuration combining with an analytical model. We have carried out initial experimental tests and the results confirmed that the proposed approach can guide placement of SSDs in HPC systems for accelerating data accesses. Our research will be helpful in guiding designs and developments for Big Data applications in current and projected HPC systems including exascale systems.\n\nTitle: Cost-intelligent application-specific data layout optimization for parallel file systems\nContent: Parallel file systems have been developed in recent years to ease the I/O bottleneck of high-end computing system. These advanced file systems offer several data layout strategies in order to meet the performance goals of specific I/O workloads. However, while a layout policy may perform well on some I/O workload, it may not perform as well for another. Peak I/O performance is rarely achieved due to the complex data access patterns. Data access is application dependent. In this study, a cost-intelligent data access strategy based on the application-specific optimization principle is proposed. This strategy improves the I/O performance of parallel file systems. We first present examples to illustrate the difference of performance under different data layouts. By developing a cost model which estimates the completion time of data accesses in various data layouts, the layout can better match the application. Static layout optimization can be used for applications with dominant data access patterns, and dynamic layout selection with hybrid replications can be used for applications with complex I/O patterns. Theoretical analysis and experimental testing have been conducted to verify the proposed cost-intelligent layout approach. Analytical and experimental results show that the proposed cost model is effective and the application-specific data layout approach can provide up to a 74% performance improvement for data-intensive applications.\n\nTitle: HMC-Sim-2.0: A Simulation Platform for Exploring Custom Memory Cube Operations\nContent: The recent advent of stacked memory devices has led to a resurgence of research associated with the fundamental memory hierarchy and associated memory pipeline. The bandwidth advantages provided by stacked logic and DRAM devices have inspired research associated with eliminating the bandwidth bottlenecks associated with many applications in high performance computing. Further, recent efforts have focused on utilizing stacked memory devices as last-level caches. In addition to the two aforementioned focus areas, a third area of research is emerging to explore augmenting the stacked memory logic layer with additional operations. This first generation of Hybrid Memory Cube (HMC) devices provided rudimentary atomic memory operations. The Gen2 Hybrid Memory Cube devices provide more expressive atomic memory operations that include primitive integer arithmetic operations. Despite the inclusion of more expressive arithmetic operations, many users have expressed interest in more complex and potentially orthogonal custom memory cube, or CMC, operations in future revisions of the Hybrid Memory Cube specification. This work presents recent development associated with the HMC-Sim Hybrid Memory Cube simulation framework that provides users a powerful infrastructure to experiment and research augmented custom memory cube, or CMC, operations within the current Gen2 Hybrid Memory Cube device infrastructure. We provide an overview of extending the original HMC-Sim simulation infrastructure to include support for CMC operations with requiring users to modify the core simulation code base. We also present three examples of building and utilizing custom, user-defined CMC operations in sample simulations to exhibit potential application speedup with future HMC device specifications. In doing so, we present a model to replace traditional thread mutexes with custom HMC mutex commands.\n\nTitle: Log-Assisted Straggler-Aware I/O Scheduler for High-End Computing\nContent: Object-based parallel file systems have emerged as promising storage solutions for high-end computing (HEC) systems. Despite the fact that object storage provides a flexible interface, scheduling highly concurrent I/O requests that access a large number of objects still remains as a challenging problem, especially in the case when stragglers (storage servers that are significantly slower than others) exist in the system. An efficient I/O scheduler needs to avoid possible stragglers to achieve low latency and high throughput. In this paper, we introduce a log-assisted straggler-aware I/O scheduling to mitigate the impact of storage server stragglers. The contribution of this study is threefold. First, we introduce a client-side, log-assisted, straggler-aware I/O scheduler architecture to tackle the storage straggler issue in HEC systems. Second, we present two scheduling algorithms that can make efficient decision on scheduling I/Os while avoiding stragglers based on such an architecture. Third, we evaluate the proposed I/O scheduler using simulations. The simulation results have confirmed the promise of the newly introduced log-assisted straggler-aware I/O scheduler in large-scale HEC systems.\n\n## Task\nCompose an abstract for the title 'Mammoth: Gearing Hadoop Towards Memory-Intensive MapReduce Applications' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The MapReduce platform has been widely used for large-scale data processing and analysis recently. It works well if the hardware of a cluster is well configured. However, our survey has indicated that common hardware configurations in smalland medium-size enterprises may not be suitable for such tasks. This situation is more challenging for memory-constrained systems, in which the memory is a bottleneck resource compared with the CPU power and thus does not meet the needs of large-scale data processing. The traditional high performance computing (HPC) system is an example of the memory-constrained system according to our survey. In this paper, we have developed Mammoth, a new MapReduce system, which aims to improve MapReduce performance using global memory management. In Mammoth, we design a novel rule-based heuristic to prioritize memory allocation and revocation among execution units (mapper, shuffler, reducer, etc.), to maximize the holistic benefits of the Map/Reduce job when scheduling each memory unit. We have also developed a multi-threaded execution engine, which is based on Hadoop but runs in a single JVM on a node. In the execution engine, we have implemented the algorithm of memory scheduling to realize global memory management, based on which we further developed the techniques such as sequential disk accessing, multi-cache and shuffling from memory, and solved the problem of full garbage collection in the JVM. We have conducted extensive experiments with comparison against the native Hadoop platform. The results show that the Mammoth system can reduce the job execution time by more than 40% in typical cases, without requiring any modifications of the Hadoop programs. When a system is short of memory, Mammoth can improve the performance by up to 5.19 times, as observed for I/O intensive applications, such as PageRank. Given the growing importance of supporting large-scale data processing and analysis and the proven success of the MapReduce platform, the- Mammoth system can have a promising potential and impact."
      }
    ]
  },
  {
    "tid":"33",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Regularized least squares temporal difference learning with nested \u21132 and \u21131 penalization\nContent: The construction of a suitable set of features to approximate value functions is a central problem in reinforcement learning (RL). A popular approach to this problem is to use high-dimensional feature spaces together with least-squares temporal difference learning (LSTD). Although this combination allows for very accurate approximations, it often exhibits poor prediction performance because of overfitting when the number of samples is small compared to the number of features in the approximation space. In the linear regression setting, regularization is commonly used to overcome this problem. In this paper, we review some regularized approaches to policy evaluation and we introduce a novel scheme (L 21 ) which uses \u21132 regularization in the projection operator and an \u21131 penalty in the fixed-point step. We show that such formulation reduces to a standard Lasso problem. As a result, any off-the-shelf solver can be used to compute its solution and standardization techniques can be applied to the data. We report experimental results showing that L 21 is effective in avoiding overfitting and that it compares favorably to existing \u21131 regularized methods.\n\nTitle: Regularized Policy Iteration\nContent: In this paper we consider approximate policy-iteration-based reinforcement learning algorithms. In order to implement a flexible function approximation scheme we propose the use of non-parametric methods with regularization, providing a convenient way to control the complexity of the function approximator. We propose two novel regularized policy iteration algorithms by adding L2-regularization to two widely-used policy evaluation methods: Bellman residual minimization (BRM) and least-squares temporal difference learning (LSTD). We derive efficient implementation for our algorithms when the approximate value-functions belong to a reproducing kernel Hilbert space. We also provide finite-sample performance bounds for our algorithms and show that they are able to achieve optimal rates of convergence under the studied conditions.\n\nTitle: Finite-sample Analysis of Bellman Residual Minimization\nContent: We consider the Bellman residual minimization approach for solving discounted Markov decision problems, where we assume that a generative model of the dynamics and rewards is available. At each policy iteration step, an approximation of the value function for the current policy is obtained by minimizing an empirical Bellman residual defined on a set of n states drawn i.i.d. from a distribution mu the immediate rewards, and the next states sampled from the model. Our main result is a generalization bound for the Bellman residual in linear approximation spaces. In particular, we prove that the empirical Bellman residual approaches the true (quadratic) Bellman residual in mu-norm with a rate of order O(1/root n). This result implies that minimizing the empirical residual is indeed a sound approach for the minimization of the true Bellman residual which guarantees a good approximation of the value function for each policy. Finally, we derive performance bounds for the resulting approximate policy iteration algorithm in terms of the number of samples n and a measure of how well the function space is able to approximate the sequence of value functions.\n\nTitle: Approximate Modified Policy Iteration and its Application to the Game of Tetris\nContent: Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that contains the two celebrated policy and value iteration methods. Despite its generality, MPI has not been thoroughly studied, especially its approximation form which is used when the state and/or action spaces are large or infinite. In this paper, we propose three implementations of approximate MPI (AMPI) that are extensions of the well-known approximate DP algorithms: fitted-value iteration, fitted-Q iteration, and classification-based policy iteration. We provide error propagation analysis that unify those for approximate policy and value iteration. We develop the finite-sample analysis of these algorithms, which highlights the influence of their parameters. In the classification-based version of the algorithm (CBMPI), the analysis shows that MPI's main parameter controls the balance between the estimation error of the classifier and the overall value function approximation. We illustrate and evaluate the behavior of these new algorithms in the Mountain Car and Tetris problems. Remarkably, in Tetris, CBMPI outperforms the existing DP approaches by a large margin, and competes with the current state-of-the-art methods while using fewer samples.\n\nTitle: Finite-sample analysis of least-squares policy iteration\nContent: In this paper, we report a performance bound for the widely used least-squares policy iteration (LSPI) algorithm. We first consider the problem of policy evaluation in reinforcement learning, that is, learning the value function of a fixed policy, using the least-squares temporal-difference (LSTD) learning method, and report finite-sample analysis for this algorithm. To do so, we first derive a bound on the performance of the LSTD solution evaluated at the states generated by the Markov chain and used by the algorithm to learn an estimate of the value function. This result is general in the sense that no assumption is made on the existence of a stationary distribution for the Markov chain. We then derive generalization bounds in the case when the Markov chain possesses a stationary distribution and is b-mixing. Finally, we analyze how the error at each policy evaluation step is propagated through the iterations of a policy iteration method, and derive a performance bound for the LSPI algorithm.\n\n## Task\nCompose an abstract for the title 'Regularized Fitted Q-Iteration: Application to Planning' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We consider planning in a Markovian decision problem, i.e., the problem of finding a good policy given access to a generative model of the environment. We propose to use fitted Q-iteration with penalized (or regularized) least-squares regression as the regression subroutine to address the problem of controlling model-complexity. The algorithm is presented in detail for the case when the function space is a reproducing-kernel Hilbert space underlying a user-chosen kernel function. We derive bounds on the quality of the solution and argue that data-dependent penalties can lead to almost optimal performance. A simple example is used to illustrate the benefits of using a penalized procedure."
      }
    ]
  },
  {
    "tid":"34",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Influence of Vertical Result in Web Search Examination\nContent: Research in how users examine results on search engine result pages (SERPs) helps improve result ranking, advertisement placement, performance evaluation and search UI design. Although examination behavior on organic search results (also known as \\\"ten blue links\\\") has been well studied in existing works, there lacks a thorough investigation on how users examine SERPs with verticals. Considering the fact that a large fraction of SERPs are served with one or more verticals in the practical Web search scenario, it is of vital importance to understand the influence of vertical results on search examination behaviors. In this paper, we focus on five popular vertical types and try to study their influences on users' examination processes in both cases when they are relevant or irrelevant to the search queries. With examination behavior data collected with an eye-tracking device, we show the existence of vertical-aware user behavior effects including vertical attraction effect, examination cut-off effect in the presence of a relevant vertical, and examination spill-over effect in the presence of an irrelevant vertical. Furthermore, we are also among the first to systematically investigate the internal examination behavior within the vertical results. We believe that this work will promote our understanding of user interactions with federated search engines and bring benefit to the construction of search performance evaluations.\n\nTitle: Constructing a Comparison-based Click Model for Web Search\nContent: ABSTRACTExtracting valuable feedback information from user behavior logs is one of the major concerns in Web search studies. Among the tremendous efforts that aim to improve search performance with user behavior modeling, constructing click models is of vital importance because it provides a direct estimation of result relevance. Most existing click models assume that whether or not users click on results only depends on the examination probability and the content of the result. However, through a carefully designed user eye-tracking study, we found that users do not make click-through decisions in isolation. Instead, they also consider the context of a result (e.g., adjacent results). This finding leads to the design of a novel click model named Comparison-based Click Model (CBCM). Different from traditional examination hypotheses, CBCM introduces the concept of an examination viewport and assumes users click results after comparing adjacent results within the same viewport. The experimental results on a publicly available user behavior dataset demonstrate the effectiveness of CBCM. We also public our code of CBCM and dataset.\n\nTitle: Constructing Click Models for Mobile Search.\nContent: Users' click-through behavior is considered as a valuable yet noisy source of implicit relevance feedback for web search engines. A series of click models have therefore been proposed to extract accurate and unbiased relevance feedback from click logs. Previous works have shown that users' search behaviors in mobile and desktop scenarios are rather different in many aspects, therefore, the click models that were designed for desktop search may not be as effective in mobile context. To address this problem, we propose a novel Mobile Click Model (MCM) that models how users examine and click search results on mobile SERPs. Specifically, we incorporate two biases that are prevalent in mobile search into existing click models: 1) the click necessity bias that some results can bring utility and usefulness to users without being clicked; 2) the examination satisfaction bias that a user may feel satisfied and stop searching after examining a result with low click necessity. Extensive experiments on large-scale real mobile search logs show that: 1) MCM outperforms existing models in predicting users' click behavior in mobile search; 2) MCM can extract richer information, such as the click necessity of search results and the probability of user satisfaction, from mobile click logs. With this information, we can estimate the quality of different vertical results and improve the ranking of heterogeneous results in mobile search.\n\n\n\nTitle: Incorporating Non-sequential Behavior into Click Models\nContent: Click-through information is considered as a valuable source of users' implicit relevance feedback. As user behavior is usually influenced by a number of factors such as position, presentation style and site reputation, researchers have proposed a variety of assumptions (i.e.~click models) to generate a reasonable estimation of result relevance. The construction of click models usually follow some hypotheses. For example, most existing click models follow the sequential examination hypothesis in which users examine results from top to bottom in a linear fashion. While these click models have been successful, many recent studies showed that there is a large proportion of non-sequential browsing (both examination and click) behaviors in Web search, which the previous models fail to cope with. In this paper, we investigate the problem of properly incorporating non-sequential behavior into click models. We firstly carry out a laboratory eye-tracking study to analyze user's non-sequential examination behavior and then propose a novel click model named Partially Sequential Click Model (PSCM) that captures the practical behavior of users. We compare PSCM with a number of existing click models using two real-world search engine logs. Experimental results show that PSCM outperforms other click models in terms of both predicting click behavior (perplexity) and estimating result relevance (NDCG and user preference test). We also publicize the implementations of PSCM and related datasets for possible future comparison studies.\n\nTitle: Global or Local: Constructing Personalized Click Models for Web Search\nContent: ABSTRACT Click models are widely used for user simulation, relevance inference, and evaluation in Web search. Most existing click models implicitly assume that users\u2019 relevance judgment and behavior patterns are homogeneous. However, previous studies have shown that different users interact with search engines in rather different ways. Therefore, a unified click model can hardly capture the heterogeneity in users\u2019 click behavior. To shed light on this research question, we propose a Click Model Personalization framework (CMP) that adaptively selects from global and local models for individual users. Different adaptive strategies are designed to personalize click behavior modeling only for specific users and queries. We also reveal that capturing personalized behavior patterns is more important than modeling personalized relevance assessments in constructing personalized click models. To evaluate the performance of the proposed CMP framework, we build a large-scale practical Personalized Web Search (PWS) dataset, which consists of the search logs of 1,249 users from a commercial search engine over six months. Experimental results show that the proposed CMP framework achieves significant performance improvements than the non-personalized click models in click prediction.\n\n## Task\nCompose an abstract for the title 'Incorporating vertical results into search click models' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In modern search engines, an increasing number of search result pages (SERPs) are federated from multiple specialized search engines (called verticals, such as Image or Video). As an effective approach to interpret users' click-through behavior as feedback information, most click models were designed to reduce the position bias and improve ranking performance of ordinary search results, which have homogeneous appearances. However, when vertical results are combined with ordinary ones, significant differences in presentation may lead to user behavior biases and thus failure of state-of-the-art click models. With the help of a popular commercial search engine in China, we collected a large scale log data set which contains behavior information on both vertical and ordinary results. We also performed eye-tracking analysis to study user's real-world examining behavior. According these analysis, we found that different result appearances may cause different behavior biases both for vertical results (local effect) and for the whole result lists (global effect). These biases include: examine bias for vertical results (especially those with multimedia components), trust bias for result lists with vertical results, and a higher probability of result revisitation for vertical results. Based on these findings, a novel click model considering these biases besides position bias was constructed to describe interaction with SERPs containing verticals. Experimental results show that the new Vertical-aware Click Model (VCM) is better at interpreting user click behavior on federated searches in terms of both log-likelihood and perplexity than existing models."
      }
    ]
  },
  {
    "tid":"35",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Joint security and QoS provisioning in cooperative vehicular ad hoc networks\nContent: In vehicular ad hoc networks (VANETs), security always comes with a price in terms of QoS performance degradation. In this paper, we take an integrated approach of optimizing both security and QoS parameters, and study the tradeoffs between them in VANETs. Specifically, we use recent advances in cooperative communication to enhance the QoS performance of VANETs. In addition, we present a prevention-based security technique that provides both hop-by-hop and end-to-end authentication and integrity protection. We derive the closed-form effective secure throughput considering both security and QoS provisioning in VANETs with cooperative communications. The system is formulated as a partially observable Markov decision process (POMDP). Simulation results are presented to show that our proposed scheme can substantially improve the effective secure throughput of VANETs with cooperative communications.\n\nTitle: Performance Analysis of High Speed Rail Wireless Communication Network\nContent: GSM-R (Global System for Mobile Communications-Rail) is employed as the main channel to transfer safety data in high speed rail control system. Its reliability directly affects train safety. In the analysis of two different GSM-R redundant networks, we use the Alta Rica language and Markov Chain in modeling. Combined with related data, system reliability and availability are calculated through simulations respectively. Simulation results: co-sited network reliability is higher than intercross network's due to differences of redundant network structures and handoff rates, co-sited network availability is lower than intercross network due to co-sited network common failure factor. Based on these conclusions, suggestions are: the co-sited network should be used in key parts which demand high reliability in short time while the intercross network should be used in most parts along the rails.\n\nTitle: Handoff Performance Improvements in an Integrated Train-Ground Communication System Based on Wireless Network Virtualization.\nContent: In existing urban rail transit systems, the train-ground communication system for different subsystems is deployed independently. Investing and constructing the communication infrastructures repeatedly not only wastes substantial social resources, but it also is difficult to maintain all these infrastructures. In this paper, we propose an integrated train-ground communication system based on wireless network virtualization for urban rail transit systems. In order to improve the communication-based train control (CBTC) subsystem performance during handoff, we propose a novel handoff scheme to support handoff between virtual networks. The application-layer quality-of-service (QoS) parameters of the CBTC, passenger information system, and closed circuit television subsystems are used as the performance measures in the handoff design. We then formulate the QoS optimization problem in the proposed integrated train-ground communication system as an approximate dynamic programming (ADP) problem. The extensive simulation results show that the proposed integrated train-ground communication system QoS can be improved substantially with our ADP-based optimization model.\n\nTitle: Cross-Layer Handoff Design in MIMO-Enabled WLANs for Communication-Based Train Control (CBTC) Systems\nContent: Communication-Based Train Control (CBTC) system is an automated train control system using bidirectional train-ground communications to ensure the safe operation of rail vehicles. Handoff design has significant impacts on the train control performance in CBTC systems based on multi-input and multi-output (MIMO)-enabled WLANs. Most of previous works use traditional design criteria, such as network capacity and communication latency, in handoff designs. However, these designs do not necessarily benefit the train control performance. In this paper, we take an integrated design approach to jointly optimize handoff decisions and physical layer parameters to improve the train control performance in CBTC systems. We use linear quadratic cost for the train controller as the performance measure. The handoff decision and physical layer parameters adaptation problem is formulated as a stochastic control process. Simulation result shows that the proposed approach can significantly improve the control performance in CBTC systems.\n\nTitle: Handoff Management in Communication-Based Train Control Networks Using Stream Control Transmission Protocol and IEEE 802.11p WLANs.\nContent: Communication-based train control (CBTC) network is an automated control network for railways that ensures the safe operation of rail vehicles using data communications. CBTC networks have stringent requirements for communication availability and latency. Wireless local area network (WLAN) is a popular choice in CBTC networks due to the available commercial-off-the-shelf WLAN equipments. However, handoffs in WLANs may result in communication interruption and long latency in WLANs-based CBTC networks. In this article, we propose a handoff management scheme for CBTC networks using stream control transmission protocol (SCTP) and IEEE 802.11p WLANs to provide high communication availability and low latency in CBTC networks. We formulate the handoff decision problem as a stochastic Semi-Markov Decision Process (SMDP) with the objectives of minimizing the handoff latency and maximizing the SCTP throughput. Simulation results are presented to show that the proposed scheme can significantly improve the handoff performance in CBTC networks.\n\n## Task\nCompose an abstract for the title 'The QoS Indicators Analysis of Integrated EUHT Wireless Communication System Based on Urban Rail Transit in High-Speed Scenario.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Nowadays, in urban rail transit systems, train wayside communication system uses Wireless Local Area Network (WLAN) as wireless technologies to achieve safety-related information exchange between trains and wayside equipment. However, according to the high speed mobility of trains and the limitations of frequency band, WLAN is unable to meet the demands of future intracity and intercity rail transit. And although the Time Division-Long Term Evolution (TD-LTE) technology has high performance compared with WLAN, only 20 MHz bandwidth can be used at most. Moreover, in high-speed scenario over 300 km/h, TD-LTE can hardly meet the future requirement as well. The equipment based on Enhanced Ultra High Throughput (EUHT) technology can achieve a better performance in high-speed scenario compared with WLAN and TD-LTE. Furthermore, it allows using the frequency resource flexibly based on 5.8 GHz, such as 20 MHz, 40 MHz, and 80 MHz. In this paper, we set up an EUHT wireless communication system for urban rail transit in high-speed scenario integrated all the traffics of it. An outdoor testing environment in Beijing-Tianjin High-speed Railway is set up to measure the performance of integrated EUHT wireless communication system based on urban rail transit. The communication delay, handoff latency, and throughput of this system are analyzed. Extensive testing results show that the Quality of Service (QoS) of the designed integrated EUHT wireless communication system satisfies the requirements of urban rail transit system in high-speed scenario. Moreover, compared with testing results of TD-LTE which we got before, the maximum handoff latency of safety-critical traffics can be decreased from 225 ms to 150 ms. The performance of throughput-critical traffics can achieve 2-way 2 Mbps CCTV and 1-way 8 Mbps PIS which are much better than 2-way 1 Mbps CCTV and 1-way 2 Mbps PIS in TD-LTE."
      }
    ]
  },
  {
    "tid":"36",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Optimal Two-Level Speed Assignment For Real-Time Systems\nContent: Reducing energy consumption is one of the main concerns in the design and implementation of embedded real-time systems. For this reason, the current generation of processors allows to vary voltage and operating frequency to balance computational speed and energy consumption. This technique is called dynamic voltage scaling (DVS).When applying DVS to hard real-time systems, it is important to provide the worst-case computational requirement; otherwise the timing constraints may be violated. However, the probability of a task executing for its worst-case execution time is very low.In this paper, we show how to exploit probabilistic information about the execution time of a task in order to reduce the energy consumed by the processor. Optimal speed assignments and transition points are found using a very general model for the processor. The model accounts for the processor idle power and time/energy overheads due to frequency transitions. We also show how these results apply to some significant cases.\n\nTitle: Minimizing CPU energy in real-time systems with discrete speed management\nContent: This article presents a general framework to analyze and design embedded systems minimizing the energy consumption without violating timing requirements. A set of realistic assumptions is considered in the model in order to apply the results in practical real-time applications. The processor is assumed to have as a set of discrete operating modes, each characterized by speed and power consumption. The energy overhead and the transition delay incurred during mode switches are considered. Task computation times are modeled with a part that scales with the speed and a part having a fixed duration, to take I/O operations into account. The proposed method allows to compute the optimal sequence of voltage/speed changes that approximates the minimum continuous speed, which guarantees the feasibility of a given set of real-time tasks, without violating the deadline constraints. The analysis is performed both under fixed and dynamic priority assignments.\n\nTitle: Exploiting Job Response-Time Information in the Co-Design of Real-Time Control Systems\nContent: We consider a real-time system of multiple tasks, each task having a plant to control. The overall quadratic control cost is to be optimized. We exploit the periodicity of the task response time, which corresponds to a periodic delay pattern in the feedback control loop. Perturbed periods are used as a tool to find a finite hyper period. We present an analytical procedure to design a periodic linear-quadratic-Gaussian (LQG) controller for tasks with fixed execution times as well as a numerical solution to the periodic -- stochastic LQG problem for tasks with variable execution times. The controllers are evaluated using simulations in real-time scheduling and control co-design examples.\n\nTitle: Partitioning Real-Time Applications Over Multicore Reservations\nContent: A full exploitation of the computational power available in a multicore platform requires the software to be specified in terms of parallel execution flows. At the same time, modern embedded systems often consist of more parallel applications with timing requirements, concurrently executing on the same platform and sharing common resources. To prevent reciprocal interference among critical activities, a resource reservation mechanism is highly desired in the kernel to achieve temporal isolation. In this paper, we propose a general methodology for abstracting the total computing power available on a multicore platform by a set of virtual processors, to allocate applications independently of the physical platform. The application, described as a set of tasks with precedence relations expressed by a directed acyclic graph, is automatically partitioned into a set of subgraphs that are selected to minimize either the overall bandwidth consumption or the required number of cores. \u00a9 2011 IEEE.\n\nTitle: Optimal Task Rate Selection in Fixed Priority Systems\nContent: The design phase of any real-time system requires balancing the limited computational resources against the functional requirements and the performance of the application. The optimal design solution can be obtained by solving an optimization problem where the system performance is maximized within the schedulability constraints. In this paper, we provide a procedure that finds the task activation rates maximizing a performance function within the deadline constraints in systems scheduled by fixed priorities. First, we describe the exact feasibility region in the domain of task frequencies. Then, we introduce a procedure that starts by finding an initial solution, and incrementally improves it by using an original branch and bound search, until the global optimum is reached. Experiments show that our algorithm finds the optimal task periods for practical problems with a remarkable speedup, if compared with existing techniques. When the size of the problem makes the global search intractable, the experiments show that the algorithm can still find a high quality solution in the very early steps.\n\n## Task\nCompose an abstract for the title 'Speed modulation in energy-aware real-time systems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper presents a general framework for analyzing and designing embedded systems with energy and timing requirements. A set of realistic assumptions is considered in the model in order to apply the results in practical realtime applications. For example, the processor is assumed to have as a set of discrete operating modes, each characterized by speed, power consumption. The transition delay between modes is considered. To take I/O operations into account, task computation times are modeled with a part that scales with the speed and a part having a fixed duration. Given a set of real-time tasks, the proposed method allows to compute the optimal sequence of voltage/speed changes that approximates the minimum continuous speed which guarantees the feasibility of the system. The analysis is performed both under fixed and dynamic priority assignments."
      }
    ]
  },
  {
    "tid":"37",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: ISABoost: A weak classifier inner structure adjusting based AdaBoost algorithm-ISABoost based application in scene categorization\nContent: AdaBoost algorithms fuse weak classifiers to be a strong classifier by adaptively determine fusion weights of weak classifiers. In this paper, an enhanced AdaBoost algorithm by adjusting inner structure of weak classifiers (ISABoost) is proposed. In the traditional AdaBoost algorithms, the weak classifiers are not changed once they are trained. In ISABoost, the inner structures of weak classifiers are adjusted before their fusion weights determination. ISABoost inherits the advantages of the AdaBoost algorithms in fusing weak classifiers to be a strong classifier. ISABoost gives each weak classifier a second chance to be adjusted stronger. The adjusted weak classifiers are more contributive to make correct classifications for the hardest samples. To show the effectiveness of the proposed ISABoost algorithm, its applications in scene categorization are evaluated. Comparisons of ISABoost and AdaBoost algorithms on three widely utilized scene datasets show the effectiveness of ISABoost algorithm.\n\nTitle: Scene categorization using boosted back-propagation neural networks\nContent: Scene categorization plays an important role in computer vision, image content understanding, and image retrieval. In this paper, back-propagation neural network (BPN) is served as the basic classifier for multi-class scene/image categorization. Four features, namely, SPM (spatial pyramid appearance descriptor represented by scale invariant feature transform), PHOG (pyramid histogram of oriented gradient), GIST, and HWVP (hierarchical wavelet packet transform) are selected as the basic inputs of BPNs. They are the appearance, shape and texture descriptors respectively. For an M (M2) classes scene categorization problem, we cascade M one-versus-all BPNs to determine the accurate label of an image. An offline multi-class Adaboost algorithm is proposed to fuse multiple BPN classifiers trained with complementary features to improve scene categorization performance. Experimental results on the widely used Scene-13 and Sport Event datasets show the effectiveness of the proposed boosted BPN based scene categorization approach. Scene categorization performances of BPN classifiers with input features: SPM, PHOG, GIST and HWVP, boosted BPN classifiers of each of the four features, and the boosted classifiers of all the four features are given. Relationships of boosted classifiers number and the scene categorization performance are also discussed. Comparisons with some existing scene categorization methods using the authors' datasets further show effectiveness of the proposed boosted BPN based approach.\n\nTitle: HWVP: hierarchical wavelet packet descriptors and their applications in scene categorization and semantic concept retrieval\nContent: Wavelet packet transform is an effective texture analysis approach by sub-band filtering. Different texture patterns have distinctive responses to the sub-bands of wavelet packets. The responses are valuable for texture description. Utilizing all the responses of the sub-bands of different resolutions can improve texture pattern discrimination power. In this paper, effective texture descriptors based on hierarchical wavelet packet (HWVP) transform are proposed. The subtle sub-bands of wavelet packet transform improve the discrimination power of HWVP descriptors for the images in different categories. Scene categorization performances of the HWVP descriptors under various decomposition levels and wavelet bases are discussed. Performances of HWVP descriptors of global and local images with different partition patterns are also analyzed. The advantages of HWVP descriptors attribute to the following two aspects. Firstly sub-band filtering is helpful for improving the discrimination power of HWVP descriptors to capture the subtle differences of texture patterns. Secondly hierarchical feature representation makes the HWVP descriptors robust to resolution variations. Comparisons are made with some existing robust descriptors on scene categorization and semantic concept retrieval. Experimental results on the widely used OT, Scene-13, Sport Event, and TRECVID 2007 datasets show the effectiveness of the proposed HWVP descriptors.\n\nTitle: An improved fusion method based on adaboost algorithm for semantic concept extraction.\nContent: In this paper, based on probability distribution of weak classifier output, an improved Adaboost-based multi-classifiers fusion algorithm is proposed for semantic concept extraction. We present a novel method to compute the error rate and the weight of each classifier. We believe that the error rate of an example should be related to its rank in a weak classifier output. First, the probability distribution of the SVM output is estimated. SVM is regarded as the weak classifier in our system. Then, based on the negative and positive examples probability distributions, we can calculate the error rates of positive and negative example respectively. We define the error rate of a positive example as the proportion of negative examples whose scores are bigger than this positive example in an SVM output. Finally, we integrate the error rate into the Adaboost algorithm and add some modification to further improve our performance. We call the proposed fusion method D-Adaboost since the distribution-based error rate computing algorithm is integrated. Experimental results on TRECVID-2007 dataset show the effectiveness of the proposed D-Adaboost.\n\nTitle: Object Categorization Using Hierarchical Wavelet Packet Texture Descriptors\nContent: Object categorization plays an important role in computer vision, semantic based image content understanding, and image retrieval. Wavelet packet transform provides a very good observation for the images by sub-band filtering. Different objects have distinctive characteristics in the sub-bands of wavelet packets, which should be discriminative for objects classification. In this paper, an object categorization method using hierarchical wavelet packet texture descriptors is proposed. Comparisons between Gabor texture descriptor, pyramid of histograms of orientation gradients (PHOG) and the proposed hierarchical wavelet packet texture descriptors on the widely used OT, Scene-13 and Sport event datasets are also given. Experimental results show that object categorization performances of the proposed texture descriptors are better than that of Gabor texture descriptor and as good as that of PHOG shape descriptor. Object categorization performances of the texture descriptors under various decomposition levels and wavelet bases are discussed. Performances of texture descriptors of global and local images with different partition patterns are also analyzed.\n\n## Task\nCompose an abstract for the title 'Boosted scene categorization approach by adjusting inner structures and outer weights of weak classifiers' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Scene categorization plays an important role in computer vision and image content understanding. It is a multi-class pattern classification problem. Usually, multi-class pattern classification can be completed by using several component classifiers. Each component classifier carries out discrimination of some patterns from the others. Due to the biases of training data, and local optimal of weak classifiers, some weak classifiers may not be well trained. Usually, some component classifiers of a weak classifier may be not act well as the others. This will make the performances of the weak classifier not as good as it should be. In this paper, the inner structures of weak classifiers are adjusted before their outer weights determination. Experimental results on three AdaBoost algorithms show the effectiveness of the proposed approach."
      }
    ]
  },
  {
    "tid":"38",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Theoretical analysis of non-linear quantization of broadband channels\nContent: Multi-user multi-antenna wireless communications require the knowledge of channel state information (CSI) in order to cancel or mitigate interference between users. The knowledge of this quantization error due to the channel quantization is of extreme importance to theoretically evaluate and improve the performance of limited feedback communications. In this paper we present theoretical expressions for the statistical characterization of the quantization error obtained when a broadband multicarrier channel is quantized in the frequency domain, taking advantage of Gaussian channel behavior for multipath channels. The derived expression is valid for any type of non-linear quantization, and it is validated through Monte Carlo simulations for an interpolated uniform and non-uniform quantization strategies, under different scenarios.\n\nTitle: Iterative interference alignment techniques for broadband wireless systems with limited feedback\nContent: Interference alignment (IA) is a promising technique that allows high capacity gains in interfering channels. In this paper we consider iterative IA techniques for the downlink of OFDM-based (Orthogonal Frequency Division Multiplexing) broadband wireless systems with limited feedback. A quantized version of the channel state information (CSI) associated to the different links between base station (BS) and user terminal (UT) is feedback from the UT to the BS, which sends it to the other BSs through a limited-capacity backhaul network. This information is employed by each BS to perform the overall IA design. Our channel quantization method requires much less complexity than random vector quantization based techniques and requires the quantization of a fraction of the channel frequency response samples. The results have shown that a small number of quantization bits per multipath component is enough to allow performance close to one obtained with perfect channel knowledge.\n\nTitle: Estimation of CFO and channels in phase-shift orthogonal pilot-aided OFDM systems with transmitter diversity\nContent: We present a CFO estimation algorithm and an associated channel estimation method for broadband OFDM systems with transmitter diversity. The CFO estimation algorithm explores the TD structure of the transmitted symbols carrying pilots and data, relying solely on the data component present on the symbols to estimate the CFO, thus avoiding additional overhead like training symbols or null subcarriers. An intermediate output of the CFO algorithm provides an easy-to-get initial CIR estimate that will be improved with the utilization of a TD LMMSE filter. The feasibility of the investigated methods is substantiated by system simulation using indoor and outdoor broadband wireless channel models. Simulation results show that the joint algorithms provide a near optimal system's performance.\n\nTitle: Efficient Uniform Channel Quantization of Sparse CIR for Downlink OFDM Systems.\nContent: Channel state information at the transmitter side is an important issue for wireless communications systems, namely when precoding techniques are employed. Recent works explored random vector quantization (RVQ) as a solution for limited feedback for multi-user systems equipped with multiple antennas. Despite of being a good option for narrowband channels, this method requires large complexity and is not efficient for sparse channels. To overcome these drawbacks we consider a strategy based on uniform quantization, denoted partial uniform quantization (P-UQ), where just part of channel frequency response is quantized. This allows an efficient feedback of channel frequency response from the receivers to the transmitter, by using a reduced number of quantization bits. The comparison between the proposed P-UQ-based method and RVQ performed in this paper leads to the conclusion that the most advantageous method for sparse channels is the P-UQ.\n\nTitle: Minimum Bit-Error-Rate Nonlinear Precoding For Multi-User Distributed Antenna Systems\nContent: The aim of this manuscript is to propose and evaluate a novel nonlinear precoding scheme for distributed MIMO based systems. We consider a high-speed backhaul network where the remote antenna units are transparently linked by optical fiber to a central unit. In the considered architecture we can assume the knowledge of the data and channel state information of all users, at the central unit. The precoder design is divided in two phases: first a solution based on minimization of bit-error-rate is computed by assuming that all remote antenna units can jointly pool their power, i.e., a total power constraint is imposed instead of a per remote antenna unit power constraint; the second phase consists in scaling the obtained transmit signal by a factor eta to satisfy the individual per remote antenna unit power constraint. Numerical results show that the algorithm achieves significant gains over the linear zero-forcing and minimum mean square error based approaches, with a slight increase in complexity, and that its performance is close to the optimum.\n\n## Task\nCompose an abstract for the title 'Analytical characterization of CSI quantization effects on broadband multipath channels.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The knowledge of channel state information (CSI) at transmitter is crucial in precoding techniques to efficiently explore wireless multiuser multi-antenna capabilities namely to mitigate the interference and/or to separate spatial streams. For frequency division duplex systems, the CSI available is a quantized version fed back from the receivers, and the nonlinear effects associated to CSI quantization can lead to significant performance degradation. The knowledge of the statistical characteristics of those nonlinear distortion effects is important for performance evaluation purposes, as well as to design robust precoding techniques."
      }
    ]
  },
  {
    "tid":"39",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Abstract: Speeding-Up Memory Intensive Applications through Adaptive Hardware Accelerators\nContent: Heterogeneous architectures are becoming an increasingly relevant component for High-Performance Computing: they combine the computational power of multi-core processors with the flexibility of reconfigurable co-processor boards. Such boards are often composed of a set of standard Field Programmable Gate Arrays (FPGAs), coupled with a distributed memory architecture. This allows the concurrent execution of memory access operations. Nevertheless, since the execution latency of these operations may be unknown at compile-time, the synthesis of such parallelizing accelerators becomes a complex task. In fact, standard approaches require the construction of Finite State Machines (FSMs) whose complexity, in terms of number of states and transitions, increases exponentially with respect to the number of unbounded operations that may execute concurrently. We propose an adaptive architecture for such accelerators which overcome this limitation, while exploiting the available parallelism. The proposed design methodology is compared with FSM-based approaches by means of a motivational example.\n\nTitle: Harnessing Adaptivity Analysis for the Automatic Design of Efficient Embedded and HPC Systems\nContent: In the past decades, design methodologies of Embedded Systems (ES) and High Performance Computing (HPC) systems have evolved following different trends. However, they are lately experiencing issues that affect both the domains, whose solutions converge to similar approaches. Examples of issues affecting both the domains are: large parallelism degrees, heterogeneity, power constraints, reliability issues, self-adaptation, and significant programming efforts to reach the desired performance on increasingly complex architectures. Systems able to dynamically adjust their behavior at run-time appear good candidates for the next computing generation, and will most probably condemn non-adaptable systems to rapid extinction. Adaptive systems can deal with uncertain and unpredictable conditions, due, for example, to reliability issues. In this paper we show how we can exploit adaptivity analysis to address several design challenges in embedded systems. The results show an average increase in performance around 34% with respect to state of the art methodology, with a limited area overhead. Furthermore, we discuss our work-in-progress on the exploitation of adaptivity analysis to address new challenges in HPC systems design.\n\nTitle: A runtime adaptive controller for supporting hardware components with variable latency\nContent: Nowadays, the design of hardware cores has to necessarily deal with unpredictable components, due to process variation or to the interaction with external modules (e.g., memories, sensors, IP cores). Adaptive systems are, thus, one of the most important solutions to substitute traditional approaches, based on analysis at design time, especially in critical environments. In this paper, we present an innovative lightweight controller architecture able to automatically adjust its behavior at run-time. It interacts with the surrounding environment by means of a simple token-based communication schema. We examine the capabilities of the proposed architectural model to adapt its behavior during the execution, compared to classical ones, such as the finite state machine.\n\nTitle: An adaptive memory interface controller for improving bandwidth utilization of hybrid and reconfigurable systems\nContent: Data mining, bioinformatics, knowledge discovery, social network analysis, are emerging irregular applications that exploits data structures based on pointers or linked lists, such as graphs, unbalanced trees or unstructured grids. These applications are characterized by unpredictable memory accesses and generally are memory bandwidth bound, but also presents large amounts of inherent dynamic parallelism because they can potentially spawn concurrent activities for each one of the element they are exploring. Hybrid architectures, which integrate general purpose processors with reconfigurable devices, appears promising target platforms for accelerating irregular applications. These systems often connect to distributed and multi-ported memories, potentially enabling parallel memory operations. However, these memory architectures introduce several challenges, such as the necessity to manage concurrency and synchronization to avoid structural conflicts on shared memory locations and to guarantee consistency. In this paper we present an adaptive Memory Interface Controller (MIC) that addresses these issues. The MIC is a general and customizable solution that can target several different memory structures, and is suitable for High Level Synthesis frameworks. It implements a dynamic arbitration scheme, which avoids conflicts on memory resources at runtime, and supports atomic memory operations, commonly exploited for synchronization directives in parallel programming paradigms. The MIC simultaneously maps multiple accesses to different memory ports, allowing fine grained parallelism exploitation and ensuring correctness also in the presence of irregular and statically unpredictable memory access patterns. We evaluated the effectiveness of our approach on a typical irregular kernel, graph Breadth First Search (BFS), exploring different design alternatives.\n\nTitle: Mapping and scheduling of parallel C applications with Ant Colony Optimization onto heterogeneous reconfigurable MPSoCs\nContent: Efficient mapping and scheduling of partitioned applications are crucial to improve the performance on today's reconfigurable multiprocessor systems-on-chip (MPSoCs) platforms. Most of existing heuristics adopt the directed acyclic (task) Graph as representation, that unfortunately, is not able to represent typical embedded applications (e.g., real-time and loop-partitioned). In this paper we propose a novel approach, based on Ant Colony Optimization, that explores different alternative designs to determine an efficient hardware-software partitioning, to decide the task allocation and to establish the execution order of the tasks, dealing with different design constraints imposed by a reconfigurable heterogeneous MPSoC. Moreover, it can be applied to any parallel C application, represented through Hierarchical Task Graphs. We show that our methodology, addressing a realistic target architecture, outperforms existing approaches on a representative set of embedded applications.\n\n## Task\nCompose an abstract for the title 'Applications Acceleration through Adaptive Hardware Components' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"High Level Synthesis (HLS) provides automatic flows for the generation of hardware accelerators starting from their behavioral description. HLS guarantees results comparable to hand-written design for some applications domains such as Digital Signal Processing. However, it is not yet able to cope with performance requirements when scaling the application complexity. One of the biggest limitation is an execution paradigm still based on the construction of a centralized Finite State Machine (FSM). Parallelism exploitation is thus bound to Instruction Level Parallelism within a single execution flow. This is in contrast to the current trends for hardware architectures and programming languages, which are progressively moving towards execution paradigms dominated other type of parallelisms, such as Task or Thread Level Parallelism. This work proposes a novel adaptive accelerator design, not based on the FSM execution paradigm, which provides support to dynamic parallel execution. Execution is parallel, because no pre-computed scheduled is considered. Operations are directly managed by dedicated lightweight hardware modules, which directly communicate to notify execution completion and to start other dependent operations. Execution is parallel, because several execution flows may run concurrently. The proposed design targets different application domains, from Embedded Systems accelerators to hybrid high-performance architectures."
      }
    ]
  },
  {
    "tid":"40",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Multi-view Matrix Factorization for Linear Dynamical System Estimation.\nContent: We consider maximum likelihood estimation of linear dynamical systems with generalized-linear observation models. Maximum likelihood is typically considered to be hard in this setting since latent states and transition parameters must be inferred jointly. Given that expectation-maximization does not scale and is prone to local minima, moment-matching approaches from the subspace identification literature have become standard, despite known statistical efficiency issues. In this paper, we instead reconsider likelihood maximization and develop an optimization based strategy for recovering the latent states and transition parameters. Key to the approach is a two-view reformulation of maximum likelihood estimation for linear dynamical systems that enables the use of global optimization algorithms for matrix factorization. We show that the proposed estimation strategy outperforms widely-used identification algorithms such as subspace identification methods, both in terms of accuracy and runtime.\n\nTitle: Rank/Norm Regularization with Closed-Form Solutions: Application to Subspace Clustering\nContent:   When data is sampled from an unknown subspace, principal component analysis (PCA) provides an effective way to estimate the subspace and hence reduce the dimension of the data. At the heart of PCA is the Eckart-Young-Mirsky theorem, which characterizes the best rank k approximation of a matrix. In this paper, we prove a generalization of the Eckart-Young-Mirsky theorem under all unitarily invariant norms. Using this result, we obtain closed-form solutions for a set of rank/norm regularized problems, and derive closed-form solutions for a general class of subspace clustering problems (where data is modelled by unions of unknown subspaces). From these results we obtain new theoretical insights and promising experimental results. \n\nTitle: Semi-supervised multi-label classification: a simultaneous large-margin, subspace learning approach\nContent: Labeled data is often sparse in common learning scenarios, either because it is too time consuming or too expensive to obtain, while unlabeled data is almost always plentiful. This asymmetry is exacerbated in multi-label learning, where the labeling process is more complex than in the single label case. Although it is important to consider semi-supervised methods for multi-label learning, as it is in other learning scenarios, surprisingly, few proposals have been investigated for this particular problem. In this paper, we present a new semi-supervised multi-label learning method that combines large-margin multi-label classification with unsupervised subspace learning. We propose an algorithm that learns a subspace representation of the labeled and unlabeled inputs, while simultaneously training a supervised large-margin multi-label classifier on the labeled portion. Although joint training of these two interacting components might appear intractable, we exploit recent developments in induced matrix norm optimization to show that these two problems can be solved jointly, globally and efficiently. In particular, we develop an efficient training procedure based on subgradient search and a simple coordinate descent strategy. An experimental evaluation demonstrates that semi-supervised subspace learning can improve the performance of corresponding supervised multi-label learning methods.\n\nTitle: Convex Relaxations of Latent Variable Training\nContent: We investigate a new, convex relaxation of an expectation-maximization (EM) variant that approximates a standard objective while eliminating local minima. First, a cautionary result is presented, showing that any convex relaxation of EM over hidden variables must give trivial results if any dependence on the missing values is retained. Although this appears to be a strong negative outcome, we then demonstrate how the problem can be bypassed by using equivalence relations in- stead of value assignments over hidden variables. In particular, we develop new algorithms for estimating exponential conditional models that only require equiv- alence relation information over the variable values. This reformulation leads to an exact expression for EM variants in a wide range of problems. We then develop a semidefinite relaxation that yields global training by eliminating local minima.\n\nTitle: Optimal reverse prediction: a unified perspective on supervised, unsupervised and semi-supervised learning\nContent: Training principles for unsupervised learning are often derived from motivations that appear to be independent of supervised learning. In this paper we present a simple unification of several supervised and unsupervised training principles through the concept of optimal reverse prediction: predict the inputs from the target labels, optimizing both over model parameters and any missing labels. In particular, we show how supervised least squares, principal components analysis, k-means clustering and normalized graph-cut can all be expressed as instances of the same training principle. Natural forms of semi-supervised regression and classification are then automatically derived, yielding semi-supervised learning algorithms for regression and classification that, surprisingly, are novel and refine the state of the art. These algorithms can all be combined with standard regularizers and made non-linear via kernels.\n\n## Task\nCompose an abstract for the title 'Scalable and Sound Low-Rank Tensor Learning.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Many real-world data arise naturally as tensors. Equipped with a low rank prior, learning algorithms can benefit from exploiting the rich dependency encoded in a tensor. Despite its prevalence in low-rank matrix learning, trace norm ceases to be tractable in tensors and therefore most existing works resort to matrix unfolding. Although some theoretical guarantees are available, these approaches may lose valuable structure information and are not scalable in general. To address this problem, we propose directly optimizing the tensor trace norm by approximating its dual spectral norm, and we show that the approximation bounds can be efficiently converted to the original problem via the generalized conditional gradient algorithm. The resulting approach is scalable to large datasets, and matches state-of-the-art recovery guarantees. Experimental results on tensor completion and multitask learning confirm the superiority of the proposed method."
      }
    ]
  },
  {
    "tid":"41",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: CBMAR: a comprehensive \u03b2-lactamase molecular annotation resource.\nContent: b-Lactam antibiotics are among the most widely used antibiotics against microbial pathogens. However, enzymatic hydrolysis of these antibiotics by bacterial beta-lactamases is increasingly compromising their efficiency. Although new generation beta-lactam antibiotics have been developed to combat antibiotic resistance, beta-lactamases have also evolved along with the new variants of the substrate. A strong selection pressure from the newer generation of beta-lactam antibiotics has resulted in evolution of different families within each class of beta-lactamase. To facilitate detailed characterization of different families of beta-lactamases, we have created a database, CBMAR, which facilitates comprehensive molecular annotation and discovery of novel beta-lactamases. As against the limited scope of other existing similar databases, CBMAR provides information useful for molecular and biochemical characterization of each family of beta-lactamase. The basic architecture of CBMAR is based on Ambler classification, which divides beta-lactamases as serine (Classes A, C and D) and metallo-beta-lactamases (Class B). Each class is further divided into several families on the basis of their hydrolytic character. In CBMAR, each family is annotated with (i) sequence variability, (ii) antibiotic resistance profile, (iii) inhibitor susceptibility, (iv) active site, (v) family fingerprints, (vi) mutational profile, (vii) variants, (viii) gene location, (ix) phylogenetic tree and several other features. Each entry also has external links to the relevant protein/nucleotide sequence and structure databases. The database also supports sequence similarity searches using BLAST and assigns a new beta-lactamase protein to its respective family on the basis of family-specific fingerprint.\n\nTitle: Bhairpred: Prediction Of Beta-Hairpins In A Protein From Multiple Alignment Information Using Ann And Svm Techniques\nContent: This paper describes a method for predicting a supersecondary structural motif, beta-hairpins, in a protein sequence. The method was trained and tested on a set of 5102 hairpins and 5131 non-hairpins, obtained from a non-redundant dataset of 2880 proteins using the DSSP and PROMOTIF programs. Two machine-learning techniques, an artificial neural network (ANN) and a support vector machine (SVM), were used to predict beta-hairpins. An accuracy of 65.5% was achieved using ANN when an amino acid sequence was used as the input. The accuracy improved from 65.5 to 69.1% when evolutionary information (PSI- BLAST profile), observed secondary structure and surface accessibility were used as the inputs. The accuracy of the method further improved from 69.1 to 79.2% when the SVM was used for classification instead of the ANN. The performances of the methods developed were assessed in a test case, where predicted secondary structure and surface accessibility were used instead of the observed structure. The highest accuracy achieved by the SVM based method in the test case was 77.9%. A maximum accuracy of 71.1% with Matthew's correlation coefficient of 0.41 in the test case was obtained on a dataset previously used by X. Cruz, E. G. Hutchinson, A. Shephard and J.M. Thornton (2002) Proc. NatlAcad. Sci. USA, 99, 11157-11162. The performance of the method was also evaluated on proteins used in the '6th community-wide experiment on the critical assessment of techniques for protein structure prediction (CASP6)'. Based on the algorithm described, a web server, BhairPred ( http:// www. imtech. res. in/ raghava/ bhairpred/), has been developed, which can be used to predict b- hairpins in a protein using the SVM approach.\n\nTitle: Identification of DNA-binding proteins using support vector machines and evolutionary profiles.\nContent: Identification of DNA-binding proteins is one of the major challenges in the field of genome annotation, as these proteins play a crucial role in gene-regulation. In this paper, we developed various SVM modules for predicting DNA-binding domains and proteins. All models were trained and tested on multiple datasets of non-redundant proteins.SVM models have been developed on DNAaset, which consists of 1153 DNA-binding and equal number of non DNA-binding proteins, and achieved the maximum accuracy of 72.42% and 71.59% using amino acid and dipeptide compositions, respectively. The performance of SVM model improved from 72.42% to 74.22%, when evolutionary information in form of PSSM profiles was used as input instead of amino acid composition. In addition, SVM models have been developed on DNAset, which consists of 146 DNA-binding and 250 non-binding chains/domains, and achieved the maximum accuracy of 79.80% and 86.62% using amino acid composition and PSSM profiles. The SVM models developed in this study perform better than existing methods on a blind dataset.A highly accurate method has been developed for predicting DNA-binding proteins using SVM and PSSM profiles. This is the first study in which evolutionary information in form of PSSM profiles has been used successfully for predicting DNA-binding proteins. A web-server DNAbinder has been developed for identifying DNA-binding proteins and domains from query amino acid sequences http://www.imtech.res.in/raghava/dnabinder/.\n\nTitle: COPid: Composition Based Protein Identification.\nContent: In the past, a large number of methods have been developed for predicting various characteristics of a protein from its composition. In order to exploit the full potential of protein composition, we developed the web-server COPid to assist the researchers in annotating the function of a protein from its composition using whole or part of the protein. COPid has three modules called search, composition and analysis. The search module allows searching of protein sequences in six different databases. Search results list database proteins in ascending order of Euclidian distance or descending order of compositional similarity with the query sequence. The composition module allows calculation of the composition of a sequence and average composition of a group of sequences. The composition module also allows computing composition of various types of amino acids (e.g. charge, polar, hydrophobic residues). The analysis module provides the following options; i) comparing composition of two classes of proteins, ii) creating a phylogenetic tree based on the composition and iii) generating input patterns for machine learning techniques. We have evaluated the performance of composition-based (or alignment-free) similarity search in the subcellular localization of proteins. It was found that the alignment free method performs reasonably well in predicting certain classes of proteins. The COPid web-server is available at http://www.imtech.res.in/raghava/copid/.\n\nTitle: Prediction of Polyadenylation Signals in Human DNA Sequences using Nucleotide Frequencies.\nContent: The polyadenylation signal plays a key role in determining the site for addition of a polyadenylated tail to nascent mRNA and its mutation(s) are reported in many diseases. Thus, identifying poly(A) sites is important for understanding the regulation and stability of mRNA. In this study, Support Vector Machine (SVM) models have been developed for predicting poly(A) signals in a DNA sequence using 100 nucleotides, each upstream and downstream of this signal. Here, we introduced a novel split nucleotide frequency technique, and the models thus developed achieved maximum Matthews correlation coefficients (MCC) of 0.58, 0.69, 0.70 and 0.69 using mononucleotide, dinucleotide, trinucleotide, and tetranucleotide frequencies, respectively. Finally, a hybrid model developed using a combination of dinucleotide, 2nd order dinucleotide and tetranucleotide frequencies, achieved a maximum MCC of 0.72. Moreover, for independent datasets this model achieved a precision ranging from 75.8-95.7% with a sensitivity of 57%, which is better than any other known methods.\n\n## Task\nCompose an abstract for the title 'LPS-annotate: complete annotation of compositionally biased regions in the protein knowledgebase.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Compositional bias (i.e. a skew in the composition of a biological sequence towards a subset of residue types) can occur at a wide variety of scales, from compositional biases of whole genomes, down to short regions in individual protein and gene-DNA sequences that are compositionally biased (CB regions). Such CB regions are made from a subset of residue types that are strewn along the length of the region in an irregular way. Here, we have developed the database server LPS-annotate, for the analysis of such CB regions, and protein disorder in protein sequences. The algorithm defines compositional bias through a thorough search for lowest-probability subsequences (LPSs) (i.e., the least likely sequence regions in terms of composition). Users can (i) initially annotate CB regions in input protein or nucleotide sequences of interest, and then (ii) query a database of greater than 1 500 000 pre-calculated protein-CB regions, for investigation of further functional hypotheses and inferences, about the specific CB regions that were discovered, and their protein disorder propensities. We demonstrate how a user can search for CB regions of similar compositional bias and protein disorder, with a worked example. We show that our annotations substantially augment the CB-region annotations that already exist in the UniProt database, with more comprehensive annotation of more complex CB regions. Our analysis indicates tens of thousands of CB regions that do not comprise globular domains or transmembrane domains, and that do not have a propensity to protein disorder, indicating a large cohort of protein-CB regions of biophysically uncharacterized types. This server and database is a conceptually novel addition to the workbench of tools now available to molecular biologists to generate hypotheses and inferences about the proteins that they are investigating. It can be accessed at http://libaio.biol.mcgill.ca/lps-annotate.html."
      }
    ]
  },
  {
    "tid":"42",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Diverse retrieval via greedy optimization of expected 1-call@k in a latent subtopic relevance model\nContent: It has been previously observed that optimization of the 1-call@k relevance objective (i.e., a set-based objective that is 1 if at least one document is relevant, otherwise 0) empirically correlates with diverse retrieval. In this paper, we proceed one step further and show theoretically that greedily optimizing expected 1-call@k w.r.t. a latent subtopic model of binary relevance leads to a diverse retrieval algorithm sharing many features of existing diversification approaches. This new result is complementary to a variety of diverse retrieval algorithms derived from alternate rank-based relevance criteria such as average precision and reciprocal rank. As such, the derivation presented here for expected 1-call@k provides a novel theoretical perspective on the emergence of diversity via a latent subtopic model of relevance --- an idea underlying both ambiguous and faceted subtopic retrieval that have been used to motivate diverse retrieval.\n\nTitle: Sequential Bayesian Optimisation For Spatial-Temporal Monitoring\nContent: Bayesian Optinisationhas received considerable attention in recent years as a general methodology to find the maximum of costly-to-evaluate objective functions. Most existing BO work focuses on where to gather a set of samples without giving special consideration to the sampling sequence, or the costs or constraints associated with that sequence. However, in real-world sequential decision problems such as robotics, the order in which samples are gathered is paramount, especially when the robot needs to optimise a temporally non-stationary objective function. Additionally, the state of the environment and sensing platform determine the type and cost of samples that can be gathered. To address these issues, we formulate Sequential Bayesian Optinisation(SBO) with side-state information within a Partially Observed Markov Decision Process (POMDP) framework that can accommodate discrete and continuous observation spaces. We build on previous work using Monte-Carlo Tree Search (MCTS) and Upper Confidence boundfor Trees (UCT) for POMDPs and extend it to work with continuous state and observation spaces. Through a series of experiments on monitoring a spatial-temporal process with a mobile robot, we show that our UCT-based SBO POMDP optimisation outperforms myopic and non-myopic alternatives.\n\nTitle: Stochastic Planning and Lifted Inference.\nContent: The paper argues that (1) stochastic planning should be used as a core problem domain for relational probabilistic models providing problems of interest that are challenging for current approaches and significant scope for extending their capabilities, (2) that symbolic dynamic programming solving such problems can be seen as a prime example of lifted inference in relational probabilistic problems, (3) that first order decision diagrams provide a useful tool to drive such lifted computations, and (4) that the resulting lifted inference is qualitatively different from what other approaches are providing. As a result, this relationship can be studied to the benefit of developing foundations for relational probabilistic models and to the benefit of stochastic planning.\n\nTitle: Expecting to be HIP: Hawkes Intensity Processes for Social Media Popularity.\nContent: Modeling and predicting the popularity of online content is a significant problem for the practice of information dissemination, advertising, and consumption. Recent work analyzing massive datasets advances our understanding of popularity, but one major gap remains: To precisely quantify the relationship between the popularity of an online item and the external promotions it receives. This work supplies the missing link between exogenous inputs from public social media platforms, such as Twitter, and endogenous responses within the content platform, such as YouTube. We develop a novel mathematical model, the Hawkes intensity process, which can explain the complex popularity history of each video according to its type of content, network of diffusion, and sensitivity to promotion. Our model supplies a prototypical description of videos, called an endo-exo map. This map explains popularity as the result of an extrinsic factor -- the amount of promotions from the outside world that the video receives, acting upon two intrinsic factors -- sensitivity to promotion, and inherent virality. We use this model to forecast future popularity given promotions on a large 5-months feed of the most-tweeted videos, and found it to lower the average error by 28.6% from approaches based on popularity history. Finally, we can identify videos that have a high potential to become viral, as well as those for which promotions will have hardly any effect.\n\nTitle: Scalable Planning with Tensorflow for Hybrid Nonlinear Domains.\nContent: Given recent deep learning results that demonstrate the ability to effectively optimize high-dimensional non-convex functions with gradient descent optimization on GPUs, we ask in this paper whether symbolic gradient optimization tools such as Tensorflow can be effective for planning in hybrid (mixed discrete and continuous) nonlinear domains with high dimensional state and action spaces? To this end, we demonstrate that hybrid planning with Tensorflow and RMSProp gradient descent is competitive with mixed integer linear program (MILP) based optimization on piecewise linear planning domains (where we can compute optimal solutions) and substantially outperforms state-of-the-art interior point methods for nonlinear planning domains. Furthermore, we remark that Tensorflow is highly scalable, converging to a strong plan on a large-scale concurrent domain with a total of 576,000 continuous action parameters distributed over a horizon of 96 time steps and 100 parallel instances in only 4 minutes. We provide a number of insights that clarify such strong performance including observations that despite long horizons, RMSProp avoids both the vanishing and exploding gradient problems. Together these results suggest a new frontier for highly scalable planning in nonlinear hybrid domains by leveraging GPUs and the power of recent advances in gradient descent with highly optimized toolkits like Tensorflow.\n\n## Task\nCompose an abstract for the title 'The 2014 International Planning Competition: Progress and Trends.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This article reviews the 2014 International Planning Competition (IPC-2014), the eighth in a series of competitions starting in 1998. IPC-2014 was held in three separate parts to assess the state of the art in three prominent areas of planning research: the deterministic (classical) part (IPCD), the learning part (IPCL), and the probabilistic part (IPPC). Each part evaluated planning systems in ways that pushed the edge of existing planner performance by introducing new challenges, novel tasks, or both. The competition surpassed again the number of competitors that participated in its predecessor, highlighting the competition's central role in shaping the landscape of ongoing developments in evaluating planning systems."
      }
    ]
  },
  {
    "tid":"43",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Optimizing relevance and revenue in ad search: a query substitution approach\nContent: The primary business model behind Web search is based on textual advertising, where contextually relevant ads are displayed alongside search results. We address the problem of selecting these ads so that they are both relevant to the queries and profitable to the search engine, showing that optimizing ad relevance and revenue is not equivalent. Selecting the best ads that satisfy these constraints also naturally incurs high computational costs, and time constraints can lead to reduced relevance and profitability. We propose a novel two-stage approach, which conducts most of the analysis ahead of time. An offine preprocessing phase leverages additional knowledge that is impractical to use in real time, and rewrites frequent queries in a way that subsequently facilitates fast and accurate online matching. Empirical evaluation shows that our method optimized for relevance matches a state-of-the-art method while improving expected revenue. When optimizing for revenue, we see even more substantial improvements in expected revenue.\n\nTitle: Joint relevance and freshness learning from clickthroughs for news search\nContent: In contrast to traditional Web search, where topical relevance is often the main selection criterion, news search is characterized by the increased importance of freshness. However, the estimation of relevance and freshness, and especially the relative importance of these two aspects, are highly specific to the query and the time when the query was issued. In this work, we propose a unified framework for modeling the topical relevance and freshness, as well as their relative importance, based on click logs. We use click statistics and content analysis techniques to define a set of temporal features, which predict the right mix of freshness and relevance for a given query. Experimental results on both historical click data and editorial judgments demonstrate the effectiveness of the proposed approach.\n\nTitle: The anatomy of an ad: structured indexing and retrieval for sponsored search\nContent: The core task of sponsored search is to retrieve relevant ads for the user's query. Ads can be retrieved either by exact match, when their bid term is identical to the query, or by advanced match, which indexes ads as documents and is similar to standard information retrieval (IR). Recently, there has been a great deal of research into developing advanced match ranking algorithms. However, no previous research has addressed the ad indexing problem. Unlike most traditional search problems, the ad corpus is defined hierarchically in terms of advertiser accounts, campaigns, and ad groups, which further consist of creatives and bid terms. This hierarchical structure makes indexing highly non-trivial, as naively indexing all possible \"displayable\" ads leads to a prohibitively large and ineffective index. We show that ad retrieval using such an index is not only slow, but its precision is suboptimal as well. We investigate various strategies for compact, hierarchy-aware indexing of sponsored search ads through adaptation of standard IR indexing techniques. We also propose a new ad retrieval method that yields more relevant ads by exploiting the structured nature of the ad corpus. Experiments carried out over a large ad test collection from a commercial search engine show that our proposed methods are highly effective and efficient compared to more standard indexing and retrieval approaches.\n\nTitle: Exploiting site-level information to improve web search\nContent: Ranking Web search results has long evolved beyond simple bag-of-words retrieval models. Modern search engines routinely employ machine learning ranking that relies on exogenous relevance signals. Yet the majority of current methods still evaluate each Web page out of context. In this work, we introduce a novel source of relevance information for Web search by evaluating each page in the context of its host Web site. For this purpose, we devise two strategies for compactly representing entire Web sites. We formalize our approach by building two indices, a traditional page index and a new site index, where each \"document\" represents the an entire Web site. At runtime, a query is first executed against both indices, and then the final page score for a given query is produced by combining the scores of the page and its site. Experimental results carried out on a large-scale Web search test collection from a major commercial search engine confirm the proposed approach leads to consistent and significant improvements in retrieval effectiveness.\n\nTitle: Ad retrieval systems in vitro and in vivo: knowledge-based approaches to computational advertising\nContent: Over the past decade, online advertising became the principal economic force behind many an Internet service, from major search engines to globe-spanning social networks to blogs. There is often a tension between online advertising and user experience, but on the other hand, advertising revenue enables a myriad of free Web services to the public and fosters a great deal of innovation. Matching the advertisers' message to a receptive and interested audience benefits both sides; indeed, literally hundreds of millions of users occasionally click on the ads, hence they should be considered relevant to the users' information needs by current IR evaluation principles. The utility of ads can be better explained by considering advertising as a medium of information [2, 3]. Similarly to aggregated search [1], which enhances users' Web search experience with relevant news, local results, user-generated content, or multimedia, online advertising provides another rich source of content. This source, however, is in a complexity class of its own, due to the brevity of bid phrases, ad text being optimized for presentation rather than indexing, and multiple, possibly contradictory utility functions.\n\n## Task\nCompose an abstract for the title 'Search advertising using web relevance feedback' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The business of Web search, a $10 billion industry, relies heavily on sponsored search, whereas a few carefully-selected paid advertisements are displayed alongside algorithmic search results. A key technical challenge in sponsored search is to select ads that are relevant for the user's query. Identifying relevant ads is challenging because queries are usually very short, and because users, consciously or not, choose terms intended to lead to optimal Web search results and not to optimal ads. Furthermore, the ads themselves are short and usually formulated to capture the reader's attention rather than to facilitate query matching. Traditionally, matching of ads to queries employed standard information retrieval techniques using the bag of words approach. Here we propose to go beyond the bag of words, and augment both queries and ads with additional knowledge-rich features. We use Web search results initially returned for the query to create a pool of relevant documents. Classifying these documents with respect to an external taxonomy and identifying salient named entities give rise to two new feature types. Empirical evaluation based on over 9,000 query-ad pairwise judgments confirms that using augmented queries produces highly relevant ads. Our methodology also relaxes the requirement for each ad to explicitly specify the exhaustive list of queries (\"bid phrases\") that can trigger it."
      }
    ]
  },
  {
    "tid":"44",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: How delays matter in an oscillatory whole-brain spiking-neuron network model for MEG alpha-rhythms at rest.\nContent: In recent years the study of the intrinsic brain dynamics in a relaxed awake state in the absence of any specific task has gained increasing attention, as spontaneous neural activity has been found to be highly structured at a large scale. This so called resting-state activity has been found to be comprised by nonrandom spatiotemporal patterns and fluctuations, and several Resting-State Networks (RSN) have been found in BOLD-fMRI as well as in MEG signal power envelope correlations. The underlying anatomical connectivity structure between areas of the brain has been identified as being a key to the observed functional network connectivity, but the mechanisms behind this are still underdetermined. Theoretical large-scale brain models for fMRI data have corroborated the importance of the connectome in shaping network dynamics, while the importance of delays and noise differ between studies and depend on the models' specific dynamics. In the current study, we present a spiking neuron network model that is able to produce noisy, distributed alpha-oscillations, matching the power peak in the spectrum of group resting-state MEG recordings. We studied how well the model captured the inter-node correlation structure of the alpha-band power envelopes for different delays between brain areas, and found that the model performs best for propagation delays inside the physiological range (5\u201310m/s). Delays also shift the transition from noisy to bursting oscillations to higher global coupling values in the model. Thus, in contrast to the asynchronous fMRI state, delays are important to consider in the presence of oscillation.\n\nTitle: Functional connectivity dynamically evolves on multiple time-scales over a static structural connectome: Models and mechanisms.\nContent: Over the last decade, we have observed a revolution in brain structural and functional Connectomics. On one hand, we have an ever-more detailed characterization of the brain's white matter structural connectome. On the other, we have a repertoire of consistent functional networks that form and dissipate over time during rest. Despite the evident spatial similarities between structural and functional connectivity, understanding how different time-evolving functional networks spontaneously emerge from a single structural network requires analyzing the problem from the perspective of complex network dynamics and dynamical system's theory. In that direction, bottom-up computational models are useful tools to test theoretical scenarios and depict the mechanisms at the genesis of resting-state activity.\n\nTitle: Structure-function discrepancy: inhomogeneity and delays in synchronized neural networks.\nContent: The discrepancy between structural and functional connectivity in neural systems forms the challenge in understanding general brain functioning. To pinpoint a mapping between structure and function, we investigated the effects of (in) homogeneity in coupling structure and delays on synchronization behavior in networks of oscillatory neural masses by deriving the phase dynamics of these generic networks. For homogeneous delays, the structural coupling matrix is largely preserved in the coupling between phases, resulting in clustered stationary phase distributions. Accordingly, we found only a small number of synchronized groups in the network. Distributed delays, by contrast, introduce inhomogeneity in the phase coupling so that clustered stationary phase distributions no longer exist. The effect of distributed delays mimicked that of structural inhomogeneity. Hence, we argue that phase (de-) synchronization patterns caused by inhomogeneous coupling cannot be distinguished from those caused by distributed delays, at least not by the naked eye. The here-derived analytical expression for the effective coupling between phases as a function of structural coupling constitutes a direct relationship between structural and functional connectivity. Structural connectivity constrains synchronizability that may be modified by the delay distribution. This explains why structural and functional connectivity bear much resemblance albeit not a one-to-one correspondence. We illustrate this in the context of resting-state activity, using the anatomical connectivity structure reported by Hagmann and others.\n\nTitle: Estimation Of Directed Effective Connectivity From Fmri Functional Connectivity Hints At Asymmetries Of Cortical Connectome\nContent: The brain exhibits complex spatio-temporal patterns of activity. This phenomenon is governed by an interplay between the internal neural dynamics of cortical areas and their connectivity. Uncovering this complex relationship has raised much interest, both for theory and the interpretation of experimental data (e.g., fMRI recordings) using dynamical models. Here we focus on the so-called inverse problem: the inference of network parameters in a cortical model to reproduce empirically observed activity. Although it has received a lot of interest, recovering directed connectivity for large networks has been rather unsuccessful so far. The present study specifically addresses this point for a noise-diffusion network model. We develop a Lyapunov optimization that iteratively tunes the network connectivity in order to reproduce second-order moments of the node activity, or functional connectivity. We show theoretically and numerically that the use of covariances with both zero and nonzero time shifts is the key to infer directed connectivity. The first main theoretical finding is that an accurate estimation of the underlying network connectivity requires that the time shift for covariances is matched with the time constant of the dynamical system. In addition to the network connectivity, we also adjust the intrinsic noise received by each network node. The framework is applied to experimental fMRI data recorded for subjects at rest. Diffusion-weighted MRI data provide an estimate of anatomical connections, which is incorporated to constrain the cortical model. The empirical covariance structure is reproduced faithfully, especially its temporal component (i.e., time-shifted covariances) in addition to the spatial component that is usually the focus of studies. We find that the cortical interactions, referred to as effective connectivity, in the tuned model are not reciprocal. In particular, hubs are either receptors or feeders: they do not exhibit both strong incoming and outgoing connections. Our results sets a quantitative ground to explore the propagation of activity in the cortex.\n\nTitle: Role of local network oscillations in resting-state functional connectivity.\nContent: Spatio-temporally organized low-frequency fluctuations (<0.1Hz), observed in BOLD fMRI signal during rest, suggest the existence of underlying network dynamics that emerge spontaneously from intrinsic brain processes. Furthermore, significant correlations between distinct anatomical regions\u2014or functional connectivity (FC)\u2014have led to the identification of several widely distributed resting-state networks (RSNs). This slow dynamics seems to be highly structured by anatomical connectivity but the mechanism behind it and its relationship with neural activity, particularly in the gamma frequency range, remains largely unknown. Indeed, direct measurements of neuronal activity have revealed similar large-scale correlations, particularly in slow power fluctuations of local field potential gamma frequency range oscillations. To address these questions, we investigated neural dynamics in a large-scale model of the human brain's neural activity. A key ingredient of the model was a structural brain network defined by empirically derived long-range brain connectivity together with the corresponding conduction delays. A neural population, assumed to spontaneously oscillate in the gamma frequency range, was placed at each network node. When these oscillatory units are integrated in the network, they behave as weakly coupled oscillators. The time-delayed interaction between nodes is described by the Kuramoto model of phase oscillators, a biologically-based model of coupled oscillatory systems. For a realistic setting of axonal conduction speed, we show that time-delayed network interaction leads to the emergence of slow neural activity fluctuations, whose patterns correlate significantly with the empirically measured FC. The best agreement of the simulated FC with the empirically measured FC is found for a set of parameters where subsets of nodes tend to synchronize although the network is not globally synchronized. Inside such clusters, the simulated BOLD signal between nodes is found to be correlated, instantiating the empirically observed RSNs. Between clusters, patterns of positive and negative correlations are observed, as described in experimental studies. These results are found to be robust with respect to a biologically plausible range of model parameters. In conclusion, our model suggests how resting-state neural activity can originate from the interplay between the local neural dynamics and the large-scale structure of the brain.\n\n## Task\nCompose an abstract for the title 'Exploring mechanisms of spontaneous functional connectivity in MEG: How delayed network interactions lead to structured amplitude envelopes of band-pass filtered oscillations.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Spontaneous (or resting-state) brain activity has attracted a growing body of neuroimaging research over the last decades. Whole-brain network models have proved helpful to investigate the source of slow (<0.1Hz) correlated hemodynamic fluctuations revealed in fMRI during rest. However, the mechanisms mediating resting-state long-distance correlations and the relationship with the faster neural activity remain unclear. Novel insights coming from MEG studies have shown that the amplitude envelopes of alpha- and beta-frequency oscillations (~8\u201330Hz) display similar correlation patterns as the fMRI signals."
      }
    ]
  },
  {
    "tid":"45",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Speeding Up Similarity Queries over Large Chinese Calligraphic Character Databases Using Data Grid\nContent: This paper proposes a novel data-grid-based k nearest neighbor query over large Chinese calligraphic character databases, which can significantly speed up the retrieval efficiency. Three steps are made. Firstly, when a user submits a query request to a query node, a process of character set reduction is performed using iDistance index in different data nodes, followed by sending the candidate characters to the executing nodes through a package-based transfer technique. Secondly, a refinement process of the candidate characters is conducted in the executing nodes in parallel to get the answer set. Finally, the answer set is transferred to the query node. The proposed method incorporates a uniform-start- distance-based character data allocation policy and character reduction algorithm. The analysis and experimental results show that the performance of the algorithm is effective in minimizing the response time by decreasing network transfer cost and increasing the parallelism of I/O and CPU.\n\nTitle: A Unified Indexing Structure for Efficient Cross-Media Retrieval\nContent: An important trend in web information processing is the support of content-based multimedia retrieval (CBMR). However, the most prevailing paradigm of CBMR, such as content-based image retrieval, content-based audio retrieval, etc, is rather conservative. It can only retrieve media objects of single modality. With the rapid development of Internet, there is a great deal of media objects of different modalities in the multimedia documents such as webpages, which exhibit latent semantic correlation. Cross-media retrieval, as a new multi-media retrieval method, is to retrieve all the related media objects with multi-modalities via submitting a query media object. To the best of our knowledge, this is the first study on how to speed up the cross-media retrieval via indexes. In this paper, based on a Cross-Reference-Graph(CRG )-based similarity retrieval method, we propose a novel unified high-dimensional indexing scheme called CIndex , which is specifically designed to effectively speedup the retrieval performance of the large cross-media databases. In addition, we have conducted comprehensive experiments to testify the effectiveness and efficiency of our proposed method.\n\nTitle: Efficient Query Execution Techniques in a 4DIS Video Database System for eLearning\nContent: In this paper, we describe our work on developing an eLearning video database system. The eLearning video database provides a temporal modeling framework for describing eLearning video data and it supports data distribution by applying vertical class partitioning techniques. Building on top of our previous work on Four Dimensional Information System (4DIS)\u2014an object-oriented temporal modeling framework, we apply vertical class partitioning techniques onto a 4DIS eLearning video database system as a means for efficient query execution. We further describe our work on dynamic fetching of multimedia eLearning video on the Internet. A detailed cost model for query execution through vertical class partitioning is developed. Finally, we demonstrate through the use of a running example the effectiveness of our vertical class partitioning approach.\n\nTitle: A multi-paradigm querying approach for a generic multimedia database management system\nContent: To truly meet the requirements of multimedia database (MMDB) management, an integrated framework for modeling, managing and retrieving various kinds of media data in a uniform way is necessary. MediaLand is an experimental MMDB platform being developed at Microsoft Research Asia for users with different levels of experiences and expertise to manage and search multimedia repositories easily, efficiently, and cooperatively. Key features of MediaLand include a uniform data model for describing all kinds of media objects and their relationships, and a 4-tier architecture based on this data model. In this paper, a multi-paradigm querying approach of MediaLand is presented, in which multimedia queries are processed based on a seamless integration of various existing search approaches. In doing so, MediaLand also offers the feature of \"media independence\" which is analogous to the notion of \"data independence\" from the classic ANSI SPARC standard. By incorporating a rich set of facilities and techniques, MediaLand lays down a good foundation for addressing further research issues, such as multimedia query rewriting, optimization, and presentation.\n\nTitle: Facilitating Spatio-Temporal Operations in a Versatile Video Database System\nContent: In a video database, large amount of information involving video, audio, and/or images needs to be stored and managed. Therefore, there is an important need for novel techniques and systems that can provide an efficient retrieval facility of the voluminous data stored in the video database. While \"content-based\" retrieval provides a direct and an intuitive approach for video data access, it is inadequate from efficient video data management viewpoint. This is because many (semantic) features of video data can not be extracted out from the video itself automatically; moreover, video objects may share annotations or descriptions. Consequently, it is n ecessary and cost-effective to complement content-based retrieval with high level (declarative) query-based retrieval in the video database system. As video objects by their nature are rich in spatial and temporal semantics, in this paper, we present a versatile video database modeling mechanism, in which it supports spatial and temporal semantics, along with the Spatio-Temporal Operations and the associated query language facilities that can accommodate a wide range of video queries. 1 Introduction Nowadays, video data management is fast becoming one of the seminal topics in multimedia information systems. An important issue to be addressed in video database management systems is video retrieval. Content-based retrieval (IEEE1995), which uses image features such as colour, texture and shapes, is alone not sufficient to fulfil the user's need. The main reaso n is due to the fact that many semantic features of video data are extremely difficult (if not impossible) to be extracted automatic ally from the video itself; moreover, video objects may share annotations and/or descriptions. Consequently, it is necessary and cos t- effective to complement content-based retrieval with declarative, efficient query-based retrieval in the video database system. As the key characteristics of video data are the spatial and temporal semantics associated with it, this makes video retrieval qui te different from, and more challenging than, other types of data access such as text, voice or image retrieval. In this paper, we concentrate on developing spatio-temporal operations for a versatile video DBMS, through an underlying object modeling mechanism called CCM/ST along with its associated query language facilities. In particular, CCM/ST is based on its previous research project (CCM) that we have developed earlier (LH1995, LS1992). CCM offers a number of versatile features such as dynamic grouping of scenes and multif aceted features modeling; these features have shown to be extremely useful for video manipulations (LL1995, LLX1997). The semantic meanings captured by the scenes and features also facilitate declarative retrieval, which results in a high-level query language called CAROL (LYC1998) which was developed to provide flexible video-retrieval in both context-dependent and context-free manners (LL1996). However, neither CCM nor CAROL has (explicit) spatial and temporal facilities for modeling and accessing video data, hence further extensions are needed to our model and language in order to support more effective and efficient video retrieval. At this stage, our main emphasis is on the feasibility of developing the new features within CCM, both in terms of expressiveness and implementability. In next section, we review our existing prototype system and its underlying model (i.e. CCM) with respect to the scene and feature facilities. In section 3, we describe the new modeling facilities and operations that are being incorporated into CCM/ST, along with a brief look at its associated query language CAROL/ST. Moreover, we describe in section 4 some implementation issues on the new features. Finally, conclusions and future work are given in section 5. 2 Background Object-oriented database (OODB) management systems model each real world entity as an object, and each of the\n\n## Task\nCompose an abstract for the title 'Towards interactive indexing for large Chinese calligraphic character databases' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, based on a novel shape-similarity-based retrieval method, we propose an interactive partial-distance-map (PDM)- based high-dimensional indexing scheme to speed up the retrieval performance of the large Chinese calligraphic character databases. Specifically, we use the approximate minimal bounding hyper- sphere of query character to search the PDM and utilize the users' relevance feedback to refine the search process. We conduct comprehensive experiments to testify the efficiency and effectiveness of the proposed method."
      }
    ]
  },
  {
    "tid":"46",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Frequency and Power Allocation for Energy Efficient OFDMA Systems with Proportional Rate Constraints\nContent: A two-step solution to the problem of finding the subchannel and power allocation that maximizes the energy efficiency of the OFDMA based transmissions from a base station, under proportional rate and total power constraints, is presented. A low complexity subchannel assignment is followed by an optimal power allocation that is obtained via a single non-linear equation. The proposed algorithm has the same computational complexity as the best algorithm in the literature for the same problem but with minimum rate constraints. While the convergence of the algorithm in the literature is not guaranteed, the algorithm in this paper is proven to converge. Simulation results show that the proposed algorithm out-performs the one in the literature when the rate constraints are ignored.\n\nTitle: A low complexity evolutionary algorithm for multi-user MIMO detection\nContent: Employing the Multi-Device Multi Input Multi Output (MIMO) communication scheme improves the wireless communication via diversity methods and spacial multiplexing. In this paper, we apply Biogeography-Based Optimization Algorithm (BBO) for Joint Symbol Detection (JSD) at the receiver station in a Multi-Device Space-Time Block Coded (STBC) MIMO system. Exhaustive search for finding an optimal detection - Maximum Likelihood (ML) detection - has a computational complexity that increases exponentially with the number of wireless devices, transmit antennas per wireless device, and the number of bits per symbols (complex modulation schemes e.g. MPSK or M-QAM), which yields to higher complexity and cost. BBO is a new population-based, biogeography inspired global optimization algorithm that mainly uses the biogeography-based migration operator to share the information among solutions. We present a low-complex migration model for partial immigration-based BBO, and apply it to JSD to find a nearly optimal solution in real time with much less computational complexity. The results of multiple independent simulation runs of the algorithms indicate a good performance/complexity trade-off compared with other well-known previously proposed algorithms for JSD, such as the near-optimal Sphere Decoding (SD), Genetic Algorithm (GA), Minimum Mean Square Error (MMSE), Vertical Bell Laboratories Layered Space-Time (V-BLAST) and Semi-Definite Relaxation (SDR). The effectiveness of BBO is verified through these simulation results.\n\nTitle: Multiuser Detection in CDMA Fast Fading Multipath Channel using Heuristic Genetic Algorithm\nContent: In this paper, a simple heuristic genetic algorithm is used for Multistage Multiuser detection in fast fading environments. Multipath channels, multiple access interference (MAI) and near far effect cause the performance of the conventional detector to degrade. Heuristic Genetic algorithms, a rapidly growing area of artificial intelligence, uses evolutionary programming for initial search, which not only helps to converge the solution towards near optimal performance efficiently but also at a very low complexity as compared with optimal detector. This holds true for Additive White Gaussian Noise (AWGN) and multipath fading channels. Experimental results are presented to show the superior performance of the proposed techque over the existing methods.\n\nTitle: Discrete artificial bee colony for computationally efficient symbol detection in multidevice STBC MIMO Systems\nContent: A Discrete Artificial Bee Colony (DABC) is presented for joint symbol detection at the receiver in a multidevice Space-Time Block Code (STBC) Mutli-Input Multi-Output (MIMO) communication system. Exhaustive search (maximum likelihood detection) for finding an optimal detection has a computational complexity that increases exponentially with the number of mobile devices, transmit antennas per mobile device, and the number of bits per symbol. ABC is a new population-based, swarm-based Evolutionary Algorithms (EA) presented for multivariable numerical functions and has shown good performance compared to other mainstream EAs for problems in continuous domain. This algorithm simulates the intelligent foraging behavior of honeybee swarms. An enhanced discrete version of the ABC algorithmis presented and applied to the joint symbol detection problem to find a nearly optimal solution in real time. The results of multiple independent simulation runs indicate the effectiveness of DABC with other well-known algorithms previously proposed for joint symbol detection such as the near-optimal sphere decoding, minimum mean square error, zero forcing, and semidefinite relaxation, along with other EAs such as genetic algorithm, estimation of distributions algorithm, and the more novel biogeography-based optimization algorithm.\n\nTitle: Symbol-by-symbol CDMA spreading gain adaptation and detection using OVSF sequences\nContent: We present a spreading gain adaptation protocol between a transmitter and a receiver communicating with each other through a Direct-Sequence Spread Spectrum system. This protocol does not require the transfer of an explicit control message indicating the change of spreading gain from the transmitter to the receiver. Also, according to this protocol, the transmitter can change the spreading gain \"symbol by symbol\" (SBS) as opposed to \"frame by frame\" (FBF); this enables extremely fast adaptation to the time-varying wireless channel and/or to the rate variation of the traffic. The proposed scheme uses the orthogonal variable spreading factor (OVSF) codes to enable the receiver to detect the spreading gain used by the transmitter. Our analysis indicates that fast adaptation can significantly improve the average throughput over a slow or no adaptation.\n\n## Task\nCompose an abstract for the title 'Minimum bit error rate multiuser detection for OFDM-SDMA using particle swarm optimization' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The Minimum Bit Error Rate (MBER) detectors outperform the conventional Minimum Mean Squared Error (MMSE) detector by minimizing the Bit Error Rate (BER) directly. In this paper an MBER multiuser detector for Orthogonal Frequency Division Multiplexing-Space Division Multiple Access (OFDM-SDMA) system is proposed employing Particle Swarm Optimization (PSO) for finding the optimum weight vectors. Simulation results show that the proposed system achieves faster convergence with lower complexity as compared to Genetic Algorithms (GA) with same Bit Error Rate (BER) performance."
      }
    ]
  },
  {
    "tid":"47",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Efficient replica maintenance for distributed storage systems\nContent: This paper considers replication strategies for storage systems that aggregate the disks of many nodes spread over the Internet. Maintaining replication in such systems can be prohibitively expensive, since every transient network or host failure could potentially lead to copying a server's worth of data over the Internet to maintain replication levels. The following insights in designing an efficient replication algorithm emerge from the paper's analysis. First, durability can be provided separately from availability; the former is less expensive to ensure and a more useful goal for many wide-area applications. Second, the focus of a durability algorithm must be to create new copies of data objects faster than permanent disk failures destroy the objects; careful choice of policies for what nodes should hold what data can decrease repair time. Third, increasing the number of replicas of each data object does not help a system tolerate a higher disk failure probability, but does help tolerate bursts of failures. Finally, ensuring that the system makes use of replicas that recover after temporary failure is critical to efficiency. Based on these insights, the paper proposes the Carbonite replication algorithm for keeping data durable at a low cost. A simulation of Carbonite storing 1 TB of data over a 365 day trace of PlanetLab activity shows that Carbonite is able to keep all data durable and uses 44% more network traffic than a hypothetical system that only responds to permanent failures. In comparison, Total Recall and DHash require almost a factor of two more network traffic than this hypothetical system.\n\nTitle: The Case for Resilient Overlay Networks\nContent: Abstract: This paper makes the case for Resilient Overlay Networks (RONs), an application-level routing and packet forwarding service that gives end-hosts and applications the ability to take advantage of network paths that traditional Internet routing cannot make use of, thereby improving their end-to-end reliability and performance. Using RON, nodes participating in a distributed Internet application configure themselves into an overlay network and cooperatively forward packets for each other. Each RON node monitors the quality of the links in the underlying Internet and propagates this information to the other nodes; this enables a RON to detect and react to path failures within several seconds rather than several minutes, and allows it to select application-specific paths based on performance. We argue that RON has the potential to substantially improve the resilience of distributed Internet applications to path outages and sustained overload.\n\nTitle: Flexible, wide-area storage for distributed systems with WheelFS\nContent: WheelFS is a wide-area distributed storage system intended to help multi-site applications share data and gain fault tolerance. WheelFS takes the form of a distributed file system with a familiar POSIX interface. Its design allows applications to adjust the tradeoff between prompt visibility of updates from other sites and the ability for sites to operate independently despite failures and long delays. WheelFS allows these adjustments via semantic cues, which provide application control over consistency, failure handling, and file and replica placement. WheelFS is implemented as a user-level file system and is deployed on PlanetLab and Emulab. Three applications (a distributed Web cache, an email service and large file distribution) demonstrate that WheelFS's file system interface simplifies construction of distributed applications by allowing reuse of existing software. These applications would perform poorly with the strict semantics implied by a traditional file system interface, but by providing cues to WheelFS they are able to achieve good performance. Measurements show that applications built on WheelFS deliver comparable performance to services such as CoralCDN and BitTorrent that use specialized wide-area storage systems.\n\nTitle: Cache craftiness for fast multicore key-value storage\nContent: We present Masstree, a fast key-value database designed for SMP machines. Masstree keeps all data in memory. Its main data structure is a trie-like concatenation of B+-trees, each of which handles a fixed-length slice of a variable-length key. This structure effectively handles arbitrary-length possiblybinary keys, including keys with long shared prefixes. +-tree fanout was chosen to minimize total DRAM delay when descending the tree and prefetching each tree node. Lookups use optimistic concurrency control, a read-copy-update-like technique, and do not write shared data structures; updates lock only affected nodes. Logging and checkpointing provide consistency and durability. Though some of these ideas appear elsewhere, Masstree is the first to combine them. We discuss design variants and their consequences. On a 16-core machine, with logging enabled and queries arriving over a network, Masstree executes more than six million simple queries per second. This performance is comparable to that of memcached, a non-persistent hash table server, and higher (often much higher) than that of VoltDB, MongoDB, and Redis.\n\nTitle: Phase reconciliation for contended in-memory transactions\nContent: Multicore main-memory database performance can collapse when many transactions contend on the same data. Contending transactions are executed serially--either by locks or by optimistic concurrency control aborts--in order to ensure that they have serializable effects. This leaves many cores idle and performance poor. We introduce a new concurrency control technique, phase reconciliation, that solves this problem for many important workloads. Doppel, our phase reconciliation database, repeatedly cycles through joined, split, and reconciliation phases. Joined phases use traditional concurrency control and allow any transaction to execute. When workload contention causes unnecessary serial execution, Doppel switches to a split phase. There, updates to contended items modify per-core state, and thus proceed in parallel on different cores. Not all transactions can execute in a split phase; for example, all modifications to a contended item must commute. A reconciliation phase merges these per-core states into the global store, producing a complete database ready for joined-phase transactions. A key aspect of this design is determining which items to split, and which operations to allow on split items. Phase reconciliation helps most when there are many updates to a few popular database records. Its throughput is up to 38x higher than conventional concurrency control protocols on microbenchmarks, and up to 3x on a larger application, at the cost of increased latency for some transactions.\n\n## Task\nCompose an abstract for the title 'Proactive replication for data durability' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Many wide\u203aarea storage systems replicate data for durability. A common way of maintaining the replicas is to detect node failures and respond by creating additional copies of objects that were stored on failed nodes and hence suffered a loss of redundancy. Reactive techniques can minimize total bytes sent since they only create replicas as needed; however, they can create spikes in network use after a failure. These spikes may overwhelm application trafc and can make it difcult to provision bandwidth. This paper explores a proactive approach that creates additional copies not in response to failures, but periodically at a x ed low rate. We introduce Tempo, a distributed hash table that allows each user to specify a maximum maintenance bandwidth and uses it to perform proactive replication. Results from a simulation study suggest that Tempo can deliver high durability despite only using several kilobytes per second of bandwidth, comparable to state\u203aof\u203a the\u203aart reactive systems."
      }
    ]
  },
  {
    "tid":"48",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: PGA: Using Graphs to Express and Automatically Reconcile Network Policies\nContent: Software Defined Networking (SDN) and cloud automation enable a large number of diverse parties (network operators, application admins, tenants/end-users) and control programs (SDN Apps, network services) to generate network policies independently and dynamically. Yet existing policy abstractions and frameworks do not support natural expression and automatic composition of high-level policies from diverse sources. We tackle the open problem of automatic, correct and fast composition of multiple independently specified network policies. We first develop a high-level Policy Graph Abstraction (PGA) that allows network policies to be expressed simply and independently, and leverage the graph structure to detect and resolve policy conflicts efficiently. Besides supporting ACL policies, PGA also models and composes service chaining policies, i.e., the sequence of middleboxes to be traversed, by merging multiple service chain requirements into conflict-free composed chains. Our system validation using a large enterprise network policy dataset demonstrates practical composition times even for very large inputs, with only sub-millisecond runtime latencies.\n\n\n\nTitle: Corybantic: towards the modular composition of SDN control programs\nContent: Software-Defined Networking (SDN) promises to enable vigorous innovation, through separation of the control plane from the data plane, and to enable novel forms of network management, through a controller that uses a global view to make globally-valid decisions. The design of SDN controllers creates novel challenges; much previous work has focused on making them scalable, reliable, and efficient. However, prior work has ignored the problem that multiple controller functions may be competing for resources (e.g., link bandwidth or switch table slots). Our Corybantic design supports modular composition of independent controller modules, which manage different aspects of the network while competing for resources. Each module tries to optimize one or more objective functions; we address the challenge of how to coordinate between these modules to maximize the overall value delivered by the controllers' decisions, while still achieving modularity.\n\nTitle: DevoFlow: scaling flow management for high-performance networks\nContent: OpenFlow is a great concept, but its original design imposes excessive overheads. It can simplify network and traffic management in enterprise and data center environments, because it enables flow-level control over Ethernet switching and provides global visibility of the flows in the network. However, such fine-grained control and visibility comes with costs: the switch-implementation costs of involving the switch's control-plane too often and the distributed-system costs of involving the OpenFlow controller too frequently, both on flow setups and especially for statistics-gathering. In this paper, we analyze these overheads, and show that OpenFlow's current design cannot meet the needs of high-performance networks. We design and evaluate DevoFlow, a modification of the OpenFlow model which gently breaks the coupling between control and global visibility, in a way that maintains a useful amount of visibility without imposing unnecessary costs. We evaluate DevoFlow through simulations, and find that it can load-balance data center traffic as well as fine-grained solutions, without as much overhead: DevoFlow uses 10--53 times fewer flow table entries at an average switch, and uses 10--42 times fewer control messages.\n\nTitle: NodeWiz: Fault-tolerant grid information service\nContent: Large scale grid computing systems may provide multitudinous services, from different providers, whose quality of service\n will vary. Moreover, services are deployed and undeployed in the grid with no central coordination. Thus, to find out the\n most suitable service to fulfill their needs, or to find the most suitable set of resources on which to deploy their services,\n grid users must resort to a Grid Information Service (GIS). This service allows users to submit rich queries that are normally\n composed of multiple attributes and range operations. The ability to efficiently execute complex searches in a scalable and\n reliable way is a key challenge for current GIS designs. Scalability issues are normally dealt with by using peer-to-peer\n technologies. However, the more reliable peer-to-peer approaches do not cater for rich queries in a natural way. On the other\n hand, approaches that can easily support these rich queries are less robust in the presence of failures. In this paper we\n present the design of NodeWiz, a GIS that allows multi-attribute range queries to be performed efficiently in a distributed\n manner, while maintaining load balance and resilience to failures.\n\nTitle: Performance analysis of the send-on-demand: A distributed database concurrency control protocol for high-speed networks\nContent: In this paper, we consider an application of high-speed wide area networks, viz. that of distributed database systems (DDBS). In a high-speed environment, although the transmission delay decreases, the propagation delay (communication latency) now becomes the throughput thwarting factor. Existing Concurrency Control (CC) protocols do not address the communication latency problem. This paper is a continuation of our research work in this area. First, we very briefly summarize our previously reported research results, viz. the issues that are important for DDBS in high-speed wide area networks, and the description of a new CC protocol called 'send-on-demand'. A detailed comparison of this protocol with that of locking is provided, using analytical and simulation techniques. The datacycle scheme performs very well in a read-oriented environment, and the rest of the paper deals with developing and comparing hybrid CC schemes using datacycling for queries. Altogether, the performance of four concurrency control schemes is compared.\n\n## Task\nCompose an abstract for the title 'Automatic Synthesis of NF Models by Program Analysis.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Network functions (NFs), like firewall, NAT, IDS, have been widely deployed in today\u2019s modern networks. However, currently there is no standard specification or modeling language that can accurately describe the complexity and diversity of different NFs. Recently there have been research efforts to propose NF models. However, they are often generated manually and thus error-prone. This paper proposes a method to automatically synthesize NF models via program analysis. We develop a tool called NFactor, which conducts code refactoring and program slicing on NF source code, in order to generate its forwarding model. We demonstrate its usefulness on two NFs and evaluate its correctness. A few applications of NFactor are described, including network verification."
      }
    ]
  },
  {
    "tid":"49",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: HRP-2W: A humanoid platform for research on support behavior in daily life environments\nContent: We introduce a concept of a real-world-oriented humanoid robot that can support humans' activities in daily life. In such environments, robots have to watch humans, understand their behavior, and support their daily life tasks. In particular, these robots must be capable of such real-world behavior as handling tableware and delivering daily commodities by hand. We developed a humanoid robot, HRP-2W, which has an upper body of HRP-2 [K. Kaneko, F. Kanehiro, S. Kajita, H. Hirukawa, T. Kawasaki, M. Hirata, K. Akachi, T. Isozumi, Humanoid Robot HRP-2, in: Proceedings of the 2004 IEEE International Conference on Robotics & Automation, 2004, pp. 1083-1090] and a wheel module instead of legs, as a research platform to fulfill this aim. We also developed basic software configuration in order to integrate our platform with other research groups. Through experiments, we demonstrated the feasibility of the humanoid robot platform and the potential of the software architecture.\n\nTitle: Description And Execution Of Humanoid'S Object Manipulation Based On Object-Environment-Robot Contact States\nContent: In the case of object manipulation by a humanoid robot, it is important to deal with contact states between objects, a robot, and an environment both to avoid falling down and to achieve objective manipulations. We propose a method to describe and uniformly execute various object manipulations by a humanoid robot. In description, we focus on the contact states and define manipulation phases according to the contact states. In execution, the humanoid's controller autonomously switches manipulation phases and substantiates the contact-force controller. According to switching of the manipulation phases, the humanoid's manipulation system switches the inputs for the contact-force controller, which includes the estimation of object's information and motion generation. We evaluated our proposed system through experiments in which the HRP-2 robot manipulates four objects without information about the objects' masses and necessary operational forces.\n\nTitle: Transparent Integration of Humanoid Robot System for Performing Various Tasks\nContent: An integrated humanoid robot system, including from low-level hardware to high-level intelligence software and user interfaces, is required to build humanoid robot system meeting the expectation that robots work in the real environment such as disaster response. For creating such an integrated system, it is important that it has sustainable development potential, partially re-usability, and transparency to any type of robot. When creating a new robot, the conventional a system is desired to use just by changing a robot hardware. In order to realize such system, it is necessary to ensure that a software system is transparent to a robot hardware. On the other hand, sustainable development and partially reusability contribute robustness of the system and quickly building a complex system. In this paper, we describe the methodology to create an integrated humanoid robot system through actual humanoid robot system we have developed. Our system achieved to have sustainable development, partially re-usability, and transparency to any type of robot.\n\nTitle: Vision Based Behavior Verification System Of Humanoid Robot For Daily Environment Tasks\nContent: This paper describes integrated/intelligent humanoid robot system for daily-life environment tasks. We have realized complex behaviors of a humanoid robot in daily-life environment based on motion planner technique using an environment and manipulation knowledge. However in order to adapt to unknown or dynamic situations, sensor based behavior variation is essentially important. In this paper, we present a design and implementation of sensor based behavior verification system using an environment and manipulation knowledge, which is also used in manipulation motion planner. We also present software architecture that allows us to write a single stream code to perform complex concurrent humanoid motions. By using this architecture, sensor based verification functions are easily integrated in motion generation functions. Finally, we demonstrated a water-pouring task and a dishwashing task of the life-sized humanoid robot HRP2-JSK in a real environment while verifying its own motion.\n\nTitle: Realization Of Dynamics Simulator Embedded Robot Brain For Humanoid Robots\nContent: This paper proposes the new robot programming environment in which robot motion programming environment and dynamics simulator are integrated. This allows robot motion programs to include simulation descriptions. Additionally, a new implementation of simulation that is composed by simulation modules is presented, on the other hand, conventional simulators are monolithic and implanted every function. This makes it difficult to add new simulation functions such as new sensors on a simulator by its users. In the new method, the users of the environment can add new modules easily. The simulation function of this system is evaluated by showing new robot motion simulations like brooming, seesaw and so on. The experiment that shows how the simulation embedded brain changes the motion planning of block moving problem is illustrated in the end this paper.\n\n## Task\nCompose an abstract for the title 'Portable Situation-Reporting System by a Palmtop Humanoid Robot for Daily Life' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We propose the portable situation reporting sys- tem by a small robot for daily life, which demands the rapidly system reconstitution for daily life support. In this paper, we established a system by an attachment mechanism assisted the robot system based on device-distributed approach and the patterned processing by the tree structuring of sensor infor- mation processing. The attachment mechanism facilitates the hardware system reconstitution and the patterned processing simplifies the software system restructuring. We evaluated the reconfigurable system not to re-write programs and judged the effectiveness of the system by the experiments. I. I NTRODUCTION"
      }
    ]
  },
  {
    "tid":"50",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Efficient processing of group-oriented connection queries in a large graph\nContent: We study query processing in large graphs that are fundamental data model underpinning various social networks and Web structures. Given a set of query nodes, we aim to find the groups which the query nodes belong to, as well as the best connection among the groups. Such a query is useful to many applications but the query processing is extremely costly. We define a new notion of Correlation Group (CG), which is a set of nodes that are strongly correlated in a large graph G. We then extract the subgraph from G that gives the best connection for the nodes in a CG. To facilitate query processing, we develop an efficient index built upon the CGs. Our experiments show that the CGs are meaningful as groups and importantly, the meaningfulness of the query results are justifiable. We also demonstrate the high efficiency of CG computation, index construction and query processing.\n\nTitle: Efficient Correlation Search from Graph Databases\nContent: Correlation mining has gained great success in many application domains for its ability to capture the underlying dependency between objects. However, research on correlation mining from graph databases is still lacking despite the proliferation of graph data in recent years. We propose a new problem of correlation mining from graph databases, called correlated graph search (CGS). CGS adopts Pearson's correlation coefficient to take into account the occurrence distributions of graphs. However, the problem poses significant challenges, since every subgraph of a graph in the database is a candidate but the number of subgraphs is exponential. We derive two necessary conditions that set bounds on the occurrence probability of a candidate in the database. With this result, we devise an efficient algorithm that mines the candidate set from a much smaller projected database and thus a significantly smaller set of candidates is obtained. Three heuristic rules are further developed to refine the candidate set. We also make use of the bounds to directly answer high-support queries without mining the candidates. Experimental results justify the efficiency of our algorithm. Finally, we generalize the CGS problem and show that our algorithm provides a general solution to most of the existing correlation measures.\n\nTitle: Cold-Start Expert Finding In Community Question Answering Via Graph Regularization\nContent: Expert finding for question answering is a challenging problem in Community-based Question Answering (CQA) systems such as Quora. The success of expert finding is important to many real applications such as question routing and identification of best answers. Currently, many approaches of expert findings rely heavily on the past question-answering activities of the users in order to build user models. However, the past question-answering activities of most users in real CQA systems are rather limited. We call the users who have only answered a small number of questions the cold-start users. Using the existing approaches, we find that it is difficult to address the cold-start issue in finding the experts.In this paper, we formulate a new problem of cold-start expert finding in CQA systems. We first utilize the \"following relations\" between the users and topical interests to build the user-to-user graph in CQA systems. Next, we propose the Graph Regularized Latent Model (GRLM) to infer the expertise of users based on both past question-answering activities and an inferred user-to-user graph. We then devise an iterative variational method for inferring the GRLM model. We evaluate our method on a well-known question-answering system called Quora. Our empirical study shows encouraging results of the proposed algorithm in comparison to the state-of-the-art expert finding algorithms.\n\nTitle: Expert Finding for Question Answering via Graph Regularized Matrix Completion\nContent: Expert finding for question answering is a challenging problem in community-based question answering (CQA) systems, arising in many real applications such as question routing and identification of best answers. In order to provide high-quality experts, many existing approaches learn the user model from their past question-answering activities in CQA systems. However, the past activities of users in most CQA systems are rather few, and thus the user model may not be well inferred in practice. In this paper, we consider the problem of expert finding from the viewpoint of missing value estimation. We then employ users' social networks for inferring user model, and thus improve the performance of expert finding in CQA systems. In addition, we develop a novel graph-regularized matrix completion algorithm for inferring the user model. We further develop two efficient iterative procedures, GRMC-EGM and GRMC-AGM, to solve the optimization problem. GRMC-EGM utilizes the Extended Gradient Method (EGM), while GRMC-AGM applies the Accelerated proximal Gradient search Method (AGM), for the optimization. We evaluate our methods on the well-known question answering system Quora, and the popular social network Twitter. Our empirical study shows the effectiveness of the proposed algorithms in comparison to the state-of-the-art expert finding algorithms.\n\nTitle: Fg-index: towards verification-free query processing on graph databases\nContent: Graphs are prevalently used to model the relationships between objects in various domains. With the increasing usage of graph databases, it has become more and more demanding to efficiently process graph queries. Querying graph databases is costly since it involves subgraph isomorphism testing, which is an NP-complete problem. In recent years, some effective graph indexes have been proposed to first obtain a candidate answer set by filtering part of the false results and then perform verification on each candidate by checking subgraph isomorphism. Query performance is improved since the number of subgraph isomorphism tests is reduced. However, candidate verification is still inevitable, which can be expensive when the size of the candidate answer set is large. In this paper, we propose a novel indexing technique that constructs a nested inverted-index, called FG-index, based on the set of Frequent subGraphs (FGs). Given a graph query that is an FG in the database, FG-index returns the exact set of query answers without performing candidate verification. When the query is an infrequent graph, FG-index produces a candidate answer set which is close to the exact answer set. Since an infrequent graph means the graph occurs in only a small number of graphs in the database, the number of subgraph isomorphism tests is small. To ensure that the index fits into the main memory, we propose a new notion of \u03b4-Tolerance Closed Frequent Graphs (\u03b4-TCFGs), which allows us to flexibly tune the size of the index in a parameterized way. Our extensive experiments verify that query processing using FG-index is orders of magnitude more efficient than using the state-of-the-art graph index.\n\n## Task\nCompose an abstract for the title 'Context-Aware Object Connection Discovery in Large Graphs' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Given a large graph and a set of objects, the task of object connection discovery is to find a subgraph that retains the best connection between the objects. Object connection discovery is useful to many important applications such as discovering the connection between different terrorist groups for counter-terrorism operations. Existing work considers only the connection between individual objects; however, in many real problems the objects usually have a context (e.g., a terrorist belongs to a terrorist group). We identify the context for the nodes in a large graph. We partition the graph into a set of communities based on the concept of modularity, where each community becomes naturally the context of the nodes within the community. By considering the context we also significantly improve the efficiency of object connection discovery, since we break down the big graph into much smaller communities. We first compute the best intra-community connection by maximizing the amount of information flow in the answer graph. Then, we extend the connection to the inter-community level by utilizing the community hierarchy relation, while the quality of the inter-community connection is also ensured by modularity. Our experiments show that our algorithm is three orders of magnitude faster than the state-of-the-art algorithm, while the quality of the query answer is comparable."
      }
    ]
  },
  {
    "tid":"51",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A novel key management protocol for RFID systems\nContent: The increasing demand to deploy efficient ways of identification has made Radio frequency Identification (RFID) technology ubiquitous. Due to the wireless nature of communication between the reader and the tag, this technology imposes major security and privacy threats. Massive work to design a powerful authentication protocol has been put to overcome various threats against privacy and security of the system. However, certain constraints on RFID tags such as limited computation capabilities, memory size and communication cost, has made most approaches fail to conduct fully secured RFID system. In this paper, we present a novel cryptographic scheme, Hacker Proof Authentication Protocol (HPAP), that allows mutual authentication between the reader and the tag as well as secure tags' information. We prove our protocol achieves full security by deploying tag static identifier, updated timestamp, a one way hash function and encryption keys with semi randomized nature as they are updated using Linear Feedback Shift Register (LFSR). Simulation using C#.NET shows that the protocol is secure against various attacks. Comparison against various existing RFID authentication protocols prove that our protocol maintain less storage, computation load and low-cost.\n\nTitle: Novel modulo based Aloha anti-collision algorithm for RFID systems\nContent: RFID (Radio frequency Identification) has become an efficient way to identify, track and/or trace objects and people. Its importance has motivated scientists and researchers to examine the challenges that are slowing its expeditious deployment in various applications. RFID collision is a major challenge imposed by the wireless links shared among a reader and the many tags in the interrogation zone. In most proposed anti-collision algorithms, tags reply randomly to time slots chosen by the reader. Since two or more tags may choose the same slot, this Random Access (RA) causes garbled data at the reader side; therefore, the identification process fails. In this paper, we propose a new anti-collision algorithm that adopts a novel method for eliminating the theory of RA to enhance system efficiency and to reduce both the number of rounds between reader and tag and the number of collided/empty slots over existing algorithms. In this algorithm, tags use modulo function to choose tag owned time slot. Another advantage of this method is that the reader estimates the next frame size and compares it with the previously selected frame sizes that are saved in the reader to ensure there is no redundancy. The performance of the algorithm is simulated and compared with existent ALOHA family algorithms.\n\nTitle: Strategic iniitialization of a hybrid particle swarm optimization-simullated annealing algorithm (HPSOSA) for PID controller design for a nonlinear system.\nContent: There exist some variations of the particle swarm optimization - simulated annealing optimization technique (PSOSA) hybrid algorithm for solving the PID control design problem, however most of these algorithms use the simulated annealing as a tool to escape local minimums that the PSO algorithm may get trapped in and also these algorithms initialize the particles within the solution space randomly. In this paper, the effects of initializing the particles strategically within the solution space along with the application of the SA algorithm to the hybrid algorithm at each iteration are explored. To test the effectiveness of the proposed modifications the algorithms are compared on common benchmark functions before the modified hybrid algorithm (MPSOSA) is used to design a PID controller for the inverted Pendulum problem.\n\nTitle: High - Resolution QRS detection algorithm for wireless ECG systems based on compressed sensing theory\nContent: Wireless electrocardiogram (ECG) systems are responsible to collect and transmit the vital signals of cardiac patients wirelessly to medical centers for diagnostic and therapeutic purposes. ECG is a noninvasive technique widely used in health care systems for diagnosis of heart diseases. However, the use of conventional ECG system is restricted by patient's mobility and the system's transmission capacity, and physical size. Aforementioned highlights the need and advantage of wireless ECG systems with low sampling-rate and low power consumption. With this in mind, Compressed Sensing (CS) procedure as a new sampling approach and the collaboration of wireless ECG framework is used to provide a robust high-resolution QRS detection algorithm in the hospitals and medical centers with high probability and enough accuracy. Advanced wireless ECG systems based on our approach will be able to deliver healthcare not only to patients in hospitals and medical centers; but also at their homes and workplaces thus offering cost saving, and improving the quality of life. Our simulation results for two records of ECG signals show an increment of 10% for sensitivity as well as 12% for the prediction level and good detection accuracy. The proposed algorithm also achieves significantly better detection rate in comparison with Empirical Mode Decomposition (EMD) method.\n\nTitle: A Novel Subcarrier Allocation Algorithm For Multiuser Ofdm System With Fairness: User'S Perspective\nContent: In wireless multiuser OFDM systems, dynamic resource allocation has been shown to improve the performance by exploiting the multiuser diversity. In this paper we propose a subcarrier allocation algorithm to increase the total data rate for the downlink of a variable bit rate multiuser OFDM system with proportional rate constraints subject to bit error rate and total transmit power. We assume that the channel is quasi-static where the channel status does not vary within each transmission block and the base station has perfect knowledge of subchannel gains. The proposed algorithm is based on prioritizing the critical (most sensitive) user in the system and the variance of the subchannel gains for each user is used to define the sensitivity of the user to the subcarrier allocation. Simulation results show that this algorithm achieves higher capacity with acceptable proportional fairness compared to the previous suboptimal solutions proposed by Rhee et al. in [9] and Shen et al. in [12].\n\n## Task\nCompose an abstract for the title 'HPAP: A novel authentication scheme for RFID systems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The increasing demand to deploy efficient ways of identification has made Radio frequency Identification (RFID) technology ubiquitous. Due to the wireless nature of communication between the reader and the tag, this technology imposes major security and privacy threats. Massive work to design a powerful authentication protocol has been put to overcome various threats against privacy and security of the system. However, certain constraints on RFID tags such as limited computation capabilities, memory size and communication cost, has made most approaches fail to conduct fully secured RFID system. In this paper, we present a novel cryptographic scheme, Hacker Proof Authentication Protocol (HPAP), that allows mutual authentication between the reader and the tag as well as secure tags' information. We prove our protocol achieves full security by deploying tag static identifier, updated timestamp, a one way hash function and encryption keys with semi randomized nature as they are updated using Linear Feedback Shift Register (LFSR). We show that the protocol insures to maintain less storage, computation load and low-cost in comparison to various existing RFID authentication protocols."
      }
    ]
  },
  {
    "tid":"52",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Concurrency Control Method for Parallel Btree Structures\nContent: A new concurrency control protocol for parallel Btree structures, MARK-OPT, is proposed. MARK-OPT marks the lowest structure-modification-operation (SMO) occurrence point during optimistic latch coupling operations, to reduce the cost of SMO compared to the conventional protocols such as ARIES/IM and INC-OPT. The marking reduces the frequency of restarts for spreading the range of X latches, which will clearly improves the system throughput. Moreover, the MARK-OPT is deadlock free and satis- fies the physical consistency requirement for Btrees. These indicate that the MARK-OPT is right and suitable as a concurrency control protocol for Btree structures. This paper also proposes three variations of the protocol, INC-MARKOPT, 2P-INT-MARK-OPT and 2P-REP-MARK-OPT, by focusing on tree structure changes from other transactions. We implement the proposed protocols, the INC-OPT, and the ARIES/IM for the Fat-Btree, a form of parallel Btree, and compare the performance of these protocols using a large-scale blade system. The experimental results indicate that the proposed protocols always improve the system throughput, and the 2P-REP-MARK-OPT is the most useful protocol in a high update environment. Moreover, in the experiment, the low frequency of restarts in the proposed protocols indicates that the marking in the proposed protocols is effective.\n\nTitle: Concurrency Control Protocol For Parallel B-Tree Structures That Improves The Efficiency Of Request Transfers And Smos Within A Node\nContent: Many concurrency control protocols for B-trees use latch-coupling because its execution is efficient on a single machine. Some studies have indicated that latch-coupling may involve a performance bottleneck when using multicore processors in a shared-everything environment, but no studies have considered the possible performance bottleneck caused by sending messages between processing elements (PEs) in shared-nothing environments. We propose two new concurrency control protocols, \"LCFB\" and \"LCFB-link\", which require no latch-coupling in optimistic processes. The LCFB-link also innovates B-link approach within each PE to reduce the cost of modifications in the PE, as a solution to the difficulty of consistency management for the side pointers in a parallel B-tree. The B-link algorithm is well known as a protocol without latch-coupling, but Blink has the difficulty of guaranteeing the consistency of the side pointers in a parallel B-tree. Experimental results in various environments indicated that the system throughput of the proposed protocols was always superior to those of the conventional protocols, particularly in large-scale configurations, and using LCFB-link was effective for higher update ratios. In addition, to mitigate access skew, data should migrate between PEs. We have demonstrated that our protocols always improve the system throughput and are effective as concurrency controls for data migration.\n\nTitle: MARK-OPT: A Concurrency Control Protocol for Parallel B-Tree Structures to Reduce the Cost of SMOs\nContent: In this paper, we propose a new concurrency control protocol for parallel B-tree structures capable reducing the cost of structure-modification-operation (SMO) compared to the conventional protocols such as ARIES/IM and INC-OPT. We call this protocol the MARK-OPT protocol, since it marks the lowest SMO occurrence point during optimistic latch-coupling operations. The marking reduces middle phases for spreading an X latch and removes needless X latches. In addition, we propose three variations of the MARK-OPT, which focus on tree structure changes from other transactions. Moreover, the proposed protocols are deadlock-free and satisfy the physical consistency requirement for B-trees. These indicate that the proposed protocols are suitable as concurrency control protocols for B-tree structures. To compare the performance of the proposed protocols, the INC-OPT, and the ARIES/IM, we implement these protocols on an autonomous disk system adopting the Fat-Btree structure, a form of parallel B-tree structure. Experimental results in various environments indicate that the proposed protocols always improve system throughput, and 2P-REP-MARK-OPT is the most useful protocol in high update environment. Additionally, to mitigate access skew, data should be migrated between PEs. We also demonstrate that MARK-OPT improves the system throughput under the data migration and reduces the time for data migration to balance load distribution.\n\nTitle: Compound treatment of chained declustered replicas using a parallel btree for high scalability and availability\nContent: Scalability and availability are key features of parallel database systems. To realize scalability, many dynamic load-balancing methods with data placement and parallel index structures on shared-nothing parallel infrastructure have been proposed. Data migration with range-partitioned placement using a parallel Btree is one solution. The combination of range partitioning and chained declustered replicas provides high availability while preserving scalability. However, independent treatment of the primary and backup data in each node results in long failover times. We propose a novel method for compound treatment of chained declustered replicas using a parallel Btree, called the Fat-Btree. In the proposed method, the single Fat-Btree provides access paths to both primary and backup data in all processor elements, which greatly reduces failover time. Moreover, it enables dynamic load balancing without physical data migration, and improves memory space utilization for processing the index. Experiments using PostgreSQL on a 160-node PC cluster demonstrate the effect.\n\nTitle: A Compound Parallel Btree For High Scalability And Availability On Chained Declustering Parallel Systems\nContent: Scalability and availability are the key features of parallel database systems. To realize scalability, many dynamic load-balancing methods with data placement and parallel index structures on shared-nothing parallel infrastructure have been proposed. Data migration with range-partitioned placement using a parallel Btree is one solution. The combination of range partitioning and chained declustered replicas provides high availability (HA) while preserving scalability. However, independent treatment of the primary and backup data in each node requires long failover times. We propose a novel method for the compound treatment of chained declustered replicas using a parallel Btree, termed the Fat-Btree. In the proposed method, a single Fat-Btree provides access paths to both the primary and backup data of all processor elements (PEs), which greatly reduces failover time. Moreover, these access paths overlap between two neighboring PEs, which enables dynamic load balancing without physical data migration by dynamically redirecting the access paths. In addition, this compound treatment improves memory space utilization to enable index processing with good scalability. Experiments using PostgreSQL on a 160-node PC cluster demonstrate the effectiveness of the high scalability and availability of our proposed method.\n\n## Task\nCompose an abstract for the title 'A concurrency control protocol for parallel B-tree structures without latch-coupling for explosively growing digital content' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"While shared-nothing parallel infrastructures provide fast processing of explosively growing digital content, managing data efficiently across multiple nodes is important. The value-range partitioning method with parallel B-tree structures in a shared-nothing environment is an efficient approach for handling large amounts of data. To handle large amounts of data, it is also important to provide an efficient concurrency control protocol for the parallel B-tree. Many studies have proposed concurrency control protocols for B-trees, which use latch-coupling. None of these studies has considered that latch-coupling contains a performance bottleneck of sending of messages between processing elements (PEs) in distributed environments because latch-coupling is efficient for a B-tree on a single machine. The only protocol without latch-coupling is the B-link algorithm, but it is difficult to use the B-link algorithm directly on an entire parallel B-tree structure because it is necessary to guarantee the consistency of the side pointers. We propose a new concurrency control protocol named LCFB that requires no latch-coupling in optimistic processes. LCFB reduces the amount of communication between PEs during a B-tree traversal. To detect access path errors in the LCFB protocol caused by removal of latch-coupling, we assign boundary values to each index page. Because a page split may cause page deletion in a Fat-Btree, we also propose an effective method for handling page deletions without latch-coupling. We then combine LCFB with the B-link algorithm within each PE to reduce the cost of Structure Modification Operations (SMOs) in a PE, as a solution to the difficulty of consistency management for the side pointers in a parallel B-tree structure. To compare the performance of the proposed protocol with conventional protocols MARK-OPT, INC-OPT, and ARIES/IM, we implemented them on an autonomous disk system with a Fat-Btree structure. Experimental results in various environments indicate that the system throughput of the proposed protocols is always superior to those of the other protocols, especially in large-scale configurations, and LCFB with the B-link algorithm is effective at higher update ratios."
      }
    ]
  },
  {
    "tid":"53",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Evolutionary Clustering in Description Logics: Controlling Concept Formation and Drift in Ontologies\nContent: We present a method based on clustering techniques to detect concept drift or novelty in a knowledge base expressed in Description Logics. The method exploits an effective and language-independent semi-distance measure defined for the space of individuals, that is based on a finite number of dimensions corresponding to a committee of discriminating features (represented by concept descriptions). In the algorithm, the possible clusterings are represented as strings of central elements (medoids, w.r.t. the given metric) of variable length. The number of clusters is not required as a parameter; the method is able to find an optimal choice by means of the evolutionary operators and of a fitness function. An experimentation with some ontologies proves the feasibility of our method and its effectiveness in terms of clustering validity indices. Then, with a supervised learning phase, each cluster can be assigned with a refined or newly constructed intensional definition expressed in the adopted language.\n\nTitle: Metric-based stochastic conceptual clustering for ontologies\nContent: A conceptual clustering framework is presented which can be applied to multi-relational knowledge bases storing resource annotations expressed in the standard languages for the Semantic Web. The framework adopts an effective and language-independent family of semi-distance measures defined for the space of individual resources. These measures are based on a finite number of dimensions corresponding to a committee of discriminating features represented by concept descriptions. The clustering algorithm expresses the possible clusterings in terms of strings of central elements (medoids, w.r.t. the given metric) of variable length. The method performs a stochastic search in the space of possible clusterings, exploiting a technique based on genetic programming. Besides, the number of clusters is not necessarily required as a parameter: a natural number of clusters is autonomously determined, since the search spans a space of strings of different length. An experimentation with real ontologies proves the feasibility of the clustering method and its effectiveness in terms of standard validity indices. The framework is completed by a successive phase, where a newly constructed intensional definition, expressed in the adopted concept language, can be assigned to each cluster. Finally, two possible extensions are proposed. One allows the induction of hierarchies of clusters. The other applies clustering to concept drift and novelty detection in the context of ontologies.\n\nTitle: Fuzzy Clustering for Semantic Knowledge Bases\nContent: This work focusses on the problem of clustering resources contained in knowledge bases represented throughmulti-relational standard languages that are typical for the context of the Semantic Web, and ultimately founded in Description Logics. The proposed solution relies on effective and language-independent dissimilarity measures that are based on a finite number of dimensions corresponding to a committee of discriminating features, that stands for a context, represented by concept descriptions in Description Logics. The proposed clustering algorithm expresses the possible clusterings in tuples of central elements: in this categorical setting, we resort to the notion of medoid, w.r.t. the given metric. These centers are iteratively adjusted following the rationale of fuzzy clustering approach, i.e. one where the membership to each cluster is not deterministic but graded, ranging in the unit interval. This better copes with the inherent uncertainty of the knowledge bases expressed in Description Logics which adopt an open-world semantics. An extensive experimentation with a number of ontologies proves the feasibility of our method and its effectiveness in terms of major clustering validity indices.\n\nTitle: Conceptual clustering and its application to concept drift and novelty detection\nContent: The paper presents a clustering method which can be applied to populated ontologies for discovering interesting groupings of resources therein. The method exploits a simple, yet effective and language-independent, semi-distance measure for individuals, that is based on their underlying semantics along with a number of dimensions corresponding to a set of concept descriptions (discriminating features committee). The clustering algorithm is a partitional method and it is based on the notion of medoids w.r.t. the adopted semi-distance measure. Eventually, it produces a hierarchical organization of groups of individuals. A final experiment demonstrates the validity of the approach using absolute quality indices. We propose two possible exploitations of these clusterings: concept formation and detecting concept drift or novelty.\n\nTitle: A Hierarchical Clustering Procedure for Semantically Annotated Resources\nContent: A clustering method is presented which can be applied to relational knowledge bases. It can be used to discover interesting groupings of resources through their (semantic) annotations expressed in the standard languages employed for modeling concepts in the Semantic Web. The method exploits a simple (yet effective and language-independent) semi-distance measure for individuals, that is based on the resource semantics w.r.t. a number of dimensions corresponding to a committee of features represented by a group of concept descriptions (discriminating features). The algorithm is an fusion of the classic Bisecting k-Meanswith approaches based on medoids since they are intended to be applied to relational representations. We discuss its complexity and the potential applications to a variety of important tasks.\n\n## Task\nCompose an abstract for the title 'Conceptual Clustering Applied to Ontologies' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"A clustering method is presented which can be applied to semantically annotated resources in the context of ontological knowledge\n bases. This method can be used to discover emerging groupings of resources expressed in the standard ontology languages. The\n method exploits a language-independent semi-distance measure over the space of resources, that is based on their semantics\n w.r.t. a number of dimensions corresponding to a committee of discriminating features represented by concept descriptions.\n A maximally discriminating group of features can be constructed through a feature construction method based on genetic programming.\n The evolutionary clustering algorithm proposed is based on the notion of medoids applied to relational representations. It\n is able to induce a set of clusters by means of a fitness function based on a discernibility criterion. An experimentation\n with some ontologies proves the feasibility of our method.\n "
      }
    ]
  },
  {
    "tid":"54",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: B+-tree index optimization by exploiting internal parallelism of flash-based solid state drives\nContent: Previous research addressed the potential problems of the hard-disk oriented design of DBMSs of flashSSDs. In this paper, we focus on exploiting potential benefits of flashSSDs. First, we examine the internal parallelism issues of flashSSDs by conducting benchmarks to various flashSSDs. Then, we suggest algorithm-design principles in order to best benefit from the internal parallelism. We present a new I/O request concept, called psync I/O that can exploit the internal parallelism of flashSSDs in a single process. Based on these ideas, we introduce B+-tree optimization methods in order to utilize internal parallelism. By integrating the results of these methods, we present a B+-tree variant, PIO B-tree. We confirmed that each optimization method substantially enhances the index performance. Consequently, PIO B-tree enhanced B+-tree's insert performance by a factor of up to 16.3, while improving point-search performance by a factor of 1.2. The range search of PIO B-tree was up to 5 times faster than that of the B+-tree. Moreover, PIO B-tree outperformed other flash-aware indexes in various synthetic workloads. We also confirmed that PIO B-tree outperforms B+-tree in index traces collected inside the Postgresql DBMS with TPC-C benchmark.\n\nTitle: Design and Implementation of a Log-Structured File System for Flash-Based Solid State Drives\nContent: Even in modern SSDs, the disparity between random and sequential write bandwidth is more than 10-fold. Moreover, random writes can shorten the limited lifespan of SSDs because they incur more NAND block erases per write. To overcome the problems of random writes, we propose a new file system, SFS, for SSDs. SFS is similar to the traditional log-structured file system (LFS) in that it transforms all random writes at the file system level to sequential ones at the SSD level, as a way to exploit the maximum write bandwidth of the SSD. But, unlike the traditional LFS, which performs hot/cold data separation on segment cleaning, SFS takes a new on writing data grouping strategy. When data blocks are to be written, SFS puts those with similar update likelihood into the same segment for sharper bimodal distribution of segment utilization, and thus aims at minimizing the inevitable segment cleaning overhead that occurs in any log-structured file system. We have implemented a prototype SFS by modifying Linux-based NILFS2 and compared it with three state-of-the-art file systems using several realistic workloads. Our experiments on SSDs show that SFS outperforms LFS by up to 2.5 times in terms of throughput. In comparison to modern file systems, SFS drastically reduces the block erase count inside SSDs by up to 23.3 times. Although SFS was targeted for SSDs, its data grouping on writing would also work well in HDDs. To confirm this, we repeated the same set of experiments over HDDs, and found that SFS is quite promising in HDDs: although the slow random reads in HDDs make SFS slightly less effective, SFS still outperforms LFS by 1.7 times.\n\nTitle: Lazy-split B+-tree: a novel B+-tree index scheme for flash-based database systems\nContent: Flash memory is rapidly being deployed as a data storage medium for embedded systems and tablet computers due to its shock resistance, fast access, and low power consumption, etc. However, it has some intractable characteristics, such as erase-before-write, asymmetric read/write/erase speed, and a limited number of write/erase cycles. Due to these hardware limitations, magnetic disk-based systems and applications can hardly make full use of the advantages of flash memory when adopting it directly for storage. For example, the frequent changes of B-tree can degrade the performance and negatively influence the lifespan of flash memory. Most state-of-the-art studies on flash-aware index design focused mainly on buffer and storage mechanisms whereby they can obtain efficient I/Os to flash memory. In this paper, we identify the problems inherent in the related studies, and then introduce the concepts of lazy-split, modify-two-node, and semi-clean, which make possible the construction of a novel index solution, the Lazy-Split B+-tree (LSB+-tree). In detail, by their introduction, the first concept of LSB+-tree can efficiently reduce the number of node splits, the second can reduce the number of node modifications, and the last can make a further improvement on buffer space utilization and flash writes reduction.\n\nTitle: Optimizing index scans on flash memory SSDs\nContent: Unlike harddisks, flash memory SSDs have very fast latency in random reads and thus the relative bandwidth gap between sequential and random read is quite small, though not negligible. For this reason, it has been believed that index scan would become more attractive access method in flash memory storage devices. In reality, however, the existing index scan can outperform the full table scan only in very selective predicates. In this paper, we investigate how to optimize the index scan on flash memory SSDs. First, we empirically show that the index scan underperforms the full table scan even when the selectivity of selection predicate is less than 5% and explain its reason. Second, we revisit the idea of sorted index scan and demonstrate that it can outperform the full table scan even when the selectivity is larger than 30%. However, one drawback of the sorted index scan is that it loses the sortedness of the retrieved records. Third, in order to efficiently resort the result from the sorted index scan, we propose a new external index-based sort algorithm, partitioned sort, which exploits the information of key value distribution in the index leaf nodes. It can sort data in one pass regardless of the available sort memory size.\n\nTitle: Memory efficient and scalable address mapping for flash storage devices.\nContent: Flash memory devices commonly rely upon traditional address mapping schemes such as page mapping, block mapping or a hybrid of the two. Page mapping is more flexible than block or hybrid mapping without being restricted by block boundaries. However, its mapping table tends to grow large quickly as the capacity of flash memory devices does. To overcome this limitation, we propose novel mapping schemes that are fundamentally different from the existing mapping strategies. We call these new schemes Virtual Extent Trie (VET) and Extent Mapping Tree (EMT), as they manage mapping information by treating each I/O request as an extent and by using extents as basic mapping units rather than pages or blocks. By storing extents instead of individual addresses, our extent mapping schemes consume much less memory to store mapping information and still remain as flexible as page mapping. We observed in our experiments that our schemes reduced memory consumption by up to an order of magnitude in comparison with the traditional mapping schemes for several real world workloads. Our extent mapping schemes also scaled well with increasing address spaces by synthetic workloads. Even though the asymptotic mapping cost of VET and EMT is higher than the O(1) time of a page mapping scheme, the amount of increased overhead was almost negligible or low enough to be hidden by an accompanying I/O operation.\n\n## Task\nCompose an abstract for the title 'h-Hash: A Hash Index Structure for Flash-Based Solid State Drives' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Flash-based solid state drives (SSDs) have been widely used as storage medium because of their fast access speed, high reliability and low power consumption. In spite of these advantages, they inherit distinct characteristics such as no in-place updates, asymmetric operation speed and unit because they consist of NAND flash memories. Therefore, a disk-based hash index structure may result in severe performance degradation if it is directly deployed on a NAND flash memory-based storage system. In this paper, we propose a new hybrid hash index structure for flash-based SSDs. It delays and reduces the split operations which cause slow block erasure and additional read and write operations by exploiting overflow buckets according to the ratio of updates and deletions. Through various performance evaluations, we show the superiority of our proposed index structure by comparing it to other hash index structures for flash-based SSDs."
      }
    ]
  },
  {
    "tid":"55",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Tracking of Point Targets in IR Image Sequence using Multiple Model Based Particle Filtering and MRF Based Data Association\nContent: Particle filtering is being investigated extensively due to its important feature of target tracking based on nonlinear and non-Gaussian model. It tracks a trajectory with a known model at a given time. It means that particle filter tracks an arbitrary trajectory only if the time instant when trajectory switches from one model to another model is known apriori. Because of this reason particle filter is not able to track any arbitrary trajectory where transition from one model to another model is not known. For real world application, trajectory is always random in nature and may follow more than one model. In this paper we propose a novel method, which overcomes the above problem. In the proposed method a multiple model based approach is used along with particle filtering, which automates the model selection process for tracking an arbitrary trajectory. In the proposed approach, there is no need to have apriori information about the exact model that a target may follow. For data association, Markov Random Field (MRF) based method has been utilized. It allows us to exploit the neighborhood concept for data association, i.e. the association of a measurement influences an association of its neighbor measurement.\n\nTitle: Robust Neural-Network-Based Data Association and Multiple Model-Based Tracking of Multiple Point Targets\nContent: Data association and model selection are important factors for tracking multiple targets in a dense clutter environment without using a priori information about the target dynamic. We propose a neural-network-based tracking algorithm, incorporating a interacting multiple model and show that it is possible to track both maneuvering and nonmaneuvering targets simultaneously in the presence of dense clutter. Moreover, it can be used for real-time application. The proposed method overcomes the problem of data association by using the method of expectation maximization and Hopfield network to evaluate assignment weights. All validated observations are used to update the target state. In the proposed approach, a probability density function (pdf) of an observed data, given target state and observation association, is treated as a mixture pdf. This allows to combine the likelihood of an observation due to each model, and the association process is defined to incorporate an interacting multiple model, and consequently, it is possible to track any arbitrary trajectory\n\nTitle: Arbitrary Trajectories Tracking using Multiple Model Based Particle Filtering in Infrared Image Sequence\nContent: Particle filtering is being investigated extensively due toits important feature of target tracking based on nonlinearand non-Gaussian model. It tracks a trajectory witha known model at a given time. It means that particle filtertracks an arbitrary trajectory only if the time instantwhen trajectory switches from one model to another modelis known apriori. Because of this reason particle filter isnot able to track any arbitrary trajectory where transitionfrom one model to another model is not known. For realworld application, trajectory is always random in natureand may follow more than one model. In this paper we proposea novel method, which overcomes the above problem.In the proposed method an interacting multiple model basedapproach is used along with particle filtering, which automatesthe model selection process for tracking an arbitrarytrajectory. We have utilized nearest neighbor (NN) methodfor data association, which is fast and easy to implement.In the proposed approach, there is no need to have aprioriinformation about the exact model that a target may follow.\n\nTitle: PMHT Based Multiple Point Targets Tracking Using Multiple Models in Infrared Image Sequence\nContent: Data association and model selection are important factors for tracking multiple targets in a dense clutter environment. We propose a sequential probabilistic multiple hypotheses tracking (PMHT) based algorithm using interacting multiple modelling (IMM), namely the IMM-PMHT algorithm. Inclusion of IMM enables any arbitrary trajectory to be tracked without any a priori information about the target dynamics. IMM allows us to incorporate different dynamic models for the targets and PMHT helps to avoid the uncertainty about the measurement origin. It operates in an iterative mode using an expectation-maximization (EM) algorithm. The proposed algorithm uses only measurement association as missing data, which simplifies E-step and M-step. It is computationally more efficient, and an important characteristic of our proposed algorithm is that it operates in a single batch model, i.e. sequential, and hence can be used for real time tracking.\n\nTitle: A Combined PMHT and IMM Approach to Multiple-Point Target Tracking in Infrared Image Sequence.\nContent: Data association and model selection are important factors for tracking multiple targets in a dense clutter environment. In this paper, we provide an effective solution to the tracking of multiple single-pixel maneuvering targets in a sequence of infrared images by developing an algorithm that combines a sequential probabilistic multiple hypothesis tracking (PMHT) and interacting multiple model (IMM). We explicitly model maneuver as a change in the target's motion model and demonstrate its effectiveness in our tracking application discussed in this paper. We show that inclusion of IMM enables tracking of any arbitrary trajectory in a sequence of infrared images without any a priori special information about the target dynamics. IMM allows us to incorporate different dynamic models for the targets and PMHT helps to avoid the uncertainty about the observation origin. It operates in an iterative mode using expectation-maximization (EM) algorithm. The proposed algorithm uses observation association as missing data.\n\n## Task\nCompose an abstract for the title 'Tracking multiple maneuvering point targets using multiple filter bank in infrared image sequence' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Performance of any tracking algorithm depends upon the model selected to capture the target dynamics. In real world applications, no a priori knowledge about the target motion is available. Moreover, it could be a maneuvering target. The proposed method is able to track maneuvering or nonmaneuvering multiple point targets with large motion (/spl mnplus/20 pixels) using multiple filter bank in an IR image sequence in the presence of clutter and occlusion due to clouds. The use of multiple filters is not new, but the novel idea here is that it uses single-step decision logic to switch over between filters. Our approach does not use any a priori knowledge about maneuver parameters, nor does it exploit a parameterized nonlinear model for the target trajectories. This is in contrast to: (i) interacting multiple model (IMM) filtering which required the maneuver parameters, and (ii) extended Kalman filter (EKF) or unscented Kaiman filter (UKF), both of which require a parameterized model for the trajectories. We compared our approach for target tracking with IMM filtering using EKF and UKF for non-linear trajectory models. UKF uses the nonlinearity of the target model, where as a first order linearization is used in case of EKF. RMS for the predicted position error (RMS-PPE) obtained using our proposed methodology is significantly less in case of highly maneuvering target."
      }
    ]
  },
  {
    "tid":"56",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Jazz2: a flexible and extensible framework for structural testing in a Java VM\nContent: This paper describes a framework for structural testing in a Java Virtual Machine (JVM), called Jazz2. The framework provides instrumentation, memory management, control flow analysis, and callback handling services for structural tests, such as branch and node coverage. Additionally, it has a language for the specification of how and where to test. These facilities allow for the efficient implementation of demand-driven and static testing techniques. Jazz2 supports an extensible test library, where new tests can be easily added. The paper describes the framework, its implementation, and evaluates how the framework interacts with a JVM, the Jikes Research Virtual Machine (RVM). We demonstrate the ease of implementing a structural test in Jazz2, extending the test library, and specifying the use of these tests with our language. Our experimental results show that Jazz2 has low overhead in both time and memory, and has minimal interaction with the RVM's garbage collector.\n\nTitle: Demand-driven structural testing with dynamic instrumentation\nContent: Producing reliable and robust software has become one of the most important software development concerns in recent years. Testing is a process by which software quality can be assured through the collection of information. While testing can improve software reliability, current tools typically are inflexible and have high over-heads, making it challenging to test large software projects. In this paper, we describe a new scalable and flexible framework for testing programs with a novel demand-driven approach based on execution paths to implement test coverage. This technique uses dynamic instrumentation on the binary code that can be inserted and removed on-the-fly to keep performance and memory overheads low. We describe and evaluate implementations of the framework for branch, node and defuse testing of Java programs. Experimental results for branch testing show that our approach has, on average, a 1.6 speed up over static instrumentation and also uses less memory.\n\nTitle: REEact: a customizable virtual execution manager for multicore platforms\nContent: With the shift to many-core chip multiprocessors (CMPs), a critical issue is how to effectively coordinate and manage the execution of applications and hardware resources to overcome performance, power consumption, and reliability challenges stemming from hardware and application variations inherent in this new computing environment. Effective resource and application management on CMPs requires consideration of user/application/hardware-specific requirements and dynamic adaption of management decisions based on the actual run-time environment. However, designing an algorithm to manage resources and applications that can dynamically adapt based on the run-time environment is difficult because most resource and application management and monitoring facilities are only available at the operating system level. This paper presents REEact, an infrastructure that provides the capability to specify user-level management policies with dynamic adaptation. REEact is a virtual execution environment that provides a framework and core services to quickly enable the design of custom management policies for dynamically managing resources and applications. To demonstrate the capabilities and usefulness of REEact, this paper describes three case studies--each illustrating the use of REEact to apply a specific dynamic management policy on a real CMP. Through these case studies, we demonstrate that REEact can effectively and efficiently implement policies to dynamically manage resources and adapt application execution.\n\nTitle: A Design Environment for Counterflow Pipeline Synthesis\nContent: The Counterflow Pipeline (CFP) organization may be a good target for synthesis of application-specific microprocessors for embedded systems because it has a regular and simple structure. This paper describes a design environment for tailoring CFP''s to an embedded application to improve performance. Our system allows exploring the design space of all possible CFP''s for a given application to understand the impact of different design decisions on performance. We have used the environment to derive heuristics that help to find the best CFP for an application. Preliminary results using our heuristics indicate that speedup for several small graphs range from 1.3 to 2.0 over a general-purpose CFP and that the heuristics find designs that are within 10% of optimal.\n\nTitle: A Model-Based Framework: An Approach for Profit-Driven Optimization\nContent: Although optimizations have been applied for a number of years to improve the performance of software, problems that have been long-standing remain, which include knowing what optimizations to apply and how to apply them. To systematically tackle these problems, we need to understand the properties of optimizations. In our current research, we are investigating the profitability property, which is useful for determining the benefit of applying an optimization. Due to the high cost of applying optimizations and then experimentally evaluating their profitability, we use an analytic model framework for predicting the profitability of optimizations. In this paper, we target scalar optimizations, and in particular, describe framework instances for Partial Redundancy Elimination (PRE) and Loop Invariant Code Motion (LICM). We implemented the framework for both optimizations and compare profit-driven PRE and LICM with a heuristic-driven approach. Our experiments demonstrate that a model-based approach is effective and efficient in that it can accurately predict the profitability of optimizations with low overhead. By predicting the profitability using models, we can selectively apply optimizations. The model-based approach does not require tuning of parameters used in heuristic approaches and works well across different code contexts and optimizations.\n\n## Task\nCompose an abstract for the title 'Jazz: a tool for demand-driven structural testing' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Software testing to produce reliable and robust software has become vitally important. Testing is a process by which quality can be assured through the collection of information about software. While testing can improve software quality, current tools typically are inflexible and have high overheads, making it a challenge to test large projects. We describe a new scalable and flexible tool, called Jazz, that uses a demand-driven structural testing approach. Jazz has a low overhead of only 17.6% for branch testing."
      }
    ]
  },
  {
    "tid":"57",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Secure Privacy-Preserving Biometric Authentication Scheme for Telecare Medicine Information Systems\nContent: Healthcare delivery services via telecare medicine information systems (TMIS) can help patients to obtain their desired telemedicine services conveniently. However, information security and privacy protection are important issues and crucial challenges in healthcare information systems, where only authorized patients and doctors can employ telecare medicine facilities and access electronic medical records. Therefore, a secure authentication scheme is urgently required to achieve the goals of entity authentication, data confidentiality and privacy protection. This paper investigates a new biometric authentication with key agreement scheme, which focuses on patient privacy and medical data confidentiality in TMIS. The new scheme employs hash function, fuzzy extractor, nonce and authenticated Diffie-Hellman key agreement as primitives. It provides patient privacy protection, e.g., hiding identity from being theft and tracked by unauthorized participant, and preserving password and biometric template from being compromised by trustless servers. Moreover, key agreement supports secure transmission by symmetric encryption to protect patient's medical data from being leaked. Finally, the analysis shows that our proposal provides more security and privacy protection for TMIS.\n\nTitle: A Novel Authentication Scheme Using Self-certified Public Keys for Telecare Medical Information Systems\nContent: Telecare medical information systems (TMIS), with the explosive growth of communication technology and physiological monitoring devices, are applied increasingly to enable and support healthcare delivery services. In order to safeguard patients' privacy and tackle the illegal access, authentication schemes for TMIS have been investigated and designed by many researchers. Many of them are promising for adoption in practice, nevertheless, they still have security flaws. In this paper, we propose a novel remote authentication scheme for TMIS using self-certified public keys, which is formally secure in the ID-mBJM model. Besides, the proposed scheme has better computational efficiency. Compared to the related schemes, our protocol is more practical for telemedicine system.\n\nTitle: An efficient and provably-secure certificateless public key encryption scheme for telecare medicine information systems.\nContent: Telecare Medicine Information Systems (TMIS) promote the traditional medical and healthcare services by information and communication technology. Since the physician and caregiver can monitor the patient's physiological condition remotely in TMIS, the confidentiality of this sensitive data should be protected, which is the key issue in the Health Insurance Portability and Accountability Act. In this paper, we propose an efficient certificateless public key encryption scheme without bilinear pairing for TMIS. Our proposal is proved to be secure in the random oracle model under the hardness assumption of computational Diffie-Hellman problem. Moreover, after modifying the original model of the certificateless encryption, this scheme achieves Girault's trust level 3. Compared with the related protocols, the perform evaluations show that our scheme is more efficient and appropriate to collocate with low power mobile devices for TMIS.\n\nTitle: An Improved Biometrics-Based Authentication Scheme for Telecare Medical Information Systems\nContent: Telecare medical information system (TMIS) offers healthcare delivery services and patients can acquire their desired medical services conveniently through public networks. The protection of patients' privacy and data confidentiality are significant. Very recently, Mishra et al. proposed a biometrics-based authentication scheme for telecare medical information system. Their scheme can protect user privacy and is believed to resist a range of network attacks. In this paper, we analyze Mishra et al.'s scheme and identify that their scheme is insecure to against known session key attack and impersonation attack. Thereby, we present a modified biometrics-based authentication scheme for TMIS to eliminate the aforementioned faults. Besides, we demonstrate the completeness of the proposed sche-me through BAN-logic. Compared to the related schemes, our protocol can provide stronger security and it is more practical.\n\nTitle: A secure and efficient authentication and key agreement scheme based on ECC for telecare medicine information systems.\nContent: In the field of the Telecare Medicine Information System, recent researches have focused on consummating more convenient and secure healthcare delivery services for patients. In order to protect the sensitive information, various attempts such as access control have been proposed to safeguard patients' privacy in this system. However, these schemes suffered from some certain security defects and had costly consumption, which were not suitable for the telecare medicine information system. In this paper, based on the elliptic curve cryptography, we propose a secure and efficient two-factor mutual authentication and key agreement scheme to reduce the computational cost. Such a scheme enables to provide the patient anonymity by employing the dynamic identity. Compared with other related protocols, the security analysis and performance evaluation show that our scheme overcomes some well-known attacks and has a better performance in the telecare medicine information system.\n\n## Task\nCompose an abstract for the title 'Privacy-preserving self-helped medical diagnosis scheme based on secure two-party computation in wireless sensor networks.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"With the continuing growth of wireless sensor networks in pervasive medical care, people pay more and more attention to privacy in medical monitoring, diagnosis, treatment, and patient care. On one hand, we expect the public health institutions to provide us with better service. On the other hand, we would not like to leak our personal health information to them. In order to balance this contradiction, in this paper we design a privacy-preserving self-helped medical diagnosis scheme based on secure two-party computation in wireless sensor networks so that patients can privately diagnose themselves by inputting a health card into a self-helped medical diagnosis ATM to obtain a diagnostic report just like drawing money from a bank ATM without revealing patients' health information and doctors' diagnostic skill. It makes secure self-helped disease diagnosis feasible and greatly benefits patients as well as relieving the heavy pressure of public health institutions."
      }
    ]
  },
  {
    "tid":"58",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Asymptotic Tracking Control of Uncertain Nonlinear Systems With Unknown Actuator Nonlinearity\nContent: This technical note is concerned with the problem of adaptive tracking control for a class of nonlinear systems with parametric uncertainty, unknown actuator nonlinearity and bounded external disturbance. Two type of actuator nonlinearities, that is, symmetric dead-zone and Bouc-Wen hysteresis, are considered, respectively. First, an adaptive control scheme with positive integrable time-varying function is presented to compensate for the dead-zone nonlinearity. Then, the actuator nonlinearity under consideration is modeled as Bouc-Wen hysteresis, and desired compensation controller is designed based on the backstepping technique and Nussbaum gain approach. In both of the two schemes, the asymptotic tracking is guaranteed with the tracking error converging to zero. Finally, an illustrative example is provided to show the effectiveness of the proposed design methods.\n\nTitle: Further Results on Adaptive Stabilization of High-Order Stochastic Nonlinear Systems Subject to Uncertainties.\nContent: This paper concerns the adaptive state-feedback control for a class of high-order stochastic nonlinear systems with uncertainties including time-varying delay, unknown control gain, and parameter perturbation. The commonly used growth assumptions on system nonlinearities are removed, and the adaptive control technique is combined with the sign function to deal with the unknown control gain. Then, with the help of the radial basis function neural network approximation approach and Lyapunov\u2013Krasovskii functional, an adaptive state-feedback controller is obtained through the backstepping design procedure. It is verified that the constructed controller can render the closed-loop system semiglobally uniformly ultimately bounded. Finally, both the practical and numerical examples are presented to validate the effectiveness of the proposed scheme.\n\nTitle: Non-fragile finite-time extended dissipative control for a class of uncertain discrete time switched linear systems.\nContent: In this paper, the issues of finite-time extended dissipative analysis and non-fragile control are investigated for a class of uncertain discrete time switched linear systems. Based on average dwell-time approach, sufficient conditions for the finite-time boundedness and finite-time extended dissipative performance of the considered systems are proposed by solving some linear matrix inequalities, where using the concept of extended dissipative, we can solve the H\u221e, L2\u2212L\u221e, Passivity and (Q, S, R)-dissipativity performance in a unified framework. Furthermore, two form of non-fragile state feedback controllers are designed to guarantee that the closed-loop systems satisfy the finite-time extended dissipative performance. Finally, simulation example is given to show the efficiency of the proposed methods.\n\nTitle: Globally adaptive control for stochastic nonlinear time-delay systems with perturbations and its application.\nContent: This paper addresses the globally adaptive state-feedback control problem for a more general class of stochastic nonlinear systems with an unknown time-varying delay and perturbations. Without imposing any assumptions on the time-varying delay, an adaptive state-feedback controller is skillfully designed by using adaptive backstepping control technique. Then, based on Lyapunov\u2013Razumikhin lemma and stochastic stability theory, it is proven that the constructed controller can guarantee the closed-loop system to be globally asymptotically stable in probability. Finally, a practical example of stochastic chemical reactor system with time delay and perturbations is presented to demonstrate the effectiveness of the proposed control scheme.\n\nTitle: Robust \u210b\u221e tracking control for uncertain Markovian jumping systems with interval time\u2010varying delay\nContent: In this article, the problem of robust H-infinity tracking control for a class of uncertain Markovian jump systems with interval time-varying delay is investigated. Based on an augmented Lyapunov-Krasovskii functional with triple integral term, partitioning the delay's lower bound and reciprocally convex approach, delay-dependent conditions for the existence of desired controller are achieved. Meanwhile, stability criteria for delayed Markovian jump systems are also provided with less conservativeness and less matrix variables than some recent results. Finally, two simulation examples are given to illustrate the effectiveness of the proposed design method. (c) 2014 Wiley Periodicals, Inc.\n\n## Task\nCompose an abstract for the title 'Adaptive tracking control for uncertain switched stochastic nonlinear pure-feedback systems with unknown backlash-like hysteresis.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, an adaptive tracking control problem is studied for a class of switched stochastic nonlinear pure-feedback systems with unknown backlash-like hysteresis under arbitrary switching. The mean-value theorem is used to overcome the difficulty arising from the pure-feedback structure. Based on neural networks\u2019 approximation capability, an adaptive tracking control approach is developed via the adaptive backstepping technique and common Lyapunov function method. It is proved that the proposed control scheme can guarantee that all signals in the closed-loop system are semi-globally uniformly ultimately bounded in probability and the tracking error converges to an adjustable neighborhood of the origin. Finally, a simulation example further shows the effectiveness of the presented control scheme."
      }
    ]
  },
  {
    "tid":"59",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Uplink Energy-Delay Trade-off under Optimized Relay Placement in Cellular Networks\nContent: Relay nodes-enhanced architectures are deemed a viable solution to enhance coverage and capacity of nowadays cellular networks. Besides a number of desirable features, these architectures reduce the average distance between users and network nodes, thus allowing for battery savings for users transmitting on the uplink. In this paper, we investigate the extent of these savings, by optimizing relay nodes deployment in terms of uplink energy consumption per transmitted bit, while taking into account a minimum uplink average user delay that has to be guaranteed. A novel performance evaluation framework for uplink relay networks is first proposed to study this energy-delay trade-off. A simulated annealing is then run to find an optimized relay placement solution under a delay constraint; exterior penalty functions are used in order to deal with a difficult energy landscape, in particular when the constraint is tight. Finally, results show that relay nodes deployment consistently improve users uplink energy efficiency, under a wide range of traffic conditions and that relays are particularly efficient in non-uniform traffic scenarios.\n\nTitle: Analytical performance evaluation of various frequency reuse and scheduling schemes in cellular OFDMA networks\nContent: In this paper, we present an analytical solution to carry out performance analysis of various frequency reuse schemes in an OFDMA based cellular network. We study the performance in downlink in terms of signal to interference (SIR) ratio and total cell data rate. The latter is analyzed while keeping in view three different scheduling schemes: equal data rate, equal bandwidth and opportunist. Analytical models are proposed for integer frequency reuse (IFR), fractional frequency reuse (FFR) and two level power control (TLPC) schemes. These models are based on a fluid model that was originally being proposed for CDMA networks. The modeling key of this approach is to consider the discrete base station entities as a continuum. To validate our approach, Monte Carlo simulations are carried out. Validation study shows that results obtained through our analytical method are in conformity with those obtained through simulations. A comparison between the above-mentioned frequency reuse schemes and scheduling policies is also presented. We also propose an optimal tuning of involved parameters (inner cell radius and power ratios).\n\nTitle: Optimal Relay Placement for Capacity and Performance Improvement using a Fluid Model for Heterogeneous Wireless Networks.\nContent:   In this paper, we address the problem of optimal relay placement in a cellular network assuming network densification, with the aim of maximizing cell capacity. In our model, a fraction of radio resources is dedicated to the base-station (BS)/relay nodes (RN) communication. In the remaining resources, BS and RN transmit simultaneously to users. During this phase, the network is densified in the sense that the transmitters density and so network capacity are increased. Intra- and inter-cell interference is taken into account in Signal to Interference plus Noise Ratio (SINR) simple formulas derived from a fluid model for heterogeneous network. Optimization can then be quickly performed using Simulated Annealing. Performance results show that cell capacity is boosted thanks to densification despite a degradation of the signal quality. Bounds are also provided on the fraction of resources dedicated to the BS-RN link. \n\nTitle: Deploy-As-You-Go Wireless Relay Placement: An Optimal Sequential Decision Approach Using the Multi-Relay Channel Model.\nContent: We use information theoretic achievable rate formulas for the multi-relay channel to study the problem of as-you-go deployment of relay nodes. The achievable rate formulas are for full-duplex radios at the relays and for decode-and-forward relaying. Deployment is done along the straight line joining a source node and a sink node at an unknown distance from the source. The problem is for a deployment agent to walk from the source to the sink, deploying relays as he walks, given the knowledge of the wireless path-loss model, and given that the distance to the sink node is exponentially distributed with known mean. As a precursor to the formulation of the deploy-as-you-go problem, we apply the multi-relay channel achievable rate formula to obtain the optimal power allocation to relays placed along a line, at fixed locations. This permits us to obtain the optimal placement of a given number of nodes when the distance between the source and sink is given. Numerical work for the fixed source-sink distance case suggests that, at low attenuation, the relays are mostly clustered close to the source in order to be able to cooperate among themselves, whereas at high attenuation they are uniformly placed and work as repeaters. We also prove that the effect of path-loss can be entirely mitigated if a large enough number of relays are placed uniformly between the source and the sink. The structure of the optimal power allocation for a given placement of the nodes, then motivates us to formulate the problem of as-you-go placement of relays along a line of exponentially distributed length, and with the exponential path-loss model, so as to minimize a cost function that is additive over hops. The hop cost trades off a capacity limiting term, motivated from the optimal power allocation solution, against the cost of adding a relay node. We formulate the problem as a total cost Markov decision process, establish results for the value function, and provide insights into the placement policy and the performance of the deployed network via numerical exploration.\n\nTitle: Optimal Relay Placement in Cellular Networks\nContent: In this paper, we address the problem of optimally placing relay nodes in a cellular network with the aim of maximizing cell capacity. In order to accurately model interference, we use a dynamic framework, in which users arrive at random time instants and locations, download a file and leave the system. A fixed point equation is solved to account for the interactions between stations. We also propose an extension of a fluid model to relay based cellular networks. This allows us to obtain quick approximations of the Signal to Interference plus Noise Ratio (SINR) that are very close to 3GPP LTE-A guideline results in terms of SINR distribution. We then use these formulas to develop a dedicated Simulated Annealing (SA) algorithm, which adapts dynamically the temperature to energy variations and uses a combination of coarse and fine grids to accelerate the search for an optimized solution. Simulations results are provided for both in-band and out-of-band relays. They show how relays should be placed in a cell in order to increase the capacity in case of uniform and non-uniform traffic. The crucial impact of the backhaul link is analyzed for in-band relays. Insights are given on the influence of shadowing.\n\n## Task\nCompose an abstract for the title 'Scheduling Impact on the Performance of Relay-Enhanced LTE-A Networks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper studies the performance of two traditional schedulers, namely, proportional fair (PF) and round robin (RR), in the context of relay-enhanced Long-Term Evolution Advanced (LTE-A) networks. These two schedulers are natural candidates for implementation in relay nodes (RNs), and following the results obtained in single-hop networks, mobile operators could be tempted to adopt PF because of the good tradeoff it offers between cell capacity and fairness. Based on a statistical throughput evaluation model, we show that this is not necessarily the right option. The number of RNs, their locations in the cell, and backhaul link quality have a decisive influence on scheduler choice. In some scenarios, it is even not desirable to deploy relays as they degrade the network performance compared with the no-relay case. For the purpose of performance evaluation, we develop a realistic and computationally tractable statistical network model that takes into account fast fading, multiple interferers, cell range expansion bias, backhaul link quality, and traffic load. We also propose optimization of the radio frame structure and a suboptimal RN placement scheme to fairly compare RR and PF."
      }
    ]
  },
  {
    "tid":"60",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Mobile Camera Array Calibration for Light Field Acquisition.\nContent:   The light field camera is useful for computer graphics and vision applications. Calibration is an essential step for these applications. After calibration, we can rectify the captured image by using the calibrated camera parameters. However, the large camera array calibration method, which assumes that all cameras are on the same plane, ignores the orientation and intrinsic parameters. The multi-camera calibration technique usually assumes that the working volume and viewpoints are fixed. In this paper, we describe a calibration algorithm suitable for a mobile camera array based light field acquisition system. The algorithm performs in Zhang's style by moving a checkerboard, and computes the initial parameters in closed form. Global optimization is then applied to refine all the parameters simultaneously. Our implementation is rather flexible in that users can assign the number of viewpoints and refinement of intrinsic parameters is optional. Experiments on both simulated data and real data acquired by a commercial product show that our method yields good results. Digital refocusing application shows the calibrated light field can well focus to the target object we desired. \n\nTitle: Light Field Distortion Feature for Transparent Object Recognition\nContent: Current object-recognition algorithms use local features, such as scale-invariant feature transform (SIFT) and speeded-up robust features (SURF), for visually learning to recognize objects. These approaches though cannot apply to transparent objects made of glass or plastic, as such objects take on the visual features of background objects, and the appearance of such objects dramatically varies with changes in scene background. Indeed, in transmitting light, transparent objects have the unique characteristic of distorting the background by refraction. In this paper, we use a single-shot light field image as an input and model the distortion of the light field caused by the refractive property of a transparent object. We propose a new feature, called the light field distortion (LFD) feature, for identifying a transparent object. The proposal incorporates this LFD feature into the bag-of-features approach for recognizing transparent objects. We evaluated its performance in laboratory and real settings.\n\nTitle: TransCut: Transparent Object Segmentation From a Light-Field Image\nContent: The segmentation of transparent objects can be very useful in computer vision applications. However, because they borrow texture from their background and have a similar appearance to their surroundings, transparent objects are not handled well by regular image segmentation methods. We propose a method that overcomes these problems using the consistency and distortion properties of a light-field image. Graph-cut optimization is applied for the pixel labeling problem. The light-field linearity is used to estimate the likelihood of a pixel belonging to the transparent object or Lambertian background, and the occlusion detector is used to find the occlusion boundary. We acquire a light field dataset for the transparent object, and use this dataset to evaluate our method. The results demonstrate that the proposed method successfully segments transparent objects from the background.\n\nTitle: Camera array calibration for light field acquisition\nContent: Light field cameras are becoming popular in computer vision and graphics, with many research and commercial applications already having been proposed. Various types of cameras have been developed with the camera array being one of the ways of acquiring a 4D light field image using multiple cameras. Camera calibration is essential, since each application requires the correct projection and ray geometry of the light field. The calibrated parameters are used in the light field image rectified from the images captured by multiple cameras. Various camera calibration approaches have been proposed for a single camera, multiple cameras, and a moving camera. However, although these approaches can be applied to calibrating camera arrays, they are not effective in terms of accuracy and computational cost. Moreover, less attention has been paid to camera calibration of a light field camera. In this paper, we propose a calibration method for a camera array and a rectification method for generating a light field image from the captured images. We propose a two-step algorithm consisting of closed form initialization and nonlinear refinement, which extends Zhang's well-known method to the camera array. More importantly, we introduce a rigid camera constraint whereby the array of cameras is rigidly aligned in the camera array and utilize this constraint in our calibration. Using this constraint, we obtained much faster and more accurate calibration results in the experiments.\n\nTitle: Background light ray modeling for change detection.\nContent: A new change detection method based on background modeling is proposed.A new concept of light field sensing is introduced to evaluate spatio-temporal consistency.Performance of change detection outperformed the state-of-the-arts in severe surveillance condition. This paper is an extension of the work that was originally reported in Shimada et al. (2013). This paper proposes a change detection method based on spatio-temporal light ray consistency. The proposed method introduces light field sensing, which is used to generate an arbitrary in-focus plane. Change detection is performed in a surveillance scene, where the background region can be filtered out by an out-focusing process. This approach resolves a longstanding issue in background modeling-based object detection, which often suffers from false positives in the background regions. To realize this new change detection method, a new feature representation, called the local ray pattern (LRP), is introduced. The LRP evaluates the spatial consistency of the light rays, and this plays an important role in distinguishing whether the light rays come from the in-focus plane or elsewhere. A combination of the LRP and Gaussian mixture model (GMM)-based background modeling realizes change detection in the in-focus plane. Experimental results demonstrate the proposed method's effectiveness and its applicability to video surveillance.\n\n## Task\nCompose an abstract for the title 'Reflectance and Shape Estimation with a Light Field Camera under Natural Illumination.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Reflectance and shape are two important components in visually perceiving the real world. Inferring the reflectance and shape of an object through cameras is a fundamental research topic in the field of computer vision. While three-dimensional shape recovery is pervasive with varieties of approaches and practical applications, reflectance recovery has only emerged recently. Reflectance recovery is a challenging task that is usually conducted in controlled environments, such as a laboratory environment with a special apparatus. However, it is desirable that the reflectance be recovered in the field with a handy camera so that reflectance can be jointly recovered with the shape. To that end, we present a solution that simultaneously recovers the reflectance and shape (i.e., dense depth and normal maps) of an object under natural illumination with commercially available handy cameras. We employ a light field camera to capture one light field image of the object, and a 360-degree camera to capture the illumination. The proposed method provides positive results in both simulation and real-world experiments."
      }
    ]
  },
  {
    "tid":"61",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Robust Trajectory Estimation for Crowdsourcing-Based Mobile Applications\nContent: Crowdsourcing-based mobile applications are becoming more and more prevalent in recent years, as smartphones equipped with various built-in sensors are proliferating rapidly. The large quantity of crowdsourced sensing data stimulates researchers to accomplish some tasks that used to be costly or impossible, yet the quality of the crowdsourced data, which is of great importance, has not received sufficient attention. In reality, the low-quality crowdsourced data are prone to containing outliers that may severely impair the crowdsourcing applications. Thus in this work, we conduct pioneer investigation considering crowdsourced data quality. Specifically, we focus on estimating user motion trajectory information, which plays an essential role in multiple crowdsourcing applications, such as indoor localization, context recognition, indoor navigation, etc. We resort to the family of robust statistics and design a robust trajectory estimation scheme, name TrMCD, which is capable of alleviating the negative influence of abnormal crowdsourced user trajectories, differentiating normal users from abnormal users, and overcoming the challenge brought by spatial unbalance of crowdsourced trajectories. Two real field experiments are conducted and the results show that TrMCD is robust and effective in estimating user motion trajectories and mapping fingerprints to physical locations.\n\nTitle: On Multipath Link Characterization and Adaptation for Device-Free Human Detection\nContent: Wireless-based device-free human sensing has raised increasing research interest and stimulated a range of novel location-based services and human-computer interaction applications for recreation, asset security and elderly care. A primary functionality of these applications is to first detect the presence of humans before extracting higher-level contexts such as physical coordinates, body gestures, or even daily activities. In the presence of dense multipath propagation, however, it is non-trivial to even reliably identify the presence of humans. The multipath effect can invalidate simplified propagation models and distort received signal signatures, thus deteriorating detection rates and shrinking detection range. In this paper, we characterize the impact of human presence on wireless signals via ray-bouncing models, and propose a measurable metric on commodity WiFi infrastructure as a proxy for detection sensitivity. To achieve higher detection rate and wider sensing coverage in multipath-dense indoor scenarios, we design a lightweight sub carrier and path configuration scheme harnessing frequency diversity and spatial diversity. We prototype our scheme with standard WiFi devices. Evaluations conducted in two typical office environments demonstrate a detection rate of 92.0% with a false positive of 4.5%, and almost 1x gain in detection range given a minimal detection rate of 90%.\n\nTitle: Ambient Rendezvous: Energy-Efficient Neighbor Discovery Via Acoustic Sensing\nContent: The continual proliferation of mobile devices has stimulated the development of opportunistic encounter-based networking and has spurred a myriad of proximity-based mobile applications. A primary cornerstone of such applications is to discover neighboring devices effectively and efficiently. Despite extensive protocol optimization, current neighbor discovery modalities mainly rely on radio interfaces, whose energy and wake up delay required to initiate, configure and operate these protocols hamper practical applicability. Unlike conventional schemes that actively emit radio tones, we exploit ubiquitous audio events to discover neighbors passively. The rationale is that spatially adjacent neighbors tend to share similar ambient acoustic environments. We propose AIR, an effective and efficient neighbor discovery protocol via low power acoustic sensing to reduce discovery latency. Especially, AIR substantially increases the discovery probability of the first time they turn the radio on. Compared with the state-of-the-art neighbor discovery protocol, AIR significantly decreases the average discovery latency by around 70%, which is promising for supporting vast proximity-based mobile applications.\n\nTitle: Mobility Increases Localizability: A Survey on Wireless Indoor Localization using Inertial Sensors\nContent: Wireless indoor positioning has been extensively studied for the past 2 decades and continuously attracted growing research efforts in mobile computing context. As the integration of multiple inertial sensors (e.g., accelerometer, gyroscope, and magnetometer) to nowadays smartphones in recent years, human-centric mobility sensing is emerging and coming into vogue. Mobility information, as a new dimension in addition to wireless signals, can benefit localization in a number of ways, since location and mobility are by nature related in the physical world. In this article, we survey this new trend of mobility enhancing smartphone-based indoor localization. Specifically, we first study how to measure human mobility: what types of sensors we can use and what types of mobility information we can acquire. Next, we discuss how mobility assists localization with respect to enhancing location accuracy, decreasing deployment cost, and enriching location context. Moreover, considering the quality and cost of smartphone built-in sensors, handling measurement errors is essential and accordingly investigated. Combining existing work and our own working experiences, we emphasize the principles and conduct comparative study of the mainstream technologies. Finally, we conclude this survey by addressing future research directions and opportunities in this new and largely open area.\n\nTitle: Locating in fingerprint space: wireless indoor localization with little human intervention\nContent: Indoor localization is of great importance for a range of pervasive applications, attracting many research efforts in the past decades. Most radio-based solutions require a process of site survey, in which radio signatures of an interested area are annotated with their real recorded locations. Site survey involves intensive costs on manpower and time, limiting the applicable buildings of wireless localization worldwide. In this study, we investigate novel sensors integrated in modern mobile phones and leverage user motions to construct the radio map of a floor plan, which is previously obtained only by site survey. On this basis, we design LiFS, an indoor localization system based on off-the-shelf WiFi infrastructure and mobile phones. LiFS is deployed in an office building covering over 1600m2, and its deployment is easy and rapid since little human intervention is needed. In LiFS, the calibration of fingerprints is crowdsourced and automatic. Experiment results show that LiFS achieves comparable location accuracy to previous approaches even without site survey.\n\n## Task\nCompose an abstract for the title 'CrossNavi: enabling real-time crossroad navigation for the blind with commodity phones' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Crossroad is among the most dangerous parts outside for the visually impaired people. Numerous studies have exploited navigating systems for the visually impaired community, providing services ranging from block detection, route planning to realtime localization. However, none of them have addressed the safety issue in crossroad and integrated three key factors necessary for a practical crossroad navigation system: detecting the crossroad, locating zebra patterns, and guiding the user within zebra crossing when passing the road. Our CrossNavi application responds to these needs, providing an integrated crossroad navigation service that incorporates all the essential functionalities mentioned above. The overall service is fulfilled by the collaboration of built-in sensors on commodity phones, and requires minimal human participation. We describe the technical aspects of its design, implementation, interface, and further improvements to make the system practical on a wider basis. Experimental results from three visually impaired volunteers show that the system exhibits promising behavior in both urban and rural areas."
      }
    ]
  },
  {
    "tid":"62",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Flexible Development of Dense Linear Algebra Algorithms on Massively Parallel Architectures with DPLASMA\nContent: We present a method for developing dense linear algebra algorithms that seamlessly scales to thousands of cores. It can be done with our project called DPLASMA (Distributed PLASMA) that uses a novel generic distributed Direct Acyclic Graph Engine (DAGuE). The engine has been designed for high performance computing and thus it enables scaling of tile algorithms, originating in PLASMA, on large distributed memory systems. The underlying DAGuE framework has many appealing features when considering distributed-memory platforms with heterogeneous multicore nodes: DAG representation that is independent of the problem-size, automatic extraction of the communication from the dependencies, overlapping of communication and computation, task prioritization, and architecture-aware scheduling and management of tasks. The originality of this engine lies in its capacity to translate a sequential code with nested-loops into a concise and synthetic format which can then be interpreted and executed in a distributed environment. We present three common dense linear algebra algorithms from PLASMA~(Parallel Linear Algebra for Scalable Multi-core Architectures), namely: Cholesky, LU, and QR factorizations, to investigate their data driven expression and execution in a distributed system. We demonstrate through experimental results on the Cray XT5 Kraken system that our DAG-based approach has the potential to achieve sizable fraction of peak performance which is characteristic of the state-of-the-art distributed numerical software on current and emerging architectures.\n\nTitle: High Performance Dense Linear System Solver with Soft Error Resilience\nContent: As the scale of modern high end computing systems continues to grow rapidly, system failure has become an issue that requires a better solution than the commonly used scheme of checkpoint and restart (C/R). While hard errors have been studied extensively over the years, soft errors are still under-studied especially for modern HPC systems, and in some scientific applications C/R is not applicable for soft error at all due to error propagation and lack of error awareness. In this work, we propose an algorithm based fault tolerance (ABFT) for high performance dense linear system solver with soft error resilience. By adapting a mathematical model that treats soft error during LU factorization as rank-one perturbation, the solution of Ax=b can be recovered with the Sherman-Morrison formula. Our contribution includes extending error model from Gaussian elimination and pair wise pivoting to LU with partial pivoting, and we provide a practical numerical bound for error detection and a scalable check pointing algorithm to protect the left factor that is needed for recovering x from soft error. Experimental results on cluster systems with ScaLAPACK show that the fault tolerance functionality adds little overhead to the linear system solving and scales well on such systems.\n\nTitle: High Performance Dense Linear System Solver with Resilience to Multiple Soft Errors.\nContent: In the multi-peta-flop era for supercomputers, the number of computing cores is growing exponentially. However, as integrated circuit technology scales below 65 nm, the critical charge required to flip a gate or a memory cell has been reduced and thus causing higher soft error rate from cosmic-radiations. Soft errors affect computers by producing silently data corruption which is hard to detect and correct. Current research of soft errors resilience for dense linear solver offers limited capability when facing large scale computing systems, and suffers from both soft error and round-off error due to floating point arithmetic. This work proposes a fault tolerant algorithm that recovers the solution of a dense linear system Ax = b from multiple spatial and temporal soft errors. Experimental results on Cray XT5 supercomputer confirm scalable performance of the proposed resilience functionality and negligible overhead in solution recovery.\n\nTitle: Reducing the time to tune parallel dense linear algebra routines with partial execution and performance modeling\nContent: We present a modeling framework to accurately predict time to run dense linear algebra calculation. We report the framework's accuracy in a number of varied computational environments such as shared memory multicore systems, clusters, and large supercomputing installations with tens of thousands of cores. We also test the accuracy for various algorithms, each of which having a different scaling properties and tolerance to low-bandwidth/high-latency interconnects. The predictive accuracy is very good and on the order of measurement accuracy which makes the method suitable for both dedicated and non-dedicated environments. We also present a practical application of our model to reduce the time required to tune and optimize large parallel runs whose time is dominated by linear algebra computations. We show practical examples of how to apply the methodology to avoid common pitfalls and reduce the influence of measurement errors and the inherent performance variability.\n\nTitle: Performance and portability with OpenCL for throughput-oriented HPC workloads across accelerators, coprocessors, and multicore processors\nContent: Ever since accelerators and coprocessors became the mainstream hardware for throughput-oriented HPC workloads, various programming techniques have been proposed to increase productivity in terms of both the performance and ease-of-use. We evaluate these aspects of OpenCL on a number of hardware platforms for an important subset of dense linear algebra operations that are relevant to a wide range of scientific applications. Our findings indicate that OpenCL portability has improved since our previous publication and many new and surprising usage scenarios are possible that rival those available after decades of software development on the CPUs. The combined performance-portability metric, even though not promised by the OpenCL standard, reflects the need for tuning performance-critical operations during the porting process and we show how a large portion of the available efficiency is lost if the tuning is not done correctly.\n\n## Task\nCompose an abstract for the title 'High Performance Development for High End Computing With Python Language Wrapper (PLW)' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper presents a design and implementation of a system that leverages interactive scripting environment to the needs of scientific computing. The system allows seamless translation of high level script codes to highly optimized native language executables that can be ported to parallel systems with high performance hardware and potential lack of the scripting language interpreter. Performance results are given to show various usage scenarios that differ in terms of invested programmer's effort and resulting performance gains."
      }
    ]
  },
  {
    "tid":"63",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Double Path Networks for Sequence to Sequence Learning.\nContent: Encoder-decoder based Sequence to Sequence learning (S2S) has made remarkable progress in recent years. Different network architectures have been used in the encoder/decoder. Among them, Convolutional Neural Networks (CNN) and Self Attention Networks (SAN) are the prominent ones. The two architectures achieve similar performances but use very different ways to encode and decode context: CNN use convolutional layers to focus on the local connectivity of the sequence, while SAN uses self-attention layers to focus on global semantics. In this work we propose Double Path Networks for Sequence to Sequence learning (DPN-S2S), which leverage the advantages of both models by using double path information fusion. During the encoding step, we develop a double path architecture to maintain the information coming from different paths with convolutional layers and self-attention layers separately. To effectively use the encoded context, we develop a cross attention module with gating and use it to automatically pick up the information needed during the decoding step. By deeply integrating the two paths with cross attention, both types of information are combined and well exploited. Experiments show that our proposed method can significantly improve the performance of sequence to sequence learning over state-of-the-art systems.\n\nTitle: Sequence Generation with Target Attention.\nContent: Source-target attention mechanism (briefly, source attention) has become one of the key components in a wide range of sequence generation tasks, such as neural machine translation, image caption, and open-domain dialogue generation. In these tasks, the attention mechanism, typically in control of information flow from the encoder to the decoder, enables to generate every component in the target sequence relying on different source components. While source attention mechanism has attracted many research interests, few of them turn eyes to if the generation of target sequence can additionally benefit from attending back to itself, which however is intuitively motivated by the nature of attention. To investigate the question, in this paper, we propose a new target-target attention mechanism (briefly, target attention). Along the progress of generating target sequence, target attention mechanism takes into account the relationship between the component to generate and its preceding context within the target sequence, such that it can better keep the coherent consistency and improve the readability of the generated sequence. Furthermore, it complements the information from source attention so as to further enhance semantic adequacy. After designing an effective approach to incorporate target attention in encoder-decoder framework, we conduct extensive experiments on both neural machine translation and image caption. Experimental results clearly demonstrate the effectiveness of our design of integrating both source and target attention for sequence generation tasks.\n\nTitle: Decoding with Value Networks for Neural Machine Translation.\nContent: Neural Machine Translation (NMT) has become a popular technology in recent years, and beam search is its de facto decoding method due to the shrunk search space and reduced computational complexity. However, since it only searches for local optima at each time step through one-step forward looking, it usually cannot output the best target sentence. Inspired by the success and methodology of AlphaGo, in this paper we propose using a prediction network to improve beam search, which takes the source sentence x, the currently available decoding output y(1), ...,y(t-1) and a candidate word w at step t as inputs and predicts the long-term value (e.g., BLEU score) of the partial target sentence if it is completed by the NMT model. Following the practice in reinforcement learning, we call this prediction network value network. Specifically, we propose a recurrent structure for the value network, and train its parameters from bilingual data. During the test time, when choosing a word w for decoding, we consider both its conditional probability given by the NMT model and its long-term value predicted by the value network. Experiments show that such an approach can significantly improve the translation accuracy on several translation tasks.\n\nTitle: Non-Autoregressive Neural Machine Translation with Enhanced Decoder Input\nContent: Non-autoregressive translation (NAT) models, which remove the dependence on previous target tokens from the inputs of the decoder, achieve significantly inference speedup but at the cost of inferior accuracy compared to autoregressive translation (AT) models. Previous work shows that the quality of the inputs of the decoder is important and largely impacts the model accuracy. In this paper, we propose two methods to enhance the decoder inputs so as to improve NAT models. The first one directly leverages a phrase table generated by conventional SMT approaches to translate source tokens to target tokens, which are then fed into the decoder as inputs. The second one transforms source-side word embeddings to target-side word embeddings through sentence-level alignment and word-level adversary learning, and then feeds the transformed word embeddings into the decoder as inputs. Experimental results show our method largely outperforms the NAT baseline (Gu et al. 2017) by 5:11 BLEU scores on WMT14 English-German task and 4:72 BLEU scores on WMT16 English-Romanian task.\n\nTitle: Dense Information Flow for Neural Machine Translation.\nContent: Recently, neural machine translation has achieved remarkable progress by introducing well-designed deep neural networks into its encoder-decoder framework. From the optimization perspective, residual connections are adopted to improve learning performance for both encoder and decoder in most of these deep architectures, and advanced attention connections are applied as well. Inspired by the success of the DenseNet model in computer vision problems, in this paper, we propose a densely connected NMT architecture (DenseNMT) that is able to train more efficiently for NMT. The proposed DenseNMT not only allows dense connection in creating new features for both encoder and decoder, but also uses the dense attention structure to improve attention quality. Our experiments on multiple datasets show that DenseNMT structure is more competitive and efficient.\n\n## Task\nCompose an abstract for the title 'Deliberation Networks: Sequence Generation Beyond One-Pass Decoding.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The encoder-decoder framework has achieved promising progress for many sequence generation tasks, including machine translation, text summarization, dialog system, image captioning, etc. Such a framework adopts an one-pass forward process while decoding and generating a sequence, but lacks the deliberation process: A generated sequence is directly used as final output without further polishing. However, deliberation is a common behavior in human's daily life like reading news and writing papers/articles/books. In this work, we introduce the deliberation process into the encoder-decoder framework and propose deliberation networks for sequence generation. A deliberation network has two levels of decoders, where the first-pass decoder generates a raw sequence and the second-pass decoder polishes and refines the raw sentence with deliberation. Since the second-pass deliberation decoder has global information about what the sequence to be generated might be, it has the potential to generate a better sequence by looking into future words in the raw sentence. Experiments on neural machine translation and text summarization demonstrate the effectiveness of the proposed deliberation networks. On the WMT 2014 English-to-French translation task, our model establishes a new state-of-the-art BLEU score of 41.5."
      }
    ]
  },
  {
    "tid":"64",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Scalability of an Ad Hoc On-Demand Routing Protocol in Very Large-Scale Mobile Wireless Networks\nContent: This article focuses on the performance and scalability study of very large-scale wireless ad hoc networks. As wireless devices become more and more popular, ad hoc networks grow in the number of nodes as well as the complexity of communication among the large number of nodes. However, due to the limitation of simulation technologies, it is either impossible or very hard to investigate the scalability of ad hoc routing protocols in very large-scale wireless networks. In this article, a comprehensive simulation study is conducted of the performance of an on-demand routing protocol on a very large scale, with as many as 50,000 nodes in the network. The authors address the scalability analysis based on various network sizes, node density, traffic load, and mobility. The reasons for packet loss are analyzed and categorized at each network layer. Based on the observations, the authors optimize the parameter selection and try to exhaust the scalability boundary of the on-demand routing protocol for wireless ad hoc networks.\n\nTitle: Enabling large-scale multicast simulation by reducing memory requirements\nContent: The simulation of large-scale multicast networks often requires a significant amount of memory that can easily exceed the capacity of current computers, both because of the inherently large amount of state necessary to simulate message routing and because of design oversights in the multicast portion of existing simulators. We describe three approaches to substantially reduce the memory required by multicast simulations: 1) We introduce a novel technique called \"negative forwarding table\" to compress multicast routing state. 2) We aggregate the routing state objects from one replicator per router per group per source to one replicator per router. 3) We employ the NIx-Vector technique to replace the original unicast IP routing table. We implemented these techniques in the ns2 simulator to demonstrate their effectiveness. Our experiments show that these techniques enable packet level multicast simulations on a scale that was previously unachievable on modern workstations using ns2.\n\nTitle: Queueing Analysis of Auxiliary-Connection-Enabled Switches for Software-Defined Networks\nContent: Software-Defined Networking (SDN) is a powerful technology to enable future network innovations, especially for data center networks. However, the limited capacity of the switch-to-controller link is a scalability issue for data center deployments. Auxiliary connections are introduced in the OpenFlow 1.3 Specification to help alleviate switch-to-controller link overflow and improve the scalability of SDN. In this paper, an analytical model is proposed to study the performance of auxiliary connections with OpenFlow switches. This model characterizes different interactions between the controller and the auxiliary-connection-enabled switches with limited buffer space. The model produces evaluation results in seconds for switches with realistic data center traffic loads, which would require days of compute time with network simulation. The analytical model is validated for lower traffic loads using ns3 and an example in-depth analysis of switch performance is carried out with typical data center traffic loads to illustrate the utility of the approach. To the best of our knowledge, this is the first paper to study auxiliary connections for SDN analytically.\n\nTitle: A New Approach to Zero-Copy Message Passing with Reversible Memory Allocation in Multi-core Architectures\nContent: We present a new Zero-Copy approach for message passing in a tightly coupled, multi -- process parallel discrete event simulation. Our approach is highly scalable and is suited for large scale distributed simulations. A Zero-Copy approach never copies message content. Rather, messages are created initially in a shared -- memory region, and only a smart pointer referring to the shared memory object is passed to a message recipient. The smart pointer can be dereferenced normally, and has reference count semantics allowing memory reuse when no references remain. This approach significantly reduces the amount of data copied between processes and results in considerable improvement in overall application performance as compared to more traditional shared --memory based message passing. We demonstrate the efficiency of our approach using two distributed discrete event simulators using conservative synchronization. However, we also included knowledge of the Global Virtual Time and the Current Simulation Time in our memory management algorithms, allowing rollbacks and reclaiming of memory that was freed prematurely.\n\nTitle: An efficient front-end for timing-directed parallel simulation of multi-core system\nContent: Manifold is a parallel simulation framework for multi-core systems. For full-system simulation, Manifold adopts the timing-directed simulation paradigm that separates the simulation into a functional front-end and a timing back-end. Components in the front-end perform functional simulation of the cores and send streams of instructions to the back-end to simulate the timing behavior. In its current design, Manifold uses the QSim multi-core emulator as the front-end, which communicates with the back-end through network sockets. Experiments have shown that the latency of the socket communications has a significant impact on the overall simulation performance. This paper presents a novel method that attempts to hide the TCP/IP latency for the back-end by creating proxy processes as an intermediary between the front-end and the back-end. The proxies serve as clients to the QSim server in the front-end, and as servers to the back-end. They interact with the QSim server through sockets, while working with the back-end in a producer-consumer manner using shared memory segments. Experiments show that this method can completely hide the TCP/IP latency for the back-end. The back-end can always get its instructions from the shared memory without waiting for the QSim server. The overhead of getting inputs for the back-end simulation is reduced to almost zero. As confirmed by our experiments, this improvement causes some side effects that together lead to significant improvements in overall simulation performance. In testing of system models with up to 64 cores, we have achieved from 29% to 51% improvement in simulation performance.\n\n## Task\nCompose an abstract for the title 'A Workload-Based Adaptive Load-Balancing Technique For Mobile Ad Hoc Networks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"A novel load-balancing technique for ad hoc on-demand routing protocols is presented. Currently, ad hoc routing protocols lack load-balancing capabilities, and thus, they often fail to provide good performance especially in the presence of a large volume of traffic. We present a simple but very effective method to achieve load balance and congestion alleviation. The new scheme is motivated by the observation that ad hoc on-demand routing protocols flood route request (RREQ) messages to acquire routes, and only nodes that respond to those messages have a potential to serve as intermediate forwarding nodes. If a node ignores RREQ messages within a specific period, it can completely be excluded from the additional communications that might have occurred for that period otherwise. Thus, a node can decide not to serve a traffic flow by dropping the RREQ for that How. In the new scheme, RREQ messages are forwarded selectively according to the load status of each node so that overloaded nodes can be excluded from the requested paths. Each node begins to allow additional traffic flows again whenever its overloaded status is dissolved. The new scheme utilizes interface queue occupancy and workload to control RREQ messages adaptively. The enhanced versions of protocols with this scheme are compared to the base protocols. Simulation results reveal that the new scheme greatly reduces packet latency as well as routing overhead without adversely affecting the network throughput, and it successfully balances the network load among nodes."
      }
    ]
  },
  {
    "tid":"65",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Beyond Overlay: Reaping Mutual Benefits for Primary and Secondary Networks Through Node-Level Cooperation.\nContent: Existing spectrum sharing paradigms have set clear boundaries between the primary and secondary networks. There is either no or very limited node-level cooperation between the primary and secondary networks. In this paper, we develop a new and bold spectrum-sharing paradigm beyond the state of the art for future wireless networks. We explore network cooperation as a new dimension for spectrum sharing between the primary and secondary users. Such network cooperation can be defined as a set of policies under which different degrees of cooperation are to be achieved. The benefits of this paradigm are numerous, as they allow integrating resources from two networks. There are many possible node-level cooperation policies that one can employ under this paradigm. For the purpose of performance study, we consider a specific policy called United cooperation of Primary and Secondary (UPS) networks. UPS allows a complete cooperation between the primary and secondary networks at the node level to relay each other's traffic. As a case study, we consider a problem with the goal of supporting the rate requirement of the primary network traffic while maximizing the throughput of the secondary sessions. For this problem, we develop an optimization model and formulate a combinatorial optimization problem. We also develop an approximation solution based on a piece-wise linearization technique. Simulation results show that UPS offers significantly better throughput performance than that under the interweave paradigm.\n\nTitle: Achieving transparent coexistence in a multi-hop secondary network through distributed computation\nContent: Transparent coexistence, also known as underlay, offers much more efficient spectrum sharing than traditional interweave coexistence paradigm. In a previous work, the transparent coexistence for a multi-hop secondary networks is studied. In this paper, we design a distributed solution to achieve this paradigm. In our design, we show how to increase the number of data streams iteratively while meeting constraints in the MIMO interference cancelation (IC) model and achieving transparent coexistence. All steps in our distributed algorithm can be accomplished based on local information exchange among the neighboring nodes. Our simulation results show that the performance of our distributed algorithm is highly competitive when compared to an upper bound solution for the centralized problem.\n\nTitle: Optimal throughput curve for primary and secondary users with node-level cooperation\nContent: A new trend in cooperation is to allow the primary and secondary networks to cooperate on the node level for data forwarding, which we call the UPS paradigm. This paper offers an in-depth study of the UPS paradigm in terms of its optimal throughput curve - the maximum achievable throughput for both primary and secondary users. We formulate the problem into a multicriteria optimization problem with the goal of maximizing the throughput of both primary and secondary users. Through a novel approach based on weighted Chebyshev norm, we transform the multicriteria optimization problem into a single criteria optimization problem and find a sequence of Pareto-optimal points iteratively. Based on the Pareto-optimal points, we find the throughput curve and show that it is \u03b5-approximation to the optimal. Through a case study, we show that the throughput region (the area under the throughput curve) under a UPS paradigm is substantially larger than that under the interweave paradigm.\n\nTitle: A Distributed Algorithm to Achieve Transparent Coexistence for a Secondary Multi-Hop MIMO Network.\nContent: The transparent coexistence (TC) paradigm allows simultaneous activation of the secondary users with the primary users as long as their interference to the primary users can be properly canceled. This paradigm has the potential to offer much more efficient spectrum sharing than the traditional interweave paradigm. In this paper, we design a distributed algorithm to achieve this paradigm for a secondary multi-hop network. For interference cancelation (IC), we employ MIMO at secondary nodes. We present a distributed iterative algorithm to maximize each secondary session\u2019s throughput while meeting all IC requirements under TC. By maintaining two local sets for each node, we can keep track of the node\u2019s IC responsibility. Although no explicit node ordering is maintained in our distributed algorithm, we prove that our distributed data structure at each node (with the use of two local sets) can be mapped to an explicit global node ordering for IC among all nodes in the network. This guarantees that each active node\u2019s degree-of-freedoms allocated for IC is feasible at the physical layer. Our algorithm is iterative in nature and all steps can be accomplished based on local information exchange among the neighboring nodes. We present the simulation results to show that the performance of our distributed algorithm is highly competitive when compared with an upper bound solution from the corresponding centralized problem.\n\nTitle: Toward Transparent Coexistence for Multihop Secondary Cognitive Radio Networks\nContent: The dominate spectrum sharing paradigm of today is interference avoidance, where a secondary network can use the spectrum only when such a use is not interfering with the primary network. However, with the advances of physical-layer technologies, the mindset of this paradigm is being challenged. This paper explores a new paradigm called \u201ctransparent coexistence\u201d for spectrum sharing between primary and secondary nodes in a multihop network environment. Under this paradigm, the secondary network is allowed to use the same spectrum simultaneously with the primary network as long as their activities are \u201ctransparent\u201d (or \u201cinvisible\u201d) to the primary network. Such transparency is accomplished through a systematic interference cancelation (IC) by the secondary nodes without any impact on the primary network. Although such a paradigm has been studied in the information theory (IT) and communications (COMM) communities, it is not well understood in the wireless networking community, particularly for multihop networks. This paper offers an in-depth study of this paradigm in a multihop network environment and addresses issues such as scheduling (both in frequency channels and time slots) and IC (to/from primary network and within the secondary network). Through a rigorous modeling and formulation, problem formulation, solution development, and simulation results, we show that transparent coexistence paradigm offers significant improvement in terms of spectrum access and throughput performance as compared to the current prevailing interference avoidance paradigm.\n\n## Task\nCompose an abstract for the title 'UPS: A United Cooperative Paradigm for Primary and Secondary Networks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The dominant spectrum sharing paradigm of today is the interweave paradigm. This paper advocates a new and alternative paradigm called United network of Primary and Secondary networks (UPS). UPS allows a complete cooperation between primary and secondary networks at the node level to relay each other's traffic, in addition to existing dynamic spectrum access (DSA) in time, space, and frequency domains. Such cooperation allows the primary and secondary networks to access a much richer network resources from the combined network. As a case study, we consider a problem with the goal of supporting the rate requirement of the primary network traffic while maximizing the minimum throughput of the secondary sessions. For this problem, we develop an optimization model and formulate a combinatorial optimization problem. Although this problem is in the form of mixed integer linear program (MILP), we can use CPLEX to solve it efficiently. Simulation results show that the UPS paradigm offers much better throughput performance than the interweave DSA paradigm."
      }
    ]
  },
  {
    "tid":"66",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Trust prediction via aggregating heterogeneous social networks\nContent: Along with the increasing popularity of social web sites, users rely more on the trustworthiness information for many online activities among users. However, such social network data often suffers from severe data sparsity and are not able to provide users with enough information. Therefore, trust prediction has emerged as an important topic in social network research. Traditional approaches explore the topology of trust graph. Previous research in sociology and our life experience suggest that people who are in the same social circle often exhibit similar behavior and tastes. Such ancillary information, is often accessible and therefore could potentially help the trust prediction. In this paper, we address the link prediction problem by aggregating heterogeneous social networks and propose a novel joint manifold factorization (JMF) method. Our new joint learning model explores the user group level similarity between correlated graphs and simultaneously learns the individual graph structure, therefore the shared structures and patterns from multiple social networks can be utilized to enhance the prediction tasks. As a result, we not only improve the trust prediction in the target graph, but also facilitate other information retrieval tasks in the auxiliary graphs. To optimize the objective function, we break down the proposed objective function into several manageable sub-problems, then further establish the theoretical convergence with the aid of auxiliary function. Extensive experiments were conducted on real world data sets and all empirical results demonstrated the effectiveness of our method.\n\nTitle: Social trust prediction using rank-k matrix recovery\nContent: Trust prediction, which explores the unobserved relationships between online community users, is an emerging and important research topic in social network analysis and many web applications. Similar to other social-based recommender systems, trust relationships between users can be also modeled in the form of matrices. Recent study shows users generally establish friendship due to a few latent factors, it is therefore reasonable to assume the trust matrices are of low-rank. As a result, many recommendation system strategies can be applied here. In particular, trace norm minimization, which uses matrix's trace norm to approximate its rank, is especially appealing. However, recent articles cast doubts on the validity of trace norm approximation. In this paper, instead of using trace norm minimization, we propose a new robust rank-k matrix completion method, which explicitly seeks a matrix with exact rank. Moreover, our method is robust to noise or corrupted observations. We optimize the new objective function in an alternative manner, based on a combination of ancillary variables and Augmented Lagrangian Multiplier (ALM) Method. We perform the experiments on three real-world data sets and all empirical results demonstrate the effectiveness of our method.\n\nTitle: Heterogeneous Image Features Integration via Multi-modal Semi-supervised Learning Model\nContent: Automatic image categorization has become increasingly important with the development of Internet and the growth in the size of image databases. Although the image categorization can be formulated as a typical multi-class classification problem, two major challenges have been raised by the real-world images. On one hand, though using more labeled training data may improve the prediction performance, obtaining the image labels is a time consuming as well as biased process. On the other hand, more and more visual descriptors have been proposed to describe objects and scenes appearing in images and different features describe different aspects of the visual characteristics. Therefore, how to integrate heterogeneous visual features to do the semi-supervised learning is crucial for categorizing large-scale image data. In this paper, we propose a novel approach to integrate heterogeneous features by performing multi-modal semi-supervised classification on unlabeled as well as unsegmented images. Considering each type of feature as one modality, taking advantage of the large amount of unlabeled data information, our new adaptive multi-modal semi-supervised classification (AMMSS) algorithm learns a commonly shared class indicator matrix and the weights for different modalities (image features) simultaneously.\n\nTitle: Heterogeneous image feature integration via multi-modal spectral clustering\nContent: In recent years, more and more visual descriptors have been proposed to describe objects and scenes appearing in images. Different features describe different aspects of the visual characteristics. How to combine these heterogeneous features has become an increasing critical problem. In this paper, we propose a novel approach to unsupervised integrate such heterogeneous features by performing multi-modal spectral clustering on unlabeled images and unsegmented images. Considering each type of feature as one modal, our new multi-modal spectral clustering (MMSC) algorithm is to learn a commonly shared graph Laplacian matrix by unifying different modals (image features). A non-negative relaxation is also added in our method to improve the robustness and efficiency of image clustering. We applied our MMSC method to integrate five types of popularly used image features, including SIFT, HOG, GIST, LBP, CENTRIST and evaluated the performance by two benchmark data sets: Caltech-101 and MSRC-v1. Compared with existing unsupervised scene and object categorization methods, our approach always achieves superior performances measured by three standard clustering evaluation metrices.\n\nTitle: Cross-language web page classification via dual knowledge transfer using nonnegative matrix tri-factorization\nContent: The lack of sufficient labeled Web pages in many languages, especially for those uncommonly used ones, presents a great challenge to traditional supervised classification methods to achieve satisfactory Web page classification performance. To address this, we propose a novel Nonnegative Matrix Tri-factorization (NMTF) based Dual Knowledge Transfer (DKT) approach for cross-language Web page classification, which is based on the following two important observations. First, we observe that Web pages for a same topic from different languages usually share some common semantic patterns, though in different representation forms. Second, we also observe that the associations between word clusters and Web page classes are a more reliable carrier than raw words to transfer knowledge across languages. With these recognitions, we attempt to transfer knowledge from the auxiliary language, in which abundant labeled Web pages are available, to target languages, in which we want classify Web pages, through two different paths: word cluster approximations and the associations between word clusters and Web page classes. Due to the reinforcement between these two different knowledge transfer paths, our approach can achieve better classification accuracy. We evaluate the proposed approach in extensive experiments using a real world cross-language Web page data set. Promising results demonstrate the effectiveness of our approach that is consistent with our theoretical analyses.\n\n## Task\nCompose an abstract for the title 'Social Trust Prediction Using Heterogeneous Networks.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Along with increasing popularity of social websites, online users rely more on the trustworthiness information to make decisions, extract and filter information, and tag and build connections with other users. However, such social network data often suffer from severe data sparsity and are not able to provide users with enough information. Therefore, trust prediction has emerged as an important topic in social network research. Traditional approaches are primarily based on exploring trust graph topology itself. However, research in sociology and our life experience suggest that people who are in the same social circle often exhibit similar behaviors and tastes. To take advantage of the ancillary information for trust prediction, the challenge then becomes what to transfer and how to transfer. In this article, we address this problem by aggregating heterogeneous social networks and propose a novel joint social networks mining (JSNM) method. Our new joint learning model explores the user-group-level similarity between correlated graphs and simultaneously learns the individual graph structure; therefore, the shared structures and patterns from multiple social networks can be utilized to enhance the prediction tasks. As a result, we not only improve the trust prediction in the target graph but also facilitate other information retrieval tasks in the auxiliary graphs. To optimize the proposed objective function, we use the alternative technique to break down the objective function into several manageable subproblems. We further introduce the auxiliary function to solve the optimization problems with rigorously proved convergence. The extensive experiments have been conducted on both synthetic and real- world data. All empirical results demonstrate the effectiveness of our method."
      }
    ]
  },
  {
    "tid":"67",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Aggressive Subchannel Allocation Algorithm for Efficient Dynamic Channel Allocation in Multi-User OFDMA System\nContent: This paper solves the problem of finding an intelligent subchannel allocation method to communicate with multiple users in OFDMA wireless systems. We propose an ASA (aggressive sub-channel allocation) algorithm, which is an intelligent dynamic channel allocation algorithm considering all users' channel state information conditionally to maximize throughput in an OFDMA system. We compare the ASA algorithm with round-robin, PF (proportional fair) and ACG (amplitude craving greedy) in the 2-tier cell environment and then analyze the performance of each algorithm through computer simulation in downlink OFDMA system. Simulation results show that the proposed ASA algorithm gets 24 %, 16 % and 14 % better ring throughput compared with the round-robin, PF and ACG, respectively\n\nTitle: A Novel Transmit Power Allocation Algorithm Combined with Dynamic Channel Allocation in Reuse Partitioning-based OFDMA/FDD System\nContent: This paper introduces a reuse partitioning-based dynamic channel allocation algorithm that guarantees quality of service (QoS) by considering fairness among mobile stations in an OFDMA/FDD system. To improve the SINR values for users around the cell edge and increase the overall system throughput compared with the conventional OFDMA/FDD system of frequency reuse factor (FRF) 1, an effective transmit power allocation algorithm is also combined with the proposed dynamic channel allocation to adjust the transmit power per user according to the average received SINR value. Simulation results show that the proposed DCA algorithm increases the sector throughput by about 25% when compared with the conventional case that do not apply the proposed DCA algorithm. When the proposed transmit power allocation algorithm is combined with the proposed DCA algorithm, a further increase in the sector throughput of about 6% is achieved than when using just the proposed DCA algorithm.\n\nTitle: Optimal Reference Vector Selection Algorithm in Subspace Interference Alignment to Maximize System Throughput\nContent: Arbitrary reference vectors have been adopted in conventional subspace alignment as transmitting vectors at the transmitter side. Inter-cell interference among users can be eliminated using orthogonal vectors to the chosen reference vectors at the receiver side. However, in this case, the sum-rate varies using different reference vectors, even though the channel values remain constant, and vice versa. In this paper, we analyzed the variation of sum-rate corresponding to reference vectors. An optimal reference vector selection algorithm based on subspace interference alignment is proposed to increase multi-cell capacity. The proposed algorithm adopts reference vectors with small variance. Simulation results show that the proposed algorithms provide about 50 % higher sum-rate than does the conventional subspace interference alignment algorithm.\n\nTitle: Effective adaptive transmit power allocation algorithm considering dynamic channel allocation in reuse partitioning-based OFDMA system\nContent: This paper introduces an transmit power allocation (TPA) algorithm considering dynamic channel allocation (DCA) for a reuse-partitioning- based Orthogonal frequency division multiple access (OFDMA)/FDD cellular system. The proposed reuse partitioning-based DCA algorithm guarantees quality of service (QoS) by considering fairness among mobile stations in an OFDMA/FDD system. However, to improve the SINR values for users around the cell edge and increase the overall system throughput compared with the conventional OFDMA/FDD system of frequency reuse factor (FRF) 1, an effective TPA algorithm is also combined with the proposed DCA to adjust the transmit power per user according to the average received SINR value. Simulation results show that the proposed DCA algorithm increases the sector throughput by about 25% when compared with the conventional case that do not apply the proposed DCA algorithm. When the proposed TPA is combined with the proposed DCA algorithm, a further increase in the sector throughput of about 6% is achieved than when using just the proposed DCA algorithm.\n\nTitle: An Algorithm for Iterative Detection and Decoding MIMO-OFDM HARQ with Antenna Scheduling\nContent: In this paper, a multiple-input-multiple-output (MIMO) hybrid-automatic repeat request (HARQ) algorithm with antenna scheduling is proposed. It retransmits the packet using scheduled transmit antennas according to the state of the communication link, instead of retransmitting the packet via the same antennas. As a result, a combination of conventional HARQ systems, viz. chase combining (CC) and incremental redundancy (IR) are used to achieve better performance and lower redundancy. The proposed MIMO-OFDM HARQ system with antenna scheduling is shown to be superior to conventional MIMO HARQ systems, due to its spatial diversity gain.\n\n## Task\nCompose an abstract for the title 'Aggressive sub-channel allocation algorithm for intelligent transmission in multi-user OFDMA system' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper solves the problem of finding an intelligent sub-channel allocation method to communicate with multiple users in OFDMA wireless systems. We propose an ASA (Aggressive Sub-channel Allocation) algorithm, which is an intelligent dynamic channel allocation algorithm considering all users' channel state information conditionally to maximize throughput in an OFDMA system. We compare the ASA algorithm with Round-robin, PF (Proportional Fair) and ACG (Amplitude Craving Greedy) in the 2-tier cell environment and then analyze the performance of each algorithm through computer simulation in downlink OFDMA system. Simulation results show that the proposed ASA algorithm gets 24 %, 16 % and 14 % better ring throughput compared with the Round-robin, PF and ACG, respectively."
      }
    ]
  },
  {
    "tid":"68",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Opportunities and challenges for better than worst-case design\nContent: The progressive trend of fabrication technologies towards the nanometer regime has created a number of new physical design challenges for computer architects. Design complexity, uncertainty in environmental and fabrication conditions, and single-event upsets all conspire to compromise system correctness and reliability. Recently, researchers have begun to advocate a new design strategy called Better Than Worst-Case design that couples a complex core component with a simple reliable checker mechanism. By delegating the responsibility for correctness and reliability of the design to the checker, it becomes possible to build provably correct designs that effectively address the challenges of deep submicron design. In this paper, we present the concepts of Better Than Worst-Case design and high light two exemplary designs: the DIVA checker and Razor logic. We show how this approach to system implementation relaxes design constraints on core components, which reduces the effects of physical design challenges and creates opportunities to optimize performance and power characteristics. We demonstrate the advantages of relaxed design constraints for the core components by applying typical-case optimization (TCO) techniques to an adder circuit. Finally, we discuss the challenges and opportunities posed to CAD tools in the context of Better Than Worst-Case design. In particular, we describe the additional support required for analyzing run-time characteristics of designs and the many opportunities which are created to incorporate typical-case optimizations into synthesis and verification.\n\nTitle: ReDEEM: A heterogeneous distributed microarchitecture for energy-efficient reliability\nContent: Diminishing energy-efficiency returns and decreasing transistor reliability are casting shadows on semiconductor scaling. Prior research has been addressing processors' energy-efficiency and transistor reliability as orthogonal problems. However, as embedded processors become more powerful and find their way into more diverse applications, both high reliability and energy-efficiency become critical. In this work, we propose ReDEEM, a novel approach to design energy-efficient and reliable microarchitectures. Our proposed solution composes processor pipelines at runtime from redundant but heterogeneous pipeline components. Our pipeline components are loosely coupled and the control logic is decentralized so as to enable fault isolation and thereby eliminate single points of failure. We equip the microarchitecture with the ability to adapt dynamically to varying application phases by constructing energy-efficient pipelines best suited for each phase. In addition, pipeline components have power management capabilities that allow for greater energy efficiency and flexibility. Our experimental evaluation shows that our solution offers up to 60% in energy savings and can operate about 1.8x longer, when subjected to the same fault rate as a state-of-the-art reliable microarchitecture.\n\nTitle: Optimizing Nonmonotonic Interconnect Using Functional Simulation and Logic Restructuring\nContent: The relatively poor scaling of interconnect in modern digital circuits necessitates a number of design optimizations, which must typically be iterated several times to meet the specified performance objectives. Such iterations are often due to the difficulty of early delay estimation, particularly before placement. Therefore, effective logic restructuring to reduce interconnect delay has been a major challenge in physical synthesis, a phase during which more accurate delay estimates can be finally gathered. In this paper, we develop a new approach that enhances modern high-performance logic synthesis techniques with flexibility and accuracy in the physical domain. This approach is based on the following: 1) a novel criterion based on path monotonicity, which identifies those interconnects that are amenable to optimization through logic restructuring, and 2) a synthesis algorithm relying on logic simulation and placement information to identify placed subcircuits that hold promise for interconnect reduction. Experiments indicate that our techniques find optimization opportunities and improve interconnect delay by 11.7% on average at less than 2% wirelength and area overhead.\n\nTitle: ForEVeR: A complementary formal and runtime verification approach to correct NoC functionality\nContent: As silicon technology scales, modern processor and embedded systems are rapidly shifting towards complex chip multi-processor (CMP) and system-on-chip (SoC) designs. As a side effect of complexity of these designs, ensuring their correctness has become increasingly problematic. Within these domains, Network-on-Chips (NoCs) are a de-facto choice to implement on-chip interconnect; their design is quickly becoming extremely complex in order to keep up with communication performance demands. As a result, design errors in the NoC may go undetected and escape into the final silicon. In this work, we propose ForEVeR, a solution that complements the use of formal methods and runtime verification to ensure functional correctness in NoCs. Formal verification, due to its scalability limitations, is used to verify smaller modules, such as individual router components. To deliver correctness guarantees for the complete network, we propose a network-level detection and recovery solution that monitors the traffic in the NoC and protects it against escaped functional bugs. To this end, ForEVeR augments the baseline NoC with a lightweight checker network that alerts destination nodes of incoming packets ahead of time. If a bug is detected, flagged by missed packet arrivals, our recovery mechanism delivers the in-flight data safely to the intended destination via the checker network. ForEVeR's experimental evaluation shows that it can recover from NoC design errors at only 4.9&percnt; area cost for an 8x8 mesh interconnect, over a time interval ranging from 0.5K to 30K cycles per recovery event, and it incurs no performance overhead in the absence of errors. ForEVeR can also protect NoC operations against soft-errors: a growing concern with the scaling of silicon. ForEVeR leverages the same monitoring hardware to detect soft-error manifestations, in addition to design-errors. Recovery of the soft-error affected packets is guaranteed by building resiliency features into our checker network. ForEVeR incurs minimal performance penalty up to a flit error rate of 0.01&percnt; in lightly loaded networks.\n\nTitle: Crashtest: A Fast High-Fidelity Fpga-Based Resiliency Analysis Framework\nContent: Extreme scaling practices in silicon technology are quickly leading to integrated circuit components with limited reliability, where phenomena such as early-transistor failures, gate-oxide wearout, and transient faults are becoming increasingly common. In order to overcome these issues and develop robust design techniques for large-market silicon ICs, it is necessary to rely on accurate failure analysis frameworks which enable design houses to faithfully evaluate both the impact of a wide range of potential failures and the ability of candidate reliable mechanisms to overcome them. Unfortunately, while failure rates are already growing beyond economically viable limits, no fault analysis framework is yet available that is both accurate and can operate on a complex integrated system.To address this void, we present CrashTest, a fast, high-fidelity and flexible resiliency analysis system. Given a hardware description model of the design under analysis, CrashTest is capable of orchestrating and performing a comprehensive design resiliency analysis by examining how the design reacts to faults while running software applications. Upon completion, CrashTest provides a high-fidelity analysis report obtained by performing a fault injection campaign at the gate-level netlist of the design. The fault injection and analysis process is significantly accelerated by the use of an FPGA hardware emulation platform. We conducted experimental evaluations on a range of systems, including a complex LEON-based system-on-chip, and evaluated the impact of gate-level injected faults at the system level. We found that CrashTest is 16-90x faster than an equivalent software-based framework, when analyzing designs through direct primary I/Os. As shown by our LEON-based SoC experiments, CrashTest exhibits emulation speeds that are six orders of magnitude faster than simulation.\n\n## Task\nCompose an abstract for the title 'Deployment of Better Than Worst-Case Design: Solutions and Needs' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The advent of nanometer feature sizes in silicon fabrication has triggered a number of new design challenges for computer designers. These challenges include design complexity and operation in the presence of environmental and device uncertainty. To make things worse, these new challenges add to the many challenges that designers already face in order to scale system performance while meeting power and reliability budgets. Current design objectives are being met by applying even more engineers and increasing overall design times, an unsustainable trend. This paper overviews a novel design strategy, called Better Than Worst-Case design, that addresses these challenges through a methodology based on separating the concerns of performance and reliability by coupling complex design components with simple reliable checker mechanisms. We present the key aspects of Better ThanWorst-Case Design and cover some recently proposed solutions that deploy this technique in application domains ranging from microprocessors to digital signal processors. We then highlight a few aspects that need to be addressed to make this approach more practical in general contexts and suggest possible solutions."
      }
    ]
  },
  {
    "tid":"69",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Layered block diagonalization for base station cooperated multiuser MIMO with partial channel state information feedback\nContent: This paper proposes a layered block diagonalization (BD) precoding method for downlink multiuser multiple-input multiple-output (MIMO) transmission employing multiple base station (BS) cooperation where some of the instantaneous channel state information (CSI) feedback between the user equipment and the respective BSs are missing depending on the location of the user equipment. In the layered BD method, the inter-user interference from the channel for which the set of BSs knows the instantaneous CSI is eliminated as in the conventional BD method with perfect CSI feedback. However, since some of the instantaneous CSI is not obtained at the BSs, the inter-user interference from these channels without the knowledge of the instantaneous CSI is allowed. To minimize the inter-user interference from the BS-unknown channel and to utilize fully the available transmission power, the layered BD method adaptively controls the set of BSs used for the signal transmission for a particular user in user-by-user manner, based on the CSI feedback state of that user and the other users. The layered BD method maximizes the power efficiency of the respective users in a partial instantaneous CSI feedback scenario, while maintaining the nulling of the inter-user interference in all of the BS-known MIMO channels. A computer simulation is conducted to verify the effectiveness of the proposed method from the viewpoint of the achievable throughput. The results show that the proposed layered BD method expands the user throughput region compared to that of the conventional approaches when accommodating the partial instantaneous CSI feedback scenario.\n\nTitle: Investigation on Beamforming Control Methods in Base Station Cooperative Multiuser MIMO Using Block-Diagonalized Beamforming Matrix\nContent: This paper investigates a beamforming control method in base station (BS) cooperative multiuser multiple-input multiple-output (MIMO) transmission assuming a block-diagonalized unitary beamforming matrix. More specifically, we comprehensively compare open-loop type random (opportunistic) beamforming control to codebook-based closed-loop type beamforming control. The proportional fair (PF)-based scheduling is assumed, and the number of users, codebook size, and delay time of the feedback signal from the user terminals are taken into account in the comparison. Based on numerical results, codebook-based beamforming control with a large codebook size achieves higher system throughput than random beamforming when the number of users is small and the feedback delay time is short. However, we show that when the number of users is large and the feedback delay time is long, random beamforming achieves higher system-level throughput than codebook-based beamforming. This implies that the transmission-rate control with random beamforming is more robust against channel variation than that with codebook-based beamforming. These results suggest that random beamforming, which requires a lower feedback overhead than the codebook-based beamforming, is a promising beamforming strategy in future radio access.\n\nTitle: CF-Based Adaptive PAPR Reduction Method for Block Diagonalization-Based Multiuser MIMO-OFDM Signals\nContent: This paper investigates the application of a previously proposed adaptive peak-to-average power ratio (PAPR) reduction method based on clipping and filtering (CF) to block diagonalization-based multiuser multiple-input multiple-output (MIMO) orthogonal frequency division multiplexing (OFDM) transmission. Assuming that the total number of receiver antennas is less than that for the transmitter antennas, we introduce virtual streams used only for PAPR reduction and the precoding matrix for these virtual streams, where the virtual streams do not impart interference to the actual data streams. Since the interference caused by the clipping operation is concentrated in the virtual streams and consequently restricts the interference to the actual data stream, the proposed method can significantly improve the sum capacity compared to the conventional CF method for a given PAPR. Simulation results show the capacity gain using the proposed method compared to the conventional method.\n\nTitle: Non-Orthogonal Multiple Access Using Intra-Beam Superposition Coding and SIC in Base Station Cooperative MIMO Cellular Downlink\nContent: This paper extends our previously proposed non-orthogonal multiple access (NOMA) scheme to the base station (BS) cooperative multiple-input multiple-output (MIMO) cellular downlink for future radio access. The proposed NOMA scheme employs intra-beam superposition coding of a multiuser signal at the transmitter and the spatial filtering of inter-beam interference followed by the intra-beam successive interference canceller (SIC) at the user terminal receiver. The intra-beam SIC cancels out the inter-user interference within a beam. This configuration achieves reduced overhead for the downlink reference signaling for channel estimation at the user terminal in the case of non-orthogonal user multiplexing and also achieves the applicability of the SIC receiver in MIMO downlink. The transmitter beamforming (precoding) matrix is controlled based on open loop-type random beamforming using a block-diagonalized beamforming matrix, which is very efficient in terms of the amount of feedback information from the user terminal. Simulation results show that the proposed NOMA scheme with block-diagonalized random beamforming in BS cooperative multiuser MIMO and the intra-beam SIC achieves better system-level throughput compared to orthogonal multiple access (OMA), which is assumed in LTE-Advanced. We also show that BS cooperative operation along with the proposed NOMA in particular achieves a high cell-edge user throughput gain which implies better user fairness and universal connectivity.\n\nTitle: Muting-Based Partially Non-Orthogonal Block Diagonalization In Multiuser Mimo With Limited Channel State Information Feedback\nContent: This paper investigates a precoding method in downlink multiuser multiple-input multiple-output (MIMO) transmission employing multiple base station (BS) cooperation where the sets of user equipment basically feedback the instantaneous channel state information (CSI) only to the nearest BS, but the users near the cell edge additionally feedback the instantaneous CSI to the second nearest BS among the cooperating BSs. The proposed precoding method is categorized into multi-cell processing, in which the transmission information sent to a user is shared by the cooperating BSs in order to utilize fully the degrees of freedom of the spatial channel, and is based on block diagonalization of the channel matrix. However, since some elements of the channel matrix are unknown, we allow inter-user interference to users with limited instantaneous CSI feedback. More specifically, in the proposed method, the elements corresponding to the unknown instantaneous CSI in the overall channel matrix are muted by setting these elements to zero, and the standard generation procedure of the block diagonalization precoding matrix is performed on the muted channel matrix. The proposed method efficiently utilizes the degrees of freedom of the spatial channel compared to the case with full orthogonal transmission at the cost of increased inter-user interference. Simulation results show that the proposed method decreases the transmission power for a given capacity requirement compared to the case with full orthogonal transmission.\n\n## Task\nCompose an abstract for the title 'Base station cooperative multiuser MIMO using block-diagonalized random beamforming with online update' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper proposes applying random (opportunistic) beamforming to base station (BS) cooperative multiuser multiple-input multiple-output (MIMO) transmission. This proposal comprises two parts. First, we propose a block-diagonalized random unitary beamforming matrix. The proposed beamforming matrix achieves better throughput distribution compared to the purely random unitary beamforming matrix when the average path loss based on distance-dependent loss and shadowing loss is largely different among transmitter antennas, which is true in BS cooperative MIMO. Second, we propose an online update algorithm for a random beamforming matrix to improve the throughput compared to the purely random and channel-independent beamforming matrix generation, especially when the number of users is low. Different from conventional approaches, the proposed online update algorithm does not increase the overhead of the reference signal transmission and control delay. Simulation results show the effectiveness of the proposed method using a block-diagonalized random unitary beamforming matrix with online updates in a BS cooperative multiuser MIMO scenario."
      }
    ]
  },
  {
    "tid":"70",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Situation understanding based on heterogeneous sensor networks and human-inspired favor weak fuzzy logic system\nContent: Humans use multiple sources of sensory information to estimate environmental properties and has innate ability to integrate information from heterogeneous data sources. How the multi-sensory and multimodal information are integrated in human brain? There is consensus that it depends on the prefrontal cortex (PFC). The PFC has top-down control (favor weak) and rule-based mechanisms, and we incorporate the favor weak mechanism into rule-based fuzzy logic systems (FLS) via using upper and lower membership functions. The inference engine of favor weak fuzzzy logic system is proposed under three different categories based on fuzzifiers. We apply the favor weak FLS to situation understanding based on heterogeneous sensor network, and it shows that our favor weak fuzzy logic system has clear advantage comparing to the type-1 FLS. The favor weak FLS can increase the probability of threat detection, and provides timely indication & warning (I&W).\n\nTitle: Designing power aware self-reconfiguring topology for mobile wireless personal area networks using fuzzy logic\nContent: In mobile wireless personal area networks (WPAN), the position of each node changes over time. A network protocol that is able to dynamically update its links in order to maintain strong connectivity is said to be \"self-reconfiguring.\" We propose a mobile wireless personal area networks (WPAN) design method with self-reconfiguring protocol for power efficiency. The WPAN is self-organized to clusters using an unsupervised clustering method, fuzzy c-means. A fuzzy logic system is applied to master/controller election for each cluster. A self-reconfiguring topology is proposed to manage the mobility and recursively update the network topology. We also modify the mobility management scheme with hysteresis to overcome the ping-pong effect. Simulation results show that our scheme performs much better than the existing algorithm.\n\nTitle: Joint Multi-Target Identification and Classification in Cognitive Radar Sensor Networks\nContent: We investigate the problem of jointly classifying and identifying multiple targets in radar sensor networks where the maximum\n number of categories and the maximum number of targets in each category are obtained a priori based on statistical data. However,\n the actual number of targets in each category and the actual number of target categories being present at any given time are\n assumed unknown. It is assumed that a given target only belongs to one category and one identification number. The target\n signals are moreover modeled as zero-mean complex Gaussian processes. In this paper, we propose a joint multi-target identification\n and classification (JMIC) algorithm for radar surveillance using the cognitive radar network. The existing target categories\n are first classified and then the targets in each category are accordingly identified. Simulation results are presented to\n evaluate the feasibility and effectiveness of the proposed JMIC algorithm in a query surveillance region.\n\nTitle: Improving performance of multi-radio frequency-hopping wireless mesh networks\nContent: In this paper, we study a wireless infrastructure mesh network where nodes are equipped multiple frequency-hopping radio interfaces, each with an array of antennas and utilizing the space-time block coding (STBC) technique for transmissions. With STBC technique on the radio, both temporal and spatial diversities can be employed on each transmission and the channel-attenuated performance can then be improved. Then, to reduce the cochannel interferences in the multiradio mesh network , we present an interference-aware channel assignment algorithm to evaluate the amount of cochannel interferences using an SER (symbol eror rate)-based cost function and assign the most appropriate hopping pattern for each radio. Using the STBC physical technique along with the MAC-layer channel management, the mesh network thus acquires the immunity against the channel attenuation and the cochannel interferences.\n\nTitle: Ad hoc wireless network traffic-self-similarity and forecasting\nContent: Lots of works have been carried out to discuss the self-similarity of Ethernet and World Wide Web traffic. In this letter, we study the ad hoc wireless network traffic collected in an ad hoc network (AHN) testbed and show that the ad hoc wireless network traffic is self-similar, which validates that AHN traffic is forecastable because self-similar time-series can be forecasted. We apply a fuzzy logic system to ad hoc wireless network traffic forecasting and simulation results show that it performs much better than does an LMS adaptive filter. All these studies are very important for evaluating network capacity and determining the battery power mode based on the forecasted traffic workload.\n\n## Task\nCompose an abstract for the title 'Improving Person Reidentification Using a Self-Focusing Network in Internet of Things' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Person reidentification (re-ID), which is a significant and potential application in the Internet of Things (IoT), aims to retrieve pedestrians of interest given a labeled image in a camera network. Now, it is still existing many challenges that severely influence feature representation in practical scenarios. Many methods adopt the attention mechanism in convolutional neural network (CNN) to improve the ability of feature learning. Although they only apply 1-D attention block in the popular deep learning architecture, the learned features are not discriminative for the feature representation. In this work, we investigate a self-focusing network (SFNet) that considers both the channel-dimensional attention and spatial-dimensional attention to adaptively learn more discriminative features. Namely, we embed the new attention module into the common backbone network, which can focus on the salient region by inhibiting the redundant features. Specifically, we design eight variants of the channel-dimensional attention and spatial-dimensional attention throughout the entire network and explore the most powerful feature representation. The heatmaps of different layers are visualized to intuitively present the performance of SFNet. Furthermore, we compare SFNet with the prior work on three popular person re-ID benchmarks by abundant experiments."
      }
    ]
  },
  {
    "tid":"71",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Efficient Dynamic Flow Tracking for Packet Analyzers\nContent: Analyzing large amounts of traffic at the packet or flow level is an important part of managing and monitoring cloud network infrastructure. Common scenarios that require low-level packet analysis are troubleshooting problems, accounting traffic, and security applications such as intrusion detection systems or firewalls. Moreover, researchers often analyze traffic for scientific purposes. For such low-level traffic analyses, tracking flows is a feature required for both commercial and scientific purposes. However, there is no good shared library available to implement this functionality in an efficient, configurable, and dynamic way that is suitable for real-time analysis. We implement a high-performant generic flow tracker that can track millions of simultaenous flows based on arbitrarily complex definitions of a flow. We make this implementation available as open source in our traffic analysis tool FlowScope. The highly efficient realtime tracking of flows by arbitrarily complex user-defined flow criteria and filters is enabled by just-in-time (JIT) compilation of flow tracking rules. The code and evaluation scripts are available as free and open source at: https://github.com/emmericp/FlowScope.\n\nTitle: Flow-inspector: a framework for visualizing network flow data using current web technologies\nContent: New web technologies led to the development of browser applications for data analysis. Modern browser engines allow for building interactive real-time visualization applications that enable efficient ways to understand complex data. We present Flow-Inspector, a highly interactive open-source web framework for visualizing network flow data using latest web technologies. Flow-Inspector includes a backend for processing and storing large-scale network flow data, as well as a JavaScript-based web application capable to display and manipulate traffic information in real-time. This work provides operators with a toolkit to analyze their networks and enables the scientific community to create new and innovative visualizations of traffic data with an extensible framework. We demonstrate the applicability of our approach by implementing several different visualization components that help to identify topological characteristics in network flows.\n\nTitle: Towards stochastic flow-level network modeling: Performance evaluation of short TCP flows\nContent: We present in this paper a stochastic flow-level network model for the performance evaluation of IP networks with multiple bottlenecks supporting short-lived and long-lived TCP flows. Flow-level network models are efficient at estimating the mean bandwidth of TCP flows in various topologies, but they are generally limited to the study of infinite flows. This paper extends such models in order to evaluate short-lived flows alternating between idle and active periods where data of random size is transferred. We study the interaction between multiple flows and derive mean bandwidths, durations of file transfer or average number of active flows. We first study a single bottleneck, and then extend our analysis to networks with multiple bottlenecks as well as the effect of slow-start. We apply our results to various networks and assess the accuracy of our approach by comparing our analytical results with results of the discrete event simulator ns-2.\n\nTitle: Automated Real Time Performance Management for Mobile Networks\nContent: Real Time Performance Management (RTPM) is expected to be a key features offuture developments of mobile networks. Although it is already requested by operators of mobile networks only basic approaches have been realized. Implementing access to real time performance data in a mobile network, with a large number of manageable nodes, leads to a huge resource consumption. Not only network bandwidth but also processing resources. Current PM schemes can hardly be extended to support real time performance management. We present a policy based approach that is capable to deal with challenges imposed by the request for real time performance data. Typical workflows are identified and analysed. Based on these analyses a policy based performance management system is introduced that allows a balanced configuration which does not exceed available resources but still satisfies the information requirements of the human operators.\n\nTitle: Carrier-Grade Anomaly Detection Using Time-to-Live Header Information.\nContent: Time-to-Live data in the IP header offers two interesting characteristics: First, different IP stacks pick different start TTL values. Second, each traversed router should decrement the TTL value. The combination of both offers host and route fingerprinting options. We present the first work to investigate Internet-wide TTL behavior at carrier scale and evaluate its fit to detect anomalies, predominantly spoofed source IP addresses. Using purpose-built software, we capture 2 weeks of raw TTL data at a 40 Gbit/s Internet uplink. For further insight, we actively measure observed hosts and conduct large-scale hitlist-based measurements, which yields three complementary data sets for IPv4 and IPv6. A majority (69% IPv4; 81% IPv6) of passively observed multi-packet hosts exhibit one stable TTL value. Active measurements on unstable hosts yield a stable anchor TTL value for more than 85% of responsive hosts. We develop a structure to further classify unstable hosts taking, for example, temporal stability into account. Correlation of TTL values with BGP data is clear, yet unpredictive. The results indicate that carrier-grade TTL anomaly detection can yield significant insights in the following categories: First, the method can flag anomalies based on TTL observations (yet likely at a difficult false positive/false negative trade-off). Second, the method can establish trust that a packet originates from its acclaimed source.\n\n## Task\nCompose an abstract for the title 'Real-time Analysis of Flow Data for Network Attack Detection' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"\u00f3 With the wide deployment of o w monitoring in IP networks, the analysis of the exported o w data has become an important research area. It has been shown that o w data can be used to detect trafc anomalies, DoS attacks, and the propagation of worms. In practice, anomalies and attacks should be detected as fast as possible in order to allow taking appropriate countermeasures. We describe the necessary steps from the raw o w data to the detection result in a systematic way. Furthermore, we present TOPAS, a system and framework for real-time analysis of o w data, that has been developed in order to meet these requirements. Performance measurements and various application examples point out the capabilities and benets of our approach. Index Terms\u00f3 network monitoring, o w analysis, anomaly and attack detection"
      }
    ]
  },
  {
    "tid":"72",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Interval-based flow watermarking for tracing interactive traffic\nContent: Tracing interactive attack traffic that traverses stepping stones (i.e., intermediate hosts) is challenging, as the packet headers, lengths, and contents can all be changed by the stepping stones. The traffic timing (delays between packets) has therefore been studied as a means of tracing traffic. One such technique uses traffic timing as a side channel into which a watermark, or identifying tag, can be embedded to aid with tracing. The effectiveness of such techniques is greatly reduced when the packet count of the traffic is changed at the stepping stone. Such transformations may occur as a result of either active countermeasures (e.g. chaff packets, flow splitting) by an adversary attempting to defeat tracing, or by incidental repacketization of the traffic by network interfaces. This paper presents a new method of embedding a watermark in traffic timing, for purposes of tracing the traffic in the presence of flow splitting, chaff packets, timing perturbation, and repacketization. This method uses an invariant characteristic of two connection flows which are part of the same stepping stone chain, namely, the elapsed time of the flows. The duration of each flow is sliced into short fixed-length intervals. Packet timing is adjusted to manipulate the packet count in specific intervals (without adding or deleting any packets), for purposes of embedding the watermark. The method is self-synchronizing and does not require clock synchronization between the watermark encoder and decoder. A statistical analysis of the method, with no assumptions or limitations concerning the distribution of packet times, proves the effectiveness of the method given a sufficient number of packets, despite natural and/or deliberate repacketization and countermeasures by an adversary. The method has been implemented and tested on a large number of SSH traffic flows. The results demonstrate that 100% detection rates and very low false positive rates are achieved under conditions of multiple countermeasures, and using only a few hundred packets.\n\nTitle: Analyzing network traffic to detect self-decrypting exploit code\nContent: Remotely-launched software exploits are a common way for attackers to intrude into vulnerable computer systems. As detection techniques improve, remote exploitation techniques are also evolving. Recent techniques for evasion of exploit detection include polymorphism (code encryption) and meta-morphism (code obfuscation). This paper addresses the problem of detecting in network traffic polymorphic remote exploits that are encrypted, and that self-decrypt before launching the intrusion. Such exploits pose a great challenge to existing malware detection techniques, partly due to the non-obvious starting location of the exploit code in the network payload.We describe a new method for detecting self-decrypting exploit codes. This method scans network traffic for the presence of a decryption routine, which is characteristic of such exploits. The proposed method uses static analysis and emulated instruction execution techniques. This improves the accuracy of determining the starting location and instructions of the decryption routine, even if self-modifying code is used. The method outperforms approaches that have been previously proposed, both in terms of detection capabilities, and in detection accuracy.The proposed method has been implemented and tested on current polymorphic exploits, including ones generated by state-of-the-art polymorphic engines. All exploits have been detected (i.e., a 100% detection rate), including those for which the decryption routine is dynamically coded, or self-modifying. The false positive rate is close to 0%. Running time is approximately linear in the size of the network payload being analyzed.\n\nTitle: BitTrickle: Defending against broadband and high-power reactive jamming attacks\nContent: Reactive jamming is not only cost effective, but also hard to track and remove due to its intermittent jamming behaviors. Frequency Hopping Spread Spectrum (FHSS) and Direct Sequence Spread Spectrum (DSSS) have been widely used to defend against jamming attacks. However, both will fail if the jammer jams all frequency channels or has high transmit power. In this paper, we propose BitTrickle, an anti-jamming wireless communication scheme that allows communication in the presence of a broadband and high power reactive jammer by exploiting the reaction time of the jammer. We develop a prototype of BitTrickle using the USRP platform running GNURadio. Our evaluation shows that when under powerful reactive jamming, BitTrickle still maintains communication, whereas other schemes such as 802.11 DSSS fail completely.\n\nTitle: Interleaved hop-by-hop authentication against false data injection attacks in sensor networks\nContent: Sensor networks are often deployed in unattended environments, thus leaving these networks vulnerable to false data injection attacks in which an adversary injects false data into the network with the goal of deceiving the base station or depleting the resources of the relaying nodes. Standard authentication mechanisms cannot prevent this attack if the adversary has compromised one or a small number of sensor nodes. We present three interleaved hop-by-hop authentication schemes that guarantee that the base station can detect injected false data immediately when no more than t nodes are compromised, where t is a system design parameter. Moreover, these schemes enable an intermediate forwarding node to detect and discard false data packets as early as possible. Our performance analysis shows that our scheme is efficient with respect to the security it provides, and it also allows a tradeoff between security and performance. A prototype implementation of our scheme indicates that our scheme is practical and can be deployed on the current generation of sensor nodes.\n\nTitle: How to misuse AODV: a case study of insider attacks against mobile ad-hoc routing protocols\nContent: This paper presents a systematic analysis of insider attacks against mobile ad-hoc routing protocols, using the Ad hoc On-Demand Distance Vector (AODV) protocol as an example. It identifies a number of attack goals and then study how to achieve these goals through misuses of the routing messages. To facilitate the analysis, this paper classifies the insider attacks into two categories: {\\em atomic misuses} and {\\em compound misuses}. Atomic misuses are performed by manipulating a single routing message, which cannot be further divided; compound misuses are composed of combinations of atomic misuses and possibly normal uses of the routing protocol. The analysis results in this paper reveal several classes of insider attacks, including {\\em route disruption, route invasion, node isolation,} and {\\em resource consumption}. This paper also includes simulation results that demonstrate the impact of these attacks.\n\n## Task\nCompose an abstract for the title 'Tracing Traffic Through Intermediate Hosts That Repacketize Flows' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Tracing interactive traffic that traverses stepping stones (i.e., intermediate hosts) is challenging, as the packet headers, lengths, and contents can all be changed by the stepping stones. The traffic timing has therefore been studied as a means of tracing traffic. One such technique uses traffic timing as a side channel into which a watermark, or identifying tag, can be embedded to aid with tracing. The effectiveness of such techniques is greatly reduced when repacketization of the traffic occurs at the stepping stones. Repacketization is a natural effect of many applications, including SSH, and therefore poses a serious challenge for traffic tracing.This paper presents a new method of embedding a watermark in traffic timing, for purposes of tracing the traffic in the presence of repacketization. This method uses an invariant characteristic of two traffic flows which are part,of the same stepping stone chain, namely, elapsed time of the flows. The duration of each flow is sliced into short fixed-length intervals. Packet timing is adjusted to manipulate the packet count in specific intervals, for purposes of embedding the watermark. A statistical analysis of the method, with no assumptions or limitations concerning the distribution of packet times, proves the effectiveness of the method given a sufficient number of packets, despite natural and/or deliberate repacketization and perturbation of the traffic timing by an adversary. The method has been implemented and tested on a large number of synthetically-generated SSH traffic flows. The results demonstrate that 100% detection rates and less than 1% false positive rates are achievable under conditions of 2 seconds of maximum timing perturbation and 12% repacketization rate, using fewer than 1000 packets."
      }
    ]
  },
  {
    "tid":"73",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Exploiting dense substructures for fast sparse matrix vector multiplication\nContent: The execution time of many scientific computing applications is dominated by the time spent in performing sparse matrix vector multiplication (SMV; y \u00e2\u8060\u806c A \u8117\u00b7 x). We consider improving the performance of SMV on multicores by exploiting the dense substructures that are inherently present in many sparse matrices derived from partial differential equation models. First, we identify indistinguishable vertices, i.e., vertices with the same adjacency structure, in a graph representation of the sparse matrix (A) and group them into a supernode. Next, we identify effectively dense blocks within the matrix by grouping rows and columns in each supernode. Finally, by using a suitable data structure for this representation of the matrix, we reduce the number of load operations during SMV while exactly preserving the original sparsity structure of A. In addition, we use ordering techniques to enhance locality in accesses to the vector, x, to yield an SMV kernel that exploits the effectively dense substructures in the matrix. We evaluate our scheme on Intel Nehalem and AMD Shanghai processors. We observe that for larger matrices on the Intel Nehalem processor, our method improves performance on average by 37.35% compared with the traditional compressed sparse row scheme (a blocked compressed form improves performance on average by 30.27%). Benefits of our new format are similar for the AMD processor. More importantly, if we pick for each matrix the best among our method and the blocked compressed scheme, the average performance improvements increase to 40.85%. Additional results indicate that the best performing scheme varies depending on the matrix and the system. We therefore propose an effective density measure that could be used for method selection, thus adding to the variety of options for an auto-tuned optimized SMV kernel that can exploit sparse matrix properties and hardware attributes for high performance.\n\nTitle: Performance of Greedy Ordering Heuristics for Sparse Cholesky Factorization\nContent: Greedy algorithms for ordering sparse matrices for Cholesky factorization can be based on different metrics. Minimum degree, a popular and effective greedy ordering scheme, minimizes the number of nonzero entries in the rank-1 update (degree) at each step of the factorization. Alternatively, minimum deficiency minimizes the number of nonzero entries introduced (deficiency) at each step of the factorization. In this paper we develop two new heuristics: modified minimum deficiency (MMDF) and modified multiple minimum degree (MMMD). The former uses a metric similar to deficiency while the latter uses a degree-like metric. Our experiments reveal that on the average, MMDF orderings result in 21% fewer operations to factor than minimum degree; MMMD orderings result in 15% fewer operations to factor than minimum degree. MMMD requires on the average 7--13% more time than minimum degree, while MMDF requires on the average 33--34% more time than minimum degree.\n\nTitle: A multilevel compressed sparse row format for efficient sparse computations on multicore processors\nContent: We seek to improve the performance of sparse matrix computations on multicore processors with non-uniform memory access (NUMA). Typical implementations use a bandwidth reducing ordering of the matrix to increase locality of accesses with a compressed storage format to store and operate only on the non-zero values. We propose a new multilevel storage format and a companion ordering scheme as an explicit adaptation to map to NUMA hierarchies. More specifically, we propose CSR-k, a multilevel form of the popular compressed sparse row (CSR) format for a multicore processor with k > 1 well-differentiated levels in the memory subsystem. Additionally, we develop Band-k, a modified form of a traditional bandwidth reduction scheme, to convert a matrix represented in CSRto our proposed CSR-k. We evaluate the performance of the widely-used and important sparse matrix-vector multiplication (SpMV) kernel using CSR-2 on Intel Westmere processors for a test suite of 12 large sparse matrices with row densities in the range 3 to 45. On 32 cores, on average across all matrices in the test suite, the execution time for SpMV with CSR-2is less than 42% of the time taken by the state-of-the-art automatically tuned SpMV resulting in energy savings of approximately 56%. Additionally, on average, the parallel speed-up on 32 cores of the automatically tuned SpMV relative to its 1-core performance is 8.18 compared to a value of 19.71 for CSR-2. Our analysis indicates that the higher performance of SpMV with CSR-2 comes from achieving higher reuse of x in the shared L3 cache without incurring overheads from fill-in of original zeroes. Furthermore, the pre-processing costs of SpMV with CSR-2 can be amortized on average over 97 iterations of SpMV using CSR and are substantially lower than the 513 iterations required for the automatically tuned implementation. Based on these results, CSR-k appears to be a promising multilevel formulation of CSR for adapting sparse computations to multico- e processors with NUMA memory hierarchies.\n\nTitle: Multi-pass mapping schemes for parallel sparse matrix computations\nContent: Consider the solution of a large sparse linear system Ax = b on multiprocessors. A parallel sparse matrix factorization is required in a direct solver. Alternatively, if Krylov subspace iterative methods are used, then incomplete forms of parallel sparse factorization are required for preconditioning. In such schemes, the underlying parallel computation is tree-structured, utilizing task-parallelism at lower levels of the tree and data-parallelism at higher levels. The proportional heuristic has typically been used to map the data and computation to processors. However, for sparse systems from finite-element methods on complex domains, the resulting assignments can exhibit significant load-imbalances. In this paper, we develop a multi-pass mapping scheme to reduce such load imbalances and we demonstrate its effectiveness for a test suite of large sparse matrices. Our scheme can also be used to generate improved mappings for tree-structured applications beyond those considered in this paper.\n\nTitle: Reducing Power with Performance Constraints for Parallel Sparse Applications\nContent: Sparse and irregular computations constitute a large fraction of applications in the data-intensive scientific domain. While every effort is made to balance the computational workload in such computations across parallel processors, achieving sustained near machine-peak performance with close-to-ideal load balanced computationto-processor mapping is inherently difficult. As a result, most of the time, the loads assigned to parallel processors can exhibit significant variations. While there have been numerous past efforts that study this imbalance from the performance viewpoint, to our knowledge, no prior study has considered exploiting the imbalance for reducing power consumption during execution. Power consumption in large-scale clusters of workstations is becoming a critical issue as noted by several recent research papers from both industry and academia. Focusing on sparse matrix computations in which underlying parallel computations and data dependencies can be represented by trees, this paper proposes schemes that save power through voltage/frequency scaling. Our goal is to reduce overall energy consumption by scaling the voltages/frequencies of those processors that are not in the critical path; i.e., our approach is oriented towards saving power without incurring performance penalties.\n\n## Task\nCompose an abstract for the title 'Time-memory trade-offs using sparse matrix methods for large-scale eigenvalue problems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Iterative methods such as Lanczos and Jacobi-Davidson are typically used to compute a small number of eigenvalues and eigenvectors of a sparse matrix. However, these methods are not effective in certain large-scale applications, for example, \"global tight binding molecular dynamics.\" Such applications require all the eigenvectors of a large sparse matrix; the eigenvectors can be computed a few at a time and discarded after a simple update step in the modeling process. We show that by using sparse matrix methods, a direct-iterative hybrid scheme can significantly reduce memory requirements while requiring less computational time than a banded direct scheme. Our method also allows a more scalable parallel formulation for eigenvector computation through spectrum slicing. We discuss our method and provide empirical results for a wide variety of sparse matrix test problems."
      }
    ]
  },
  {
    "tid":"74",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Fixation prediction through multimodal analysis.\nContent: In this article, we propose to predict human eye fixation through incorporating both audio and visual cues. Traditional visual attention models generally make the utmost of stimuli\u2019s visual features, yet they bypass all audio information. In the real world, however, we not only direct our gaze according to visual saliency, but also are attracted by salient audio cues. Psychological experiments show that audio has an influence on visual attention, and subjects tend to be attracted by the sound sources. Therefore, we propose fusing both audio and visual information to predict eye fixation. In our proposed framework, we first localize the moving--sound-generating objects through multimodal analysis and generate an audio attention map. Then, we calculate the spatial and temporal attention maps using the visual modality. Finally, the audio, spatial, and temporal attention maps are fused to generate the final audiovisual saliency map. The proposed method is applicable to scenes containing moving--sound-generating objects. We gather a set of video sequences and collect eye-tracking data under an audiovisual test condition. Experiment results show that we can achieve better eye fixation prediction performance when taking both audio and visual cues into consideration, especially in some typical scenes in which object motion and audio are highly correlated.\n\nTitle: Visual attention analysis and prediction on human faces.\nContent: Human faces are almost always the focus of visual attention because of the rich semantic information therein. While some visual attention models incorporating face cues indeed perform better in images with faces, yet there is no systematic analysis of the deployment of visual attention on human faces in the context of visual attention modelling, nor is there any specific attention model designed for face images. On faces, many high-level factors have influence on visual attention. To investigate visual attention on human faces, we first construct a Visual Attention database for Faces (VAF database), which is composed of 481 face images along with eye-tracking data of 22 viewers. Statistics of the eye-movement data show that some high-level factors such as face size, facial features and face pose have impact on visual attention. Thus we propose to build visual attention models specifically for face images through combining low-level saliency calculated by traditional saliency models with high-level facial features. Efficiency of the built models is verified on the VAF database. When combined with high-level facial features, most saliency models can achieve better performance.\n\nTitle: Quality Assessment of Free-Viewpoint Videos by Quantifying the Elastic Changes of Multi-Scale Motion Trajectories\nContent: Virtual viewpoints synthesis is an essential process for many immersive applications including Free-viewpoint TV (FTV). A widely used technique for viewpoints synthesis is Depth-Image-Based-Rendering (DIBR) technique. However, such technique may introduce challenging non-uniform spatial-temporal structure-related distortions. Most of the existing state-of-the-art quality metrics fail to handle these distortions, especially the temporal structure inconsistencies observed during the switch of different viewpoints. To tackle this problem, an elastic metric and multi-scale trajectory based video quality metric (EM-VQM) is proposed in this paper. Dense motion trajectory is first used as a proxy for selecting temporal sensitive regions, where local geometric distortions might significantly diminish the perceived quality. Afterwards, the amount of temporal structure inconsistencies and unsmooth viewpoints transitions are quantified by calculating 1) the amount of motion trajectory deformations with elastic metric and, 2) the spatial-temporal structural dissimilarity. According to the comprehensive experimental results on two FTV video datasets, the proposed metric outperforms the state-of-the-art metrics designed for free-viewpoint videos significantly and achieves a gain of 12.86% and 16.75% in terms of median Pearson linear correlation coefficient values on the two datasets compared to the best one, respectively.\n\nTitle: A new no-reference stereoscopic image quality assessment based on ocular dominance theory and degree of parallax\nContent: In recent years, while stereoscopic images were becoming widely applied and the corresponding technologies were substantially developed, very few stereoscopic image quality assessment metrics were proposed, especially under the condition that there is no reference image available. This paper proposes a new no-reference stereoscopic image quality assessment algorithm based on the ocular dominance theory and degree of parallax. All of our tests using the Toyama database draw two valuable conclusions. First, the performances of stereoscopic image quality assessment methods are significantly affected by difference of image qualities between the left and right images. Second, to offset the discriminations of different degrees of parallax caused by various 3D image contents is required indeed. Experiments and comparative studies are provided to confirm the effectiveness of our proposed new stereoscopic image quality metric.\n\nTitle: A Large-Scale Compressed 360-Degree Spherical Image Database: From Subjective Quality Evaluation to Objective Model Comparison\nContent: 360-degree images/videos have been dramatically increasing in recent years. But the high resolution makes it difficult to be transported, compressed and stored, and thus constrains the development of 360-degree images/videos. Therefore, it is important to study how popular coding technologies influence the quality of 360-degree images. In this paper, we present a study on subjective assessment of compressed 360-degree images and investigate whether existing objective image quality assessment (IQA) methods can effectively evaluate the quality of compressed 360-degree images. We first construct the largest compressed 360-degree image database (CVIQD2018) including 16 source images and 528 compressed ones with three prevailing coding technologies. Then, we implement 16 full reference (FR) IQA metrics, which include 10 traditional IQA metrics for 2D images and 3 PSNR-based metrics for 360-degree images, as well as 5 no reference (NR) IQA metrics and calculate the correlation between each above metric and subjective assessment in terms of three commonly used performance indices. The experiment results reveal structure information, visual saliency information and compensation for geometric distortion are crucial for evaluating the quality of compressed 360-degree images.\n\n## Task\nCompose an abstract for the title 'The prediction of head and eye movement for 360 degree images.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Estimating salient areas of visual stimuli which are liable to attract viewers\u2019 visual attention is a challenging task because of the high complexity of cognitive behaviors in the brain. Many researchers have been dedicated to this field and obtained many achievements. Some application areas, ranging from computer vision, computer graphics, to multimedia processing, can benefit from saliency detection, considering that the detected saliency has depicted the visual importance of different areas of the visual stimuli. As for the 360 degree visual stimuli, images and videos should record the whole scene in the 3D world, so the resolutions of panoramic images and videos are usually very high. However, when watching 360 degree stimuli, observers can only see part of the scene in the view port, which is presented to the eyes of the observers through the Head Mounted Display (HMD). So sending the whole video, or rendering the whole scene may result in the waste of resources. Thus if we can predict the current field of view, then focuses can be put to the streaming and rendering of the scene in the current field of view. Further more, if we can predict salient areas in the scene, then more fine processing can be done to the visually important areas. The prediction of salient regions for traditional images and videos have been extensively studied. However, conventional saliency prediction methods are not fully adequate for 360 degree contents, because 360 degree stimuli own some unique characteristics. Related study in this area is limited. In this paper, we study the problem of predicting head movement, head\u2013eye motion, and scanpath of viewers when they are watching 360 degree images in the commodity HMDs. Three types of data are specifically analyzed. The first is the head movement data, which can be regarded as the movement of the view port. The second is the head\u2013eye motion data which combines the motion of the head and the movement of the eye within the view port. The third is the scan-paths data of observers in the entire panorama which record the position information as well as the time information. And our model is designed to predict the saliency maps for the first two, and the scanpaths for the last one. Experimental results demonstrate the effectiveness of our model."
      }
    ]
  },
  {
    "tid":"75",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: An adaptive development framework for webbased enterprise information system\nContent: The rapid evolutions of software environment and business requirements place a high demand on the adaptiveness of enterprise information systems (EIS). Over the last few years, more and more EIS adopted the distributed multi-tiered web-based application architecture. Crosscutting concerns and clone codes make the web-based EIS difficult to evolve and maintain. The traditional Model-View-Controller (MVC) model imposes design-level restrictions that give a clean separation between the presentation, functional and control concerns. However it does not modularize the structure crosscutting concerns. In this paper, we propose a novel development model named Meta Data Object (MDO) that modularizes the structure concerns and reduces some specific clone codes. Panther is a new domain-driven web development framework that implements the MDO model. Panther has been used to develop many web-based EIS. Development with Panther benefits from a significant improvement in code reuse, adaptability, and maintainability. \u00a9 2008 IEEE.\n\nTitle: A Web Service Recommendation Approach Based on QoS Prediction Using Fuzzy Clustering\nContent: Web services, as loosely-coupled software systems, are increasingly being published to the web and there are a large number of services with similar functions. Therefore, service users compare the non-functional properties of services, e.g., Quality of Service (QoS), when they make service selection. This paper aims at generating a more comprehensive web service recommendation to users with a novel approach to fulfill more accurate prediction of unknown services' QoS values. We accomplish the QoS prediction by using fuzzy clustering method with calculating the users' similarity. Our approach improves the prediction accuracy and this is confirmed by comparing experiments with other methods. In addition, the quality of web services is considered as a multi-dimensional object, and each dimension is one aspect of the web service's non-functional properties. We also provide an application example to demonstrate how to utilize our approach to rank services by a score function and map multi-dimensional QoS properties into a single dimensional value.\n\nTitle: Service4All: a scalable PaaS platform for service-oriented software\nContent: Software developers with service-oriented technologies usually put a lot of efforts to deploy and manage supporting middleware and tools. Meanwhile PaaS in cloud computing aims at providing efficient support for software developers. In the light of this consideration, we have designed and implemented Service4All, a service cloud platform targeting at improving productivity of service-oriented software developers. In this paper, we describe the key technical issues, architecture design and implementation technology of SAE, a key component in Service4All, in terms of scalability. First, we propose a software appliance-based mechanism for elastic middleware management. Second, we describe a micro-kernel-based AppEngine core for efficient coordination of various components. Third, we present a scalable replication framework to achieve highly scalable application services. Finally, through a real application deployed on Service4All, we demonstrate the effectiveness of our solution.\n\nTitle: Building a TaaS Platform for Web Service Load Testing\nContent: Web services are widely known as the building blocks of typical service oriented applications. The performance of such an application system is mainly dependent on that of component web services. Thus the effective load testing of web services is of great importance to understand and improve the performance of a service oriented system. However, existing Web Service load testing tools ignore the real characteristics of the practical running environment of a web service, which leads to inaccurate test results. In this work, we present WS-TaaS, a load testing platform for web services, which enables load testing process to be as close as possible to the real running scenarios. In this way, we aim at providing testers with more accurate performance testing results than existing tools. WS-TaaS is developed on the basis of our existing Cloud PaaS platform: Service4All. First, we provide detailed analysis of the requirements of Web Service load testing and present the conceptual architecture and design of key components. Then we present the implementation details of WS-TaaS on the basis of Service4All. Finally, we perform a set of experiments based on the testing of real web services, and the experiments illustrate that WS-TaaS can efficiently facilitate the whole process of Web Service load testing.\n\nTitle: Personalized QoS-Aware Web Service Recommendation and Visualization\nContent: With the proliferation of web services, effective QoS-based approach to service recommendation is becoming more and more important. Although service recommendation has been studied in the recent literature, the performance of existing ones is not satisfactory, since (1) previous approaches fail to consider the QoS variance according to users' locations; and (2) previous recommender systems are all black boxes providing limited information on the performance of the service candidates. In this paper, we propose a novel collaborative filtering algorithm designed for large-scale web service recommendation. Different from previous work, our approach employs the characteristic of QoS and achieves considerable improvement on the recommendation accuracy. To help service users better understand the rationale of the recommendation and remove some of the mystery, we use a recommendation visualization technique to show how a recommendation is grouped with other choices. Comprehensive experiments are conducted using more than 1.5 million QoS records of real-world web service invocations. The experimental results show the efficiency and effectiveness of our approach.\n\n## Task\nCompose an abstract for the title 'A Novel Approach for Developing Web-Based Enterprise Information Systems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"All over the world software industry is facing the threatening failure rate of software projects. The scenario is worst in Pakistan having very immature software industry where projects are mostly over budgeted, over scheduled, and get low customer satisfaction. ..."
      }
    ]
  },
  {
    "tid":"76",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: SLAW: self-similar least-action human walk\nContent: Many empirical studies of human walks have reported that there exist fundamental statistical features commonly appearing in mobility traces taken in various mobility settings. These include: 1) heavy-tail flight and pause-time distributions; 2) heterogeneously bounded mobility areas of individuals; and 3) truncated power-law intercontact times. This paper reports two additional such features: a) The destinations of people (or we say waypoints) are dispersed in a self-similar manner; and b) people are more likely to choose a destination closer to its current waypoint. These features are known to be influential to the performance of human-assisted mobility networks. The main contribution of this paper is to present a mobility model called Self-similar Least-Action Walk (SLAW) that can produce synthetic mobility traces containing all the five statistical features in various mobility settings including user-created virtual ones for which no empirical information is available. Creating synthetic traces for virtual environments is important for the performance evaluation of mobile networks as network designers test their networks in many diverse network settings. A performance study of mobile routing protocols on top of synthetic traces created by SLAW shows that SLAW brings out the unique performance features of various routing protocols.\n\nTitle: On the Levy-Walk Nature of Human Mobility\nContent: We report that human walk patterns contain statistically similar features observed in Levy walks. These features include heavy-tail flight and pause-time distributions and the super-diffusive nature of mobility. Human walks are not random walks, but it is surprising that the patterns of human walks and Levy walks contain some statistical similarity. Our study is based on 226 daily GPS traces collected from 101 volunteers in five different outdoor sites. The heavy-tail flight distribution of human mobility induces the super-diffusivity of travel, but up to 30 min to 1 h due to the boundary effect of people's daily movement, which is caused by the tendency of people to move within a predefined (also confined) area of daily activities. These tendencies are not captured in common mobility models such as random way point (RWP). To evaluate the impact of these tendencies on the performance of mobile networks, we construct a simple truncated Levy walk mobility (TLW) model that emulates the statistical features observed in our analysis and under which we measure the performance of routing protocols in delay-tolerant networks (DTNs) and mobile ad hoc networks (MANETs). The results indicate the following. Higher diffusivity induces shorter intercontact times in DTN and shorter path durations with higher success probability in MANET. The diffusivity of TLW is in between those of RWP and Brownian motion (BM). Therefore, the routing performance under RWP as commonly used in mobile network studies and tends to be overestimated for DTNs and underestimated for MANETs compared to the performance under TLW.\n\nTitle: On the Critical Delays of Mobile Networks Under L\u00e9vy Walks and L\u00e9vy Flights\nContent: Delay-capacity tradeoffs for mobile networks have been analyzed through a number of research works. However, Le\u0301vy mobility known to closely capture human movement patterns has not been adopted in such work. Understanding the delay-capacity tradeoff for a network with Le\u0301vy mobility can provide important insights into understanding the performance of real mobile networks governed by human mobility. This paper analytically derives an important point in the delay-capacity tradeoff for Le\u0301vy mobility, known as the critical delay. The critical delay is the minimum delay required to achieve greater throughput than what conventional static networks can possibly achieve (i.e., O(1/\u221an) per node in a network with n nodes). The Le\u0301vy mobility includes Le\u0301vy flight and Le\u0301vy walk whose step-size distributions parametrized by \u03b1 \u2208 (0,2] are both heavy-tailed while their times taken for the same step size are different. Our proposed technique involves: 1) analyzing the joint spatio-temporal probability density function of a time-varying location of a node for Le\u0301vy flight, and 2) characterizing an embedded Markov process in Le\u0301vy walk, which is a semi-Markov process. The results indicate that in Le\u0301vy walk, there is a phase transition such that for \u03b1 \u2208 (0,1), the critical delay is always \u0398(n[1/2]), and for \u03b1 \u2208 [1,2] it is \u0398(n[(\u03b1)/2]). In contrast, Le\u0301vy flight has the critical delay \u0398(n[(\u03b1)/2]) for \u03b1 \u2208 (0,2].\n\nTitle: Routing performance analysis of human-driven delay tolerant networks using the truncated levy walk model\nContent: The routing performance of delay tolerant networks (DTN) is highly correlated with the distribution of inter-contact times (ICT), the time period between two successive contacts of the same two mobile nodes. As humans are often carriers of mobile communication devices, studying the patterns of human mobility is an essential tool to understand the performance of DTN protocols. From measurement studies of human contact behaviors, we find that their distributions closely resemble a form of power-law distributions called truncated Pareto. Human walk traces has a dichotomy distribution pattern of ICT; it has a power-law tendency up to some time, and decays exponentially after that time. Truncated Pareto distributions offer a simple yet cohesive mathematical model to express this dichotomy in the measured data. Using the residual and relaxation time theory [17] [4], we apply truncated Pareto distributions to quantify the performance of opportunistic routing in DTN. We further show that Truncated Levy walk (TLW) mobility model [22] commonly used in biology to describe the foraging patterns of animals [25], provide the same truncated power-law ICT distributions as observed from the empirical data, especially when mobility is confined within a finite area. This result confirms our recent finding that human walks contain similar statistical characteristics as Levy walks [22].\n\nTitle: Delay-capacity tradeoffs for mobile networks with L\u00e9vy walks and L\u00e9vy flights\nContent: This paper analytically derives the delay-capacity tradeoffs for Le\u0301vy mobility: Le\u0301vy walks and Le\u0301vy flights. Le\u0301vy mobility is a random walk with a power-law flight distribution. \u03b1 is the power-law slope of the distribution and 0 <;; \u03b1 \u2264 2. While in Le\u0301vy flight, each flight takes a constant flight time, in Le\u0301vy walk, it has a constant velocity which incurs strong spatio-temporal correlation as flight time depends on traveling distance. Le\u0301vy mobility is of special interest because it is known that Le\u0301vy mobility and human mobility share several common features including heavy-tail flight distributions. Humans highly influence the mobility of nodes (smartphones and cars) in real mobile networks as they carry or drive mobile nodes. Understanding the fundamental delay-capacity tradeoffs of Le\u0301vy mobility provides important insight into understanding the performance of real mobile networks. However, its power-law nature and strong spatio-temporal correlation make the scaling analysis non-trivial. This is in contrast to other random mobility models including Brownian motion, random waypoint and i.i.d. mobility which are amenable for a Markovian analysis. By exploiting the asymptotic characterization of the joint spatio-temporal probability density functions of Le\u0301vy models, the order of critical delay, the minimum delay required to achieve more throughput than \u0398(1/\u221an) where n is the number of nodes in the network, is obtained. The results indicate that in Le\u0301vy walk, there is a phase transition that for 0 <; \u03b1 <; 1, the critical delay is constantly \u0398(n1/2) and for 1 \u2264 \u03b1 \u2264 2, is \u0398(n\u03b1/2). In contrast, Le\u0301vy flight has critical delay \u0398(n\u03b1/2) for 0 <; \u03b1 \u2264 2.\n\n## Task\nCompose an abstract for the title 'SLAW: A New Mobility Model for Human Walks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Simulating human mobility is important in mobile networks because many mobile devices are either attached to or controlled by humans and it is very hard to deploy real mobile networks whose size is controllably scalable for performance evaluation. Lately various measurement studies of human walk traces have discovered several significant statistical patterns of human mobility. Namely these include truncated power-law distributions of flights, pause-times and inter-contact times, fractal way-points, and heterogeneously defined areas of individual mobility. Unfortunately, none of existing mobility models effectively captures all of these features. This paper presents a new mobility model called SLAW (self-similar least action walk) that can produce synthetic walk traces containing all these features. This is by far the first such model. Our performance study using using SLAW generated traces indicates that SLAW is effective in representing social contexts present among people sharing common interests or those in a single community such as university campus, companies and theme parks. The social contexts are typically common gathering places where most people visit during their daily lives such as student unions, dormitory, street malls and restaurants. SLAW expresses the mobility patterns involving these contexts by fractal way points and heavy-tail flights on top of the way points. We verify through simulation that SLAW brings out the unique performance features of various mobile network routing protocols."
      }
    ]
  },
  {
    "tid":"77",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Using Minimum Bounding Cube To Discover Valuable Salinity/Temperature Patterns From Ocean Science Data\nContent: A novel data mining techniques for finding interesting spatial-temporal patterns in ocean data is presented. The data consist of time series measurements of upper ocean science variables (e.g., salinity and temperature). Extracting interesting patterns from ocean variables is of important for understanding the relationship between ocean salinity/ temperature structures and climate variability. Association rules mining is applied in the search for these spatial-temporal patterns. Most traditional data mining models focus on mining association rules among attributes within one transaction. For example, if salinity rose, then temperature rises. However, people may be interested in discovering additional relations among transactions and take context such as time or location into consideration. An example of such a rule might be \"if the salinity in area A rose from 5 % to 7 %, then the temperature in area B will rise from 0 % to 2.5 % in the next month.\" In this case, the associated salinity/temperature variations among different locations and days are revealed. To overcome these issues, a multi-dimensional inter-transaction association rules mining framework was developed. Unlike other mining algorithms that suffer from a large number of inter-transaction items, the proposed Apriori-like method treats each event from the ocean science data as a transaction and applies the MBC (Minimum Bounding Cube) to form inter-transactions within the maxspan. Since there is no need to slide the maxspan window, the proposed method is easy to implement and it is computationally efficient.\n\nTitle: Data mining and fuzzy inference based salinity and temperature variation prediction\nContent: The ARGO project archives huge quantities of upper ocean salinity/temperature time series measurements that are related to climate issues such as global warming. Fuzzy inter-transaction association rules are derived from ARGO data using a reduced prefix-projected item set algorithm that has a small space and time complexity. After mining the frequent 1-itemsets the proposed algorithm exploits a reduced prefix projection strategy to extract the frequent inter-itemsets. Based on the extracted fuzzy inter-transaction association rules a fuzzy inference model is proposed for identifying salinity/temperature anomalies. Experimental results verify that the proposed model is effective in predicting the occurrence of abnormal salinity/temperature variations.\n\nTitle: Identifying Insightful Salinity And Temperature Variations In Ocean Data\nContent: Global ocean salinity and temperature variations are attracting increasing attention as scientists are trying to extract more knowledge from collected ocean data to better understand global change. Association rules mining can be applied to ocean salinity and temperature data to discover spatial-temporal patterns that reveal salinity and temperature variations. Since we are addressing the associations Of salinity/temperature events among different time and locations, the events can be grouped into clusters before mining starts, and the discovered association rule that has its antecedent and consequent from different clusters will be of most interest. However, are the discovered rules important or insightful? In this paper, an importance measurement for association rules with antecedent and consequent from different clusters is proposed. The importance measurement quantifies the rule's antecedent and consequent impact to their clusters, respectively. A rule is insightful if its importance measurement is above a predefined threshold. The insightful rules can then be presented to experts for further analysis.\n\nTitle: Temporal-spatial association analysis of ocean salinity and temperature variations\nContent: Ocean circulation plays an important role in global climate change. In an effort to monitor ocean circulation an infrastructure of more than 3,000 buoys have been deployed in the open water to measure ocean salinity and temperature variations. Some of these data are made freely available by Argo. The focus of this study is extracting previously unknown patterns of abnormal ocean salinity and temperature variations from Argo data that can be further applied to predict ocean current variations. First, Argo data are converted to market-basket type data that are used to find temporal-spatial association rules. The discovered rules reveal the associations of abnormal ocean salinity and temperature variations. Next, the discovered temporal and spatial variation patterns are used to predict future ocean salinity and temperature variations surrounding Taiwan. A 3-D visualization model is developed to present a) the interactions between events at different dates, concentric circles and ocean depths, and b) relationships between ocean temperature and salinity variations. The proposed 3-D visualization model help researchers determine whether the ocean temperature and salinity variations occurred in the same water mass and the relative importance of each attribute. Having established the informative relationships among different attributes an early warning system for climate changes can be established such that their impact on property and loss of life is reduced. The discovered association rules are compared with traditional association rules to illustrate their strength in analyzing global climate change.\n\nTitle: A prefix tree-based model for mining association rules from quantitative temporal data\nContent: There are two problems as we use conventional Boolean association rules mining algorithm to discover temporal association rules over the stock market to predict stock price variation. The first problem is that the discovered rules only consider associations between the presence and absence of variations of stock prices and the second problem is that the associations among stock price variations are within the same transaction day. For example, if stock A raises, then stock B raises the same day. This Boolean temporal association rule reveals no information of quantitative variations of stock prices and can only predict price trend in the same day. In this paper, we deal with the problem of mining temporal association rules in stock databases containing quantitative price variations to discover the associations among different transactions day. Our algorithm first employs data discretization concept to partition quantitative attributes into intervals and an adaptive a priori method that cooperates with time sliding window concept and prefix tree is developed to find quantitative temporal association rules. An example of such a rule might be \"if stock A price variation raised 5% to 7% and stock B raised 2.5% to 5% the same day, then stock C will raise 0% to 2.5% in the next two days.\" In this case, the stock price variation is taking into consideration and the associated stock price variations belong to different transaction days. As compared with conventional methods, more useful results can be found from the proposed quantitative temporal association rules.\n\n## Task\nCompose an abstract for the title 'Efficient mining of salinity and temperature association rules from ARGO data' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper presents an efficient technique for analyzing ARGO ocean data comprising time series of salinity/temperature measurements where informative salinity/temperature patterns are extracted. Most traditional mining techniques focus on finding associations among items within one transaction and are therefore unable to discover rich contextual patterns related to location and time. In order to show the associated salinity/temperature variations among different locations and time intervals, for example, ''if the salinity rose from 0.15psu to 0.25psu in the area that is in the east-northeast direction and is near Taiwan, then the temperature will rise from 0^oC to 1.2^oC in the area that is in the east-northeast direction and is far away from Taiwan next month'', a quantitative inter-transaction association rules mining algorithm is proposed. The FITI and the PrefixSpan algorithms are adopted to maximize the mining efficiency. The strategy is applied to ocean salinity measurements obtained from the waters surrounding Taiwan. These experimental evaluations show that the proposed algorithm achieves better performance than other inter-transaction association rule mining algorithms."
      }
    ]
  },
  {
    "tid":"78",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Cost-effective localization in underground mines using new SIMO/MIMO-like fingerprints and artificial neural networks\nContent: Safety measures have always been a main concern in the mining industry that, despite the modern practices, utilizes old-fashioned surveillance and monitoring systems. Our mission in underground mines stems from the profound need of geo-positioning systems that can accurately localize endangered miners and their heavy machinery in one of Earth's most harsh and rough environments. In underground mines, complex channels' responses to wireless transmitted signals challenge traditional localization techniques, yet they fail to defeat our innovative, cost-effective and accurate fingerprint-based positioning techniques that use artificial neural networks (ANNs) and exploit space-time diversity. Being among the pioneers in underground communications research, we bring forward a more sophisticated and accurate fingerprint-based positioning technique that exploits spatial transmission diversity in the presence of more than one transmitter Tx and/or receiver Rx antenna, such as in the case of single/multiple input multiple output (SIMO/MIMO) communication systems. More importantly, an advanced study is conducted to reduce the cost of fingerprint-acquisition trading off pinpoint accuracy for lower complexity and better ANNs' design. By challenging the localization system using less data measurements, we prove that ANNs, when properly designed, succeed to attain high positioning accuracies even when localizing in measurement gaps that were not seen in the training phase.\n\nTitle: Smart spatio-temporal fingerprinting for cooperative ANN-based wireless localization in underground narrow-vein mines\nContent: One of the main concerns in the mining industry is ensuring the safety and security of miners and their equipment. Being aware of the real-time position of personnel in such harsh environments within a special quasi-curvilinear topology is challenging and requires a sophisticated localization system. While traditional triangulation techniques fail to accurately localize in such indoor scenarios, new approaches that rely on fingerprints extracted from the Channel Impulse Response (CIR) succeed to localize with high accuracy using Artificial Neural Networks (ANNs) for fingerprint-location matching. Signatures collected from different locations in space, at different instances in time, are concatenated to form spatio-temporal fingerprints for improved localization accuracy. In this paper, we overview these novel and very promising localization techniques then investigate the impact of the spatial sampling grid's resolution in fingerprint collection on their accuracy in underground narrow-vein mines. We show by simulations that the significant accuracy gains reaped from the new exploitation of spatio-temporal diversity, if not needed in some applications, can be alternatively traded for remarkable and extremely useful cost reductions in the fingerprint collection step.\n\nTitle: Cooperative Geo-Location In Underground Mines: A Novel Fingerprint Positioning Technique Exploiting Spatio-Temporal Diversity\nContent: Underground narrow-vein mines result in complex indoor scenarios which require sophisticated localization techniques to maintain basic security measures. While some traditional localization systems use the triangulation techniques for outdoor channels, fingerprint positioning techniques are mostly used in more complex indoor environments like mines. One of the techniques exploited in the quasi-curvilinear topology of underground mines is the Channel Impulse Response (CIR) based fingerprint positioning combined with Artificial Neural Networks (ANNs). This article innovates a CIR-based positioning technique within a cooperative memory-assisted approach that exploits both the temporal (from different time instances) and spatial (from different space positions) diversities of the collected fingerprints. Introducing memory-type signatures in a cooperative localization technique within the spatial confinements of the tunnel-shaped narrow-vein mines significantly increases the accuracy, precision and robustness of the localization system. The cooperative memory-assisted technique is capable of localizing a transmitter with an accuracy of less than 25 cm 90% of the time.\n\nTitle: Radio-localization in underground narrow-vein mines using neural networks with in-built tracking and time diversity\nContent: In the mining industry, knowing the position of miners and/or equipments is an important safety measure that reduces risks and improves the security of that facility. Being an indoor environment, wireless transmitted signals in underground narrow-vein mines suffer multiple kinds of distortions due to extreme multipath and non-line of sight (NLOS) conditions. One of the proposed solutions to accurate localization in such challenging environments is based on extracting the channel impulse response (CIR) of the received signal and using the fingerprinting technique combined with cooperative artificial neural networks (ANNs). Such localization systems use the spatial domain where the reference localizing units are implemented at different positions away from the transmitter. In this article, we introduce a localization technique that uses fingerprints successively recorded in time with in-built tracking as an alternative method to localize. Unlike the spatial-domain technique where cooperative localizing units collect memoryless fingerprints from different locations, this technique uses one localizing unit and is capable of estimating the position of a transmitter precisely using its current and previous registered fingerprints in time. Localization using time-domain fingerprinting (i.e., tracking) and ANNs is introduced as a new method that exploits time diversity and improves the accuracy, precision and scalability of the positioning system.\n\nTitle: Indoor Geolocation with Received Signal Strength Fingerprinting Technique and Neural Networks\nContent: The location of people, mobile terminals and equipments is highly desirable for operational enhancements and safety reasons in indoor environments. In an in-building environment, the multipath caused by reflection and diffraction, and the obstruction and/or the blockage of the shortest path between transmitter and receiver are the main sources of range measurement errors. Due to the harsh indoor environment, unreliable measurements of location metrics such as RSS, AOA and TOA/TDOA result in the deterioration of the positioning performance. Hence, alternatives to the traditional parametric geolocation techniques have to be considered. In this paper, we present a method for mobile station location using narrowband channel measurement results applied to an artificial neural network (ANN). The proposed system learns off-line the location 'signatures' from the extracted location-dependent features of the measured data for LOS and NLOS situations. It then matches online the observation received from a mobile station against the learned set of 'signatures' to accurately locate its position. The location precision of the proposed system, applied in an in-building environment, has been found to be 0.5 meter for 90% of trained data and about 5 meters for 45% of untrained data.\n\n## Task\nCompose an abstract for the title 'Cooperative Localization in Mines Using Fingerprinting and Neural Networks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Localizing people in confined and underground areas is one of the topics under research in mining labs and industries. The position of personnel and equipments in areas such as mines is of high importance because it improves industrial safety and security. Due to the special nature of underground environments, signals transmitted in a mine gallery/tunnel suffer from severe multipath effects caused by reflection, refraction, diffraction and collision with humid rough surfaces. In such cases and in cases where the signals are blocked due to the non-line of sight (NLOS) regions, traditional localization techniques based on the RSS, AOA and TOA/TDOA lead to high position estimation errors. One of the proposed solutions to such challenging situations is based on extracting channel impulse response (CIR) fingerprints with reference to one wireless receiver and using an artificial neural network as a matching algorithm to localize. In this article we study this approach in a multiple access network where multiple access points are present. The diversity of the collected fingerprints will allow us to create artificial neural networks that will work separately or cooperatively using the same localization technique. The results will show that using cooperative artificial intelligence in the presence of multiple signatures from different reference points improves significantly the accuracy, precision, scalability and the overall performance of the localization system."
      }
    ]
  },
  {
    "tid":"79",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Sensorizer: an architecture for regenerating cyber physical data streams from the web\nContent: World Wide Web contains a huge amount of periodically-updated values originally sensed from the physical world; they include, for example, density of air pollutant, road traffic condition, and car park occupancy. In many cases, however, those data are not easily accessible from a computer program due to the lack of APIs to fetch them. In this paper, to cope with this problem, we propose an architecture for discovering, excavating, and streaming the entombed web contents (EWC). This architecture, called Sensorizer, leverages crowd sourcing for accurate EWC discovery, periodic web scraping with a headless browser for excavation from dynamic web pages, and a standardized communication protocol (XMPP) for data streaming to wide variety of applications.\n\n\n\nTitle: Spinning Sensors: A Middleware for Robotic Sensor Nodes with Spatiotemporal Models\nContent: This paper proposes Spinning Sensors middleware that realizes a robotic sensor node mechanism comprised of a sensor node and a robotic actuator node such as a motor or a mobile robot. We can increase sensing space, time, and accuracy of a sensor node by attaching them onto robotic actuators. To realize a robotic sensor node, we need to achieve collaborative utilization of arbitrary sensors and actuators, and automated calculation of sensing area and time. We stated these problems as spontaneous coordination problem and sensing area calculation problem. The Spinning Sensors middleware provides the mechanism of device coordination, data processing, and management of spatiotemporal model of robotic sensor nodes. In this paper, we discuss a robotic sensor node model, and design and implementation of the middleware. We introduce three kinds of applications using the middleware such as environment monitoring, sensor controlled robot, and context-aware service. The experiments using the robotic sensor node and the middleware are conducted to evaluate and measure the possibility, performance, and practicality of a robotic sensor node mechanism.\n\nTitle: SENSeTREAM: enhancing online live experience with sensor-federated video stream using animated two-dimensional code\nContent: We propose a novel technique that aggregates multiple sensor streams generated by totally different types of sensors into a visually enhanced video stream. This paper shows major features of SENSeTREAM and demonstrates enhancement of user experience in an online live music event. Since SENSeTREAM is a video stream with sensor values encoded in a two-dimensional graphical code, it can transmit multiple sensor data streams while maintaining their synchronization. A SENSeTREAM can be transmitted via existing live streaming services, and can be saved into existing video archive services. We have implemented a prototype SENSeTREAM generator and deployed it to an online live music event. Through the pilot study, we confirmed that SENSeTREAM works with popular streaming services, and provide a new media experience for live performances. We also indicate future direction for establishing visual stream aggregation and its applications.\n\nTitle: Help Me!: Valuing and Visualizing Participatory Sensing Tasks with Physical Sensors\nContent: Recent progress of mobile devices such as smartphones enables human to leverage their perception ability as a part of sensing framework. This sensing framework, so called participatory sensing, distributes various sensing tasks (e.g., whether report, waiting time in a queue, traffic conditions etc.) to possible participants. Then, participants can select and achieve the sensing tasks. However, in the coming future with the rapid growth of participatory sensing and increasing number of sensing tasks, it must be very hard for users to choose appropriate sensing tasks around them. To solve this problem, we propose a system called Help Me!, which can value and visualize importance of sensing tasks by quantifying them in cooperation with physical sensors. Since Help Me! system provides objective index for sensing tasks, it enhances opportunity for users to participate to sensing tasks. We designed and implemented Help Me! system as an integrated architecture of physical sensors and participatory sensors. Through initial in-lab experiment, we confirmed that Help Me! system can enhance opportunity to participate to sensing tasks for users.\n\nTitle: Spinning sensors: middleware for robotic sensor network\nContent: This paper proposes Spinning Sensors, a middleware system for creating robotic sensor network applications. This middleware enables application programmers to easily write application software which utilizes both sensors and actuators in their network. In the Spinning Sensors model, a sensor node is attached to a robotic actuator to change its position and/or direction. The continuous position change enables the single robotic sensor node to cover greater sensing area and sensing time. This paper describes robotic sensor node model that the sensor and the actuator are attached together as one node, then present our design and implementation of the software which achieved both versatility and functionality simultaneously. We constructed three applications using the middleware: an environment monitoring, a radio control robot, and context-aware services. We show the result of experiment in which we utilized multiple robotic sensor nodes to monitor the environment.\n\n## Task\nCompose an abstract for the title 'EverCopter: continuous and adaptive over-the-air sensing with detachable wired flying objects' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The paper proposes EverCopter, which provides continuous and adaptive over-the-air sensing with detachable wired flying objects. While a major advantage of sensing systems with battery-operated MAVs is a wide sensing coverage, sensing time is limited due to its limited amount of energy. We propose dynamically rechargeable flying objects, called EverCopter. EverCopter achieves both long sensing time and wide sensing coverage by the following two characteristics. First, multiple EverCopters can be tied in a row by power supply cables. Since the root EverCopter in a row is connected to DC power supply on the ground, each EverCopter can fly without battery. This makes their sensing time forever, unless the power supply on the ground fails. Second, the leaf EverCopter can detach itself from the row in order to enjoy wider sensing coverage. An EverCopter, while it is detached, runs with its own battery-supplied energy. When the remaining energy becomes low, it flies back to the row to recharge the battery."
      }
    ]
  },
  {
    "tid":"80",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: An adaptive DCT-based Mod-4 steganographic method\nContent: This paper presents a novel Mod-4 steganographic method in discrete cosine transform (DCT) domain. A group of 2 \u00d7 2 quantized DCT coefficients (GQC) is selected as the valid embedding area if more than two DCT coefficients are outside the interval of [-1,1]. The modulo 4 arithmetic operation is further applied to all the valid GQCs to embed a pair of binary bits using the shortest-route modification scheme. Each secret message is also encrypted to provide the system with more security. The proposed system has been extensively tested on a variety of images with different textures. Experimental results demonstrate that our system successfully preserves the quality of the images and stays undetected by the well-known steganalysis methods.\n\nTitle: A DCT-based Mod4 steganographic method\nContent: This paper presents a novel Mod4 steganographic method in discrete cosine transform (DCT) domain. Mod4 is a blind steganographic method. A group of 2x2 spatially adjacent quantized DCT coefficients (GQC) is selected as the valid message carrier. The modulus 4 arithmetic operation is then applied to the valid GQC to embed a pair of bits. When modification is required for data embedding, the shortest route modification scheme is applied to reduce distortion as compared to the ordinary direct modification scheme. Mod4 is capable in embedding information into both uncompressed and JPEG-compressed image. To compare Mod4 with other existing methods, carrier capacity, stego image quality, and results of blind steganalysis for 500 various images are shown. Visual comparison of three additional metrics is also presented to show the relative performance of Mod4 among other existing methods.\n\nTitle: Multiple messages embedding using DCT-Based mod4 steganographic method\nContent: This paper proposes an extension of DCT-based Mod4 steganographic method to realize multiple messages embedding (MME). To implement MME, we utilize the structural feature of Mod4 that uses vGQC (valid group of 2 \u00d72 adjacent quantized DCT coefficients) as message carrier. vGQC's can be partitioned into several disjoint sets by differentiating the parameters where each set could serve as an individual secret communication channel. A maximum number of 14 independent messages can be embedded into a cover image without interfering one message and another. We can generate stego images with image quality no worse than conventional Mod4. Results for blind steganalysis are also shown.\n\nTitle: An efficient sign prediction method for DCT coefficients and its application to reversible data embedding in scrambled JPEG image\nContent: In this paper, an efficient DCT sign prediction method is proposed. Unlike the conventional methods that depend on information from both spatial and frequency domains, the proposed method operates solely in the frequency domain by exploiting the pixel value patterns represented by the corresponding DCT basis vectors. In particular, each block is classified into five categories, namely, complex-pattern, complex-nonpattern, simple-smooth, simple-pattern and simple-texture, and each is treated differently using the proposed predictor. The proposed sign prediction method is then applied to realize reversible data embedding using sign information in a scrambled JPEG compressed image. This work is the first of its kind in using sign information for data embedding purposes. Basic performance of the proposed sign prediction and the proposed reversible data embedding method in scrambled JPEG image are verified using standard test images.\n\nTitle: Bitstream size suppression for DCT-based information hiding method\nContent: Although ScaScraIH [1] is able to scalably scramble image/video and offer scalable carrier capacity for reversible data embedding [2], the product of ScaScraIH suffers from bitstream size increment of ~ 9.17% on average. The main cause of bitstream size increment in ScaScraIH is storing the number of nonzero coefficients in each 8 \u00d7 8 block which is utilized during the descrambling process. This paper proposes two techniques in suppressing bitstream size increment for ScaScraIH. The first technique exploits the distribution of nonzero DCT coefficients in the image to construct a scanning order, aiming to shorten the distance between nonzero coefficients in a block. For the second technique, combination of predictive and entropy coding is used to encode the number of nonzero coefficients. Experiments are conducted by using standard test images to verify the effectiveness of both techniques in suppressing bitstream size increment. On average, bitstream size increment is significantly suppressed to ~ 0.91%, with some of the processed images assuming smaller bitstream size than its original counter part.\n\n## Task\nCompose an abstract for the title 'StegErmelc: A Novel DCT-Based Steganographic Method Using Three Strategies' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper proposes a DCT-based steganographic method named StegErmelc in the JPEG domain. Three strategies are proposed, namely (i) edge-like block selection, (ii) recursive matrix encoding, and (iii) largest coefficient serving, to form a novel steganographic method for achieving scalable carrier capacity, low detectability by universal blind steganalyzer, and high image quality, simultaneously. For a given message length, StegErmelc flexibly scales its carrier capacity to accommodate the message while trading off with stego detectability. At full capacity, StegErmelc has comparable carrier capacity relative to the existing methods. When embedding the same amount of information, StegErmelc remarkably reduces the stego detection rate to about 0.3\u20130.5 lower than that of the existing methods considered, and consequently StegErmelc can withstand blind steganalyzer when embedding up to 0.10 bpc. Under the same condition, StegErmelc produces stego image with quality higher than that of the existing methods considered. Graphical comparison with three additional evaluation metrics is also presented to show the relative performance of StegErmelc with respect to the existing methods considered."
      }
    ]
  },
  {
    "tid":"81",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Implementation of a motion sickness evaluation system based on EEG spectrum analysis\nContent: Motion sickness is a normal response to real, perceived, or even anticipated movement. People tend to get motion sickness on a moving boat, train, airplane, car, or amusement park rides. Motion sickness occurs when the body, the inner ear, and the eyes send conflicting signals to the brain. Sensory conflict theory that came about in the 1970's has become the most widely accepted theorem of motion-sickness among scientists [1]. The theory proposed that the conflict between the incoming sensory inputs could induce motion- sickness. However, some new research studies have appeared to tackle the issue of the vestibular function in central nervous system (CNS). In the previous human subject studies, researchers attempt to confirm the brain areas involved in the conflict in multi-modal sensory systems by means of clinical or anatomical methods. Our past studies had investigated the EEG activities correlated with motion sickness in a virtual-reality based driving simulator. We found that the parietal, motor, occipital brain regions exhibited significant EEG power changes in response to vestibular and visual stimuli. Based on these experimental results, we attempt to implement an EEG-based evaluation system to estimate subject's motion sickness level upon the major EEG power spectra from these motion sickness related brain area in this study. The evaluation system can be applied to early detect the subject's motion sickness level and prevent the uncomfortable syndromes occurred in advance in our daily life.\n\nTitle: Spatial and temporal EEG dynamics of motion sickness.\nContent: This study investigates motion-sickness-related brain responses using a VR-based driving simulator on a motion platform with six degrees of freedom, which provides both visual and vestibular stimulations to induce motion sickness in a manner that is close to that in daily life. Subjects' brain dynamics associated with motion sickness were measured using a 32-channel EEG system. Their degree of motion sickness was simultaneously and continuously reported using an onsite joystick, providing non-stop behavioral references to the recorded EEG changes. The acquired EEG signals were parsed by independent component analysis (ICA) into maximally independent processes. The decomposition enables the brain dynamics that are induced by the motion of the platform and motion sickness to be disassociated. Five MS-related brain processes with equivalent dipoles located in the left motor, the parietal, the right motor, the occipital and the occipital midline areas were consistently identified across all subjects. The parietal and motor components exhibited significant alpha power suppression in response to vestibular stimuli, while the occipital components exhibited MS-related power augmentation in mainly theta and delta bands; the occipital midline components exhibited a broadband power increase. Further, time series cross-correlation analysis was employed to evaluate relationships between the spectral changes associated with different brain processes and the degree of motion sickness. According to our results, it is suggested both visual and vestibular stimulations should be used to induce motion sickness in brain dynamic studies.\n\nTitle: Motion-Sickness Related Brain Areas and EEG Power Activates\nContent: This study investigates electroencephalographic (EEG) correlates of motion sickness in a virtual-reality based driving simulator. The driving simulator comprised an actual automobile mounted on a Stewart motion platform with six degrees of freedom, providing both visual and vestibular stimulations to induce motion-sickness in a manner that is close to that in daily life. EEG data were acquired at a sampling rate of 500 Hz using a 32-channel EEG system. The acquired EEG signals were analyzed using independent component analysis (ICA) and time-frequency analysis to assess EEG correlates of motion sickness. Subject's degree of motion-sickness was simultaneously and continuously reported using an onsite joystick, providing non-stop psychophysical references to the recorded EEG changes. Five Motion-sickness related brain processes with equivalent dipoles located in the left motor, the parietal, the right motor, the occipital and the occipital midline areas were consistently identified across all subjects. These components exhibited distinct spectral suppressions or augmentation in motion sickness. The results of this study could lead to a practical human-machine interface for noninvasive monitoring of motion sickness of drivers or passengers in real-world environments.\n\nTitle: Genetic feature selection in EEG-based motion sickness estimation\nContent: Motion sickness is a common symptom that occurs when the brain receives conflicting information about the sensation of movement. Many motion sickness biomarkers have been identified, and electroencephalogram (EEG)-based motion sickness level estimation was found feasible in our previous study. This study employs genetic feature selection to find a subset of EEG features that can further improve estimation performance over the correlation-based method reported in the previous studies. The features selected by genetic feature selection were very different from those obtained by correlation analysis. Results of this study demonstrate that genetic feature selection is a very effective method to optimize the estimation of motion-sickness level. This demonstration could lead to a practical system for noninvasive monitoring of the motion sickness of individuals in real-world environments.\n\nTitle: EEG activities of dynamic stimulation in VR driving motion simulator\nContent: The purpose of this study is to investigate Electroencephalography dynamics in response to kinesthetic stimuli during driving. We used a Virtual Reality driving simulator consisted of a hydraulic hexapod motion platform to create practical driving events. We compared the EEG dynamics in response to kinesthetic stimulus while the platform was in motion, to that while the platform was stationary. The scalp-recorded EEG channel signals were first separated into independent brain sources using Independent Component Analysis (ICA), and then studied with time-frequency analysis. Our results showed that independent brain processes near the somatomotor cortex exhibited alpha power decreases across sessions and subjects. Negative potentials phase-locked to the onsets of deviation events under motion conditions were observed in a central midline component. The results allow us to better understand different brain networks involved in driving, and provide a foundation for studying event-related EEG activities in the presence of kinesthetic stimuli.\n\n## Task\nCompose an abstract for the title 'Estimating the level of motion sickness based on EEG spectra' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Motion sickness (MS) is a normal response to real, perceived, or even anticipated movement. People tend to get motion sickness on a moving boat, train, airplane, car, or amusement park rides. Although many motion sicknessrelated biomarkers have been identified, but how to estimate human's motion sickness level (MSL) is a big challenge in the operational environment. Traditionally, questionnaire and physical check are the common ways to passively evaluate subject's sickness level. Our past studies had investigated the EEG activities correlated with motion sickness in a virtual-reality based driving simulator. The driving simulator comprised an actual automobile mounted on a Stewart motion platform with six degrees of freedom, providing both visual and vestibular stimulations to induce motion-sickness in a manner that is close to that in daily life. EEG data were acquired at a sampling rate of 500 Hz using a 32- channel EEG system. The acquired EEG signals were analyzed using independent component analysis (ICA) and time-frequency analysis to assess EEG correlates of motion sickness. Subject's degree of motion-sickness was simultaneously and continuously reported using an onsite joystick, providing non-stop psychophysical references to the recorded EEG changes. We found that the parietal, motor, occipital brain regions exhibited significant EEG power changes in response to vestibular and visual stimuli. Based on these findings and experimental results, this study aims to develop an EEG-based system to estimate subject's motion sickness level upon the EEG power spectra from motion-sickness related brain areas. The MS evaluation system can be applied to early detection of the subject's motion sickness and prevent its uncomfortable syndromes in our daily life. Furthermore, the experiment results could also lead to a practical human-machine interface for noninvasive monitoring of motion sickness of drivers or passengers in real-world environments."
      }
    ]
  },
  {
    "tid":"82",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Optimal time-resource allocation for activity-detection via multimodal sensing.\nContent: The optimal allocation of measurements for activity-level detection in a wireless body area network (WBAN) for health-monitoring applications is considered. The WBAN with heterogeneous sensors is deployed in a simple star topology with the fusion center receiving a fixed number of measurements from the sensors; the number of measurements allocated to each sensor is optimized to minimize the probability of detection error at the fusion center. An analysis of the two-sensor case with binary hypotheses is presented. Since the number of measurements is an integer, an exhaustive search (grid search) is traditionally employed to determine the optimal allocation of measurements. However, such a search is computationally expensive. To this end, an alternate continuous-valued vector optimization is derived which yields approximately optimal allocations which can be found with lower complexity. Numerical case studies based on experimental data for different key activity-states are presented. It is observed that the Kullback-Leibler (KL) distances between the distributions associated with the hypotheses dominate the optimal allocation of measurements.\n\nTitle: NFRA: Generalized Network Flow-Based Resource Allocation for Hosting Centers\nContent: Due to prohibitive cost of data center setup and maintenance, many small-scale businesses rely on hosting centers to provide the cloud infrastructure to run their workloads. Hosting centers host services of the clients on their behalf and guarantee quality of service as defined by service level agreements (SLAs.) To reduce energy consumption and to maximize profit it is critical to optimally allocate resources to meet client SLAs. Optimal allocation is a nontrivial task due to 1) resource heterogeneity where energy consumption of a client task varies depending on the allocated resources 2) lack of energy proportionality where energy cost for a task varies based on server utilization. In this paper, we introduce a generalized Network Flow-based Resource Allocation framework, called NFRA, for energy minimization and profit maximization. NFRA provides a unified framework to model profit maximization under a wide range of SLAs. We will demonstrate the simplicity of this unified framework by deriving optimal resource allocations for three different SLAs. We derive workload demands and server energy consumption data from SPECWeb2009 benchmark results to demonstrate the efficiency of NFRA framework.\n\nTitle: Markov-optimal sensing policy for user state estimation in mobile devices\nContent: Mobile device based human-centric sensing and user state recognition provide rich contextual information for various mobile applications and services. However, continuously capturing this contextual information consumes significant amount of energy and drains mobile device battery quickly. In this paper, we propose a computationally efficient algorithm to obtain the optimal sensor sampling policy under the assumption that the user state transition is Markovian. This Markov-optimal policy minimizes user state estimation error while satisfying a given energy consumption budget. We first compare the Markov-optimal policy with uniform periodic sensing for Markovian user state transitions and show that the improvements obtained depend upon the underlying state transition probabilities. We then apply the algorithm to two different sets of real experimental traces pertaining to user motion change and inter-user contacts and show that the Markov-optimal policy leads to an approximately 20% improvement over the naive uniform sensing policy.\n\nTitle: Exploiting Simulation Slack to Improve Parallel Simulation Speed\nContent: Parallel simulation is a technique to accelerate microarchitecture simulation of CMPs by exploiting the inherent parallelism of CMPs. In this paper, we explore the simulation paradigm of simulating each core of a target CMP in one thread and then spreading the threads across the hardware thread contexts of a host CMP. We start with cycle-by-cycle simulation and then relax the synchronization condition in various schemes, which we call slack simulations. In slack simulations, the Pthreads simulating different simulated cores do not synchronize after each simulated cycle, but rather they are given some slack. The slack is the difference in cycle between the simulated times of any two target cores. Small slacks, such as a few cycles, greatly improve the efficiency of parallel CMP simulations, with no or negligible simulation error. We have developed a simulation framework called SlackSim to experiment with various slack simulation schemes. Unlike previous attempts to parallelize multiprocessor simulations on distributed memory machines, SlackSim takes advantage of the efficient sharing of data in the host CMP architecture. We demonstrate the efficiency and accuracy of some well-known slack simulation schemes and of some new ones on SlackSim running on a state-of-the-art CMP platform.\n\nTitle: Scaling the Energy Proportionality Wall with KnightShift\nContent: Measuring energy proportionality accurately and understanding the reasons for disproportionality are critical first steps in designing future energy-efficient servers. This article presents two metrics\u2014linear deviation and proportionality gap\u2014that let system designers analyze and understand server energy consumption at various utilization levels. An analysis of published SPECpower results shows that energy proportionality improvements are not uniform across various server utilization levels. Even highly energy proportional servers suffer significantly at nonzero but low utilizations. To address the lack of energy proportionality at low utilization, the authors present KnightShift, a server-level heterogeneous server providing an active low-power mode. KnightShift is tightly coupled with a low-power compute node called Knight. Knight responds to low-utilization requests whereas the primary server responds only to high-utilization requests, enabling two energy-efficient operating regions. The authors evaluate KnightShift against a variety of real-world datacenter workloads using a combination of prototyping and simulation.\n\n## Task\nCompose an abstract for the title 'Optimal Allocation of Time-Resources for Multihypothesis Activity-Level Detection' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The optimal allocation of samples for activity-level detection in a wireless body area network for health-monitoring applications is considered. A wireless body area network with heterogeneous sensors is deployed in a simple star topology with the fusion center receiving biometric samples from each of the sensors. The number of samples collected from each of the sensors is optimized to minimize the probability of misclassification between multiple hypotheses at the fusion center. Using experimental data from our pilot study, we find equally allocating samples amongst sensors is normally suboptimal. A lower probability of error can be achieved by allocating a greater fraction of the samples to sensors which can better discriminate between certain activity-levels. As the number of samples is an integer, prior work employed an exhaustive search to determine the optimal allocation of integer samples. However, such a search is computationally expensive. To this end, an alternate continuous-valued vector optimization is derived which yields approximately optimal allocations which can be found with significantly lower complexity."
      }
    ]
  },
  {
    "tid":"83",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Graph Regularized Feature Selection with Data Reconstruction.\nContent: Feature selection is a challenging problem for high dimensional data processing, which arises in many real applications such as data mining, information retrieval, and pattern recognition. In this paper, we study the problem of unsupervised feature selection. The problem is challenging due to the lack of label information to guide feature selection. We formulate the problem of unsupervised feature selection from the viewpoint of graph regularized data reconstruction. The underlying idea is that the selected features not only preserve the local structure of the original data space via graph regularization, but also approximately reconstruct each data point via linear combination. Therefore, the graph regularized data reconstruction error becomes a natural criterion for measuring the quality of the selected features. By minimizing the reconstruction error, we are able to select the features that best preserve both the similarity and discriminant information in the original data. We then develop an efficient gradient algorithm to solve the corresponding optimization problem. We evaluate the performance of our proposed algorithm on text clustering. The extensive experiments demonstrate the effectiveness of our proposed approach.\n\nTitle: Graph-based local concept coordinate factorization.\nContent: Ubiquitous data are increasingly expanding in large volumes due to human activities, and grouping them into appropriate clusters is an important and yet challenging problem. Existing matrix factorization techniques have shown their significant power in solving this problem, e.g., nonnegative matrix factorization, concept factorization. Recently, one state-of-the-art method called locality-constrained concept factorization is put forward, but its locality constraint does not well reveal the intrinsic data structure since it only requires the concept to be as close to the original data points as possible. To address this issue, we present a graph-based local concept coordinate factorization (GLCF) method, which respects the intrinsic structure of the data through manifold kernel learning in the warped Reproducing Kernel Hilbert Space. Besides, a generalized update algorithm is developed to handle data matrices containing both positive and negative entries. Since GLCF is essentially based on the local coordinate coding and concept factorization, it inherits many advantageous properties, such as the locality and sparsity of the data representation. Moreover, it can better encode the locally geometrical structure via graph Laplacian in the manifold adaptive kernel. Therefore, a more compact and better structured representation can be obtained in the low-dimensional data space. Extensive experiments on several image and gene expression databases suggest the superiority of the proposed method in comparison with some alternatives.\n\nTitle: A-Optimal Projection for Image Representation\nContent: We consider the problem of image representation from the perspective of statistical design. Recent studies have shown that images are possibly sampled from a low dimensional manifold despite of the fact that the ambient space is usually very high dimensional. Learning low dimensional image representations is crucial for many image processing tasks such as recognition and retrieval. Most of the existing approaches for learning low dimensional representations, such as Principal Component Analysis (PCA) and Locality Preserving Projections (LPP), aim at discovering the geometrical or discriminant structures in the data. In this paper, we take a different perspective from statistical experimental design, and propose a novel dimensionality reduction algorithm called A-Optimal Projection (AOP). AOP is based on a linear regression model. Specifically, AOP finds the optimal basis functions so that the expected prediction error of the regression model can be minimized if the new representations are used for training the model. Experimental results suggest that the proposed approach provides a better representation and achieves higher accuracy in image retrieval.\n\nTitle: Expert Finding for Question Answering via Graph Regularized Matrix Completion\nContent: Expert finding for question answering is a challenging problem in community-based question answering (CQA) systems, arising in many real applications such as question routing and identification of best answers. In order to provide high-quality experts, many existing approaches learn the user model from their past question-answering activities in CQA systems. However, the past activities of users in most CQA systems are rather few, and thus the user model may not be well inferred in practice. In this paper, we consider the problem of expert finding from the viewpoint of missing value estimation. We then employ users' social networks for inferring user model, and thus improve the performance of expert finding in CQA systems. In addition, we develop a novel graph-regularized matrix completion algorithm for inferring the user model. We further develop two efficient iterative procedures, GRMC-EGM and GRMC-AGM, to solve the optimization problem. GRMC-EGM utilizes the Extended Gradient Method (EGM), while GRMC-AGM applies the Accelerated proximal Gradient search Method (AGM), for the optimization. We evaluate our methods on the well-known question answering system Quora, and the popular social network Twitter. Our empirical study shows the effectiveness of the proposed algorithms in comparison to the state-of-the-art expert finding algorithms.\n\nTitle: Fast Sparse Least-Squares Regression with Non-Asymptotic Guarantees\nContent: In this paper, we study a fast approximation method for {it large-scale high-dimensional} sparse least-squares regression problem by exploiting the Johnson-Lindenstrauss (JL) transforms, which embed a set of high-dimensional vectors into a low-dimensional space. In particular, we propose to apply the JL transforms to the data matrix and the target vector and then to solve a sparse least-squares problem on the compressed data with a {it slightly larger regularization parameter}. Theoretically, we establish the optimization error bound of the learned model for two different sparsity-inducing regularizers, i.e., the elastic net and the $ell_1$ norm. Compared with previous relevant work, our analysis is {it non-asymptotic and exhibits more insights} on the bound, the sample complexity and the regularization. As an illustration, we also provide an error bound of the {it Dantzig selector} under JL transforms.\n\n## Task\nCompose an abstract for the title 'Graph regularized sparse coding for image representation.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Sparse coding has received an increasing amount of interest in recent years. It is an unsupervised learning algorithm, which finds a basis set capturing high-level semantics in the data and learns sparse coordinates in terms of the basis set. Originally applied to modeling the human visual cortex, sparse coding has been shown useful for many applications. However, most of the existing approaches to sparse coding fail to consider the geometrical structure of the data space. In many real applications, the data is more likely to reside on a low-dimensional submanifold embedded in the high-dimensional ambient space. It has been shown that the geometrical information of the data is important for discrimination. In this paper, we propose a graph based algorithm, called graph regularized sparse coding, to learn the sparse representations that explicitly take into account the local manifold structure of the data. By using graph Laplacian as a smooth operator, the obtained sparse representations vary smoothly along the geodesics of the data manifold. The extensive experimental results on image classification and clustering have demonstrated the effectiveness of our proposed algorithm."
      }
    ]
  },
  {
    "tid":"84",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Generative Modelling and Classification of Students' E-Learning and E-Assessment Results\nContent: In this paper we propose an intelligent modelling of the students' knowledge collected from the e-Learning and e-Assessment processes of a particular course. The paper is focused on proposing a methodology for extracting the students' knowledge from the e-Learning activities, which we refer to as Profiling, then modifying it in compliance with their e-Assessment results and eventually, using it to model the probability distributions of the students Profiles that have passed and of those that have failed the course. The probability distributions of the students Profiles are then applied in the Bayes' theorem to perform binary classification analysis, i.e., to classify the students, pass or fail. The purpose of the proposed methodology is to simulate a real teacher, more precisely, to observe the activities of the particular student during the whole course in order to derive a decision of his or hers overall success.\n\nTitle: Intelligent modelling for predicting students' final grades\nContent: The main objective of this paper is producing an intelligent virtual teacher who will be able to predict the students' final grades at the end of the semester. Our approach is based on continual observation of the student's activities on the particular course during the semester. In order to achieve realistic modelling of the students' devotion to the given lectures and also the degree of how much the student has learned from the given lecture, we take into account both the e-Learning and the e-Assessment results through the semester. In our previous work we did an intelligent students' Profiling to classify the students into a pass, or, fail category. In this paper we go deeper into the problem, achieving more precise modelling according to which we will be able to determine the student's most likely final grade, using multi classification methodology. The advantage of our model is in its ability to take into account all the assessments during the semester, not relying only on the results from the last student's assessment. It can be a good indicator whether the teacher needs to perform additional testing of the student's knowledge in order to derive an overall conclusion on the most appropriate grade.\n\nTitle: User satisfaction evaluation of the electronic student services system \u2014 iKnow\nContent: In this paper, we report a survey that gathered the opinions on electronic student services by implementing the iKnow system. The methodology to evaluate user satisfaction was based on realization of questionnaires, and surveys with students, as well as interviews with the teaching and administration staff. We have realized several surveys and questionnaires, not just during the development cycle, but also during first two years of exploitation. The results show that most respondents have a positive attitude towards the iKnow software system. A very interesting fact is the one about swing of the attitude towards better recognition of the system, mostly due to the understanding of iKnow possibilities at all. Overall, our findings imply that iKnow has alleviated much of the basic and long-range problems that have challenged students in the past. Additionally, the study also reveals some shortcomings of iKnow, and lays ground for defining and building an even better user experience with the iKnow system.\n\nTitle: Technologies for Interactive Learning and Assessment Content Development\nContent: AbstractRecent technology trends evolved the student assessment from traditional ones \"pen-and-paper\" and \"face-to-face\" to modern e-Assessment system. These modern approaches allow the teachers to conduct and evaluate an exam with huge number of students in a short period of time. Even more important, both the teacher and the students achieve the evaluation results immediately after the assessment has finished. Although the e-Assessment system speeds up the evaluation, teachers face a huge challenge to prepare, organize and generate a huge set of questions. The questions must cover all learning objectives and their number should be as large as possible to prevent cheating by guessing or memorization of correct answers from previous exams. This paper presents several technologies that can efficiently realize strategies to develop a huge question database with minimal teacher efforts. It also describes the methodologies and strategies based on a specific technology. The technologies are categorized in two classes of e-Assessment systems that are used at the authors' faculty: the traditional e-Assessment system with usual multiple-choice answers and the newest e-Assessment system with interactive images. The question generation is based on defining the questions and answers as XML files for more advanced users and MS Word-based files for users with basic IT background. The question database can be used both for efficient and effective e-Assessment and e-Learning.\n\nTitle: Performance Evaluation of a New Approach for Automatic Question Production\nContent: Nowadays, e - testing is an often used method for evaluation in the process of learning. In this paper, we discuss the e -\n testing problem of creating large question set that will reflect the knowledge of some domain. A new model of E - testing\n is introduced with a proposal of a new solution to the problem of creation a large question set for a given domain. Then,\n we present a methodology for comparison of the results and the contribution of the new model and realization on the automated\n creation of large number of questions, and we evaluate the quality and the vulnerability of the question set, as well. It\n is shown that the new model increases the speed of question production by more 10 times.\n \n\n## Task\nCompose an abstract for the title 'Intelligent student profiling for predicting e-Assessment outcomes' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The main objective of this paper is introducing intelligence in the e-Learning and e-Assessment processes. Therefore, we present an existing adaptive e-Learning and e-Assessment strategies, verify them with machine learning (ML) algorithms, build students Profile and eventually, we present our new model that will be able to estimate the final result of the overall students' work during the semester, taking into account all the learning objectives that the students have passed. Thus, our idea is creating an intelligent agent that will simulate the behavior of a real professor as much as possible."
      }
    ]
  },
  {
    "tid":"85",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Two-Level Reorder Buffers: Accelerating Memory-Bound Applications on SMT Architectures\nContent: We propose a low complexity mechanism for accelerating memory-bound threads on SMT processors without adversely impacting the performance of other concurrently running applications. The main idea is to provide a two-level organization of the Reorder Buffer (ROB), where the first level is comprised of small private per-thread ROBs which are used in the normal course of execution in the absence of last level cache misses. The second ROB level is a much larger storage that can be used on demand by threads experiencing last level cache misses. The key feature of our scheme is that the allocation of the second-level ROB partition occurs to a thread experiencing a miss into the last level cache only if the number of instructions dependent on the missing load is below a predetermined threshold. We introduce a novel low-complexity mechanism to count the number of load-dependent instructions and propose two schemes for allocating second level ROB: predictive and reactive. Our results demonstrate about 30% improvement over DCRA resource distribution mechanism in terms of \"harmonic mean of weighted IPCs\" metric.\n\nTitle: Exploiting Operand Availability for Efficient Simultaneous Multithreading\nContent: We propose several schemes to improve the scalability, reduce the complexity and delays, and increase the throughput of dynamic scheduling in SMT processors. Our first design is an adaptation of the recently proposed instruction packing to SMT. Instruction packing opportunistically packs two instructions (possibly from different threads), each with at most one nonready source operand at the time of dispatch, into the same issue queue entry. Our second design, termed 2OP_BLOCK, takes these ideas one step further and completely avoids the dispatching of the instructions with two nonready source operands. This technique has several advantages. First, it reduces the scheduling complexity (and the associated delays) as the logic needed to support the instructions with two nonready source operands is eliminated. More surprisingly, 2OP_BLOCK simultaneously improves the performance as the same issue queue entry may be reallocated multiple times to the instructions with at most one nonready source (which usually spends fewer cycles in the queue) as opposed to hogging the entry with an instruction which enters the queue with two nonready sources. For schedulers with the capacity to hold 64 instructions on a 4-way SMT, the 2OP_BLOCK design outperforms the traditional queue by 14 percent, on average, and at the same time results in a 10 percent reduction in the overall scheduling delay. We also present mechanisms to support speculative scheduling with 2OP_BLOCK and introduce the hybrid scheme that dynamically switches between 2OP_BLOCK and instruction packing modes depending on the workload characteristics, to achieve further performance gains.\n\nTitle: An L2-miss-driven early register deallocation for SMT processors\nContent: The register file is one of the most critical datapath components limiting the number of threads that can be supported on a Simultaneous Multithreading (SMT) processor. To allow the use of smaller register files without degrading performance, techniques that maximize the efficiency of using registers through aggressive register allocation/deallocation can be considered. In this paper, we propose a novel technique to early deallocate physical registers allocated to threads that experience L2 cache misses. This is accomplished by speculatively committing the load-independent instructions and deallocating the registers corresponding to the previous mappings of their destinations, without waiting for the cache miss request to be serviced. The early deallocated registers are then made immediately available for allocation to instructions within the same thread as well as within other threads, thus improving the overall processor throughput. On the average across the simulated mixes of multiprogrammed SPEC 2000 workloads, our technique results in 33% improvement in throughput and 25% improvement in terms of harmonic mean of weighted IPCs over the baseline SMT with the state-of-the-art DCRA policy. This is achieved without creating checkpoints, maintaining per-register counters of pending consumers, performing tag re-broadcasts, register re-mappings and/or additional associative searches.\n\nTitle: SIFT: a low-overhead dynamic information flow tracking architecture for SMT processors\nContent: Dynamic Information Flow Tracking (DIFT) is a powerful technique that can protect unmodified binaries from a broad range of vulnerabilities such as buffer overflow and code injection attacks. Software DIFT implementations incur very high performance overhead, while comprehensive hardware implementations add substantial complexity to the microarchitecture, making it unlikely for chip manufacturers to adopt them. In this paper, we propose SIFT (SMT-based DIFT), where a separate thread performing taint propagation and policy checking is executed in a spare context of an SMT processor. However, the instructions for the checking thread are generated in hardware using self-contained off-the-critical path logic at the commit stage of the pipeline. We investigate several optimizations to the base design including: (1) Prefetching of the taint data from shadow memory when the corresponding data is accessed by the primary thread; (2) Optimizing the generation of the taint instructions to remove unneeded instructions. Together, these optimizations reduce the performance penalty of SIFT to 26% on SPEC CPU 2006 benchmarks--much lower than the overhead of previously proposed software-based DIFT schemes. To demonstrate the feasibility of SIFT, we design and synthesize a core with SIFT logic and show that the area overhead of SIFT is only 4.5% and that instruction generation can be performed in one additional cycle at commit time.\n\nTitle: Efficient instruction schedulers for SMT processors\nContent: We propose dynamic scheduler designs to improve the scheduler scalability and reduce its complexity in the SMT processors. Our first design is an adaptation of the recently proposed instruction packing to SMT Instruction packing opportunistically packs two instructions (possibly from different threads), each with at most one non-ready source operand at the time of dispatch, into the same issue queue entry. Our second design, termed 20P_BLOCK, takes these ideas one step further and completely avoids the dispatching of the instructions with two non-ready source operands. This technique has several advantages. First, it reduces the scheduling complexity (and the associated delays) as the logic needed to support the instructions with 2 non-ready source operands is eliminated More surprisingly, 20P_BLOCK simultaneously improves the performance as the same issue queue entry may be reallocated multiple times to the instructions with at most one non-ready source (which usually spend fewer cycles in the queue) as opposed to hogging the entry with an instruction which enters the queue with two non-ready sources. For the schedulers with the capacity to hold 64 instructions, the 20P_BLOCK design outperforms the traditional queue by 11%, on the average, and at the same time results in a 10% reduction in the overall scheduling delay.\n\n## Task\nCompose an abstract for the title 'Balancing ILP and TLP in SMT Architectures through Out-of-Order Instruction Dispatch' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Simultaneous Multi-threading (SMT) architectures open up new avenues for datapath optimizations due to the presence of thread-level parallelism (TLP). One recent proposal for exploiting such parallelism is the 2OP_BLOCK scheduler design, which completely avoids the dispatch of instructions with two non-ready source operands into the issue queue. This technique reduces the scheduler complexity and also provides performance benefits for workloads with sufficient TLP, as the issue queue is more efficiently utilized. In this paper we first revisit the 2OP_BLOCK scheduler and show that this design actually results in performance losses for workloads with a limited number of threads because the constraints imposed on the exploitable ILP within each thread outweigh its advantages. To balance the ILP and TLP in SMT processors supporting such schedulers, we propose out-of-order dispatch of instructions within each thread. This simple augmentation naturally allows the 2OP_BLOCK scheduler to perform well even when the number of threads is small. Furthermore, for environments with a larger number of threads, the out-of-order dispatch mechanism improves the performance of the original proposal by up to 15% on the average across simulated multithreaded mixes of SPEC 2000 benchmarks."
      }
    ]
  },
  {
    "tid":"86",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Achieving Target Equilibria in Network Routing Games without Knowing the Latency Functions\nContent: The analysis of network routing games typically assumes, right at the onset, precise and detailed information about the latency functions. Such information may, however, be unavailable or difficult to obtain. Moreover, one is often primarily interested in enforcing a desirable target flow as the equilibrium by suitably influencing player behavior in the routing game. We ask whether one can achieve target flows as equilibria without knowing the underlying latency functions. Our main result gives a crisp positive answer to this question. We show that, under fairly general settings, one can efficiently compute edge tolls that induce a given target multicommodity flow in a nonatomic routing game using a polynomial number of queries to an oracle that takes candidate tolls as input and returns the resulting equilibrium flow. This result is obtained via a novel application of the ellipsoid method, and applies to arbitrary multicommodity settings and non-linear latency functions. Our algorithm extends easily to many other settings, such as (i) when certain edges cannot be tolled or there is an upper bound on the total toll paid by a user, and (ii) general nonatomic congestion games. We obtain tighter bounds on the query complexity for series-parallel networks, and single-commodity routing games with linear latency functions, and complement these with a query-complexity lower bound applicable even to single-commodity routing games on parallel-link graphs with linear latency functions. We also explore the use of Stackelberg routing to achieve target equilibria and obtain strong positive results for series-parallel graphs. Our results build upon various new techniques that we develop pertaining to the computation of, and connections between, different notions of approximate equilibrium, properties of multicommodity flows and tolls in series-parallel graphs, and sensitivity of equilibrium flow with respect to tolls. Our results demonstrate that one can indeed circumvent the po- entially-onerous task of modeling latency functions, and yet obtain meaningful results for the underlying routing game.\n\nTitle: The effectiveness of stackelberg strategies and tolls for network congestion games\nContent: It is well known that in a network with arbitrary (convex) latency functions that are a function of edge traffic, the worst-case ratio, over all inputs, of the system delay caused due to selfish behavior versus the system delay of the optimal centralized solution may be unbounded even if the system consists of only two parallel links. This ratio is called the price of anarchy (PoA). In this article, we investigate ways by which one can reduce the performance degradation due to selfish behavior. We investigate two primary methods (a) Stackelberg routing strategies, where a central authority, for example, network manager, controls a fixed fraction of the flow, and can route this flow in any desired way so as to influence the flow of selfish users; and (b) network tolls, where tolls are imposed on the edges to modify the latencies of the edges, and thereby influence the induced Nash equilibrium. We obtain results demonstrating the effectiveness of both Stackelberg strategies and tolls in controlling the price of anarchy. For Stackelberg strategies, we obtain the first results for nonatomic routing in graphs more general than parallel-link graphs, and strengthen existing results for parallel-link graphs. (i) In series-parallel graphs, we show that Stackelberg routing reduces the PoA to a constant (depending on the fraction of flow controlled). (ii) For general graphs, we obtain latency-class specific bounds on the PoA with Stackelberg routing, which give a continuous trade-off between the fraction of flow controlled and the price of anarchy. (iii) In parallel-link graphs, we show that for any given class L of latency functions, Stackelberg routing reduces the PoA to at most \u03b1 + (1-\u03b1)&cdot;\u03c1(L), where \u03b1 is the fraction of flow controlled and \u03c1(L) is the PoA of class L (when \u03b1 = 0). For network tolls, motivated by the known strong results for nonatomic games, we consider the more general setting of atomic splittable routing games. We show that tolls inducing an optimal flow always exist, even for general asymmetric games with heterogeneous users, and can be computed efficiently by solving a convex program. This resolves a basic open question about the effectiveness of tolls for atomic splittable games. Furthermore, we give a complete characterization of flows that can be induced via tolls.\n\nTitle: Hardness Results for Signaling in Bayesian Zero-Sum and Network Routing Games.\nContent: We study the optimization problem faced by a perfectly informed principal in a Bayesian game, who reveals information to the players about the state of nature to obtain a desirable equilibrium. This signaling problem is the natural design question motivated by uncertainty in games and has attracted much recent attention. We present new hardness results for signaling problems in (a) Bayesian two-player zero-sum games, and (b) Bayesian network routing games. For Bayesian zero-sum games, when the principal seeks to maximize the equilibrium utility of a player, we show that it is NP-hard to obtain an additive FPTAS. Our hardness proof exploits duality and the equivalence of separation and optimization in a novel way. Further, we rule out an additive PTAS assuming planted clique hardness, which states that no polynomial time algorithm can recover a planted clique from an Erd\\\\H{o}s-R\\\\'enyi random graph. Complementing these, we obtain a PTAS for a structured class of zero-sum games (where obtaining an FPTAS is still NP-hard) when the payoff matrices obey a Lipschitz condition. Previous results ruled out an FPTAS assuming planted-clique hardness, and a PTAS only for implicit games with quasi-polynomial-size strategy sets. For Bayesian network routing games, wherein the principal seeks to minimize the average latency of the Nash flow, we show that it is NP-hard to obtain a (multiplicative) (4/3 - epsilon)-approximation, even for linear latency functions. This is the optimal inapproximability result for linear latencies, since we show that full revelation achieves a (4/3)-approximation for linear latencies.\n\nTitle: Approximation Algorithms for Data Placement Problems\nContent: We develop approximation algorithms for the problem of placing replicated data in arbitrary networks, where the nodes may both issue requests for data objects and have capacity for storing data objects so as to minimize the average data-access cost. We introduce the data placement problem to model this problem. We have a set of caches $\\mathcal{F}$, a set of clients $\\mathcal{D}$, and a set of data objects $\\mathcal{O}$. Each cache $i$ can store at most $u_i$ data objects. Each client $j\\in\\mathcal{D}$ has demand $d_j$ for a specific data object $o(j)\\in\\mathcal{O}$ and has to be assigned to a cache that stores that object. Storing an object $o$ in cache $i$ incurs a storage cost of $f_i^o$, and assigning client $j$ to cache $i$ incurs an access cost of $d_jc_{ij}$. The goal is to find a placement of the data objects to caches respecting the capacity constraints, and an assignment of clients to caches so as to minimize the total storage and client access costs. We present a 10-approximation algorithm for this problem. Our algorithm is based on rounding an optimal solution to a natural linear-programming relaxation of the problem. One of the main technical challenges encountered during rounding is to preserve the cache capacities while incurring only a constant-factor increase in the solution cost. We also introduce the connected data placement problem to capture settings where write-requests are also issued for data objects, so that one requires a mechanism to maintain consistency of data. We model this by requiring that all caches containing a given object be connected by a Steiner tree to a root for that object, which issues a multicast message upon a write to (any copy of) that object. The total cost now includes the cost of these Steiner trees. We devise a 14-approximation algorithm for this problem. We show that our algorithms can be adapted to handle two variants of the problem: (a) a $k$-median variant, where there is a specified bound on the number of caches that may contain a given object, and (b) a generalization where objects have lengths and the total length of the objects stored in any cache must not exceed its capacity.\n\nTitle: Primal\u2013Dual Algorithms for Connected Facility Location Problems\nContent: We consider the Connected Facility Location problem. We are given a graph $G = (V,E)$ with costs $\\{c_e\\}$ on the edges, a set of facilities $\\F \\subseteq V$, and a set of clients $\\D \\subseteq V$. Facility $i$ has a facility opening cost $f_i$ and client $j$ has $d_j$ units of demand. We are also given a parameter $M\\geq 1$. A solution opens some facilities, say $F$, assigns each client $j$ to an open facility $i(j)$, and connects the open facilities by a Steiner tree $T$. The total cost incurred is ${\\sum}_{i\\in F} f_i+ sum_{j\\in\\D} d_jc_{i(j)j}+M\\sum_{e\\in T}c_e$. We want a solution of minimum cost. A special case of this problem is when all opening costs are 0 and facilities may be opened anywhere, i.e., $\\F=V$. If we know a facility $v$ that is open, then the problem becomes a special case of the single-sink buy-at-bulk problem with two cable types, also known as the rent-or-buy problem. We give the first primal\u2013dual algorithms for these problems and achieve the best known approximation guarantees. We give an 8.55-approximation algorithm for the connected facility location problem and a 4.55-approximation algorithm for the rent-or-buy problem. Previously the best approximation factors for these problems were 10.66 and 9.001, respectively. Further, these results were not combinatorial\u2014they were obtained by solving an exponential size linear rogramming relaxation. Our algorithm integrates the primal\u2013dual approaches for the facility location problem and the Steiner tree problem. We also consider the connected $k$-median problem and give a constant-factor approximation by using our primal\u2013dual algorithm for connected facility location. We generalize our results to an edge capacitated variant of these problems and give a constant-factor approximation for these variants.\n\n## Task\nCompose an abstract for the title 'Network design for information networks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We define a new class of network design problems motivated by designing information networks. In our model, the cost of transporting flow for a set of users (or servicing them by a facility) depends on the amount of information requested by the set of users. We assume that the aggregation cost follows economies of scale, that is, the incremental cost of a new user is less if the set of users already served is larger. Naturally, information requested by some sets of users might aggregate better than that of others, so our cost is now a function of the actual set of users. not just their total demand.We provide constant-factor approximation algorithms to two important problems in this general model. In the Group Facility Location problem, each user needs information about a resource. and the cost is a linear function of the number of resources involved (instead of the number of clients served). The Dependent Maybecast Problem extends the Karger-Minkoff maybecast model to probabilities with limited correlation and also contains the 2-stage stochastic optimization problem as a special case. We also give an O(ln n)-approximation algorithm for the Single Sink Information Network Design problem.We show that the Stochastic Steiner Tree problem can be approximated by dependent maybecast, and using this we obtain an O(1)-approximation algorithm for the k-stage stochastic Steiner tree problem for any fixed k. This is the first approximation algorithm for multi-stage stochastic optimization. Our algorithm allows scenarios to have different inflation factors, and works for any distribution provided that we can sample the distribution."
      }
    ]
  },
  {
    "tid":"87",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Recursive hierarchical swapped networks: versatile interconnection architectures for highly parallel systems\nContent: The authors propose a new class of interconnection networks called recursive hierarchical swapped networks (RHSN) for general-purpose parallel processing. The node degrees of RHSNs can vary from a small number to as large as required, depending on recursive and hierarchical composition parameters and the nucleus graph chosen. The diameter of an RHSN can be asymptotically optimal within a small constant factor. They present efficient routing, semigroup computation, ascend/descend, matrix-matrix multiplication, and emulation algorithms, thus proving the versatility of RHSNs. In particular on suitably constructed RHSNs, matrix multiplication can be performed faster than the DNS algorithm on a hypercube. Furthermore, ascend/descend algorithms, semigroup computation, and parallel prefix computation can be done using algorithms with asymptotically fewer communication steps than on a hypercube\n\nTitle: Swapped networks: unifying the architectures and algorithms of a wide class of hierarchical parallel processors\nContent: In this paper, we propose a new class of interconnection networks, called swapped networks, for general-purpose parallel processing. Swapped networks not only generate a wide class of high-performance interconnection networks, but also generalize, and serve to unify, many proposed parallel architectures as well as their algorithms. We show that swapped networks can efficiently emulate hypercubes, high-dimensional meshes, or generalized hypercubes, while having node degrees significantly smaller than the emulated network in each case. We also show that some subclasses of swapped networks can achieve asymptotically optimal diameters. Swapped networks are highly modularized, make the use of fixed-degree building blocks possible for any practically realizable system, and lead to the construction of high-performance scalable networks with reasonable cost\n\nTitle: Macro-Star Networks: Efficient Low-Degree Alternatives to Star Graphs\nContent: We propose a new class of interconnection networks, called macro-star networks, which belong to the class of Cayley graphs and use the star graph as a basic building module. A macro-star network can have node degree that is considerably smaller than that of a star graph of the same size, and diameter that is sublogarithmic and asymptotically within a factor of 1.25 from a universal lower bound (given its node degree). We show that algorithms developed for star graphs can be emulated on suitably constructed macro-stars with asymptotically optimal slowdown. This enables us to obtain through emulation a variety of efficient algorithms for the macro-star network, thus proving its versatility. Basic communication tasks, such as the multinode broadcast and the total exchange, can be executed in macro-star networks in asymptotically optimal time under both the single-port and the all-port communication models. Moreover, no interconnection network with similar node degree can perform these communication tasks in time that is better by more than a constant factor than that required in a macro-star network. We show that macro-star networks can embed trees, meshes, hypercubes, as well as star, bubble-sort, and complete transposition graphs with constant dilation. We introduce several variants of the macro-star network that provide more flexibility in scaling up the number of nodes. We also discuss implementation issues and compare the new topology with the star graph and other popular topologies.\n\nTitle: The Index-Permutation Graph Model for Hierarchical Interconnection Networks\nContent: In this paper, we present the index-permutation (IP) graph model, and apply it to the systematic development of efficient hierarchical networks. We derive several classes of interconnection networks based on IP graphs to achieve desired properties; the results compare favorably with popular interconnection networks, as measured by topological (e.g., node degree and diameter) and algorithmic properties, and are particularly efficient in view of their sparse inter-module communication patterns. In particular, the diameters of suitably-constructed super-IP graphs, a subclass of IP graphs, are optimal within a factor of 1 +o(1), given their node degrees. The IP graph model can also be used as a common platform that unifies the architectures and algorithms for a vast variety of interconnection networks.\n\nTitle: Cyclic Networks: A Family of Versatile Fixed-Degree Interconnection Architectures\nContent: In this paper, we propose a new family of interconnection networks, called cyclic networks (CNs), in which an intercluster connection is defined on a set of nodes whose addresses are cyclic shifts of one another The node degrees of basic CNs are independent of system size, but cart vary from a small constant (e.g., 3) to as large as required, thus providing flexibility and effective tradeoff between cost and performance. The diameters of suitably constructed CNs can be asymptotically optimal within their lower bounds, given the degrees. We show that packet routing and ascend/descend algorithms can be performed in Theta(log(d) N) communication steps on some CNs with N nodes of degree Theta(d). Moreover CNs can also efficiently emulate homogeneous product networks (e.g., hypercubes and high dimensional meshes). As a consequence, we obtain a variety of efficient algorithms on such networks, thus proving the versatility of CNs.\n\n## Task\nCompose an abstract for the title 'Hierarchical swapped networks: efficient low-degree alternatives to hypercubes and generalized hypercubes' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a new class of interconnection networks called hierarchical swapped networks (HSNs). We show that some subclasses of HSNs can efficiently emulate hypercubes, or generalized hypercubes, while having node degrees significantly smaller than the emulated networks. In particular, a suitably constructed HSN can emulate a hypercube or generalized hypercube with constant slowdown under the single-dimension communication model and asymptotically optimal slowdown with respect to its node degree under the all-port communication model. As a consequence, we obtain a variety of efficient algorithms on HSNs through emulation, thus proving the versatility of HSN. Some subclasses of HSNs are also shown to have asymptotically optimal diameters with respect to their node degrees. HSNs appear to be attractive low-degree alternatives to hypercubes and generalized hypercubes for general-purpose parallel computers"
      }
    ]
  },
  {
    "tid":"88",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: UPL: Opportunistic Localization in Urban Districts\nContent: We propose an opportunistic ad hoc localization algorithm called Urban Pedestrians Localization (UPL), for estimating locations of mobile nodes in urban districts. The design principles of UPL are twofold. First, we assume that location landmarks are deployed sparsely due to deployment-cost constraints. Thus, most mobile nodes cannot expect to meet these location landmarks frequently. Each mobile node in UPL relies on location information received from its neighboring mobile nodes instead in order to estimate its area of presence in which the node is expected to exist. Although the area of presence of each mobile node becomes inexact as it moves, it can be used to reduce the areas of presence of the others. Second, we employ information about obstacles such as walls, and present an algorithm to calculate the movable areas of mobile nodes considering obstacles for predicting the area of presence of mobile nodes accurately under mobility. This also helps to reduce each node's area of presence. The experimental results have shown that UPL could be limited to $(0.7r)$ positioning error in average, where $(r)$ denotes the radio range by the above two ideas.\n\nTitle: A novel scheduling algorithm for densely-deployed wireless stations in urban areas\nContent: This paper presents a scheduling algorithm for a set of wireless stations such as road-side access points for vehicular networks and outdoor WiFi stations, which are deployed in wide urban areas and may compete with each other for limited wireless resources. Different from a number of conventional approaches most of which consider detailed information on individual stations and signal interference among them, we focus more on geography of the areas of interest, and provide a novel algorithm that pursues the best balance among (i) optimality of resource utilization, (ii) robustness to new station installation and traffic demand changes, and (iii) scalability to the population of stations and area size. We have confirmed the performance by experimental simulations with several scenarios, and the applicability of approach has been testified by a case study on a scheduling problem for roadside access points of vehicular networks in cooperation with a manufacturing corporation.\n\nTitle: Scalable and robust channel allocation for densely-deployed urban wireless stations\nContent: This paper presents a scalable and robust scheduling algorithm for a set of wireless stations. Recently, road-side access points for vehicular networks and outdoor WiFi stations are deployed in wide urban areas and compete with each other for limited wireless resources. We focus on pursuing the best balance among (i) optimality of resource utilization, (ii) robustness to new station installation and traffic demand, and (iii) scalability to the population of stations and area size. The algorithm is designed based on a vector space theory with correctness proof. Compared with our previous work that presents the basic algorithm and theory, we extend the algorithm to a multi-layer (i.e. hierarchical) version to handle more realistic situations. Additionally, we have conducted extensive simulations with a variety of scenarios. Finally, the applicability of approach has been testified by a case study on a scheduling problem for roadside access points of vehicular networks in cooperation with a manufacturing corporation.\n\nTitle: Context-supported local crowd mapping via collaborative sensing with mobile phones\nContent: Positioning and tracking of mobile devices have been a fundamental building block of people-centric mobile applications. As well as global (or absolute) positioning, recognizing one's current position with respect to surrounding crowd of people at a busy station or in an event place is increasingly needed for emerging services like mobile social navigation. In this paper, we propose a novel positioning system that provides a local map of surrounding people based on sensing data gathered from smartphones in the crowd, without relying on any infrastructure or exhaustive fingerprinting. To cope with large position errors due to sensor noise and other environmental factors, we introduce a heuristic error correction algorithm based on collective activity context of mobile phone users. Analyzing recent history of the sensing data, it detects ''groups'' of people who move together and then corrects deviation of estimated traces of individual users by harmonizing with the traces of other group members. Through a field experiment using Android smartphones, we have shown that our error correction mechanism successfully enhances positioning accuracy by 28% (from 4.16m to 3.01m). Furthermore, we have analyzed the performance of our method in detail through extensive simulations.\n\nTitle: An efficient localization algorithm focusing on stop-and-go behavior of mobile nodes\nContent: This paper presents a cooperative localization approach for mobile nodes using wireless and ranging devices. We consider scenarios where node mobility follows stop-and-go behavior; we can then utilize the different movement states of nodes as an input to our localization approach. In the proposed method, each node autonomously finds among its surrounding nodes the ones that do not seem to move, and treats them as static nodes. Only nodes that are deemed static are then used as reference points for position estimation. Furthermore, each node adjusts its localization frequency automatically according to its estimated velocity. Performance evaluation results based on a realistic sensor model and actual mobility traces show that our method could achieve sufficient accuracy and efficiency for an exhibition scenario where people need to be tracked.\n\n## Task\nCompose an abstract for the title 'Ad-Hoc Localization In Urban District' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, we present a range-free ad-hoc localization algorithm called UPL (Urban Pedestrians Localization), for positioning mobile nodes in urban district. The design principle of UPL, is two-fold. (1) We assume that location seeds are deployed sparsely due to deployment-cost constraints. Thus most mobile nodes cannot expect to meet these location seeds frequently. Therefore, each mobile node in UPL relies on location information received from its neighboring mobile nodes in order to estimate its area of presence. The area of presence of each mobile node becomes inexact as it moves, but it is helpful to reduce the areas of presence of the other mobile nodes. (2) To predict the area of presence of mobile nodes accurately under mobility, we employ information about obstacles such as walls, and present an algorithm to calculate the movable areas of mobile nodes considering obstacles. This also helps to reduce each node's area of presence. The experimental results have shown that by the above two ideas UPL could achieve 8m positioning error in average with 10m of radio range."
      }
    ]
  },
  {
    "tid":"89",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Filling the Gaps: Improving Wikipedia Stubs\nContent: The availability of only a limited number of contributors on Wikipedia cannot ensure consistent growth and improvement of the online encyclopedia. With information being scattered on the web, our goal is to automate the process of generation of content for Wikipedia. In this work, we propose a technique of improving stubs on Wikipedia that do not contain comprehensive information. A classifier learns features from the existing comprehensive articles on Wikipedia and recommends content that can be added to the stubs to improve the completeness of such stubs. We conduct experiments using several classifiers - Latent Dirichlet Allocation (LDA) based model, a deep learning based architecture (Deep belief network) and TFIDF based classifier. Our experiments reveal that the LDA based model outperforms the other models (~6% F-score). Our generation approach shows that this technique is capable of generating comprehensive articles. ROUGE-2 scores of the articles generated by our system outperform the articles generated using the baseline. Content generated by our system has been appended to several stubs and successfully retained in Wikipedia.\n\nTitle: WikiWrite: Generating Wikipedia Articles Automatically.\nContent: The growth of Wikipedia, limited by the availability of knowledgeable authors, cannot keep pace with the ever increasing requirements and demands of the readers. In this work, we propose Wiki-Write, a system capable of generating content for new Wikipedia articles automatically. First, our technique obtains feature representations of entities on Wikipedia. We adapt an existing work on document embeddings to obtain vector representations of words and paragraphs. Using the representations, we identify articles that are very similar to the new entity on Wikipedia. We train machine learning classifiers using content from the similar articles to assign web retrieved content on the new entity into relevant sections in the Wikipedia article. Second, we propose a novel abstractive summarization technique that uses a two-step integer-linear programming (ILP) model to synthesize the assigned content in each section and rewrite the content to produce a well-formed informative summary. Our experiments show that our technique is able to reconstruct existing articles in Wikipedia with high accuracies. We also create several articles using our approach in the English Wikipedia, most of which have been retained in the online encyclopedia.\n\nTitle: Playscript Classification and Automatic Wikipedia Play Articles Generation\nContent: In this work, we aim to create Wikipedia pages on plays automatically by extracting relevant information from various web sources. Our approach involves building an efficient classifier that can classify web documents as play scripts. From the set of correctly classified instances of play scripts, we extract relevant play-related information from the documents and use it to obtain additional information from various sources on the web. This information is aggregated and human-readable Wikipedia pages are created using a bot. The results of our experiments show that classifiers trained by combining our designed features along with \\\"bag-of-words\\\" (bow) features outperform classifiers trained using only bow features. Our approach further shows that good quality human-readable pages can be created using our bot. Such automatic page generation process can eventually ensure a more complete Wikipedia.\n\nTitle: TableSeer: automatic table metadata extraction and searching in digital libraries\nContent: Tables are ubiquitous in digital libraries. In scientific documents, tables are widely used to present experimental results or statistical data in a condensed fashion. However, current search engines do not support table search. The difficulty of automatic extracting tables from un-tagged documents, the lack of a universal table metadata specification, and the limitation of the existing ranking schemes make table search problem challenging. In this paper, we describe TableSeer, a search engine for tables. TableSeer crawls digital libraries, detects tables from documents, extracts tables metadata, indexes and ranks tables, and provides a user-friendly search interface. We propose an extensive set of medium-independent metadata for tables that scientists and other users can adopt for representing table information. In addition, we devise a novel page box-cutting method to improve the performance of the table detection. Given a query, TableSeer ranks the matched tables using an innovative ranking algorithm - TableRank. TableRank rates each \u20edquery, table\u2102 pair with a tailored vector space model and a specific term weighting scheme. Overall, TableSeer eliminates the burden of manually extract table data from digital libraries and enables users to automatically examine tables. We demonstrate the value of TableSeer with empirical studies on scientific documents.\n\nTitle: Improving Researcher Homepage Classification with Unlabeled Data\nContent: A classifier that determines if a webpage is relevant to a specified set of topics comprises a key component for focused crawling. Can a classifier that is tuned to perform well on training datasets continue to filter out irrelevant pages in the face of changing content on the Web? We investigate this question in the context of identifying researcher homepages. We show experimentally that classifiers trained on existing datasets of academic homepages underperform on \u201cnon-homepages\u201d present on current-day academic websites. As an alternative to obtaining labeled datasets to retrain classifiers for the new content, in this article we ask the following question: \u201cHow can we effectively use the unlabeled data readily available from academic websites to improve researcher homepage classification?\u201d We design novel URL-based features and use them in conjunction with content-based features for representing homepages. Within the co-training framework, these sets of features can be treated as complementary views enabling us to effectively use unlabeled data and obtain remarkable improvements in homepage identification on the current-day academic websites. We also propose a novel technique for \u201clearning a conforming pair of classifiers\u201d that mimics co-training. Our algorithm seeks to minimize a loss (objective) function quantifying the difference in predictions from the two views afforded by co-training. We argue that this loss formulation provides insights for understanding co-training and can be used even in the absence of a validation dataset. Our next set of findings pertains to the evaluation of other state-of-the-art techniques for classifying homepages. First, we apply feature selection (FS) and feature hashing (FH) techniques independently and in conjunction with co-training to academic homepages. FS is a well-known technique for removing redundant and unnecessary features from the data representation, whereas FH is a technique that uses hash functions for efficient encoding of features. We show that FS can be effectively combined with co-training to obtain further improvements in identifying homepages. However, using hashed feature representations, a performance degradation is observed possibly due to feature collisions. Finally, we evaluate other semisupervised algorithms for homepage classification. We show that although several algorithms are effective in using information from the unlabeled instances, co-training that explicitly harnesses the feature split in the underlying instances outperforms approaches that combine content and URL features into a single view.\n\n## Task\nCompose an abstract for the title 'Wikikreator: Improving Wikipedia Stubs Automatically' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Stubs on Wikipedia often lack comprehensive information. The huge cost of editing Wikipedia and the presence of only a limited number of active contributors curb the consistent growth of Wikipedia. In this work, we present WikiKreator, a system that is capable of generating content automatically to improve existing stubs on Wikipedia. The system has two components. First, a text classifier built using topic distribution vectors is used to assign content from the web to various sections on a Wikipedia article. Second, we propose a novel abstractive summarization technique based on an optimization framework that generates section-specific summaries for Wikipedia stubs. Experiments show that WikiKreator is capable of generating well-formed informative content. Further, automatically generated content from our system have been appended to Wikipedia stubs and the content has been retained successfully proving the effectiveness of our approach."
      }
    ]
  },
  {
    "tid":"90",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: An efficient page-level FTL to optimize address translation in flash memory\nContent: Flash-based solid state disks (SSDs) have been very popular in consumer and enterprise storage markets due to their high performance, low energy, shock resistance, and compact sizes. However, the increasing SSD capacity imposes great pressure on performing efficient logical to physical address translation in a page-level flash translation layer (FTL). Existing schemes usually employ a built-in RAM cache for storing mapping information, called the mapping cache, to speed up the address translation. Since only a fraction of the mapping table can be cached due to limited cache space, a large number of extra operations to flash memory are required for cache management and garbage collection, degrading the performance and lifetime of an SSD. In this paper, we first apply analytical models to investigate the key factors that incur extra operations. Then, we propose an efficient page-level FTL, named TPFTL, which employs two-level LRU lists to organize cached mapping entries to minimize the extra operations. Inspired by the models, we further design a workload-adaptive loading policy combined with an efficient replacement policy to increase the cache hit ratio and reduce the writebacks of replaced dirty entries. Finally, we evaluate TPFTL using extensive trace-driven simulations. Our evaluation results show that compared to the state-of-the-art FTLs, TPFTL reduces random writes caused by address translation by an average of 62% and improves the response time by up to 24%.\n\nTitle: ROP: Alleviating Refresh Overheads via Reviving the Memory System in Frozen Cycles\nContent: DRAM memory performs periodic refreshes to prevent data loss due to charge leakage, while memory refreshes cause performance degradation and energy consumption, referred to as refresh overheads. In this paper, we propose Refresh-Oriented Prefetching (ROP) to alleviate memory refresh overheads. Before a refresh starts, ROP prefetches cache lines from the tobe-refreshed rank into an added SRAM buffer. In doing so, when a rank is undergoing refresh, memory requests can still be serviced rather than being blocked. At the core of ROP is a probabilistic prefetch model determining which cache lines are prefetched for a refresh based on the access patterns appearing in an observational window ahead of the refresh. A Pattern Profiler collects statistics about memory traffic occurring before and after the starting time of each refresh operation in a period of training time and it outputs two conditional probabilities which are used to control subsequent prefetch decisions. A Prefetcher maintains a prediction table which helps to ascertain access patterns appearing around refresh operations. The prediction table is updated every time an access occurs to the to-be-nextrefreshed ran during the observational window and is consulted to decide which cache lines are prefetched. Extensive evaluation results with benchmarks from SPEC CPU2006 on a DDR4 memory have demonstrated that with ROP memory performance can be improved by up to 9.2% (3.3% on average) for singlecore simulations, while reducing the overall memory energy by up to 6.7% (3.6% on average), relative to an auto-refresh baseline memory. Moreover, it increases the Weighted Speedup by up to 2.22X (1.32X on average) for 4-core multiprogram simulations, while reducing energy by up to 48.8% (24.4% on average).\n\nTitle: An aggressive worn-out flash block management scheme to alleviate SSD performance degradation\nContent: Since NAND flash cannot be updated in place, SSDs must perform all writes in pre-erased pages. Consequently, pages containing superseded data must be invalidated and garbage collected. This garbage collection adds significant cost in terms of the extra writes necessary to relocate valid pages from erasure candidates to clean blocks, causing the well-known write amplification problem. SSDs reserve a certain amount of flash space which is invisible to users, called over-provisioning space, to alleviate the write amplification problem. However, NAND blocks can support only a limited number of program/erase cycles. As blocks are retired due to exceeding the limit, the reduced size of the over-provisioning pool leads to degraded SSD performance. In this work, we propose a novel system design that we call the Smart Retirement FTL (SR-FTL) to reuse the flash blocks which have been cycled to the maximum specified P/E endurance. We take advantage of the fact that the specified P/E limit guarantees data retention time of at least one year while most active data becomes stale in a period much shorter than one year, as observed in a variety of disk workloads. Our approach aggressively manages worn blocks to store data that requires only short retention time. In the meantime, the data reliability on worn blocks is carefully guaranteed. We evaluate the SR-FTL by both simulation on an SSD simulator and prototype implementation on an OpenSSD platform. Experimental results show that the SR-FTL successfully maintains consistent over-provisioning space levels as blocks wear and thus the degree of SSD performance degradation near end-of-life. In addition, we show that our scheme reduces block wear near end-of-life by as much as 84% in some scenarios.\n\nTitle: Delta-FTL: improving SSD lifetime via exploiting content locality\nContent: NAND flash-based SSDs suffer from limited lifetime due to the fact that NAND flash can only be programmed or erased for limited times. Among various approaches to address this problem, we propose to reduce the number of writes to the flash via exploiting the content locality between the write data and its corresponding old version in the flash. This content locality means, the new version, i.e., the content of a new write request, shares some extent of similarity with its old version. The information redundancy existing in the difference (delta) between the new and old data leads to a small compression ratio. The key idea of our approach, named Delta-FTL (Delta Flash Translation Layer), is to store this compressed delta in the SSD, instead of the original new data, in order to reduce the number of writes committed to the flash. This write reduction further extends the lifetime of SSDs due to less frequent garbage collection process, which is a significant write amplification factor in SSDs. Experimental results based on our Delta-FTL prototype show that Delta-FTL can significantly reduce the number of writes and garbage collection operations and thus improve SSD lifetime at a cost of trivial overhead on read latency performance.\n\nTitle: An adaptive write buffer management scheme for flash-based SSDs\nContent: Solid State Drives (SSD's) have shown promise to be a candidate to replace traditional hard disk drives. The benefits of SSD's over HDD's include better durability, higher performance, and lower power consumption, but due to certain physical characteristics of NAND flash, which comprise SSD's, there are some challenging areas of improvement and further research. We focus on the layout and management of the small amount of RAM that serves as a cache between the SSD and the system that uses it. Of the techniques that have previously been proposed to manage this cache, we identify several sources of inefficient cache space management due to the way pages are clustered in blocks and the limited replacement policy. We find that in many traces hot pages reside in otherwise cold blocks, and that the spatial locality of most clusters can be fully exploited in a limited time period, so we develop a hybrid page/block architecture along with an advanced replacement policy, called BPAC, or Block-Page Adaptive Cache, to exploit both temporal and spatial locality. Our technique involves adaptively partitioning the SSD on-disk cache to separately hold pages with high temporal locality in a page list and clusters of pages with low temporal but high spatial locality in a block list. In addition, we have developed a novel mechanism for flash-based SSD's to characterize the spatial locality of the disk I/O workload and an approach to dynamically identify the set of low spatial locality clusters. We run trace-driven simulations to verify our design and find that it outperforms other popular flash-aware cache schemes under different workloads. For instance, compared to a popular flash aware cache algorithm BPLRU, BPAC reduces the number of cache evictions by up to 79.6&percnt; and 34&percnt; on average.\n\n## Task\nCompose an abstract for the title 'Understanding and Alleviating the Impact of the Flash Address Translation on Solid State Devices.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Flash-based solid state devices (SSDs) have been widely employed in consumer and enterprise storage systems. However, the increasing SSD capacity imposes great pressure on performing efficient logical to physical address translation in a page-level flash translation layer (FTL). Existing schemes usually employ a built-in RAM to store mapping information, called mapping cache, to speed up the address translation. Since only a fraction of the mapping table can be cached due to limited cache space, a large number of extra flash accesses are required for cache management and garbage collection, degrading the performance and lifetime of an SSD. In this paper, we first apply analytical models to investigate the key factors that incur extra flash accesses during address translation. Then, we propose a novel page-level FTL with an efficient translation page-level caching mechanism, named TPFTL, to minimize the extra flash accesses. TPFTL employs a two-level least recently used (LRU) list with space-efficient optimizations to organize cached mapping entries. Inspired by the models, we further design a workload-adaptive loading policy combined with an efficient replacement policy to increase the cache hit rate and reduce the writebacks of replaced dirty entries. Finally, we evaluate TPFTL using extensive trace-driven simulations. Our evaluation results show that compared to the state-of-the-art FTLs, TPFTL significantly reduces the extra operations caused by address translation, achieving reductions on system response time and write amplification by up to 27.1% and 32.2%, respectively."
      }
    ]
  },
  {
    "tid":"91",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Bridging the gap: transcoding from single-layer H.264/AVC to scalable SVC video streams\nContent: Video scalability plays an increasingly important role in the disclosure of digital video content. Currently, the scalable extension of the H.264/AVC video coding standard (SVC) is being finalized, which provides scalability layers for state-of-the-art H.264/AVC video streams. Existing video content that is coded using single-layer H.264/AVC, however, cannot benefit from the newly developed scalability features. Here, we discuss our architecture for H.264/AVC-to-SVC transcoding, which is able to derive SNR scalability layers from existing H.264/AVC bitstreams. Results show that the rate-distortion performance of our architecture approaches the optimal decoder-encoder cascade within 1 to 2 dB. Timing results indicate that intelligent conversion techniques are required, and that transcoding can significantly reduce the required computation time.\n\nTitle: Architectures for fast transcoding of H.264/AVC to quality-scalable SVC streams\nContent: The scalable extension of H.264/AVC (SVC) was recently standardized, and offers scalability at a minor penalty in rate-distortion efficiency when compared to single-layer H.264/AVC coding. In SVC, a scaled version of the original video sequence can easily be extracted by dropping layers from the stream. However, most of the video content nowadays is still produced in a single-layer format. While decoding and reencoding is a possible solution to introduce scalability in the existing bitstreams, this is an approach which requires a tremendous amount of time and effort. In this paper, we show that transcoding can be used to intelligently derive scalable bitstreams from existing single-layer streams. We focus on SNR scalability, and introduce techniques that are able to create multiple quality layers in the bitstreams. We also discuss bitstream rewriting from SVC to H.264/AVC, and examine how our newly proposed architectures can benefit from the changes that were introduced for bitstream rewriting. Architectures with different rate distribution flexibility and computational complexity are discussed. Rate-distortion performance of transcoding is shown to be comparable to that of reencoding at a fraction of the time needed for the latter.\n\nTitle: H.264/AVC-to-SVC temporal video transcoder for video broadcasting in wireless networks\nContent: Over the last few decades multimedia usage has changed dramatically, with networks and terminals of diverse bandwidths and capabilities coexisting. To ensure a good quality of experience, this diverse environment requires adaptability of the video stream. If video streams were compressed using Scalable Video Coding (SVC) schemes, they would be able to adapt to these heterogeneous networks and a wide range of terminals. However, since the majority of multimedia contents are compressed using H.264/AVC, they cannot benefit from this scalability. This paper makes some proposals for improving a temporal H.264/AVC-to-SVC video transcoder so that it can convert an H.264/AVC bitstream without scalability to scalable bitstreams with temporal scalability. The idea behind the proposed transcoder is to accelerate interprediction, focusing on reducing the coding complexity of the mode decision and motion estimation tasks of the encoder stage by using information available after the H.264/AVC decoding stage. To accelerate the mode decision task, a decision tree for narrowing down the Macroblock (MB) types to be checked by the SVC encoder has been developed. On the other hand, to accelerate the motion estimation task, a reduced area is created dynamically for every MB and sub-MB. Moreover, time savings increase considerably when both proposals are adjusted to work together, as opposed to doing so in an isolated way. The results show that when our joint proposal is applied, the complexity is reduced by 98\u00a0% while maintaining coding efficiency.\n\nTitle: Fast mode decision algorithm for H.264/AVC-to-SVC transcoding with temporal scalability\nContent: Scalable Video Coding (SVC) uses a notion of layers within the encoded bitstream for providing temporal, spatial and quality scalability, separately or combined. By truncating layers the bitstream can be adapted to devices with different characteristics and to varying network constraints. Since the majority of the existing video content is encoded using H.264/AVC without scalability, they cannot benefit from these scalability tools, so a transcoding process should be applied to provide scalability to this existing encoded content. In this paper, an algorithm based on Machine Learning techniques for temporal scalability transcoding from H.264/AVC to SVC focusing on mode decision task is discussed. The results show that when our technique is applied, the complexity is reduced by 82% while maintaining coding efficiency.\n\nTitle: Video transcoding for mobile digital television\nContent: Mobile digital television is one of the new services introduced recently by telecommunications operators. Due to the possibilities of personalization and interaction provided, together with the increasing demand for this type of portable services, it will undoubtedly be a successful technology in the near future. Multimedia content is generally encoded by reducing the storage capacity necessary and bandwidth consumption in order to be transmitted. In order to adapt to the different characteristics of the networks and the varying capabilities of the devices, scalable video coding schemes have been proposed that provide temporal, spatial, and quality scalability, or a combination of these. Most of the existing video content is compressed using H.264/AVC, which is a single-layer codec, so these contents cannot benefit from the scalability tools due to the lack of intrinsic scalability provided in the bitstream at encoding time. This paper proposes a technique to convert from the single-layer H.264/AVC bitstream to a scalable bitstream with temporal scalability. Applying this approach, a reduction of 60% in coding complexity is achieved while maintaining the coding efficiency.\n\n## Task\nCompose an abstract for the title 'Scalable video transcoding for mobile communications' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Mobile multimedia contents have been introduced in the market and their demand is growing every day due to the increasing number of mobile devices and the possibility to watch them at any moment in any place. These multimedia contents are delivered over different networks that are visualized in mobile terminals with heterogeneous characteristics. To ensure a continuous high quality it is desirable that this multimedia content can be adapted on-the-fly to the transmission constraints and the characteristics of the mobile devices. In general, video contents are compressed to save storage capacity and to reduce the bandwidth required for its transmission. Therefore, if these compressed video streams were compressed using scalable video coding schemes, they would be able to adapt to those heterogeneous networks and a wide range of terminals. Since the majority of the multimedia contents are compressed using H.264/AVC, they cannot benefit from that scalability. This paper proposes a technique to convert an H.264/AVC bitstream without scalability to a scalable bitstream with temporal scalability as part of a scalable video transcoder for mobile communications. The results show that when our technique is applied, the complexity is reduced by 98 % while maintaining coding efficiency."
      }
    ]
  },
  {
    "tid":"92",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Correlation Based Random Subspace Ensembles For Predicting Number Of Axillary Lymph Node Metastases In Breast Dce-Mri Tumors\nContent: An important problem in quantitative medical image analysis is a large number of features (often highly correlated) to instance ratio. To handle this, we developed a feature selector and an ensemble classifier based on a modified version of random subspace method. We propose using a fusion of feature selection concepts: ranking based, correlation based and random subspaces, to develop a concordance correlation coefficient based random subspace method (CCC_RSM) feature selector. It forms random feature subsets with weakly correlated yet relevant features while the ensemble classification is achieved by training the base classifier with these feature subsets. Axillary lymph node (ALNs) metastases is one of the most important prognostic factors in breast cancer. We applied CCC_RSM for four binary class classifications based on the number of metastatic ALNs: (i) >= 1 vs 0 (ii) 1-3 vs 0 (iii) 1-3 vs >= 4; and (iv)>= 4 vs 0. We extracted textural kinetics from habitats of fifty eight dynamic contrast enhanced magnetic resonance imaging breast tumors. We used three classifiers to compare the accuracies achieved by CCC_RSM with random subspaces(RS), wrappers and correlation based feature selector (CFS). For each binary classification we achieved the best accuracy (>= 78%) using CCC_RSM.\n\nTitle: A Texture Feature Ranking Model for Predicting Survival Time of Brain Tumor Patients\nContent: Automated prediction of patient-specific disease progression can significantly contribute to clinical treatment. This paper presents a computer-assisted framework to tackle the survival time prediction problem. Inspired by the assumption that niche tumor regions may play a significant role in cancer diagnosis, we explore local visual variations from multiple MRI sequences. The research consists of three parts: 1) the extraction of multi-scale Local Binary Patterns (LBP) to describe the visual variations, 2) a supervised forward feature selection approach, called the Feature Ranking Model (FRM) which captures single feature predictive ability efficiently, and combines the top features to form a feature subset, 3) We cast the clinical survival time prediction task as a binary category classification problem. We tested the framework using a dataset of 32 cases collected from The Cancer Genome Atlas (TCGA). We obtained a 93.75% accuracy rate for the prediction of survival time.\n\nTitle: Automated Delineation of Lung Tumors from CT Images Using a Single Click Ensemble Segmentation Approach.\nContent: A single click ensemble segmentation (SCES) approach based on an existing \"Click&Grow\" algorithm is presented. The SCES approach requires only one operator selected seed point as compared with multiple operator inputs, which are typically needed. This facilitates processing large numbers of cases. Evaluation on a set of 129 CT lung tumor images using a similarity index (SI) was done. The average SI is above 93% using 20 different start seeds, showing stability. The average SI for 2 different readers was 79.53%. We then compared the SCES algorithm with the two readers, the level set algorithm and the skeleton graph cut algorithm obtaining an average SI of 78.29%, 77.72%, 63.77% and 63.76% respectively. We can conclude that the newly developed automatic lung lesion segmentation algorithm is stable, accurate and automated.\n\nTitle: Identifying metastatic breast tumors using textural kinetic features of a contrast based habitat in DCE-MRI\nContent: The ability to identify aggressive tumors from indolent tumors using quantitative analysis on dynamic contrast enhanced magnetic resonance imaging (DCE-MRI) would dramatically change the breast cancer treatment paradigm. With this prognostic information, patients with aggressive tumors that have the ability to spread to distant sites outside of the breast could be selected for more aggressive treatment and surveillance regimens. Conversely, patients with tumors that do not have the propensity to metastasize could be treated less aggressively, avoiding some of the morbidity associated with surgery, radiation and chemotherapy. We propose a computer aided detection framework to determine which breast cancers will metastasize to the loco-regional lymph nodes as well as which tumors will eventually go on to develop distant metastses using quantitative image analysis and radiomics. We defined a new contrast based tumor habitat and analyzed textural kinetic features from this habitat for classification purposes. The proposed tumor habitat, which we call combined-habitat, is derived from the intersection of two individival tumor sub-regions: one that exhibits rapid initial contrast uptake and the other that exhibits rapid delayed contrast wahout. Hence the combined-habitat represents the tumor sub-region within which the pixels undergo both rapid initial uptake and rapid delayed washout. We analyzed a dataset of twenty-seven representative two dimensional (2D) images from volumetric DCE-MRI of breast tumors, for classification of tumors with no lymph nodes from tumors with positive number of axillary lymph nodes. For this classification an accuracy of 88.9% was achieved. Twenty of the twenty-seven patients were analyzed for classification of distant metastatic tumors from indolent cancers (tumors with no lymph nodes), for which the accuracy was 84.3%.\n\nTitle: Imbalanced learning for clinical survival group prediction of brain tumor patients\nContent: Accurate computer-aided prediction of survival time for brain tumor patients requires a thorough understanding of clinical data, since it provides useful prior knowledge for learning models. However, to simplify the learning process, traditional settings often assume datasets with equally distributed classes, which clearly does not reflect a typical distribution. In this paper, we investigate the problem of mining knowledge from an imbalanced dataset (i.e., a skewed distribution) to predict survival time. In particular, we propose an algorithmic framework to predict survival groups of brain tumor patients using multi-modality MRI data. Both an imbalanced distribution and classifier design are jointly considered: 1) We used the Synthetic Minority Over-sampling Technique to compensate for the imbalanced distribution; 2) A predictive linear regression model was adopted to learn a pair of class-specific dictionaries, which were derived from reformulated balanced data. We tested the proposed framework using a dataset of 42 patients with Glioblastoma Multiforme (GBM) tumors whose scans were obtained from the cancer genome atlas (TCGA). Experimental results showed that the proposed method achieved 95.24% accuracy.\n\n## Task\nCompose an abstract for the title 'Predicting Outcomes of Nonsmall Cell Lung Cancer Using CT Image Features' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Nonsmall cell lung cancer is a prevalent disease. It is diagnosed and treated with the help of computed tomography (CT) scans. In this paper, we apply radiomics to select 3-D features from CT images of the lung toward providing prognostic information. Focusing on cases of the adenocarcinoma nonsmall cell lung cancer tumor subtype from a larger data set, we show that classifiers can be built to predict survival time. This is the first known result to make such predictions from CT scans of lung cancer. We compare classifiers and feature selection approaches. The best accuracy when predicting survival was 77.5% using a decision tree in a leave-one-out cross validation and was obtained after selecting five features per fold from 219."
      }
    ]
  },
  {
    "tid":"93",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Determination of the Repeatability of a Kuka Robot Using the Stochastic Ellipsoid Approach\nContent: In this paper, we display experimental results about the measurement of repeatability of an industrial Kuka robot. We first study the distributions of the angular positions and show that these distributions can be considered as Gaussian.We compute repeatability in different locations of the workspace using the experimental angular covariance matrix and the stochastic ellipsoid modeling.We measure repeatability and observe a high variability. We explain the phenomenon by drawing the distribution of the 30 sample repeatability index.We then compare the computed and measured repeatability and conclude that our modeling gives good results.We analyse the incidence of weight and workspace location on repeatability and bring additional results to the existing literature.\n\nTitle: Forward Kinematics And Geometric Control Of A Medical Robot - Application To Dental Implantation\nContent: Recently, robotics has found a new field of research in surgery in which it is used as an assistant of the surgeon in order to promote less traumatic surgery and minimal incision of soft tissue. In accordance with the requirements of dental surgeons, we offer a robotic system dedicated to dental implants. A dental implant is a mechanical device fixed into the patient's jaw. It is used to replace a single tooth or a set of missing teeth. Fitting the implant is a difficult operation that requires great accuracy. This work concerns the prototype of a medical robot. Forward and inverse kinematics as dynamics are considered in order to drive a control algorithm which is as accurate and safe as possible.\n\nTitle: Design of an XY-theta platform held by a planar manipulator with four revolute joints and evaluation of its precision performances\nContent: This paper is about the precision performance evaluation of an XY-theta platform, with a patented kinematic design. Indeed, this platform is held by a serial redundant robot arm actuated with four revolute joints. The platform offers a wide 300 x 300 mm workspace, whereas the whole mechanism is extremely compact. Any workpiece on the platform can be positioned under a vertical axis to be grasped or manufactured in a two-step approach: in a coarse positioning mode, the four revolute joints are controlled to position and orientate the workpiece with a position error of less than 10 mu m; in a fine positioning mode, two revolute joints are mechanically blocked while two others are controlled to reduce the final error to below 2 mu m. The mechanism design and the choice of the blocked and moving joints are optimized to enhance the positioning performances in the two-step positioning method. In this paper, the platform positioning repeatability and its spatial resolution are characterized with the help of a camera. The advantage of this method is that it avoids any mechanical contact and can be implemented easily. Then, these results are compared to our previous precision performance evaluation obtained with the stationary cube method. Finally, the positioning repeatability is estimated at 6.5 mu m in the coarse positioning mode and 1.4 mu m in the fine positioning mode. Between the coarse and fine mode, the repeatability is thus improved by the factor of four, as predicted by the theory.\n\nTitle: Indirect neural control for plant-wide systems: Application to the Tennessee Eastman Challenge Process\nContent: An autonomous indirect scheme is proposed for multivariable process control and is extended to unstable open-loop plant-wide processes. Our principal objective in this work is to prove the feasibility to control an industrial plant by a small size neural system without any a priori training. The control scheme is made of an adaptive instantaneous neural model, a Neural Controller based on fully connected \u201cReal-Time Recurrent Learning\u201d networks and an on-line parameters updating law. This control scheme is applied to the Tennessee Eastman Challenge Process. Performances such as set point stabilisation, mode switching and disturbances rejection are pointed out. Results are discussed according to the Down and Vogel control objectives.\n\nTitle: Approximated timed reachability graphs for the robust control of discrete event systems.\nContent: This paper is about control sequences design for Discrete Event Systems (DES) modeled with Time Petri nets (TPN) including a set of temporal specifications. Petri nets are known as efficient mathematical and graphical models that are widely used to describe distributed DES including choices, synchronizations and parallelisms. The domains of application include but are not restricted to manufacturing systems, computer science and transportation networks. Incorporating the time in the model is important to consider many control problems such as scheduling and planning. This paper solves some control issues in timed context and uncertain environments that include unexpected events modeled with uncontrollable transitions. To deal with such uncertainties, we propose first to build an Approximated Timed Reachability Graph that includes the time specifications and model all feasible timed trajectories at a given accuracy under earliest firing policy. Then, this graph is used to search optimal paths by using an approach based on Markov Decision Processes that encode the environment uncertainties. Such optimal paths lead to near-optimal solutions for the TPN. Several simulations illustrate the benefit of the proposed method from the performance and computational points of view.\n\n## Task\nCompose an abstract for the title 'Granular Space Structure on a Micrometric Scale for Industrial Robots' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We study the statistical relationship between angular position and target for industrial robots on a micrometric scale and this leads us to understand the angular position stochastic structure. The concept of granular angular space is introduced and transposed in the Cartesian space. Modeling is based on experimental work performed for a Kuka and a Samsung robot. The influence of workspace location, posture and angular granularity ratio on the Cartesian granular space are then analysed."
      }
    ]
  },
  {
    "tid":"94",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Pareto-based search methodology for multi-objective nurse scheduling.\nContent: In this paper, we propose a search technique for nurse scheduling, which deals with it as a multi-objective problem. For each nurse, we first randomly generate a set of legal shift patterns which satisfy all shift-related hard constraints. We then employ an adaptive heuristic to quickly find a solution with the least number of violations on the coverage-related hard constraint by assigning one of the available shift patterns of each nurse. Next, we apply a coverage repairing procedure to make the resulting solution feasible, by adding/removing any under-covered/over-covered shifts. Finally, to satisfy the soft constraints (or preferences), we present a simulated annealing based search method with the following two options: one with a weighted-sum evaluation function which encourages moves towards users\u2019 predefined preferences, and another one with a domination-based evaluation function which encourages moves towards a more diversified approximated Pareto set. Computational results demonstrate that the proposed technique is applicable to modern hospital environments.\n\nTitle: A shift sequence based approach for nurse scheduling and a new benchmark dataset\nContent: This paper investigates an adaptive constructive method for solving nurse rostering problems. The constraints considered in the problems are categorised into three classes: those that are sequence related, those that are nurse schedule related and those that are roster related. We propose a decomposition approach (to construct solutions) that consists of two stages: (1) to construct high quality sequences for nurses by only considering the sequence constraints, and (2) to iteratively construct schedules for nurses and the overall rosters, based on the sequences built and considering the schedule and roster constraints. In the second stage of the schedule construction, nurses are ordered and selected adaptively according to the quality of the schedules they were assigned to in the last iteration. Greedy local search is carried out during and after the roster construction, in order to improve the (partial) rosters built. We show that the local search heuristic during the roster construction can further improve the constructed solutions for the benchmark problems tested.In addition, we introduce new benchmark nurse rostering datasets which are based upon real world data. The data sets represent a variety of real world constraints. The publication of this problem data to the research community is aimed at closing the gap between theory and practice in nurse scheduling research. One of the main objectives is to encourage more research on these data sets.\n\nTitle: A scatter search methodology for the nurse rostering problem\nContent: The benefits of automating the nurse scheduling process in hospitals include reducing the planning workload and associated costs and being able to create higher quality and more flexible schedules. This has become more important recently in order to retain nurses and to attract more people into the profession. Better quality rosters also reduce fatigue and stress due to overwork and poor scheduling and help to maximise the use of leisure time by satisfying more requests. A more contented workforce will lead to higher productivity, increased quality of patient service and a better level of healthcare. This paper presents a scatter search approach for the problem of automatically creating nurse rosters. Scatter search is an evolutionary algorithm, which has been successfully applied across a number of problem domains. To adapt and apply scatter search to nurse rostering, it was necessary to develop novel implementations of some of scatter search's subroutines. The algorithm was then tested on publicly available real-world benchmark instances and compared against previously published approaches. The results show the proposed algorithm is a robust and effective method on a wide variety of real-world instances. Journal of the Operational Research Society (2010) 61, 1667-1679. doi: 10.1057/jors.2009.118 Published online 4 November 2009\n\nTitle: A Constraint-Directed Local Search Approach To Nurse Rostering Problems\nContent: In this paper, we investigate the hybridization of constraint programming and local search techniques within a large neighbourhood search scheme for solving highly constrained nurse rostering problems. As identified by the research, a crucial part of the large neighbourhood search is the selection of the fragment (neighbourhood, i. e. the set of variables), to be relaxed and re- optimized iteratively. The success of the large neighbourhood search depends on the adequacy of this identified neighbourhood with regard to the problematic part of the solution assignment and the choice of the neighbourhood size. We investigate three strategies to choose the fragment of different sizes within the large neighbourhood search scheme. The first two strategies are tailored concerning the problem properties. The third strategy is more general, using the information of the cost from the soft constraint violations and their propagation as the indicator to choose the variables added into the fragment. The three strategies are analyzed and compared upon a benchmark nurse rostering problem. Promising results demonstrate the possibility of future work in the hybrid approach.\n\nTitle: Hierarchical Method For Nurse Rostering Based On Granular Pre-Processing Of Constraints\nContent: Nurse Rostering problems represent a subclass of scheduling problems that are hard to solve. Their complexity is due to the large solution spaces and the many objectives and constraints that need to be fulfilled. In this study, we propose a hierarchical method of granulation of problem domain through preprocessing of constraints. A set of zero-cost patterns in the granulated search space provides a basis for the generation of work schedules. Feasible schedules calculated for week 1 are used to define zero-cost shift patterns that can be deployed in week 2. These in turn are used for the generation of feasible schedules for week 2. The process can be applied over the extended time frame. We show that the granulation of the problem description in terms of constraints and scheduling time frames leads to a more manageable computing task.\n\n## Task\nCompose an abstract for the title 'A Study Of Cost Effective Scheduling Of Nurses Based On The Domain Transformation Method' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper discusses and analyses the tradeoff between the flexibility afforded with greater number of staff and the implied cost of employing extra staff in the context of the nurse-scheduling problem. If the number of staff is constant, our study allows quantification of the degree of pressure put on the staff resulting from the schedules that do not satisfy their preferences for shift allocation. We present a practical approach, based on our domain transformation methodology that achieves good quality schedules without high computational requirements."
      }
    ]
  },
  {
    "tid":"95",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Extended direct mode for hierarchical B picture coding\nContent: In 3D subband coding, motion-compensated temporal filtering (MCTF) technique, especially with the adaptive 5/3 wavelet kernel, efficiently exploits the temporal correlation among neighboring pictures. It can be achieved alternatively by the hierarchical B picture coding technique with the same decomposition as the temporal 5/3 lifting structure but without update step. This technique has been supported by H.264/AVC (2003) with stored B technique. The performance of hierarchical B obviously outperforms that of the conventional IBBP...GOP structure. Direct mode which derives motion vectors from the co-located block of backward reference and does not require any bits for coding motion vectors is a very efficient bi-prediction technique for B pictures coding. Considering that the forward motion vectors in the backward reference is not available when it belongs to block only with backward motion vector, in this paper, we propose an extended direct mode for hierarchical B picture coding to further improve the accuracy of derived motion vectors. It effectively utilizes the forward and backward motion vectors of the co-located blocks in references for scaling.\n\nTitle: Enhanced direct mode coding for bi-predictive pictures\nContent: The direct mode coding in the bi-predictive pictures (B-pictures) can efficiently improve the performance of bi-predictive coding, because it exploits the temporal correlation by bi-directional prediction from both forward and backward reference pictures, and meanwhile it does not require any bits for coding the motion vectors. Accordingly, how to accurately calculate the motion vectors in terms of direct mode coding is very important to obtain better prediction values. In other words, it is more desirable to obtain the true motion vectors, because the derived motion vectors need not be coded. The traditional direct mode coding usually derives the motion vector of the current block by scaling the motion vector of the co-located block in the subsequent reference pictures. It is based on the assumption that the near blocks would have the same motion trajectory. In this paper, we present an improved method based on combining motion vector tracking technique and spatial motion vector prediction technique. The proposed method can accurately calculate the motion vectors for the direct mode coding.\n\nTitle: Rate Control for Hierarchical B-picture Coding with Scaling-factors\nContent: The coding performance can be further improved when the hierarchical B-picture coding is introduced into H.264/AVC. However, the existing rate control schemes can not work efficiently in such new coding framework. This paper proposes a novel rate control algorithm when hierarchical B-picture coding is used in H.264/AVC. Firstly, a set of scaling-factors applied in designing cascaded quantizer for the B frames at different temporal levels is introduced. Based on the designed scaling-factors, an efficient bit-allocation strategy for hierarchical B-picture coding is presented. The experiments show that the proposed rate control algorithm can further improve PSNR up to 0.7dB compared to the existing hierarchical B-picture coding in H.264/AVC, while the mismatch of target bit rate and real bit rate does not exceed 2%.\n\nTitle: An Efficient Coding Method For Intra Prediction Mode Information\nContent: In H.264/AVC, the bits used for the Intra_4x4 prediction mode information usually occupy a high percentage in intra coding. Towards this issue, we present an efficient coding method for the Intra_4x4 prediction mode information. Firstly, a 3-order Markov random field is introduced to model the correlation among neighboring 4x4 blocks at picture level. Secondly, based on the conditional probabilities learned in this model, we build up a context adaptive coding scheme to code the Intra_4x4 prediction mode information. Although the probabilities and the coding scheme are initialized off-line, they can be revised by automatic adjustments. Thus the proposed algorithm is robust to a variety of video sequences. Experimental results demonstrate that the proposed method can obtain a gain up to 0.3 dB in all I-frames coding without involving any serious computational burden.\n\nTitle: Block-Wise Adaptive Motion Accuracy Based B-Picture Coding With Low-Complexity Motion Compensation\nContent: This paper presents a novel B-picture coding based on block-wise adaptive motion accuracy (BAMA) with low-complexity motion compensation (MC). BAMA is able to adaptively select the motion accuracy for each inter-block in a B-picture depending on whether it is of bidirectional prediction, in which compared to the unidirectionally predicted block, lower motion accuracy is applied to the bidirectionally predicted block to reduce MC complexity. To further reduce MC complexity for the bidirectionally predicted luminance block with subpixel motion vectors in both directions, the forward and backward MC interpolations are merged according to the linear operation property of MC interpolation. In addition, a simplified MC interpolation method is also introduced for chrominance components for the bidirectionally predicted block. When integrating the proposed methods into H.264/AVC reference software, the experimental results demonstrate that they can significantly reduce MC complexity for a B-picture while yielding the comparable rate distortion performance in comparison with H.264/AVC although they no longer generate H.264/AVC compliant bitstream.\n\n## Task\nCompose an abstract for the title 'New scaling technique for direct mode coding in B pictures' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"To leave the maximum flexibility in encoder to optimize the trade-off between coding performance and complexity, in video coding standards such as H.264 AVC, H.263 and MPEG-4 etc, any number of B pictures and any arrangement of P pictures within a group of pictures (GOP) of arbitrary length are permitted. In addition, multiple reference picture prediction is also permitted in some video coding systems such as H.264/AVC to achieve efficient coding by allowing the encoder to select reference pictures among a large number of coded pictures. Both of the above cases without fixing the temporal distance between forward and backward reference pictures require the division operation for deriving the motion vectors of direct mode, which can efficiently exploit the temporal correlation among pictures and does not require any bits for coding the motion vectors. However, the division is an expensive and undesired operation in video decoder hardware design. Although the H.264/AVC video standard has provided a scaling technique to tackle this problem, unfortunately, its performance is also deteriorated. This paper presents a new scaling technique to both remove the division operation for deriving direct mode motion vectors and efficiently improve the accuracy of derived direct mode motion vectors compared with the scaling technique in H.264/AVC."
      }
    ]
  },
  {
    "tid":"96",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Modeling and Analysis for Reactive-Decision Spectrum Handoff in Cognitive Radio Networks\nContent: This paper investigates how to characterize the effect of multiple spectrum handoff delay on the extended data delivery time of the secondary users' connections in cognitive radio (CR) networks. Whenever a primary user appears, the spectrum handoff procedures are initiated for the interrupted secondary users. Through spectrum sensing, the secondary user can reactively decide the target channel for spectrum handoff to resume its unfinished transmission. Clearly, many interruptions from the primary users will result in multiple handoffs, thereby increase the extended data delivery time of a secondary connection. In this paper, we develop a Markov transition model integrating with the preemptive resume priority (PRP) M/G/1 queueing network to characterize the multiple handoff delay, resulting from the sensing time, the handshaking time, the channel switching time, and the waiting time. The analytical results can facilitate the designs of admission control rules for the secondary users subject to their latency requirements and provide a framework to determine whether the spectrum sensing technology can effectively shorten the extended data delivery time under various traffic arrival rates and service time as well as sensing time(1).\n\nTitle: Analysis of Reactive Spectrum Handoff in Cognitive Radio Networks.\nContent: In this paper, we present an analytical framework to evaluate the effects of multiple spectrum handoffs on channel utilization and latency performances in cognitive radio (CR) networks. During the transmission period of a secondary connection, multiple interruptions from the primary users result in multiple spectrum handoffs. In order to decide the target channel for each spectrum handoff and resume the unfinished transmission, wideband sensing is performed in an on-demand reactive manner. Although spectrum handoff procedure can enhance channel utilization, transmission latency of the secondary users is prolonged due to multiple handoffs. Thus, two fundamental issues in CR networks with multiple spectrum handoffs arise: (1) to what extent the channel utilization can be improved; and (2) how long the transmission latency will be extended for the secondary users. To solve the first problem, we introduce the preemptive resume priority (PRP) M/G/1 queueing network to characterize the channel usage behaviors of CR networks. Based on this queueing network, channel utilization under various traffic arrival rates and service time distributions can be evaluated. Furthermore, on top of the proposed queueing network, a state diagram is developed to characterize the effects of multiple handoff delay on the transmission latency of the secondary users. The analytical results can provide a helpful insight to study the effects of traffic arrival rates and service time on channel utilization and transmission latency and then facilitate the designs of admission control rules for the secondary users subject to their performance requirements.\n\nTitle: Modeling and analysis of multi-user spectrum selection schemes in cognitive radio networks\nContent: In this paper, we study the spectrum selection problem in cognitive radio network with emphasis on resolving the channel contention and the spectrum sharing issues of multiple secondary users. For the traditional channel selection methods, the secondary users select their operating channels based on various criteria. However, these methods neglect the effect that multiple secondary users may content for the same channel if they have the same consensus on one particular good channel. Compared to the existing spectrum selection methods, we consider the sensing-based and the probability-based spectrum selection schemes which can prevent too many secondary users from contending the same channel. An analytical model integrated with the preemptive resume priority M/G/1 queuing network theory is developed to evaluate the overall transmission time of the both schemes. Based on this model, we discuss how to find the optimal selection probability for the probability-based scheme. Furthermore, we also analyze in which condition dependent of sensing time and traffic parameters that the sensing- or the probability-based scheme should be used. Based on the analytical results, we provide a principle to guide system operators which scheme should be used in CR networks. Then, we conclude that channel selection scheme should be adaptive to the variations of the traffic characteristics.\n\nTitle: A queueing-theoretical framework for QoS-enhanced spectrum management in cognitive radio networks\nContent: This article outlines the fundamental modeling issues of opportunistic spectrum access in cognitive radio networks. In particular, we identify the effects of connection-based channel usage on the QoS performance of spectrum management techniques. During the transmission period of a secondary user's connection, the phenomenon of multiple spectrum handoffs due to interruptions of primary users arises quite often. In addition to multiple interruptions, the connection-based channel usage behaviors are also affected by spectrum sensing time, switching between different channels, generally distributed service time, and channel contention between multiple secondary users. An analytical framework based on the preemptive resumption priority M/G/1 queueing theory is introduced to characterize the effects of the above factors simultaneously. The proposed generalized analytical framework can incorporate various system parameters into the design of very broad spectrum management techniques, including spectrum sensing, spectrum decision, spectrum sharing, and spectrum mobility. The applications of this analytical framework on spectrum decision as well as spectrum mobility are discussed, and some open issues using this framework are suggested in this article.\n\nTitle: Load-Balancing Spectrum Decision for Cognitive Radio Networks\nContent: In this paper, we present an analytical framework to design system parameters for load-balancing multiuser spectrum decision schemes in cognitive radio (CR) networks. Unlike the non-load-balancing methods that multiple secondary users may contend for the same channel, the considered load-balancing schemes can distribute the traffic loads of secondary users to multiple channels. Based on the preemptive resume priority (PRP) M/G/1 queueing theory, a spectrum decision analytical model is proposed to evaluate the effects of multiple interruptions from the primary user during each link connection, the sensing errors (i.e., missed detection and false alarm) of the secondary users, and the heterogeneous channel capacity. With the objective of minimizing the overall system time of the secondary users, we derive the optimal number of candidate channels and the optimal channel selection probability for the sensing-based and the probability-based spectrum decision schemes, respectively. We find that the probability-based scheme can yield a shorter overall system time compared to the sensing-based scheme when the traffic loads of the secondary users is light, whereas the sensing-based scheme performs better in the condition of heavy traffic loads. If the secondary users can intelligently adopt the best spectrum decision scheme according to sensing time and traffic conditions, the overall system time can be improved by 50% compared to the existing methods.\n\n## Task\nCompose an abstract for the title 'Latency Analysis for Dynamic Spectrum Access in Cognitive Radio: Dedicated or Embedded Control Channel?' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Dynamic spectrum access (DSA) is the key feature of cognitive radio (CR) networks, but it also poses many new challenges on the medium access control (MAC) design. One of key challenges is the fact that the secondary CR users can only borrow the licensed spectrum from the primary users for a short period of time. Hence, unlike many available multi-channel MAC protocols for ad hoc networks where throughput is the main performance issue, the DSA protocols in CR networks shall place more emphases on the access latency. Hence, one fundamental issue arises: how can the spectrum be dimensioned for control channels in order to minimize the access delay of DSA protocol in CR networks? In this paper, we provide a comparative study in an analytical manner on the latency performance of two DSA protocols: 1) dedicated control channel, and 2) embedded dedicated control channel approaches. Our results show that an optimal ratio of the control channel bandwidth over the total channel bandwidth can be found to minimize the latency of DSA with dedicated control channels. However, the delay performance of DSA with dedicated control channels is more sensitive to the variations of the data lengths than that of DSA with embedded control channels. Hence, we conclude that the way of dimensioning the spectrum for control frames for DSA in CR networks should be adaptive to the variations of the traffic characteristics and the number of users."
      }
    ]
  },
  {
    "tid":"97",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Exponential H-Infinity Synchronization Of General Discrete-Time Chaotic Neural Networks With Or Without Time Delays\nContent: This brief studies exponential H-infinity synchronization of a class of general discrete-time chaotic neural networks with external disturbance. On the basis of the drive-response concept and H-infinity control theory, and using Lyapunov-Krasovskii (or Lyapunov) functional, state feedback controllers are established to not only guarantee exponential stable synchronization between two general chaotic neural networks with or without time delays, but also reduce the effect of external disturbance on the synchronization error to a minimal H-infinity norm constraint. The proposed controllers can be obtained by solving the convex optimization problems represented by linear matrix inequalities. Most discrete-time chaotic systems with or without time delays, such as Hopfield neural networks, cellular neural networks, bidirectional associative memory networks, recurrent multilayer perceptrons, Cohen-Grossberg neural networks, Chua's circuits, etc., can be transformed into this general chaotic neural network to be H-infinity synchronization controller designed in a unified way. Finally, some illustrated examples with their simulations have been utilized to demonstrate the effectiveness of the proposed methods.\n\nTitle: Optimal exponential synchronization of general chaotic delayed neural networks: an LMI approach.\nContent: This paper investigates the optimal exponential synchronization problem of general chaotic neural networks with or without time delays by virtue of Lyapunov-Krasovskii stability theory and the linear matrix inequality (LMI) technique. This general model, which is the interconnection of a linear delayed dynamic system and a bounded static nonlinear operator, covers several well-known neural networks, such as Hopfield neural networks, cellular neural networks (CNNs), bidirectional associative memory (BAM) networks, and recurrent multilayer perceptrons (RMLPs) with or without delays. Using the drive-response concept, time-delay feedback controllers are designed to synchronize two identical chaotic neural networks as quickly as possible. The control design equations are shown to be a generalized eigenvalue problem (GEVP) which can be easily solved by various convex optimization algorithms to determine the optimal control law and the optimal exponential synchronization rate. Detailed comparisons with existing results are made and numerical simulations are carried out to demonstrate the effectiveness of the established synchronization laws.\n\nTitle: Robust h\u221e control for delayed nonlinear systems based on standard neural network models\nContent: A neural-network-based robust output feedback H\u221e control design is suggested for control of a class of nonlinear systems with time delays. The design approach employs a neural network, of which the activation functions satisfy the sector conditions, to approximate the delayed nonlinear system. A full-order dynamic output feedback controller is designed for the approximating neural network. The closed-loop neural control system is transformed into a novel neural network model termed standard neural network model (SNNM). Based on the robust H\u221e performance analysis of the SNNM, the parameters of output feedback controllers can be obtained by solving some lilinear matrix inequalities (LMIs). The well-designed controller ensures the asymptotic stability of the closed-loop system and guarantees an optimal H\u221e norm bound constraint on disturbance attenuation for all admissible uncertainties.\n\nTitle: Robust H\u221e control for uncertain delayed nonlinear systems based on standard neural network models.\nContent: A neural-network-based robust output feedback H\"~ control design is suggested for control of a class of nonlinear systems both with time delays and with uncertainties. In this paper, a full-order dynamic output feedback controller is designed for the delayed uncertain nonlinear system approximated by the neural network (e.g. multilayer perceptron, recurrent neural network, etc.), of which the activation functions satisfy the sector conditions. The closed-loop neural control system is transformed into a novel neural network model both with uncertainties and with time delays termed standard neural network model (SNNM). Based on the optimal robust H\"~ performance analysis of the SNNM, the parameters of output feedback controllers can be obtained by solving some linear matrix inequalities (LMIs). The optimal H\"~ controller ensures the robust global asymptotic stability of the closed-loop system and eliminates the effect of approximation errors, parametric uncertainties, and external disturbances. Finally, a simple example is presented to illustrate the effectiveness and the applicability of the proposed design approach.\n\nTitle: H-Infinity Output Tracking Control Of Discrete-Time Nonlinear Systems Via Standard Neural Network Models\nContent: This brief proposes an output tracking control for a class of discrete-time nonlinear systems with disturbances. A standard neural network model is used to represent discrete-time nonlinear systems whose nonlinearity satisfies the sector conditions. H-infinity control performance for the closed-loop system including the standard neural network model, the reference model, and state feedback controller is analyzed using Lyapunov-Krasovskii stability theorem and linear matrix inequality (LMI) approach. The H infinity controller, of which the parameters are obtained by solving LMIs, guarantees that the output of the closed-loop system closely tracks the output of a given reference model well, and reduces the influence of disturbances on the tracking error. Three numerical examples are provided to show the effectiveness of the proposed H infinity output tracking design approach.\n\n## Task\nCompose an abstract for the title 'Optimal H\u221e synchronization of general discrete-time delayed chaotic neural networks via dynamic output feedback' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper deals with robust H\u221e synchronization of general discrete-time chaotic neural networks with external disturbance. This general discrete-time model, which is the interconnection of a linear delayed dynamic system and a bounded static nonlinear operator, covers not only several well-known discrete-time delayed neural networks, such as Hopfield neural networks, cellular neural networks (CNNs), bidirectional associative memory (BAM) networks, and recurrent multilayer perceptrons (RMLPs), but also Lur'e systems. Based on Lyapunov stability and H\u221e control theories, dynamic output feedback controllers are established to not only guarantee exponentially stable synchronization of both master and slave systems with time delays, but also reduce the effect of external disturbance to an H\u221e-norm constraint. Furthermore, two classes of optimal controllers are presented, one minimizing the H\u221e-norm bound, the other maximizing the exponential synchronization rate. The control design equations are shown to be a set of linear matrix inequality (LMI) standard problems which can be easily solved by various convex optimization algorithms to determine the optimal H\u221e control laws and the optimal exponential synchronization rates."
      }
    ]
  },
  {
    "tid":"98",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: NetFPGA\u2014An Open Platform for Teaching How to Build Gigabit-Rate Network Switches and Routers\nContent: The NetFPGA platform enables students and researchers to build high-performance networking systems using field-programmable gate array (FPGA) hardware. A new version of the NetFPGA platform has been developed and is available for use by the academic community. The NetFPGA platform has modular interfaces that enable development of complex hardware designs by integration of simple building blocks. FPGA logic is used to implement the core data processing functions while software running on an attached host computer or embedded cores within the device implement control functions. Reference designs and component libraries have been developed for the CS344 course at Stanford University, Stanford, CA, and taught at a series of tutorials held in the United States, United Kingdom, India, China, Australia, and Europe. The open-source Verilog, C, Perl, and Java reference design is available for download from the project website.\n\nTitle: NetFPGA-An Open Platform for Gigabit-Rate Network Switching and Routing.\nContent: The NetFPGA platform enables students and researchers to build high-performance networking systems in hardware. A new version of the NetFPGA platform has been developed and is available for use by the academic community. The NetFPGA 2.1 platform now has interfaces that can be parameterized, therefore enabling development of modular hardware designs with varied word sizes. It also includes more logic and faster memory than the previous platform. Field Programmable Gate Array (FPGA) logic is used to implement the core data processing functions while software running on embedded cores within the FPGA and/or programs running on an attached host computer implement only control functions. Reference designs and component libraries have been developed for the CS344 course at Stanford University. Open-source Verilog code is available for download from the project website.\n\nTitle: Implementing an OpenFlow switch on the NetFPGA platform\nContent: We describe the implementation of an OpenFlow Switch on the NetFPGA platform. OpenFlow is a way to deploy experimental or new protocols in networks that carry production traffic. An OpenFlow network consists of simple flow-based switches in the datapath, with a remote controller to manage several switches. In practice, OpenFlow is most often added as a feature to an existing Ethernet switch, IPv4 router or wireless access point. An OpenFlow-enabled device has an internal flow-table and a standardized interface to add and remove flow entries remotely. Our implementation of OpenFlow on the NetFPGA is one of several reference implementations we have implemented on different platforms. Our simple OpenFlow implementation is capable of running at line-rate and handling all the traffic that is going through the Stanford Electrical Engineering and Computer Science building. We compare our implementation's complexity to a basic IPv4 router implementation and a basic Ethernet learning switch implementation. We describe the OpenFlow deployment into the Stanford campus and the Internet2 backbone.\n\nTitle: A Packet Generator on the NetFPGA Platform\nContent: A packet generator and network traffic capture system has been implemented on the NetFPGA. The NetFPGA is an open networking platform accelerator that enables rapid development of hardware-accelerated packet processing applications. The packet generator application allows Internet packets to be transmitted at line rate on up to four Gigabit Ethernet ports simultaneously. Data transmitted is specified in a standard PCAP file, transferred to local memory on the NetFPGA card, then sent on the Gigabit links using a precise data rate, inter-packet delay, and number of iterations specified by the user. The hardware circuit also simultaneously operates as a packet capture system, allowing traffic to be captured from up to all four of the Gigabit Ethernet ports. Timestamps are recorded and traffic can be transferred back to the host and stored using the same PCAP format. The project has been implemented as a fully open-source project and serves as an exemplar project on how to build and distribute NetFPGA applications. All of the code (Verilog hardware, system software, verification scripts, makefiles, and support tools) can be freely downloaded from the NetFPGA.org website. Benchmarks comparing this hardware-accelerated application to the fastest available PC with a PCIe NIC shows that the FPGA-based hardware-accelerator far exceeds the performance possible using TCP-reply software.\n\nTitle: The P4->NetFPGA Workflow for Line-Rate Packet Processing.\nContent: P4 has emerged as the de facto standard language for describing how network packets should be processed, and is becoming widely used by network owners, systems developers, researchers and in the classroom. The goal of the work presented here is to make it easier for engineers, researchers and students to learn how to program using P4, and to build prototypes running on real hardware. Our target is the NetFPGA SUME platform, a 4x10 Gb/s PCIe card designed for use in universities for teaching and research. Until now, NetFPGA users have needed to learn an HDL such as Verilog or VHDL, making it off limits to many software developers and students. Therefore, we developed the P4->NetFPGA workflow, allowing developers to describe how packets are to be processed in the high-level P4 language, then compile their P4 programs to run at line rate on the NetFPGA SUME board. The P4->NetFPGA workflow is built upon the Xilinx P4-SDNet compiler and the NetFPGA SUME open source code base. In this paper, we provide an overview of the P4 programming language and describe the P4->NetFPGA workflow. We also describe how the workflow is being used by the P4 community to build research prototypes, and to teach how network systems are built by providing students with hands-on experience working with real hardware.\n\n\n\n## Task\nCompose an abstract for the title 'NetFPGA: reusable router architecture for experimental research' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Our goal is to enable fast prototyping of networking hardware (e.g. modified Ethernet switches and IP routers) for teaching and research. To this end, we built and made available the NetFPGA platform. Starting from open-source reference designs, students and researchers create their designs in Verilog, and then download them to the NetFPGA board where they can process packets at line-rate for 4-ports of 1GE. The board is becoming widely used for teaching and research, and so it has become important to make it easy to re-use modules and designs. We have created a standard interface between modules, making it easier to plug modules together in pipelines, and to create new re-usable designs. In this paper we describe our modular design, and how we have used it to build several systems, including our IP router reference design and some extensions to it."
      }
    ]
  },
  {
    "tid":"99",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Coping with a Smart Jammer in Wireless Networks: A Stackelberg Game Approach\nContent: Jamming defense is an important yet challenging problem. In this paper, we study the jamming defense problem in the presence of a smart jammer, who can quickly learn the transmission power of the user and adaptively adjust its transmission power to maximize the damaging effect. We consider both the single-channel model and the multi-channel model. By modeling the problem as a Stackelberg game, we compute the optimal transmission power for the user to maximize its utility, in the presence of a smart jammer. For the single-channel model, we prove the existence and uniqueness of the Stackelberg Equilibrium (SE) by giving closed-form expressions for the SE strategies of both the user and the player. For the multi-channel model, we prove the existence of the SE. We design algorithms for computing the jammer's best response strategy and approximating the user's optimal strategy. Finally, we validate our theoretical analysis through extensive simulations.\n\nTitle: Game theoretic analysis of multiparty access control in online social networks\nContent: Existing online social networks (OSNs) only allow a single user to restrict access to her/his data but cannot provide any mechanism to enforce privacy concerns over data associated with multiple users. This situation leaves privacy conflicts largely unresolved and leads to the potential disclosure of users' sensitive information. To address such an issue, a MultiParty Access Control (MPAC) model was recently proposed, including a systematic approach to identify and resolve privacy conflicts for collaborative data sharing in OSNs. In this paper, we take another step to further study the problem of analyzing the strategic behavior of rational controllers in multiparty access control, where each controller aims to maximize her/his own benefit by adjusting her/his privacy setting in collaborative data sharing in OSNs. We first formulate this problem as a multiparty control game and show the existence of unique Nash Equilibrium (NE) which is critical because at an NE, no controller has any incentive to change her/his privacy setting. We then present algorithms to compute the NE and prove that the system can converge to the NE in only a few iterations. A numerical analysis is also provided for different scenarios that illustrate the interplay of controllers in the multiparty control game. In addition, we conduct user studies of the multiparty control game to explore the gap between game theoretic approaches and real human behaviors.\n\nTitle: Routing in max-min fair networks: A game theoretic approach\nContent: In this paper, we study the problem of routing in networks with max-min fair congestion control at the link level. The goal of each user is to maximize its own bandwidth by selecting its path. The problem is formulated as a non-cooperative game. We first prove the existence of Nash Equilibria. This is important, because at a Nash Equilibrium (NE), no user has the incentive to change its routing strategy. In addition, we investigate how the selfish behavior of the users may affect the performance of the network as a whole. We next introduce a novel concept of observed available bandwidth on each link. It allows a user to find a path with maximum bandwidth under max-min fair congestion control in polynomial time. We then present a game based algorithm to compute an NE and prove that by following the natural game course the network converges to an NE. Extensive experiments show that the network can converge to an NE in less than 10 iterations and also significantly improves the fairness compared with other algorithms. Our results have the implication for the future routing protocol design.\n\nTitle: A Game-Theoretic Approach to Stable Routing in Max-Min Fair Networks\nContent: In this paper, we present a game-theoretic study of the problem of routing in networks with max-min fair congestion control at the link level. The problem is formulated as a noncooperative game, in which each user aims to maximize its own bandwidth by selecting its routing path. We first prove the existence of Nash equilibria. This is important, because at a Nash equilibrium (NE), no user has any incentive to change its routing strategy-leading to a stable state. In addition, we investigate how the selfish behavior of users may affect the performance of the network as a whole. We next introduce a novel concept of observed available bandwidth on each link. It allows a user to find a path with maximum bandwidth under max-min fair congestion control in polynomial time, when paths of other users are fixed. We then present a game-based algorithm to compute an NE and prove that by following the natural game course, the network converges to an NE. Extensive simulations show that the algorithm converges to an NE within 10 iterations and also achieves better fairness compared to other algorithms .\n\nTitle: Privacy-Enhanced and Practical Truth Discovery in Two-Server Mobile Crowdsensing\nContent: In mobile crowdsensing, truth discovery (TD) enables a crowdsensing server to extract truthful information from possibly conflicting crowdsensing data. TD provides a more accurate truth estimation than traditional truth inference methods like majority voting and averaging. However, there still exist crucial data privacy (including sensory data, inferred truths, and intermediates) and practicability (\n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">e.g.</i>\n, efficiency, utility, and non-interaction) concerns in real-world crowdsensing applications. Existing researches either fail to provide adequate data privacy protection throughout the entire TD procedure or suffer from low practicability. In this paper, we propose two schemes: a basic privacy-aware TD scheme (BPTD) and a privacy-enhanced TD scheme (PETD) with two servers for mobile crowdsensing, comprehensively considering both privacy and practicability. BPTD is straightforwardly conducted on shared data with few user-side interactions, while achieving high efficiency. To further liberate mobile users and prevent disclosure of the intermediates, PETD incorporates a novel partial decryption-based Paillier Cryptosystem to work with secret sharing, offering enhanced privacy protection without relying on any user-side involvement. Additionally, we improve the efficiency of PETD via data packing. Security analysis shows the desired privacy goals. Compared to prior studies with the best security guarantees, our extensive experiments demonstrate a comparable and superior performance regarding different metrics.\n\n## Task\nCompose an abstract for the title 'Game-Theory-Based Active Defense for Intrusion Detection in Cyber-Physical Embedded Systems.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Cyber-Physical Embedded Systems (CPESs) are distributed embedded systems integrated with various actuators and sensors. When it comes to the issue of CPES security, the most significant problem is the security of Embedded Sensor Networks (ESNs). With the continuous growth of ESNs, the security of transferring data from sensors to their destinations has become an important research area. Due to the limitations in power, storage, and processing capabilities, existing security mechanisms for wired or wireless networks cannot apply directly to ESNs. Meanwhile, ESNs are likely to be attacked by different kinds of attacks in industrial scenarios. Therefore, there is a need to develop new techniques or modify the current security mechanisms to overcome these problems. In this article, we focus on Intrusion Detection (ID) techniques and propose a new attack-defense game model to detect malicious nodes using a repeated game approach. As a direct consequence of the game model, attackers and defenders make different strategies to achieve optimal payoffs. Importantly, error detection and missing detection are taken into consideration in Intrusion Detection Systems (IDSs), where a game tree model is introduced to solve this problem. In addition, we analyze and prove the existence of pure Nash equilibrium and mixed Nash equilibrium. Simulations show that the proposed model can both reduce energy consumption by up to 50% compared with the existing All Monitor (AM) model and improve the detection rate by up to 10% to 15% compared with the existing Cluster Head (CH) monitor model."
      }
    ]
  },
  {
    "tid":"100",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Motion Segmentation Via a Sparsity Constraint.\nContent: Motion segmentation is an important task for intelligent transportation systems. In this paper, inspired by the fact that a feature point trajectory can be sparsely represented as a combination of several feature point trajectories that share coherent transformations, an efficient and effective motion segmentation method with a sparsity constraint is proposed. Specifically, we first propose an accumulated scheme to efficiently integrate motion information from all the frames of a video sequence to construct a correlation matrix. Then, a sparse affinity matrix is built on the correlation matrix by using information-theoretic principles, where the nonzero elements in the same row of the sparse affinity matrix correspond to the feature point trajectories more likely belonging to the same motion. Thereafter, a segment and merge procedure is proposed to effectively estimate the number of motions via the sparse affinity matrix. Finally, by applying spectral clustering on the sparse affinity matrix, different motions in the video sequence are accurately segmented based on the estimated number of motions. Experimental results on the Hopkins 155 and the 62-clip datasets demonstrate that the proposed method achieves superior performance compared with several state-of-the-art methods.\n\nTitle: A Model-Selection Framework for Multibody Structure-and-Motion of Image Sequences\nContent: Given an image sequence of a scene consisting of multiple rigidly moving objects, multi-body structure-and-motion (MSaM) is the task to segment the image feature tracks into the different rigid objects and compute the multiple-view geometry of each object. We present a framework for multibody structure-and-motion based on model selection. In a recover-and-select procedure, a redundant set of hypothetical scene motions is generated. Each subset of this pool of motion candidates is regarded as a possible explanation of the image feature tracks, and the most likely explanation is selected with model selection. The framework is generic and can be used with any parametric camera model, or with a combination of different models. It can deal with sets of correspondences, which change over time, and it is robust to realistic amounts of outliers. The framework is demonstrated for different camera and scene models.\n\nTitle: MDPE: A Very Robust Estimator for Model Fitting and Range Image Segmentation\nContent: In this paper, we propose a novel and highly robust estimator, called MDPE1 (Maximum Density Power Estimator). This estimator applies nonparametric density estimation and density gradient estimation techniques in parametric estimation (\u201cmodel fitting\u201d). MDPE optimizes an objective function that measures more than just the size of the residuals. Both the density distribution of data points in residual space and the size of the residual corresponding to the local maximum of the density distribution, are considered as important characteristics in our objective function. MDPE can tolerate more than 85% outliers. Compared with several other recently proposed similar estimators, MDPE has a higher robustness to outliers and less error variance.We also present a new range image segmentation algorithm, based on a modified version of the MDPE (Quick-MDPE), and its performance is compared to several other segmentation methods. Segmentation requires more than a simple minded application of an estimator, no matter how good that estimator is: our segmentation algorithm overcomes several difficulties faced with applying a statistical estimator to this task.\n\nTitle: Stage-Aware Feature Alignment Network for Real-Time Semantic Segmentation of Street Scenes\nContent: Over the past few years, deep convolutional neural network-based methods have made great progress in semantic segmentation of street scenes. Some recent methods align feature maps to alleviate the semantic gap between them and achieve high segmentation accuracy. However, they usually adopt the feature alignment modules with the same network configuration in the decoder and thus ignore the different roles of stages of the decoder during feature aggregation, leading to a complex decoder structure. Such a manner greatly affects the inference speed. In this paper, we present a novel Stage-aware Feature Alignment Network (SFANet) based on the encoder-decoder structure for real-time semantic segmentation of street scenes. Specifically, a Stage-aware Feature Alignment module (SFA) is proposed to align and aggregate two adjacent levels of feature maps effectively. In the SFA, by taking into account the unique role of each stage in the decoder, a novel stage-aware Feature Enhancement Block (FEB) is designed to enhance spatial details and contextual information of feature maps from the encoder. In this way, we are able to address the misalignment problem with a very simple and efficient multi-branch decoder structure. Moreover, an auxiliary training strategy is developed to explicitly alleviate the multi-scale object problem without bringing additional computational costs during the inference phase. Experimental results show that the proposed SFANet exhibits a good balance between accuracy and speed for real-time semantic segmentation of street scenes. In particular, based on ResNet-18, SFANet respectively obtains 78.1% and 74.7% mean of class-wise Intersection-over-Union (mIoU) at inference speeds of 37 FPS and 96 FPS on the challenging Cityscapes and CamVid test datasets by using only a single GTX 1080Ti GPU.\n\nTitle: Robust object tracking in crowd dynamic scenes using explicit stereo depth\nContent: In this paper, we exploit robust depth information with simple color-shape appearance model on single object tracking in crowd dynamic scenes. Since binocular video streams are captured from a moving camera rig, background subtraction cannot provide a reliable enhancement of region of interest. Our main contribution is a novel tracking strategy to employ explicit stereo depth to track and segment object in crowd dynamic scenes with occlusion handling. Appearance cues including color and shape play a secondary role to further extract the foreground acquired by previous depth-based segmentation. The proposed depth-driven tracking approach can largely alleviate the drifting issue, especially when the object frequently interacts with similar background in long sequence tracking. The problems caused by rapid object appearance change can also be avoided due to the stability of the depth cue. Furthermore, we propose a new, yet simple and effective depth-based scheme to cope with complete occlusion in tracking. From experiments on a large collection of challenging outdoor and indoor sequences, our algorithm demonstrates accurate and reliable tracking performance which outperforms other state-of-the-art competing algorithms.\n\n## Task\nCompose an abstract for the title 'The State-of-the-Art Research Progress on Motion Segmentation' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Motion segmentation is currently one of the most active research topics in the areas of computer vision and multimedia. It has a wide spectrum of potential applications, including action recognition, visual surveillance, video indexing, video compression, and many other applications. In this paper, we review the recent developments (especially for those in recent five years) of motion segmentation methods and analyze their respective pros and cons. Several valuable research directions are discussed and some suggestions are put forward in the end."
      }
    ]
  },
  {
    "tid":"101",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: FaReCast: fast, reliable application layer multicast for flash dissemination\nContent: To disseminate messages from a single source to a large number of targeted receivers, a natural approach is the tree-based application layer multicast (ALM). However, in time-constrained flash dissemination scenarios, e.g. earthquake early warning, where time is of the essence, the tree-based ALM has a single point of failure; its reliable extensions using ack-based failure recovery protocols cannot support reliable dissemination in the timeframe needed. In this paper, we exploit path diversity, i.e. exploit the use of multiple data paths, to achieve fast and reliable data dissemination. First, we design a forest-based M2M (Multiple parents-To-Multiple children) ALM structure where every node has multiple children and multiple parents. The intuition is to enable lower dissemination latency through multiple children, while enabling higher reliability through multiple parents. Second, we design multidirectional multicasting algorithms that effectively utilize the multiple data paths in the M2M ALM structure. A key aspect of our reliable dissemination mechanism is that nodes, in addition to communicating the data to children, also selectively disseminate the data to parents and siblings. As compared to trees using traditional multicasting algorithm, we observe an 80% improvement in reliability under 20% of failed nodes with no significant increase in latency for over 99% of the nodes.\n\nTitle: ReCREW: A Reliable Flash-Dissemination System\nContent: In this paper, we explore a new form of dissemination that arises in distributed, mission-critical applications called Flash Dissemination. This involves the rapid dissemination of rich information to a large number of recipients in a very short period of time. A key characteristic of Flash Dissemination is its unpredictability (e.g., natural hazards), but when invoked it must harness all possible resources to ensure timely delivery of information. Additionally, it must scale to a large number of recipients and perform efficiently in highly heterogeneous (data, network) and failure prone environments. We investigate a peer-based approach based on the simple principle of transferring dissemination load to information receivers using foundations from broadcast networks, gossip theory, and random networks. Gossip-based protocols are well known for being stateless, scalable, and fault-tolerant; however, their performance degrades as content size increases, because of the propagation of redundant gossip messages. In this paper, we propose Concurrent Random Expanding Walkers (CREW), a smart gossip protocol designed to maximize the speed of dissemination by transmitting data only as needed, and by exploiting both intra- and internode concurrency. CREW is designed to support both content and network heterogeneity and deal with transmission failures without sacrificing dissemination speed. We implemented CREW on top of a scalable middleware environment that allows for deployment across several platforms and developed optimizations without compromising on the stateless nature of CREW. We evaluated CREW empirically and compared it to optimized implementations of popular gossip and peer-based systems. Our experiments show that CREW significantly outperforms both traditional gossip and current large content dissemination systems while sustaining its performance in the presence of network errors.\n\nTitle: Towards Reliable Application Data Broadcast in Wireless Ad Hoc Networks\nContent: Application data broadcast in ad hoc networks is an important primitive that has received little systematic research - the main focus of prior research being on control data broadcast. In this paper, we show why control data broadcast and even multicast techniques are insufficient for reliable application data broadcast; in fact their reliability degrades sharply with increasing application data size. We discover the root cause of this to be IP fragmenting the application data but not providing good reliability control on the fragments. We hence propose READ (reliable and efficient application-data dissemination), a protocol based on higher-layer fragmentation with fragment-level reliability control. READ splits a data packet into fragments, and disseminates them separately at dynamically adaptive intervals. Receivers piggyback implicit NACKs when propagating the fragments, and retrieve missing fragments from neighbors. Through experiments, we show that READ consistently achieves high delivery ratio and short latency, outperforming all other examined protocols.\n\nTitle: Efficient and Reliable Application Layer Multicast for Flash Dissemination\nContent: To disseminate messages from a single source to a large number of targeted receivers, a natural approach is the tree-based application layer multicast (ALM). In time-constrained flash dissemination scenarios, e.g. earthquake early warning, where time is of the essence, the reliable extensions of the tree-based ALM using ack-based failure recovery protocols cannot support reliable dissemination in the timeframe needed. In this paper, we propose FaReCast which exploits path diversity, i.e., exploit the use of multiple data paths, to achieve fast and reliable data dissemination. First, we design a forest-based M2M (Multiple parents-To-Multiple children) ALM structure where every node has multiple children and multiple parents. The intuition is to enable lower dissemination latency through multiple children, while enabling higher reliability through multiple parents. In order to maintain the M2M ALM structure in a scalable and reliable manner, we develop a DHT-based Distributed Configuration Manager. Second, we design multidirectional multicasting algorithms that effectively utilize the multiple data paths in the M2M ALM structure. A key aspect of our reliable dissemination mechanism is that nodes, in addition to communicating the data to children, also selectively disseminate the data to parents and siblings. As compared to trees using traditional multicasting algorithm, we observe an 80 percent improvement in reliability under 20 percent of failed nodes with no significant increase in latency for over 99 percent of the nodes. Moreover, we notice that FaReCast can reduce the network overhead more than 50 percent by tuning the M2M structure, as compared to the other reliable ALM based disseminations.\n\nTitle: HybCAST: Rich Content Dissemination in Hybrid Cellular and 802.11 Ad Hoc Networks\nContent: We design, implement, and evaluate a middleware system, HybCAST, that leverages a hybrid cellular and ad hoc network to disseminate rich contents from a source to all mobile devices in a predetermined region. HybCAST targets information dissemination over a range of scenarios (e.g., military operations, crisis alerting, and popular sporting events) in which high reliability and low latency are critical and existing fixed infrastructures such as wired networks, 802.11 access points are heavily loaded or partially destroyed. HybCAST implements a suite of protocols that: (i) structures the hybrid network into a hierarchy of two-level ad hoc clusters for better scalability, (ii) employ both data push and pull mechanisms for high reliability and low latency dissemination of rich content, and (iii) implement a near-optimal gateway selection algorithm to minimize the transmission redundancy. To demonstrate its practicality and efficiency, we have implemented and deployed the HybCAST middleware on several Android smart phones and an in-network Linux machine that acts as a dissemination server. The system is evaluated via real experiments using a UMTS network and extensive packet-level simulations. Our experimental results from a live network show that HybCAST achieves 100% reliability with shorter latencies and lower overall energy consumption. Simulation results confirm that HybCAST outperforms other state-of-the-art systems in the literature. For example, HybCAST exhibits a 5 times reduction in the dissemination latencies as compared to other hybrid dissemination protocols, while its energy consumption is a third of a cellular-only dissemination system. Furthermore, the simulation results demonstrate that HybCAST scales well and maintains good performance under varying numbers of mobile devices, diverse content sizes, and device mobility.\n\n## Task\nCompose an abstract for the title 'RADcast: Enabling Reliability Guarantees for Content Dissemination in Ad Hoc Networks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper deals with the problem of reliable and fast broadcast of mission-critical data with rich content over ad hoc networks. Existing approaches to dissemination reliability often assume network size knowledge, or that receivers know about the dissemination in advance. Without making similar assumptions, we propose a distinct approach which accommodates the varying reliability needs of applications. We develop the RADcast (reliable application data broadcast) protocol as an integration of two components: (a) Peddler, which ensures that receivers obtain the dissemination metadata, and (b) Pryer, which delivers the actual data to dissemination-aware receivers. We indicate how reliability guarantees/performance tradeoffs can be achieved by a careful instantiation of Peddler and Pryer. We implement RADcast on mobile devices inside a middleware and determine its feasibility. Furthermore, through extensive simulations, we show that RADcast achieves desired reliability in all cases, and performs consistently under varying network conditions and device mobilities. As compared to existing approaches, RADcast either incurs significantly lower latency/message overhead, or reduces latency by 50% with a tradeoff in message overhead."
      }
    ]
  },
  {
    "tid":"102",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Like Partying? Your Face Says It All. Predicting the Ambiance of Places with Profile Pictures\nContent:   To choose restaurants and coffee shops, people are increasingly relying on social-networking sites. In a popular site such as Foursquare or Yelp, a place comes with descriptions and reviews, and with profile pictures of people who frequent them. Descriptions and reviews have been widely explored in the research area of data mining. By contrast, profile pictures have received little attention. Previous work showed that people are able to partly guess a place's ambiance, clientele, and activities not only by observing the place itself but also by observing the profile pictures of its visitors. Here we further that work by determining which visual cues people may have relied upon to make their guesses; showing that a state-of-the-art algorithm could make predictions more accurately than humans at times; and demonstrating that the visual cues people relied upon partly differ from those of the algorithm. \n\nTitle: Tracking \"gross community happiness\" from tweets\nContent: Policy makers are calling for new socio-economic measures that reflect subjective well-being, to complement traditional measures of material welfare as the Gross Domestic Product (GDP). Self-reporting has been found to be reasonably accurate in measuring one's well-being and conveniently tallies with sentiment expressed on social media (e.g., those satisfied with life use more positive than negative words in their Facebook status updates). Social media content can thus be used to track well-being of individuals. A question left unexplored is whether such content can be used to track well-being of entire physical communities as well. To this end, we consider Twitter users based in a variety of London census communities, and study the relationship between sentiment expressed in tweets and community socio-economic well-being. We find that the two are highly correlated: the higher the normalized sentiment score of a community's tweets, the higher the community's socio-economic well-being. This suggests that monitoring tweets is an effective way of tracking community well-being too.\n\nTitle: Chatty Maps: Constructing sound maps of urban areas from social media data.\nContent: Urban sound has a huge influence over how we perceive places. Yet, city planning is concerned mainly with noise, simply because annoying sounds come to the attention of city officials in the form of complaints, whereas general urban sounds do not come to the attention as they cannot be easily captured at city scale. To capture both unpleasant and pleasant sounds, we applied a new methodology that relies on tagging information of georeferenced pictures to the cities of London and Barcelona. To begin with, we compiled the first urban sound dictionary and compared it with the one produced by collating insights from the literature: ours was experimentally more valid (if correlated with official noise pollution levels) and offered a wider geographical coverage. From picture tags, we then studied the relationship between soundscapes and emotions. We learned that streets with music sounds were associated with strong emotions of joy or sadness, whereas those with human sounds were associated with joy or surprise. Finally, we studied the relationship between soundscapes and people's perceptions and, in so doing, we were able to map which areas are chaotic, monotonous, calm and exciting. Those insights promise to inform the creation of restorative experiences in our increasingly urbanized world.\n\nTitle: Chatty, Happy, and Smelly Maps\nContent: Mapping apps are the greatest game-changer for encouraging people to explore the city. You take your phone out and you know immediately where to go. However, the app also assumes there are only a handful of directions to the destination. It has the power to make those handful of directions the definitive direction to that destination. A few years ago, my research started to focus on understanding how people psychologically experience the city. I used computer science tools to replicate social science experiments at scale, at web scale [4,5]. I became captivated by the beauty and genius of traditional social science experiments done by Jane Jacobs, Stanley Milgram, Kevin Lynch[1,2,3]. The result of that research has been the creation of new maps, maps where one does not only find the shortest path but also the most enjoyable path [6,9]. We did so by building a new city map weighted for human emotions. On this cartography, one is not only able to see and connect from point A to point B the shortest segments, but one is also able to see the happy path, the beautiful path, the quiet path. In tests, participants found the happy, the beautiful, the quiet paths far more enjoyable than the shortest one, and that just by adding a few minutes to travel time. Participants also recalled how some paths smelled and sounded. So what if we had a mapping tool that would return the most enjoyable routes based not only on aesthetics but also based on smell and sound? That is the research question this talk will start to address [7,8].\n\nTitle: Partisan sharing: facebook evidence and societal consequences\nContent: The hypothesis of selective exposure assumes that people seek out information that supports their views and eschew information that conflicts with their beliefs, and that has negative consequences on our society. Few researchers have recently found counter evidence of selective exposure in social media: users are exposed to politically diverse articles. No work has looked at what happens after exposure, particularly how individuals react to such exposure, though. Users might well be exposed to diverse articles but share only the partisan ones. To test this, we study partisan sharing on Facebook: the tendency for users to predominantly share like-minded news articles and avoid conflicting ones. We verified four main hypotheses. That is, whether partisan sharing: 1) exists at all; 2) changes across individuals (e.g., depending on their interest in politics); 3) changes over time (e.g., around elections); and 4) changes depending on perceived importance of topics. We indeed find strong evidence for partisan sharing. To test whether it has any consequence in the real world, we built a web application for BBC viewers of a popular political program, resulting in a controlled experiment involving more than 70 individuals. Based on what they share and on survey data, we find that partisan sharing has negative consequences: distorted perception of reality. However, we do also find positive aspects of partisan sharing: it is associated with people who are more knowledgeable about politics and engage more with it as they are more likely to vote in the general elections.\n\n## Task\nCompose an abstract for the title 'Don't worry, be happy: the geography of happiness on Facebook' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Social media sites have been used to track the expression of emotion words across nations. A question left unexplored is whether, at a similar scale, these sites can be used to collect reliable well-being data. To tackle this question, we collect Satisfaction With Life (SWL) test results from a Facebook application and show that aggregate country-level results significantly vary across twelve rich countries and strongly correlate with official well-being scores. To then show that collecting data on Facebook offers an informative look at the sociology of well-being, we study the impact of (un)happiness on the twelve countries by relating the test results on Facebook to reputable international indicators of social and health problems. We find that countries where happiness is lower have increased problems across the board: decreasing well-being is associated with increasing homicide, obesity, drug use, mental illness, and anxiety. In addition to offering these findings, this work hints at the conditions under which social media could be used for data-driven social science research."
      }
    ]
  },
  {
    "tid":"103",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Quadratic regularization design for fan beam transmission tomography\nContent: Statistical methods for tomographic image reconstruction have shown considerable potential for improving image quality in X-ray CT. Penalized-likelihood (PL) image reconstruction methods require maximizing an objective function that is based on the log-likelihood of the sinogram measurements and on a roughness penalty I unction to control noise. In transmission tomography, PL methods (and MAP methods) based on conventional quadratic regularization functions lead to nonuniform and anisotropic spatial resolution, even for idealized shift-invariant imaging systems. We have previously addressed this problem for parallel-beam emission tomography by designing data-dependent, shift-variant regularizers that improve resolution uniformity. This paper extends those methods to the fan-beam geometry used in X-ray CT imaging Simulation results demonstrate that the new method for regularization design requires very modest computation and leads to nearly uniform and isotropic spatial resolution in the fan-beam geometry when using quadratic regularization.\n\nTitle: Penalized weighted least-squares image reconstruction for positron emission tomography.\nContent: This paper presents an image reconstruction method for positron-emission tomography (PET) based on a penalized, weighted least- squares (PWLS) objective. For PET measurements that are precorrected for accidental coincidences, we argue statistically that a least-squares ob- jective function is as appropriate, if not more so, than the popular Pois- son likelihood objective. We propose a simple data-based method for determining the weights that accounts for attenuation and detector effi- ciency. A nonnegative successive over-relaxation (+SOR) algorithm con- verges rapidly to the global minimum of the PWLS objective. Quan- titative simulation results demonstrate that the bias/var iance tradeoff of the PWLS+SOR method is comparable to the maximum-likelihood expectation-maximization (ML-EM) method (but with fewer iterations), and is improved relative to the conventional filtered backpr ojection (FBP) method. Qualitative results suggest that the streak artifa cts common to the FBP method are nearly eliminated by the PWLS+SOR method, and indicate that the proposed method for weighting the measurements is a sig- nificant factor in the improvement over FBP.\n\nTitle: Regularized MR coil sensitivity estimation using augmented Lagrangian methods\nContent: Several magnetic resonance (MR) parallel imaging techniques require explicit estimates of the receive coil sensitivity profiles. These estimates must be accurate over both the object and its surrounding regions to avoid generating artifacts in the reconstructed images. Statistical estimation methods provide robust sensitivity estimates but can be computationally expensive. In this paper, we propose an augmented Lagrangian (AL) based method that estimates the coil sensitivity by minimizing a quadratic cost function. This method reformulates the finite differencing matrix to allow for exact alternating minimization steps. We also explore a variation of our algorithm that involves intermediate updating of the Lagrange multipliers. We demonstrate that our proposed algorithm converges in half the time of the traditional conjugate gradient method with a circulant preconditioner (PCG) on a real data set.\n\nTitle: Regularization Design For Isotropic Spatial Resolution In Motion-Compensated Image Reconstruction\nContent: Patient motion degrades image quality in medical imaging. Gating can reduce motion artifacts by using part of the acquired data, but can increase noise. Motion-compensated image reconstruction (MCIR) utilizes all collected data with motion information to reduce motion artifacts and noise.Interactions between Poisson log-likelihood and quadratic regularizers lead to nonuniform and anisotropic spatial resolution in the static case. These undesirable problems can become worse in MCIR due to local motion. We previously compensated for local volume changes in MCIR to provide approximately uniform spatial resolution, but achieved isotropic resolution only for the static case.This paper proposes a quadratic spatial regularizer design that achieves nearly uniform and isotropic spatial resolution in MCIR. We consider \"analytical approach\" to regularization design that was developed for static image reconstruction and extend it to MCIR methods based on a general parametric motion model. Our proposed regularizer can compensate not only for the effects of interactions between the Poisson log-likelihood and the spatial regularizer but also for the effects of nonrigid motion. A 2D PET simulation demonstrates the theoretical results.\n\nTitle: Regularization Parameter Selection for Nonlinear Iterative Image Restoration and MRI Reconstruction Using GCV and SURE-Based Methods\nContent: Regularized iterative reconstruction algorithms for imaging inverse problems require selection of appropriate regularization parameter values. We focus on the challenging problem of tuning regularization parameters for nonlinear algorithms for the case of additive (possibly complex) Gaussian noise. Generalized cross-validation (GCV) and (weighted) mean-squared error (MSE) approaches [based on Stein's unbiased risk estimate (SURE)] need the Jacobian matrix of the nonlinear reconstruction operator (representative of the iterative algorithm) with respect to the data. We derive the desired Jacobian matrix for two types of nonlinear iterative algorithms: a fast variant of the standard iterative reweighted least-squares method and the contemporary split-Bregman algorithm, both of which can accommodate a wide variety of analysis- and synthesis-type regularizers. The proposed approach iteratively computes two weighted SURE-type measures: predicted-SURE and projected-SURE (which require knowledge of noise variance $\\sigma^{2}$), and GCV (which does not need $\\sigma^{2}$) for these algorithms. We apply the methods to image restoration and to magnetic resonance image (MRI) reconstruction using total variation and an analysis-type $\\ell_{1}$-regularization. We demonstrate through simulations and experiments with real data that minimizing predicted-SURE and projected-SURE consistently lead to near-MSE-optimal reconstructions. We also observe that minimizing GCV yields reconstruction results that are near-MSE-optimal for image restoration and slightly suboptimal for MRI. Theoretical derivations in this paper related to Jacobian matrix evaluations can be extended, in principle, to other types of regularizers and reconstruction algorithms.\n\n## Task\nCompose an abstract for the title 'Comparison of SIRT and SQS for regularized weighted least squares image reconstruction' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Tomographic image reconstruction is often formulated as a regularized weighted least squares (RWLS) problem optimized by iterative algorithms that are either inherently algebraic or derived from a statistical point of view. This paper compares a modified version of SIRT (Simultaneous Iterative Reconstruction Technique), which is of the former type, with a version of SQS (Separable Quadratic Surrogates), which is of the latter type. We show that the two algorithms minimize the same criterion function using similar forms of preconditioned gradient descent. We present near-optimal relaxation for both based on eigenvalue bounds and include a heuristic extension for use with ordered subsets. We provide empirical evidence that SIRT and SQS converge at the same rate for all intents and purposes. For context, we compare their performance with an implementation of preconditioned conjugate gradient. The illustrative application is X-ray CT of luggage for aviation security."
      }
    ]
  },
  {
    "tid":"104",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Region-of-Interest Based Resource Allocation for Conversational Video Communication of H.264/AVC\nContent: Due to the complexity of H.264/AVC, it is very challenging to apply this standard to design a conversational video communication system. This problem is addressed in this paper by using region-of-interest (ROI) based bit allocation and computational power allocation schemes. In our system, the ROI is first detected by using the direct frame difference and skin-tone information. Several coding parameters including quantization parameter, candidates for mode decision, the number of referencing frames, accuracy of motion vectors and the search range of motion estimation are adaptively adjusted at the macroblock (MB) level according to the relative importance of each MB. Subsequently, the encoder could allocate more resources such as bits and computational power to the ROI, and the decoding complexity is also optimized at the encoder side by utilizing an ROI based rate-distortion-complexity (R-D-C) cost function. The encoder is thus simplified and decoding-friendly, and the overall subjective visual quality can also be improved.\n\nTitle: Adaptive Mad Prediction And Refined R-Q Model For H.264/Avc Rate Control\nContent: This paper presents an improved rate control scheme for the H.264/AVC video coding scheme. By analyzing the relationship between direct mean absolute difference (MAD) and actual MAD, a new MAD prediction scheme is introduced to enhance traditional linear MAD prediction model, which is unable to predict abrupt MAD fluctuations. Our proposed adaptive model could reduce MAD prediction error by up to 34%. One simple sum bit quadratic R-Q model is also presented to solve the problem caused by inaccurate texture bits estimation of H.264/AVC. With the new MAD prediction model and R-Q model, our proposed scheme could reduce the mismatch between actual frame bits and target frame bits by up to 32%, and the buffer occupancy is much closer to the ideal status. Meanwhile, reconstructed video quality is also improved by up to 0.21dB at low bitrate.\n\nTitle: Fast Mode Decision For Coarse Grain Snr Scalable Video Coding\nContent: Scalable Video Coding (SVC) is an on-going standard and the current working draft (WD) is an extension of H.264/AVC. In the WD, exhaustive search technique is employed to select the best coding mode for each macroblock (MB). This technique achieves highest possible coding efficiency, but it results in higher computational complexity. To overcome this, we propose a novel fast mode decision scheme for coarse grain SNR scalability (CGS) in SVC. In this scheme, the mode distribution relationships between the base layer and enhancement layers are employed to reduce the candidate mode set at enhancement layers. The experimental results show that the proposed scheme provides significant reduction in computational complexity with negligible coding loss.\n\nTitle: Fast Mode Decision Algorithm for Inter-Frame Coding in Fully Scalable Video Coding\nContent: Scalable video coding is an ongoing standard, and the current working draft (WD) is an extension of H.264/AVC. In the WD, an exhaustive search technique is employed to select the best coding mode for each macroblock. This technique achieves the highest possible coding efficiency, but it results in extremely large encoding time which obstructs it from practical use. This paper proposes a fast mode decision algorithm for inter-frame coding for spatial, coarse grain signal-to-noise ratio, and temporal scalability. It makes use of the mode-distribution correlation between the base layer and enhancement layers. Specifically, after the exhaustive search technique is performed at the base layer, the candidate modes for enhancement layers can be reduced to a small number based on the correlation. Experimental results show that the fast mode decision scheme reduces the computational complexity significantly with negligible coding loss and bit-rate increases\n\nTitle: Early detection of all-zero block in H.264 with new rate-quantization models\nContent: This paper presents a new algorithm for detecting all-zero DCT coefficient blocks (AZB) prior to DCT and quantization for H.264. The early detection criterion is derived from a new rate-quantization model which is established by considering the unique features of quantization process in H.264. The proposed algorithm aims to eliminate the redundant computations in AZB and consequently speed up the encoding process of H.264 video codec. Simulation results show that the proposed algorithm achieves a high detection ratio up to 95.88% while maintain a very low false detection ratio. The results confirm that a more effective AZB detection for H.264 can be achieved by taking the unique feature of quantization into consideration.\n\n## Task\nCompose an abstract for the title 'A joint source-channel decoder for H.264 SPS and PPS headers' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper proposes a joint source-channel decoder (JSCD) to decode the Sequence Parameter Set (SPS) and Picture Parameter Set (PPS) headers for the H.264 encoded video sequence. Erroneously received headers can render the entire video sequence un-decodable. By exploiting the syntax and semantics of the headers as well as certain a priori information, our proposed JSCD is capable of reducing the probability of erroneous headers and hence improving the overall PSNR of the video by as high as 3 dB."
      }
    ]
  },
  {
    "tid":"105",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Design and analysis of a regenerative magnetorheological actuator for gait assistance in knee joint\nContent: In this paper, we propose and analyze a Regenerative Magnetorheological Actuator (RMRA) for gait assistance in knee joint. The actuator has motor and magnetorheological (MR) brake parts working in parallel, and can harvest energy through regenerative braking. This novel design provides multiple functions with good energy efficiency. The design of the RMRA is first introduced, and its multiple functions are modeled. Different working modes of the RMRA are analyzed for mimicking knee joint functions within gait cycle. To achieve an effective and energy efficient braking function, cooperation between regenerative braking and MR braking is investigated. The energy harvesting (EH) circuit equivalent load is further optimized for maximal energy harvested within a gait cycle. Then impedance based braking control of the RMRA for gait assistance is proposed. Preliminary simulation shows that the developed RMRA with proposed control strategy can provide desired torque assistance during normal gait.\n\nTitle: Human level walking gait modeling and analysis based on semi-Markov process\nContent: Evaluation of individual gait pattern is important for both abnormal gait diagnosis and gait rehabilitation in mobility impaired people. In this paper, semi-Markov process (SMP) is applied to model and analyze human gait in level walking. Gait states are detected from ground reaction forces (GRFs), and gait cycles are described as state transitions in a gait Markov chain (GMC) with sojourn times. Several gait features are defined and online estimated based on the SMP model. With this model, abnormal gait patterns are further analyzed and indexes for gait abnormality assessment are proposed. Experiments of gait analyses with proposed method are conducted on subjects with different health conditions. Results show that individual gait pattern can be successfully obtained and evaluated. Potential applications in gait diagnosis and powered lower limb orthosis (PLLO) control for gait assistance are also discussed.\n\nTitle: Frequency adaptive repetitive controller for grid-connected inverter with an all-pass infinite impulse response (IIR) filter\nContent: The repetitive controller is widely employed in grid-connected inverters to improve the power quality. Its resonant frequencies are predesigned to the nominal grid frequency and its harmonics to achieve excellent reference tracking and disturbance eliminating. However, the performance degrades when the grid frequency varies. In this paper, a frequency adaptive repetitive controller with a specially designed all-pass infinite impulse response (IIR) filter is proposed to improve the steady-state performance considering grid frequency variation. The IIR filter approximates a fractional delay. By adjusting the delay, the resonant frequencies can be corrected to the actual grid fundamental and harmonic frequencies. The filter design is based on the Thiran formula, which is simple with a closed form, and online real-time computation burden is highly reduced. The IIR filter has a wide band of linear phase region with low order. As the system stability is independent on the IIR filter for its all-pass characteristic, the system synthesis is simplified. Results from a single-phase grid-connected inverter are provided to validate the effectiveness of the proposed control scheme.\n\nTitle: Analysis of Networked Control Schemes and Data-Processing Method for Parallel Inverters\nContent: This paper presents two schemes and correlated issues for parallel inverters of distributed generation units in an ac distribution system. The proposed control schemes are based on droop control method in the form of modified weighting power and only use communication bus with limited capacity. A potential improvement in current sharing can be achieved by introducing power data of other inverters via network into a conventional droop scheme. According to the transmitted data, Averaged Power Scheme and Instantaneous Power Scheme are introduced. The impact of time-delay and data dropouts during transmission on system stability is necessarily considered. Consequently, mathematical modeling of systems is built and the maximum time bounds of stability are addressed. Moreover, one self-adaptive data-processing method is developed to optimize the current sharing, which establishes two channels of control data depending on the communication condition, thereby keeping system operation smooth and achieving satisfactory performance even in communication failures. To further the description, the comparison is carried out by related experimental results of systems with time-delay, data dropouts and the others. Experimental results show the effectiveness of these proposed schemes.\n\nTitle: Wireless current sharing scheme considering phase synchronization requirement for paralleled online UPS inverters\nContent: In this paper, a wireless current sharing Scheme for paralleled online uninterruptible power system (UPS) inverters is proposed, based on which effective power sharing under phase locking condition and fast dynamic response of sharing load are obtained. The output impedance of UPS inverter is designed to be resistive, and the droop method based on resistive output impedance is employed. Moreover, an adaptive virtual resistor is added to regulate the magnitude of the output impedance according to the requirement of reactive power sharing, which makes the power sharing less sensitive to frequency change. Besides, the root-mean-square (RMS) loop is adopted to improve the output voltage quality. And the secondary P-V droop method is put forward to decrease the amplitude deviation and improve the dynamic performance of current sharing. Simulation and experimental results are presented to demonstrate the validity of the proposed scheme.\n\n## Task\nCompose an abstract for the title 'Hip-Knee Control For Gait Assistance With Powered Knee Orthosis' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"A Powered Knee Orthosis (PKO) was developed for the elderly and patients with disordered gait to regain normal walking. In order to enhance the PKO performance and reduce system complexity especially for people with muscle weakness in their knee joints, an algorithm named HIP-KNEE control is proposed. This algorithm is based on the analysis of kinematic gait model, and the desired knee joint angle (KNEE) is estimated from the measurements of hip joint angle (HIP). The relationship between HIP and KNEE is modeled as a polynomial, which can be easily implemented to an embedded controller for real-time control. This control method is suitable to subjects with good function in hip joint, and it can provide help in walking without special training. An Inertia Measurement Units (IMU) is used for obtaining HIP input, and integrated with a footswitch for checking the heel condition; the gait assistance performance can be further improved."
      }
    ]
  },
  {
    "tid":"106",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: G-IK-SVD: parallel IK-SVD on GPUs for sparse representation of spatial big data.\nContent: Sparse representation is a building block for many image processing applications such as compression, denoising, fusion and so on. In the era of \\\"Big data\\\", the current spare representation methods generally do not meet the demand of time-efficiently processing the large image dataset. Aiming at this problem, this paper employed the contemporary general-purpose computing on the graphics processing unit (GPGPU) to extend a sparse representation method for big image datasets, IK-SVD, namely G-IK-SVD. The GPU-aided IK-SVD parallelized IK-SVD with three GPU optimization methods: (1) a batch-OMP algorithm based on GPU-aided Cholesky decomposition algorithm, (2) a GPU sparse matrix operation optimization method and (3) a hybrid parallel scheme. The experimental results indicate that (1) the GPU-aided batch-OMP algorithm shows speedups of up to 30 times than the sparse coding part of IK-SVD, (2) the optimized sparse matrix operations improve the whole procedure of IK-SVD up to 15 times,(3) the proposed parallel scheme can further accelerate the procedure of sparsely representing one large image dataset up to 24 times, and (4) G-IK-SVD can gain the same quality of dictionary learning as IK-SVD.\n\nTitle: IK-SVD: Dictionary Learning for Spatial Big Data via Incremental Atom Update\nContent: A large group of dictionary learning algorithms focus on adaptive sparse representation of data. Almost all of them fix the number of atoms in iterations and use unfeasible schemes to update atoms in the dictionary learning process. It's difficult, therefore, for them to train a dictionary from Big Data. A new dictionary learning algorithm is proposed here by extending the classical K-SVD method. In the proposed method, when each new batch of data samples is added to the training process, a number of new atoms are selectively introduced into the dictionary. Furthermore, only a small group of new atoms as subspace controls the current orthogonal matching pursuit, construction of error matrix, and SVD decomposition process in every training cycle. The information, from both old and new samples, is explored in the proposed incremental K-SVD (IK-SVD) algorithm, but only the current atoms are adaptively updated. This makes the dictionary better represent all the samples without the influence of redundant information from old samples.\n\nTitle: Parallel programing templates for remote sensing image processing on GPU architectures: design and implementation.\nContent: Remote sensing image processing is characterized with features of massive data processing, intensive computation, and complex processing algorithms. These characteristics make the rapid processing of remote sensing images very difficult and inefficient. The rapid development of general-purpose graphic process unit (GPGPU) computing technology has resulted in continuous improvement in GPU computing performance. Its strong floating point calculating capability, high intensive computation, small volume, and excellent performance-cost ratio provide an effective solution to the problems faced in remote sensing image processing. However, current usage of GPU in remote sensing image processing applications has been limited to specific parallel algorithms and their optimization of processes, rather than formed well-established models and methods. This has introduced serious problems to the development of remote sensing image processing algorithms on GPU architectures. For example, GPU parallel strategies and algorithms are highly coupled and non-reusable. The processing system is closely associated with the GPU hardware so that programming for remote sensing algorithms on GPU is nothing but easy. In this paper, we attempt to explore a reusable GPU-based remote sensing image parallel processing model and to establish a set of parallel programming templates, which provides programmers with a more simple and effective way for programming parallel remote sensing image processing algorithms.\n\nTitle: Fields of Experts Based Multichannel Compressed Sensing.\nContent: Image reconstruction from under-sampled data had always been challenging due to its implicit ill-posed nature until, that is, the emergence of compressed sensing. This paper proposes a new compressed sensing-based model and an efficient algorithm for image reconstruction from sparse observations, based upon a generalized multiple sparse restrictions framework to describe both local similarities and nonlocal similarities at the regularization stage. An efficient split Bregman method is employed to decouple the nonlinear optimization problem in order to iteratively approach the optimum solution. To obtain better restoration for multiband images, a principle component analysis is suggested to improve the signal-to-noise ratio of auxiliary parameters, and the noise variance in the wavelet domain is calculated adaptively. A comparison between filters showed that eight 3\u00d73 sized filters learned from the model of fields of experts are by far the best filters to earn high peak signal-to-noise ratio. The proposed split Bregman optimization compressed sensing algorithm with fields of experts trained filters is compared to alternative methods such as Orthogonal Matching Pursuit, Compressive Sampling Matching Pursuit, Group-based Sparse Representation, and Compressive Sensing based on Bregman Split and principal component analysis. The experimental results of nine images show that the new method has competitive results for single-band images and outweighs all the other methods for color images, and so it is an appropriate model for reconstructing multichannel images.\n\nTitle: Foreground Detection Utilizing Structured Sparse Model via l1,2 Mixed Norms\nContent: Foreground object detection is a crucial technique of intelligent surveillance systems, and it is still a challenging problem in complex scenes with illumination variations and dynamic backgrounds. Intuitively, the foreground object pixels are often not sparsely distributed but tend to be clustered. Motivated by this hypothesis, we present a new structured sparse model to extract foreground objects, which introduces the spatial neighborhood information into a unified optimization framework by l1,2 mixed norms. Simultaneously, we also give the solving method of the proposed model in details. Moreover, we apply the model to the sparse signal recovery and background subtraction in videos. In the experiments, better performance is obtained over previous methods. The experimental results validate the hypothesis and the effectiveness of the proposed method.\n\n## Task\nCompose an abstract for the title 'Large-Scale Paralleled Sparse Principal Component Analysis.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Principal component analysis (PCA) is a statistical technique commonly used in multivariate data analysis. However, PCA can be difficult to interpret and explain since the principal components (PCs) are linear combinations of the original variables. Sparse PCA (SPCA) aims to balance statistical fidelity and interpretability by approximating sparse PCs whose projections capture the maximal variance of original data. In this paper we present an efficient and paralleled method of SPCA using graphics processing units (GPUs), which can process large blocks of data in parallel. Specifically, we construct parallel implementations of the four optimization formulations of the generalized power method of SPCA (GP-SPCA), one of the most efficient and effective SPCA approaches, on a GPU. The parallel GPU implementation of GP-SPCA (using CUBLAS) is up to eleven times faster than the corresponding CPU implementation (using CBLAS), and up to 107 times faster than a MatLab implementation. Extensive comparative experiments in several real-world datasets confirm that SPCA offers a practical advantage."
      }
    ]
  },
  {
    "tid":"107",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Ontology-Based Querying with Bio2RDF's Linked Open Data.\nContent: A key activity for life scientists in this post \"-omics\" age involves searching for and integrating biological data from a multitude of independent databases. However, our ability to find relevant data is hampered by non-standard web and database interfaces backed by an enormous variety of data formats. This heterogeneity presents an overwhelming barrier to the discovery and reuse of resources which have been developed at great public expense.To address this issue, the open-source Bio2RDF project promotes a simple convention to integrate diverse biological data using Semantic Web technologies. However, querying Bio2RDF remains difficult due to the lack of uniformity in the representation of Bio2RDF datasets.We describe an update to Bio2RDF that includes tighter integration across 19 new and updated RDF datasets. All available open-source scripts were first consolidated to a single GitHub repository and then redeveloped using a common API that generates normalized IRIs using a centralized dataset registry. We then mapped dataset specific types and relations to the Semanticscience Integrated Ontology (SIO) and demonstrate simplified federated queries across multiple Bio2RDF endpoints.This coordinated release marks an important milestone for the Bio2RDF open source linked data framework. Principally, it improves the quality of linked data in the Bio2RDF network and makes it easier to access or recreate the linked data locally. We hope to continue improving the Bio2RDF network of linked data by identifying priority databases and increasing the vocabulary coverage to additional dataset vocabularies beyond SIO.\n\nTitle: RKB: a Semantic Web knowledge base for RNA.\nContent: Increasingly sophisticated knowledge about RNA structure and function requires an inclusive knowledge representation that facilitates the integration of independently -generated information arising from such efforts as genome sequencing projects, microarray analyses, structure determination and RNA SELEX experiments. While RNAML, an XML-based representation, has been proposed as an exchange format for a select subset of information, it lacks domain-specific semantics that are essential for answering questions that require expert knowledge. Here, we describe an RNA knowledge base (RKB) for structure-based knowledge using RDF/OWL Semantic Web technologies. RKB extends a number of ontologies and contains basic terminology for nucleic acid composition along with context/model-specific structural features such as sugar conformations, base pairings and base stackings. RKB (available at http://semanticscience.org/projects/rkb) is populated with PDB entries and MC-Annotate structural annotation. We show queries to the RKB using description logic reasoning, thus opening the door to question answering over independently-published RNA knowledge using Semantic Web technologies.\n\nTitle: The Semanticscience Integrated Ontology (SIO) for biomedical research and knowledge discovery.\nContent: The Semanticscience Integrated Ontology (SIO) is an ontology to facilitate biomedical knowledge discovery. SIO features a simple upper level comprised of essential types and relations for the rich description of arbitrary (real, hypothesized, virtual, fictional) objects, processes and their attributes. SIO specifies simple design patterns to describe and associate qualities, capabilities, functions, quantities, and informational entities including textual, geometrical, and mathematical entities, and provides specific extensions in the domains of chemistry, biology, biochemistry, and bioinformatics. SIO provides an ontological foundation for the Bio2RDF linked data for the life sciences project and is used for semantic integration and discovery for SADI-based semantic web services. SIO is freely available to all users under a creative commons by attribution license. See website for further information: http://sio.semanticscience.org.\n\nTitle: An RDF/OWL knowledge base for query answering and decision support in clinical pharmacogenetics.\nContent: Genetic testing for personalizing pharmacotherapy is bound to become an important part of clinical routine. To address associated issues with data management and quality, we are creating a semantic knowledge base for clinical pharmacogenetics. The knowledge base is made up of three components: an expressive ontology formalized in the Web Ontology Language (OWL 2 DL), a Resource Description Framework (RDF) model for capturing detailed results of manual annotation of pharmacogenomic information in drug product labels, and an RDF conversion of relevant biomedical datasets. Our work goes beyond the state of the art in that it makes both automated reasoning as well as query answering as simple as possible, and the reasoning capabilities go beyond the capabilities of previously described ontologies.\n\nTitle: Integrating systems biology models and biomedical ontologies.\nContent: Systems biology is an approach to biology that emphasizes the structure and dynamic behavior of biological systems and the interactions that occur within them. To succeed, systems biology crucially depends on the accessibility and integration of data across domains and levels of granularity. Biomedical ontologies were developed to facilitate such an integration of data and are often used to annotate biosimulation models in systems biology.We provide a framework to integrate representations of in silico systems biology with those of in vivo biology as described by biomedical ontologies and demonstrate this framework using the Systems Biology Markup Language. We developed the SBML Harvester software that automatically converts annotated SBML models into OWL and we apply our software to those biosimulation models that are contained in the BioModels Database. We utilize the resulting knowledge base for complex biological queries that can bridge levels of granularity, verify models based on the biological phenomenon they represent and provide a means to establish a basic qualitative layer on which to express the semantics of biosimulation models.We establish an information flow between biomedical ontologies and biosimulation models and we demonstrate that the integration of annotated biosimulation models and biomedical ontologies enables the verification of models as well as expressive queries. Establishing a bi-directional information flow between systems biology and biomedical ontologies has the potential to enable large-scale analyses of biological systems that span levels of granularity from molecules to organisms.\n\n## Task\nCompose an abstract for the title 'The RNA Ontology RNAO: An ontology for integrating RNA sequence and structure data' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Biomedical Ontologies integrate diverse biomedical data and enable intelligent data-mining and help translate basic research into useful clinical knowledge. We present the RNA Ontology RNAO, an ontology for integrating diverse RNA data, including RNA sequences and sequence alignments, three-dimensional structures, and biochemical and functional data. For example, individual atomic resolution RNA structures have broader significance as representatives of classes of homologous molecules, which can differ significantly in sequence while sharing core structural features and common roles or functions. Thus, structural data gain value by being linked to homologous sequences in genomic data and databases of sequence alignments. Likewise, the value of genomic data is enhanced by annotation of shared structural features, especially when these can be linked to specific functions. Moreover, the significance of biochemical, functional and mutational analyses of RNA molecules are most fully understood when linked to molecular structures and phylogenies. To achieve these goals, RNAO provides logically rigorous definitions of the components of RNA primary, secondary and tertiary structure and the relations between these entities. RNAO is being developed to comply with the developing standards of the Open Biomedical Ontologies OBO Consortium. The RNAO can be accessed at http://code.google.com/p/rnao/."
      }
    ]
  },
  {
    "tid":"108",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Application of variational Bayesian estimation and clustering to acoustic model adaptation\nContent: In this paper, we apply Variational Bayesian Estimation and Clustering for speech recognition (VBEC) to an acoustic model adaptation. VBEC can estimate parameter posteri- ors even when a model includes hidden variables, by using Variational Bayesian approach. In addition, VBEC can se- lect an appropriate model structure in clustering triphone states, according to the amount of available adaptation data. Unlike a conventional Bayesian method such as Maximum A Posteriori (MAP), VBEC is useful even in the case of small amounts of data, because the amount of data per one Gaussian increases due to the model structure selection, and over-training is suppressed. We conduct an off-line super- vised adaptation experiment on isolated word recognition, and show the advantage of the proposed method over the conventional method, especially when dealing with small amounts of adaptation data.\n\nTitle: Selection Of Shared-State Hidden Markov Model Structure Using Bayesian Criterion\nContent: A Shared-State Hidden Markov Model (SS-HMM) has been widely used as an acoustic model in speech recognition. In this paper, we propose a method for constructing SS-HMMs within a practical Bayesian framework. Our method derives the Bayesian model selection criterion for the SS-HMM based on the variational Bayesian approach. The appropriate phonetic decision tree structure of the SS-HMM is found by using the Bayesian criterion. Unlike the conventional asymptotic criteria, this criterion is applicable even in the case of an insufficient amount of training data. The experimental results on isolated word recognition demonstrate that the proposed method does not require the tuning parameter that must be tuned according to the amount of training data, and is useful for selecting the appropriate SS-HMM structure for practical use.\n\nTitle: Constructing shared-state hidden Markov models based on a Bayesian approach\nContent: In this paper, we propose a method for constructing shared- state triphone HMMs (SST-HMMs) within a practical Bay- esian framework. In our method, Bayesian model selection criterion is derived for SST-HMM based on the Variational Bayesian approach. The appropriate phonetic decision tree structure of SST-HMM is found by using the criterion ac- cording to a given data set. This criterion, unlike the con- ventional MDL criterion, is applicable even in the case of insufficient amounts of data. We conduct two experiments on speaker independent word recognition in order to prove the effectiveness of the proposed method. The first exper- iment demonstrates that the Bayesian approach is valid for determining the tree structure. The second experiment dem- onstrates that the Bayesian criterion can design SST-HMMs with higher recognition performance than the MDL crite- rion when dealing with small amounts of data.\n\nTitle: Averaged collapsed variational bayes inference\nContent: This paper presents the Averaged CVB (ACVB) inference and oers convergence-guaranteed and practically useful fast Collapsed Variational Bayes (CVB) inferences. CVB inferences yield more precise inferences of Bayesian probabilistic models than Variational Bayes (VB) inferences. However, their convergence aspect is fairly unknown and has not been scrutinized. To make CVB more useful, we study their convergence behaviors in a empirical and practical approach. We develop a convergence-guaranteed algorithm for any CVB-based inference called ACVB, which enables automatic convergence detection and frees non-expert practitioners from the difficult and costly manual monitoring of inference processes. In experiments, ACVB inferences are comparable to or better than those of existing inference methods and deterministic, fast, and provide easier convergence detection. These features are especially convenient for practitioners who want precise Bayesian inference with assured convergence.\n\nTitle: Variational bayesian estimation and clustering for speech recognition\nContent: In this paper we propose Variational Bayesian Estimation and Clustering for speech recognition (VBEC), which is based on the Variational Bayesian (VB) approach. VBEC is a total Bayesian framework: all speech recognition procedures (acoustic modeling and speech classification) are based on VB posterior distribution, unlike the Maximum Likelihood (ML) approach based on ML parameters. The total Bayesian framework generates two major Bayesian advantages over the ML approach for the mitigation of over-training effects, as it can select an appropriate model structure without any data set size condition, and can classify categories robustly using a predictive posterior distribution. By using these advantages, VBEC (1) allows the automatic construction of acoustic models along two separate dimensions, namely, clustering triphone Hidden Markov Model states and determining the number of Gaussians, and (2) enables robust speech classification, based on Bayesian Predictive Classification using VB posterior distributions (VB-BPC). The capabilities of the VBEC functions were confirmed in large vocabulary continuous speech recognition experiments for read and spontaneous speech tasks. The experiments confirmed that VBEC automatically constructed accurate acoustic models and robustly classified speech, i.e., totally mitigated the over-training effects with high word accuracies due to the VBEC functions. I. INTRODUCTION Accurate acoustic modeling contributes greatly to improving speech recognition performance. The conventional framework for accurate modeling is based on Hidden Markov Models (HMMs) and Gaussian Mixture Models (GMMs) where the statistical model parameters are obtained by the Maximum Likelihood (ML) approach. The conventional ML-based framework presents problems for real speech recognition tasks because it cannot select an appropriate model structure in principle, and often estimates incorrect parameters when the amounts of training data assigned for the parameters are small (over-training), which in turn degrades the speech recognition performance. This is especially true for spontaneous speech recognition tasks, which exhibit larger speech variability compared with read speech (1). The Bayesian approach is based on a posterior distribution estimation, which can utilize prior information via a prior distribu- tion. The posterior distribution estimation provides two major advantages over the ML approach. First, it enables the selection of an appropriate model structure based on a posterior distribution for a model structure, which excludes the over-trained models. Second, it offers a robust classification based on a predictive posterior distribution, which mitigates the over-trained parameter effects. Together, these two advantages totally mitigate the effects of over-training. By using these advantages, the Bayesian approach-based speech recognition framework is expected to solve the problems originating from the ML's limitations. How- ever, the Bayesian approach requires complex integral and expectation computations to obtain posterior distributions, and has not been applied to speech recognition without losing the above advantages provided by the posterior distribution estimation. For example, the Maximum A Posteriori (MAP) based framework approximates the posterior distribution estimation to the parameter estimation, which loses both advantages although MAP can utilize prior information. Bayesian Information Criterion (BIC) (2) 1 and Bayesian Predictive Classification (BPC) based frameworks partially realize Bayesian advantages for the model selection and robust speech classification, respectively, in speech recognition by boldly approximating the posterior distribution calculation (4), (5), (6), (7), (8), (9), (10). However, since each approximation loses the other advantage, these frameworks cannot provide both advantages at once (partial Bayesian framework). Recently, a Variational Bayesian (VB) approach was proposed, which aims to avoid complex computations by employing the variational approximation technique (11), (12), (13), (14). In the VB approach, approximate posterior distributions (VB posterior distributions) can be obtained effectively by iterative calculations, while both advantages of the Bayesian approaches are retained. In this paper, we construct a total Bayesian framework: Variational Bayesian Estimation and Clustering for speech recognition (VBEC) (15), where all speech recognition procedures (acoustic modeling and speech classification) are based on the VB posterior distribution, and apply it to large-vocabulary continuous speech recognition. The VBEC framework in acoustic modeling includes the estimation of the VB posterior distributions and the selection of the appropriate model structure. In speech classification, predictive posterior distributions for input speech are obtained from the VB posterior distributions and output distributions. Any speech can then be recognized based on classifications using the predictive posterior distributions (VB-BPC; Bayesian predictive classification using VB posterior distributions). Consequently, VBEC can\n\n## Task\nCompose an abstract for the title 'Bayesian model search for mixture models based on optimizing variational bounds.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"When learning a mixture model, we suffer from the local optima and model structure determination problems. In this paper, we present a method for simultaneously solving these problems based on the variational Bayesian (VB) framework. First, in the VB framework, we derive an objective function that can simultaneously optimize both model parameter distributions and model structure. Next, focusing on mixture models, we present a deterministic algorithm to approximately optimize the objective function by using the idea of the split and merge operations which we previously proposed within the maximum likelihood framework. Then, we apply the method to mixture of expers (MoE) models to experimentally show that the proposed method can find the optimal number of experts of a MoE while avoiding local maxima."
      }
    ]
  },
  {
    "tid":"109",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Educational webcasts' acceptance: Empirical examination and the role of experience\nContent: Educational webcasts have enhanced the value of training procedures in institutions and business organizations. In this study, variables from the Unified Theory of Acceptance and Use of Technology, Social Cognitive Theory and Theory of Planed Behavior were chosen as important predictors of e-Learning tools acceptance. This hybrid framework aims to verify the effect of the selected key variables on webcast acceptance. Moreover, the effect of experience on learners' intention to adopt webcasts is explored, as well as the moderating effect of experience on the relationships between the selected key variables and the intention to adopt webcasts. Responses from 248 webcast-based learners were used to examine webcasts' adoption and the differences between learners with low and high levels of webcast usage experience. Results confirmed the effects of the key variables and experience on learners' intention to use webcasts and indicated the moderating effect of learners' experience on the relationships between (1) Perceived Behavioral Control and Behavioral Intention and (2) Social Norm and Behavioral Intention. The overall outcomes are expected to contribute to theoretical development and promote the acceptance of educational webcasts.\n\nTitle: Learning Analytics For Learning Design: Towards Evidence-Driven Decisions To Enhance Learning\nContent: As the fields of learning analytics and learning design mature, the convergence and synergies between them become an important area for research. Collecting and combining learning analytics coming from different channels can clearly provide valuable information in designing learning. Hence, this paper intends to summarize the main outcomes of a systematic literature review of empirical evidence on learning analytics for learning design. The search was performed in seven academic databases, resulting in 38 papers included in the main analysis. The review demonstrates ongoing design patterns and learning phenomena that improve learning, by providing more comprehensive background of the current landscape of learning analytics for learning design and its impact on the current status of learning technologies. Consequently, future research should consider how to capture and systematize learning design data. Moreover, it should evaluate and document what learning design choices made by educators using what learning analytics techniques influence learning experiences and learning performances over time.\n\nTitle: Explaining learning performance using response-time, self-regulation and satisfaction from content: an fsQCA approach.\nContent: This study focuses on compiling studentsu0027 response-time allocated to answer correctly or wrongly, their self-regulation, as well as their satisfaction from content, in order to explain high or medium/low learning performance. To this end, it proposes a conceptual model in conjunction with research propositions. For the evaluation of the approach, an empirical study with 452 students was conducted. The fuzzy set qualitative comparative analysis (fsQCA) revealed five configurations driven by the admitted factors that explain studentsu0027 high performance, as well as five additional patterns, interpreting studentsu0027 medium/low performance. These findings advance our understanding of the relations between actual usage and latent behavioral factors, as well as their combined effect on studentsu0027 test score. Limitations and potential implications of these findings are also discussed.\n\nTitle: Learning in smart environments: user-centered design and analytics of an adaptive learning system\nContent: The complexity of today\u2019s learning processes and practices entails various challenges. It is becoming much harder for teachers to observe, control, and adjust the learning process. Moreover, contemporary teaching is enhanced with different technologies and systems that not only support information-transfer, but also make this process more effective. In this paper we present the Programming Tutoring System (ProTuS), which provides smart and interactive content, personalization options, adaptive features, and learning analytics as a support for users engaged in learning complex cognitive skills. Our contribution in this paper is twofold, conceptual and empirical. The paper presents the interactive learning analytics component developed in ProTuS and the results from the empirical study. The study shows that students find adaptive learning systems to be useful in monitoring progress, promoting reflective practices, and receiving feedback to better understand their actions and learning strategies.\n\nTitle: Gamifying informal learning activities using interactive displays: an empirical investigation of students\u2019 learning and engagement\nContent: Interactive displays (IDs) are increasingly employed in informal learning environments, where they are seen as a medium for enhancing students\u2019 creativity, and engagement. Due to the larger space they provide and thus the larger interaction area, they allow for group-work, working in parallel, co-creating artifacts or co-experiencing the interaction in a playful manner. In particular, gaming activities in IDs enhance students\u2019 mental exercise and fantasy and promote students engagement through rewards and collaboration. However, despite the increased prevalence of interactive displays and gamification, we know very little about how designers and instructors can gamify their learning activities by taking advantage of the IDs. In this paper, a framework for developing gamified activities for interactive displays is presented. For the empirical evaluation, pre-post attitudinal surveys and cognitive tests along with photos and observations were recorded and used. The contribution of this article is twofold: 1) an adaptable framework for developing gamified activities on interactive displays (GAID), and 2) the results of a field study where students have been engaged with an interactive display application during an extracurricular activity. By incorporating GAID to a traditional informal learning activity, it is found that students\u2019 knowledge acquisition, satisfaction, enjoyment and intention to participate on similar events in the future are significantly improved.\n\n## Task\nCompose an abstract for the title 'In the Face (book) of Social Learning' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"  Social networks have risen to prominence over the last years as the predominant form of electronic interaction between individuals. In an attempt to harness the power of the large user base which they have managed to attract, this study proposes an e-learning prototype which integrates concepts of the social and semantic web. A selected set of services are deployed which have been scientifically proven to positively impact the learning process of users via electronic means. The integrability of these services into a social network platform application is visualized through an exploratory prototype. The Graphical User Interface (GUI) which is developed to implement these key features is in alignment with User-Centered principles. The designed prototype proves that a number of services can be integrated in a user-friendly application and can potentially serve to gain feedback regarding additional aspects that should be included. "
      }
    ]
  },
  {
    "tid":"110",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Fast disparity and motion estimation based on correlations for multiview video coding\nContent: Recently, multiview video coding has attracted great attention from industries and research institutes. However, the heavy computational complexity limits its practical applications. In this paper, a fast disparity and motion estimation for multiview video coding is presented, based on the correlations between the neighboring cameras and between the motion and the disparity. In the proposed approach, first, a search region estimation is proposed to reduce the disparity estimation complexity according to that the camera set is usually fixed and therefore the disparity between the two neighboring views can be limited to an estimable range. Second, a motion vector derivation is given based on the geometric relationship between the motion and the disparity. In addition, an early termination scheme is provided to further reduce the number of reference frames. The experimental results show that roughly 50% time saving for disparity and motion computing can be reached when compared to the anchor in multiview video coding test model JSVM only with negligible coding efficiency loss.\n\nTitle: Joint Rate-Distortion-Complexity Optimization For H.264 Motion Search\nContent: A joint rate-distortion-complexity H.264 motion search framework is proposed to balance the encoder's coding efficiency and complexity in an embedded system environment. Under our framework, the complexity of H.264 motion search is primarily measured by the execution time of the sum of absolute differences (SAD) calculation. Two Lagrange parameters are used to terminate the complexity-inefficient motion search rounds and skip redundant motion search of small block modes, respectively. Then, the relationship between the weighted complexity and the Lagrange parameters is explored to allocate the complexity cost among different coding units. It is demonstrated by experimental results that the proposed method can reduce the complexity without much sacrifice in coding efficiency.\n\nTitle: Enhanced Ctu-Level Inter Prediction with Deep Frame Rate Up-Conversion for High Efficiency Video Coding\nContent: Inter prediction serves as the foundation of prediction based hybrid video coding framework. The state-of-the-art video coding standards employ the reconstructed frames as the references, and the motion vectors which convey the relative position shift between the current block and the prediction block are explicitly signalled in the bitstream. In this paper, we propose a high efficient inter prediction scheme by introducing a new methodology based on virtual reference frame, which is effectively generated with the deep neural network such that the motion data does not need to be explicitly signalled. In particular, the high quality virtual reference frame is generated with the deep learning based frame rate up-conversion (FRUC) algorithm from two reconstructed bi-prediction frames. Subsequently, a novel CTU level coding mode termed as direct virtual reference frame (DVRF) mode, is proposed to adaptively compensate for the current to-be-coded block in the sense of rate-distortion optimization (RDO). The proposed scheme is integrated into the HM-16.6 software, and experimental results demonstrate significant superiority of the proposed method, which provides more than 3% coding gains on average for HEVC test sequences.\n\nTitle: Fast intra-encoding algorithm for High Efficiency Video Coding\nContent: The emerging High Efficiency Video Coding (HEVC) standard provides equivalent subjective quality with about 50% bit rate reduction compared to the H.264/AVC High profile. However, the improvement of coding efficiency is obtained at the expense of increased computational complexity. This paper presents a fast intra-encoding algorithm for HEVC, which is composed of the following four techniques. Firstly, an early termination technique for coding unit (CU) depth decision is proposed based on the depth of neighboring CUs and the comparison results of rate distortion (RD) costs between the parent CU and part of its child CUs. Secondly, the correlation of intra-prediction modes between neighboring PUs is exploited to accelerate the intra-prediction mode decision for HEVC intra-coding and the impact of the number of mode candidates after the rough mode decision (RMD) process in HM is studied in our work. Thirdly, the TU depth range is restricted based on the probability of each TU depth and one redundant process is removed in the TU depth selection process based on the analysis of the HEVC reference software. Finally, the probability of each case for the intra-transform skip mode is studied to accelerate the intra-transform skip mode decision. Experimental results show that the proposed algorithm can provide about 50% time savings with only 0.5% BD-rate loss on average when compared to HM 11.0 for the Main profile all-intra-configuration. Parts of these techniques have been adopted into the HEVC reference software. HighlightsAn early termination technique for coding unit (CU) depth decision.The correlation of intra-prediction modes between neighboring PUs is exploited to accelerate the intra-mode decision.The TU depth range is restricted based on the probability of each TU depth.The probability of each case for the intra-transform skip mode is firstly studied.\n\nTitle: Feature-matching based motion prediction for high efficiency video coding in cloud\nContent: Visual features of images and video frames have become pervasive and maturely developed in extensive research fields such as computer vision and visual search. For realtime retrieval applications, the compact visual features should be transmitted and stored at server side in cloud. These local feature descriptors are characterized by the invariance properties for the variances caused by camera motion, illumination changing, occlusion and different viewpoints. Inspired by these properties, the typical scale-invariant feature transform (SIFT) descriptor is leveraged to improve the video coding efficiency in this work. In particular the predicted motion using SIFT matching is used for merge mode and motion vector prediction (MVP) in the high efficiency video coding (HEVC) standard. A hierarchical motion derivation framework aiming at achieving robust and effective MVP is further proposed. Experimental results have shown that the proposed method can efficiently improve the coding performance according to the accurate feature-matching.\n\n## Task\nCompose an abstract for the title 'Fast multi reference frame motion estimation for high efficiency video coding' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, a fast multi reference frame motion estimation algorithm is proposed to reduce the computational complexity for high efficiency video coding (HEVC). Firstly, according to the reference frame distribution, we define the motion complexity for a frame. Based on the motion complexity, an initial reference frame set (RFS) is constructed to reduce the number of the reference frame for motion estimation. Then for each prediction unit (PU), the average distortion per pixel together with the motion vector difference (MVD) in the first reference frame is employed to shrink the RFS in order to early terminate the motion estimation process. In addition, to enhance the robustness of the proposed scheme, an expansion method for RFS is proposed to guarantee the video quality. Experimental results demonstrate that the proposed scheme can significantly save the encoding time. For Low Delay testing configuration, over 30% of time saving can be achieved on average with ignorable performance loss."
      }
    ]
  },
  {
    "tid":"111",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Inversion-free design algorithms for multivariable quantitative feedback theory: an application to robust control of a CD-ROM\nContent: This paper describes an application of multivariable quantitative feedback theory (QFT) to the robust performance design of a compact disc mechanism. Robust control design is based on new QFT design algorithms that do not involve the loop transmission inversion found in present QFT design algorithms. Their advantage is that even when the designer prefers inversion-based QFT loop shaping, the associated bounds can still be computed more efficiently using our inversion-free design algorithms. The plant uncertainty consists of parametric variations in the natural frequencies of dominant mechanical resonances and variation in the interaction between the focus and radial loops. Robust performance specifications are given in terms of the multivariable closed-loop sensitivity function. The QFT controller is implemented in an experimental setup. (C) 1997 Elsevier Science Ltd.\n\nTitle: Adaptive Iterative Learning Control for High Precision Motion Systems\nContent: Iterative learning control (ILC) is a very effective technique to reduce systematic errors that occur in systems that repetitively perform the same motion or operation. However, several characteristics have prevented standard ILC from being widely used for high precision motion systems. Most importantly, the learned feedforward signal depends on the motion profile (setpoint trajectory) and if this is altered, the learning process has to be repeated. Secondly, ILC amplifies non-repetitive disturbances and noise. Finally, its performance may be limited due to position-dependent dynamics. This paper presents the design and implementation of a time-frequency adaptive ILC that is applicable for motion systems which executes the same motion or operation. It employs the same control system block diagram as standard ILC, but instead of a fixed robustness filter it uses a time-varying filter. By using the results of the time-frequency adaptive ILC, i.e., the shape of the learned feedforward signal, a ldquopiecewise ILCrdquo is proposed that leads to the design of a single learned feedforward signal suitable for different setpoints. The results are experimentally shown to work for a high precision motion system.\n\nTitle: Design of noise and period-time robust high-order repetitive control, with application to optical storage\nContent: Repetitive control is useful if periodic disturbances or setpoints act on a control system. Perfect (asymptotic) disturbance rejection is achieved if the period time is exactly known. The improved disturbance rejection at the periodic frequency and its harmonics is achieved at the expense of a degraded system sensitivity at intermediate frequencies. A convex optimization problem is defined for the design of high-order repetitive controllers, where a trade-off can be made between robustness for changes in the period time and for reduction of the error spectrum in-between the harmonic frequencies. The high-order repetitive control algorithms are successfully applied in experiments with the tracking control of a CD-player system.\n\nTitle: Stability and performance of a variable gain controller with application to a dvd storage drive\nContent: This paper deals with the control design for optical storage drives. A nonlinear design is suggested to overcome the tradeoff between disturbance rejection, in the sense of tracking error reduction during low-frequency shock and vibration, and playability, in the sense of sensor noise tracking during high-frequency disc surface defects. Based on a variable gain control design, it is shown that improvements in disturbance rejection can be obtained without unnecessarily affecting playability. With this design, additional control is applied beyond a pre-defined error level. Accordingly, it is shown that large vibrations induce additional control effort giving improved disturbance rejection while, at the same time, small vibrations hardly induce any additional control effort thus leaving the playability properties unaffected. The variable gain strategy is studied regarding closed-loop stability and regarding performance. Closed-loop stability is derived on the basis of absolute stability theory. Performance is quantified using a measure derived from the linear sensitivity function. Based on the amplitude of the linear sensitivity function, the maximum absolute values of the periodic nonlinear response subjected to harmonic excitation are computed within a frequency range of interest. Experiments are performed on an industrial setup to validate the numerical results, and to illustrate the applicability of the nonlinear control design in a dvd application.\n\nTitle: Compensation Of Mode Shapes With A Piezo Electric Actuator For An Optical Drive\nContent: In miniaturized systems (e.g. optical disk drives) much effort is made to design a structure in such a way that the demands on dimensions (height, width) and desired bandwidth are satisfied. Easier, and often cheaper designs can not be used, because they suffer from lower resonance frequencies, which limit the bandwidth. A method is presented for mechatronic structures that compensates unwanted bandwidth limiting resonance frequencies with an additional piezo electric actuator. Advantage of this method is that no extra sensor is used and that the contribution of the piezo is much easier to tune (when compared to a notch filter), since it is part of the structure. Disadvantage is that the piezo influences the dynamics of the original system, which complicates the choice of a suitable piezo actuator.\n\n## Task\nCompose an abstract for the title 'Adaptive control of the radical servo system of a compact disc player' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The radial servo system of a compact disc player has to cope with large gain variations which are due to disc dependent optical characteristics, tolerances in mechanical and electrical components and nonlinearity in the generation of the position index. In current players this problem has been solved by using an adaptive control scheme based on an injected signal to identify the process gain. This paper discusses the application of the general Self Tuning Regulator (STR) concept to the control problem of the radial servo system. The STR principle depends on parametric identification and control adaptation without additional excitation of the control loop. The concept has been implemented in an experimental arrangement; it gives good control performance and is insensitive to practical perturbations."
      }
    ]
  },
  {
    "tid":"112",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Parallelizing Explicit Formula for Arithmetic in the Jacobian of Hyperelliptic Curves.\nContent: One of the recent thrust areas in research on hyperelliptic curve cryptography has been to obtain explicit formulae for performing arithmetic in the Jacobian of such curves. We continue this line of research by obtaining parallel versions of such formulae. Our first contribution is to develop a general methodology for obtaining parallel algorithm of any explicit formula. Any parallel algorithm obtained using our methodology is provably optimal in the number of multiplication rounds. We next apply this methodology to Lange's explicit formula for arithmetic in genus 2 hyperelliptic curve - both for the affine coordinate and inversion free arithmetic versions. Since encapsulated add-and-double algorithm is an important countermeasure against side channel attacks, we develop parallel algorithms for encapsulated add-and-double for both of Lange's versions of explicit formula. For the case of inversion free arithmetic, we present parallel algorithms using 4, 8 and 12 multipliers. All parallel algorithms described in this paper are optimal in the number of parallel rounds. One of the conclusions from our work is the fact that the parallel version of inversion free arithmetic is more efficient than the parallel version of arithmetic using affine coordinates.\n\nTitle: A Simple Method for Obtaining Relations Among Factor Basis Elements for Special Hyperelliptic Curves.\nContent: Nagao proposed a decomposition method for divisors of hyperelliptic curves defined over a field $${\\\\mathbb {F}}_{q^n}$$Fqn with $$n\\\\ge 2$$n\u00bf2. Joux and Vitse later proposed a variant which provided relations among the factor basis elements. Both Nagao's and the Joux---Vitse methods require solving a multi-variate system of polynomial equations. In this work, we revisit Nagao's approach with the idea of avoiding the requirement of solving a multi-variate system. While this cannot be done in general, we are able to identify special cases for which this is indeed possible. Our main result is for curves $$C:y^2=f(x)$$C:y2=f(x) of genus g defined over $${\\\\mathbb {F}}_{q^2}$$Fq2 having characteristic 2. If there is no restriction on f(x), we show that it is possible to obtain a relation in $$(4g+4)!$$(4g+4)! trials. The number of trials, though high, quantifies the computation effort needed to obtain a relation. This is in contrast to the methods of Nagao and Joux---Vitse which are based on solving systems of polynomial equations, for which the computation effort is hard to precisely quantify. The new method combines well with a sieving technique proposed by Joux and Vitse. If f(x) has a special form, then the number of trials can be significantly lower. For example, if f(x) has at most g consecutive coefficients which are in $${\\\\mathbb {F}}_{q^2}$$Fq2 while the rest are in $${\\\\mathbb {F}}_q$$Fq, then we show that it is possible to obtain a single relation in about $$(2g+3)!$$(2g+3)! trials. Our implementation of the resulting algorithm provides examples of factor basis relations for $$g=5$$g=5 and $$g=6$$g=6. To the best of our knowledge, none of the previous methods known in the literature can provide such relations faster than our method. Other than obtaining such decompositions, we also explore the applicability of our approach for $$n2$$n2 and for binary characteristic fields.\n\nTitle: New Table Look-Up Methods for Faster Frobenius Map Based Scalar Multiplication Over GF(pn)\nContent: \n We describe a new scalar multiplication algorithm for elliptic and hyperelliptic curve cryptosystems. The algorithm is obtained\n by combining Koblitz\u2019s idea of using Frobenius automorphism along with a very special kind of look-up table. In the case where\n the base point is unknown, we present an efficient algorithm to compute the look-up table online. Our algorithm applies to\n prime power fields GF(p\n \n n\n ). One important subclass of such fields are Optimal Extension Fields (OEF\u2019s) which are believed to be ideal for efficient\n implementation of cryptographic primitives. Over prime power fields, our algorithm compares favourably to other known algorithms\n for scalar multiplication.\n \n \n\nTitle: Application of Montgomery's Trick to Scalar Multiplication for Elliptic and Hyperelliptic Curves Using a Fixed Base Point\nContent: We propose a scalar multiplication algorithm for elliptic and hyperelliptic curve cryptosystems, which uses affine arithmetic and is resistant against simple power attacks. Also, using a modification of known techniques the algorithm can be made immune against differential power attacks. The algorithm uses Montgomery's trick and a precomputed table consisting of multiples of the base point. Consequently, the algorithm is useful in a scenario where the base point is fixed, like Elgamal encryption or signature generation. Under such circumstances, for hyperelliptic curves, the algorithm compares favourably with other known algorithms over all fields. For elliptic curves, under similar circumstances, the algorithm performs better than other algorithms over prime fields. The increase in speed is due to a proper application of Montgomery's trick to efficiently perform the simultaneous inversion of several field elements.\n\nTitle: A General Polynomial Selection Method and New Asymptotic Complexities for the Tower Number Field Sieve Algorithm.\nContent: In a recent work, Kim and Barbulescu had extended the tower number field sieve algorithm to obtain improved asymptotic complexities in the medium prime case for the discrete logarithm problem on $$\\\\mathbb {F}_{p^{n}}$$ where n is not a prime power. Their method does not work when n is a composite prime power. For this case, we obtain new asymptotic complexities, e.g., $$L_{p^{n}}1/3,64/9^{1/3}$$ resp. $$L_{p^{n}}1/3,1.88$$ for the multiple number field variation when n is composite and a power of 2; the previously best known complexity for this case is $$L_{p^{n}}1/3,96/9^{1/3}$$ resp. $$L_{p^{n}}1/3,2.12$$. These complexities may have consequences to the selection of key sizes for pairing based cryptography. The new complexities are achieved through a general polynomial selection method. This method, which we call Algorithm-$$\\\\mathcal {C}$$, extends a previous polynomial selection method proposed at Eurocrypt 2016 to the tower number field case. As special cases, it is possible to obtain the generalised Joux-Lercier and the Conjugation method of polynomial selection proposed at Eurocrypt 2015 and the extension of these methods to the tower number field scenario by Kim and Barbulescu. A thorough analysis of the new algorithm is carried out in both concrete and asymptotic terms.\n\n## Task\nCompose an abstract for the title 'A New Method for Decomposition in the Jacobian of Small Genus Hyperelliptic Curves.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Decomposing a divisor over a suitable factor basis in the Jacobian of a hyperelliptic curve is a crucial step in an index calculus algorithm for the discrete log problem in the Jacobian. For small genus curves, in the year 2000, Gaudry had proposed a suitable factor basis and a decomposition method. In this work, we provide a new method for decomposition over the same factor basis. The advantage of the new method is that it admits a sieving technique which removes smoothness checking of polynomials required in Gaudry\u2019s method. Also, the total number of additions in the Jacobian required by the new method is less than that required by Gaudry\u2019s method. The new method itself is quite simple and we present some example decompositions and timing results of our implementation of the method using Magma."
      }
    ]
  },
  {
    "tid":"113",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Tensor Decompositions for Signal Processing Applications: From two-way to multiway component analysis.\nContent:   The widespread use of multi-sensor technology and the emergence of big datasets has highlighted the limitations of standard flat-view matrix models and the necessity to move towards more versatile data analysis tools. We show that higher-order tensors (i.e., multiway arrays) enable such a fundamental paradigm shift towards models that are essentially polynomial and whose uniqueness, unlike the matrix methods, is guaranteed under verymild and natural conditions. Benefiting fromthe power ofmultilinear algebra as theirmathematical backbone, data analysis techniques using tensor decompositions are shown to have great flexibility in the choice of constraints that match data properties, and to find more general latent components in the data than matrix-based methods. A comprehensive introduction to tensor decompositions is provided from a signal processing perspective, starting from the algebraic foundations, via basic Canonical Polyadic and Tucker models, through to advanced cause-effect and multi-view data analysis schemes. We show that tensor decompositions enable natural generalizations of some commonly used signal processing paradigms, such as canonical correlation and subspace techniques, signal separation, linear regression, feature extraction and classification. We also cover computational aspects, and point out how ideas from compressed sensing and scientific computing may be used for addressing the otherwise unmanageable storage and manipulation problems associated with big datasets. The concepts are supported by illustrative real world case studies illuminating the benefits of the tensor framework, as efficient and promising tools for modern signal processing, data analysis and machine learning applications; these benefits also extend to vector/matrix data through tensorization. Keywords: ICA, NMF, CPD, Tucker decomposition, HOSVD, tensor networks, Tensor Train. \n\nTitle: Kernelization of Tensor-Based Models for Multiway Data Analysis: Processing of Multidimensional Structured Data\nContent: Tensors (also called multiway arrays) are a generalization of vectors and matrices to higher dimensions based on multilinear algebra. The development of theory and algorithms for tensor decompositions (factorizations) has been an active area of study within the past decade, e.g., [1] and [2]. These methods have been successfully applied to many problems in unsupervised learning and exploratory data analysis. Multiway analysis enables one to effectively capture the multilinear structure of the data, which is usually available as a priori information about the data. Hence, it might provide advantages over matrix factorizations by enabling one to more effectively use the underlying structure of the data. Besides unsupervised tensor decompositions, supervised tensor subspace regression and classification formulations have been also successfully applied to a variety of fields including chemometrics, signal processing, computer vision, and neuroscience.\n\nTitle: Bayesian CP Factorization of Incomplete Tensors with Automatic Rank Determination.\nContent:   CANDECOMP/PARAFAC (CP) tensor factorization of incomplete data is a powerful technique for tensor completion through explicitly capturing the multilinear latent factors. The existing CP algorithms require the tensor rank to be manually specified, however, the determination of tensor rank remains a challenging problem especially for CP rank. In addition, existing approaches do not take into account uncertainty information of latent factors, as well as missing entries. To address these issues, we formulate CP factorization using a hierarchical probabilistic model and employ a fully Bayesian treatment by incorporating a sparsity-inducing prior over multiple latent factors and the appropriate hyperpriors over all hyperparameters, resulting in automatic rank determination. To learn the model, we develop an efficient deterministic Bayesian inference algorithm, which scales linearly with data size. Our method is characterized as a tuning parameter-free approach, which can effectively infer underlying multilinear factors with a low-rank constraint, while also providing predictive distributions over missing entries. Extensive simulations on synthetic data illustrate the intrinsic capability of our method to recover the ground-truth of CP rank and prevent the overfitting problem, even when a large amount of entries are missing. Moreover, the results from real-world applications, including image inpainting and facial image synthesis, demonstrate that our method outperforms state-of-the-art approaches for both tensor factorization and tensor completion in terms of predictive performance. \n\nTitle: Low-Rank Approximation Based Non-Negative Multi-Way Array Decomposition On Event-Related Potentials\nContent: Non-negative tensor factorization (NTF) has been successfully applied to analyze event-related potentials (ERPs), and shown superiority in terms of capturing multi-domain features. However, the time-frequency representation of ERPs by higher-order tensors are usually large-scale, which prevents the popularity of most tensor factorization algorithms. To overcome this issue, we introduce a non-negative canonical polyadic decomposition (NCPD) based on low-rank approximation (LRA) and hierarchical alternating least square (HALS) techniques. We applied NCPD (LRAHALS and benchmark HALS) and CPD to extract multi-domain features of a visual ERP. The features and components extracted by LRAHALS NCPD and HALS NCPD were very similar, but LRAHALS NCPD was 70 times faster than HALS NCPD. Moreover, the desired multi-domain feature of the ERP by NCPD showed a significant group difference (control versus depressed participants) and a difference in emotion processing (fearful versus happy faces). This was more satisfactory than that by CPD, which revealed only a group difference.\n\nTitle: Linked Component Analysis From Matrices to High-Order Tensors: Applications to Biomedical Data\nContent: With the increasing availability of various sensor technologies, we now have access to large amounts of multiblock (also called multiset, multirelational, or multiview) data that need to be jointly analyzed to explore their latent connections. Various component analysis methods have played an increasingly important role for the analysis of such coupled data. In this article, we first provide a brief review of existing matrix-based (two-way) component analysis methods for the joint analysis of such data with a focus on biomedical applications. Then, we discuss their important extensions and generalization to multiblock multiway (tensor) data. We show how constrained multiblock tensor decomposition methods are able to extract similar or statistically dependent common features that are shared by all blocks, by incorporating the multiway nature of data. Special emphasis is given to the flexible common and individual feature analysis of multiblock data with the aim to simultaneously extract common and individual latent components with desired properties and types of diversity. Illustrative examples are given to demonstrate their effectiveness for biomedical data analysis.\n\n## Task\nCompose an abstract for the title 'Canonical Polyadic Decomposition: From 3-way to N-Way' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Canonical Polyadic (or CANDECOMP/PARAFAC, CP) decompositions are widely applied to analyze high order data, i.e. N-way tensors. Existing CP decomposition methods use alternating least square (ALS) iterations and hence need to compute the inverse of matrices and unfold tensors frequently, which are very time consuming for large-scale data and when N is large. Fortunately, once at least one factor has been correctly estimated, all the remaining factors can be computed efficiently and uniquely by using a series of rank-one approximations. Motivated by this fact, to perform a full N-way CP decomposition, we run 3-way CP decompositions on a sub-tensor to estimate two factors first. Then the remaining factors are estimated via an efficient Khatri-Rao product recovering procedure. In this way the whole ALS iterations with respect to each mode are avoided and the efficiency can be significantly improved. Simulations show that, compared with ALS based CP decomposition methods, the proposed method is more efficient and is easier to escape from local solutions for high order tensors."
      }
    ]
  },
  {
    "tid":"114",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Investigating Bell Inequalities For Multidimensional Relevance Judgments In Information Retrieval\nContent: Relevance judgment in Information Retrieval is influenced by multiple factors. These include not only the topicality of the documents but also other user oriented factors like trust, user interest, etc. Recent works have identified and classified these various factors into seven dimensions of relevance. In a previous work, these relevance dimensions were quantified and user's cognitive state with respect to a document was represented as a state vector in a Hilbert Space, with each relevance dimension representing a basis. It was observed that relevance dimensions are incompatible in some documents, when making a judgment. Incompatibility being a fundamental feature of Quantum Theory, this motivated us to test the Quantum nature of relevance judgments using Bell type inequalities. However, none of the Bell-type inequalities tested have shown any violation. We discuss our methodology to construct incompatible basis for documents from real world query log data, the experiments to test Bell inequalities on this dataset and possible reasons for the lack of violation.\n\nTitle: Bias-variance analysis in estimating true query model for information retrieval\nContent: The estimation of query model is an important task in language modeling (LM) approaches to information retrieval (IR). The ideal estimation is expected to be not only effective in terms of high mean retrieval performance over all queries, but also stable in terms of low variance of retrieval performance across different queries. In practice, however, improving effectiveness can sacrifice stability, and vice versa. In this paper, we propose to study this tradeoff from a new perspective, i.e., the bias-variance tradeoff, which is a fundamental theory in statistics. We formulate the notion of bias-variance regarding retrieval performance and estimation quality of query models. We then investigate several estimated query models, by analyzing when and why the bias-variance tradeoff will occur, and how the bias and variance can be reduced simultaneously. A series of experiments on four TREC collections have been conducted to systematically evaluate our bias-variance analysis. Our approach and results will potentially form an analysis framework and a novel evaluation strategy for query language modeling.\n\nTitle: Learning To Improve Affinity Ranking For Diversity Search\nContent: Search diversification plays an important role in modern search engine, especially when user-issued queries are ambiguous and the top ranked results are redundant. Some diversity search approaches have been proposed for reducing the information redundancy of the retrieved results, while do not consider the topic coverage maximization. To solve this problem, the Affinity ranking model has been developed aiming at maximizing the topic coverage meanwhile reducing the information redundancy. However, the original model does not involve a learning algorithm for parameter tuning, thus limits the performance optimization. In order to further improve the diversity performance of Affinity ranking model, inspired by its ranking principle, we propose a learning approach based on the learning-to-rank framework. Our learning model not only considers the topic coverage maximization and redundancy reduction by formalizing a series of features, but also optimizes the diversity metric by extending a well-known learning-to-rank algorithm LambdaMART. Comparative experiments have been conducted on TREC diversity tracks, which show the effectiveness of our model.\n\nTitle: Approximating true relevance distribution from a mixture model based on irrelevance data\nContent: Pseudo relevance feedback (PRF), which has been widely applied in IR, aims to derive a distribution from the top n pseudo relevant documents D. However, these documents are often a mixture of relevant and irrelevant documents. As a result, the derived distribution is actually a mixture model, which has long been limiting the performance of PRF. This is particularly the case when we deal with difficult queries where the truly relevant documents in D are very sparse. In this situation, it is often easier to identify a small number of seed irrelevant documents, which can form a seed irrelevant distribution. Then, a fundamental and challenging problem arises: solely based on the mixed distribution and a seed irrelevance distribution, how to automatically generate an optimal approximation of the true relevance distribution? In this paper, we propose a novel distribution separation model (DSM) to tackle this problem. Theoretical justifications of the proposed algorithm are given. Evaluation results from our extensive simulated experiments on several large scale TREC data sets demonstrate the effectiveness of our method, which outperforms a well respected PRF Model, the Relevance Model (RM), as well as the use of RM on D with the seed negative documents directly removed.\n\nTitle: Investigating Order Effects in Multidimensional Relevance Judgment using Query Logs.\nContent: There is a growing body of research which has investigated relevance judgment in IR being influenced by multiple factors or dimensions. At the same time, the Order Effects in sequential decision making have been quantitatively detected and studied in Mathematical Psychology. Combining the two phenomena, there have been some user studies carried out which investigate the Order Effects and thus incompatibility in different dimensions of relevance. In this work, we propose a methodology for carrying out such an investigation in large scale and real world data using query logs of a web search engine, and device a test to detect the presence of an irrational user behavior in relevance judgment of documents. We further validate this behavior through a Quantum Cognitive explanation of the Order and Context effects.\n\n## Task\nCompose an abstract for the title 'Extending the Balance Theory by Measuring Bidirectional Opinions with Interactive Language.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The signs of social relationship in balance theory is objective and undirected which cannot characterize participants' bidirectional subjective opinions on social relationships. By redefining the signs with the bidirectional opinions according to homogeneity principle, we extend the balance theory by measuring the opinions with interactive language. Initial experimental results on Enron emails shows that the extended model supports the homogeneity principle of balance theory and make it possible to use balance theory to predict the directed opinions besides undirected signs."
      }
    ]
  },
  {
    "tid":"115",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Toolkit for Design and Performance Analysis of Real-Time Component-Based Software Systems\nContent: Software tools supporting the design and analysis of complex software-intensive systems are highly desirable, since they enable earlier decision making about system realization. This paper presents a tooling environment that supports the design and performance analysis of time-critical component-based software architectures deployed on complex multiprocessor platforms. The tooling environment contains a set of integrated tools for (a) component storage and retrieval, (b) graphics-based design of software and hardware architectures, (c) performance analysis of the defined architectures and, (d) automated code generation. The cornerstone of the toolkit is a performance analyzer that provides efficient simulation of the designed architectures and enables design-time prediction of key performance properties like response time, data throughout, and usage of hardware resources (processor, memory and bus). For every architecture alternative, the performance predictions can be quickly obtained, thereby enabling a fast and yet broad design space exploration. We demonstrate the efficiency and robustness of this toolkit on a Car Radio Navigation benchmark case.\n\nTitle: Compositional Performance Analysis of Component-Based Systems on Heterogeneous Multiprocessor Platforms\nContent: This paper presents a compositional performance analysis technique, enabling predictable deployment of software components on heterogeneous multiprocessor architectures. This analysis technique introduces (a) composable software and hardware component models representing abstract specification of the component behaviour and corresponding resources, (b) operational semantics enabling composition of the models into an executable system model, and (c) simulation-based analysis of the obtained executable model resulting in predicted performance attributes. Example attributes are response time, throughput, utilization of processors, memory and communication lines. Special attention is paid to modeling of both passive and active components exploiting synchronous method invocation and asynchronous message passing interaction. We made an experimental validation of the above framework for two case studies: an MPEG-4 decoder and a car navigation system. It was found that the prediction error on task latencies and processor usage was within 10%.\n\nTitle: Low-Level Profiling and MARTE-Compatible Modeling of Software Components for Real-Time Systems\nContent: In this paper, we present a method for (a) profiling of individual components at high accuracy level, (b) modeling of the components with the accurate data obtained from profiling, and (c) model conversion to the MARTE profile. The resulting performance models of individual components are used at the component composition (design) phases for detailed evaluation of the performance of the designed system. Furthermore, the profiled models serve as a valid source for architecture optimization of the composed system. The presented method is a constituent part of our complete Design Space Exploration (DSE) methodology [1, 2], which involves modeling of individual components, component composition, performance analysis of the designed composition, and architecture optimization. The contribution of this new profiling method is attractive in various ways: (a) The profiling is fast and detailed, which leads to accurate models, (b) it is generic, since it allows multiplatform execution and (c) it can be used by MARTE-based analysis tools, due to the model compatibility. Our discussed experiment has resulted in cycle-accurate models in less than 1 hour profiling effort per component.\n\nTitle: ProMARTES: Accurate network and computation delay prediction for component-based distributed systems.\nContent: This paper proposes a cycle-accurate performance analysis method for real-time component-based distributed systems (CB-RTDS). The method involves the following phases: (a) profiling SW components at cycle execution level and modeling the obtained performance measurements in MARTE-compatible component resource models, (b) guided composition of the system architecture from available SW and HW components, (c) automated generation of a system model, specifying both computation and network loads, and (d) performance analysis (scheduling, simulation and network analysis) of the composed system model. The method is demonstrated for a real-world case study of 3 autonomously navigating robots with advanced sensing capabilities. The case study is challenging because of the SW/HW mapping, real-time requirements and data synchronization among multiple nodes. This case-study proved that, thanks to the adopted low-level performance metrics, we are able to obtain accurate performance predictions of both computation and network delays. Moreover, the combination of analytical and simulation analysis methods enables the computation of both the guaranteed Worst Case Execution Time (WCET) and the detailed execution time-line data for real-time tasks. As a result, the analysis yields the identification of an optimal architecture, with respect to real-time deadlines, robustness and system costs. The paper main contributions are the cycle-accurate performance analysis workflow and supportive open-source ProMARTES tool-chain, both incorporating a network prediction model in all the performance analysis phases.\n\nTitle: Guided Rule-Based Multi-objective Optimization for Real-Time Distributed Systems\nContent: Automated optimization of real-time architectures with respect to cost, performance, robustness and safety has received considerable attention in the last decade. In this paper, we present an automated Design Space Exploration (DSE)method based on both a multi-objective genetic algorithm and a heuristic particle-swarm-optimization technique. The optimization process is guided to desired solutions by weight coefficients that are assigned to the system objectives. The proposed method automatically generates architecture alternatives by changing hardware topology and mapping the tasks on different nodes, CPUs and by modifying their execution priority. Based on multiple quality objectives, the optimization method concludes to the Pareto-optimal solution set of the architecture alternatives. Moreover, in this paper we present an addition to the pre-existing optimization heuristics, targeting the reduction of the exploration time and maintaining a high-quality Pareto-optimal solution set. Finally, we compare the NSGA-II algorithm against the OMOPSO and their \\\"Rule-Based Initial Population versions\\\", for the convergence speed and the quality of their solutions, by comparing the hyper volume and the epsilon quality indicators. The proposed DSE approach has been applied to an autonomously navigating robot system consisting of several processing nodes (real-time distributed system) and resulting into better optimized and balanced solutions when compared to the proposed system architecture by an architect specialist.\n\n## Task\nCompose an abstract for the title 'CARAT: a toolkit for design and performance analysis of component-based embedded systems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Solid frameworks and toolkits for design and analysis of embedded systems are of high importance, since they enable early reasoning about critical properties of a system. This paper presents a software toolkit that supports the design and performance analysis of real-time component-based software architectures deployed on heterogeneous multi-processor platforms. The tooling environment contains a set of integrated tools for (a) component storage and retrieval, (b) graphics-based design of software and hardware architectures, (c) performance analysis of the designed architectures and, (d) automated code generation. The cornerstone of the toolkit is a performance analysis framework that automates composition of the individual component models into a system executable model, allows simulation of the system model and gives design-time predictions of key performance properties like response time, data throughput, and usage of hardware resources. We illustrate the efficiency of this toolkit on a Car Radio Navigation benchmark system."
      }
    ]
  },
  {
    "tid":"116",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Towards The Evaluation of OSS Trustworthiness: Lessons Learned From The Observation of Relevant OSS Projects\nContent: To facilitate the adoption of open-source software (OSS) in industry, it is important to provide potential users (i.e., those who could decide to adopt OSS) with the means for evaluating the trustworthiness of OS products. This paper presents part of the work done in the QualiPSo project for this purpose. A set of factors that are believed to affect the perception of trustworthiness are introduced. In order to test the feasibility of deriving a correct, complete and reliable evaluation or trustworthiness on the basis of these factors, a set of well-known OSS projects have been chosen. Then, the possibility to assess the proposed factors on each project was verified: not all the factors appear to be observable or measurable. The paper reports what information is available to support the evaluation and what is not. This knowledge is considered to be useful to users, who are warned that there are still dark areas in the characterization of OSS products, and to developers, who should provide more data and characteristics on their products in order to support their adoption.\n\nTitle: OpenBQR: a framework for the assessment of OSS\nContent: People and organizations that are considering the adoption of OSS, or that need to choose among different OS products face the problem of evaluating OSS in a systematic, sound and complete way. While several proposals concerning the evaluation of costs and benefits exist, little attention has been given to the evaluation of technical qualities and, in general, to the \"usage-oriented\" issues. In this paper the existing proposals are examined, the different types of qualities and issues that are relevant to potential users are described, and a coherent and innovative method for the evaluation of OSS is proposed. The proposed method is expected to support the potential user in the evaluation and choice of OSS in a flexible way, taking into account all the aspects that are relevant to the user.\n\nTitle: Beyond Total Cost of Ownership: Applying Balanced Scorecards to Open-Source Software\nContent: Potential users of Open Source Software (OSS) face the problem of evaluating OSS, in order to assess the convenience of adopting OSS instead of commercial software, or to choose among different OSS proposals. Different metrics were defined, addressing different OSS properties: the Total Cost of Ownership (TCO) addresses the cost of acquiring, adapting and operating OSS; the Total Account Ownership (TAO) represents the degree of freedom of the user with respect to the technology provider; indexes like the Open Business Quality Rating (Open BQR) assess the quality of the software with respect to the user's needs. However, none of the proposed methods and models addresses all the aspects of OSS in a balanced and complete way. For this purpose, the paper explores the possibility of adapting the Balanced Scorecard (BSC) technique to OSS. A preliminary definition of the BSC for OSS is given and discussed.\n\nTitle: An Investigation of the Users' Perception of OSS Quality\nContent: The quality of Open Source Software (OSS) is generally much debated. Some state that it is generally higher than closed-source counterparts, while others are more skeptical. The authors have collected the opinions of the users concerning the quality of 44 OSS products in a systematic manner, so that it is now possible to present the actual opinions of real users about the quality of OSS products. Among the results reported in the paper are: the distribution of trustworthiness of OSS based on our survey; a comparison of the trustworthiness of the surveyed products with respect to both open and closed-source competitors; the identification of the qualities that affect the perception of trustworthiness, based on rigorous statistical analysis.\n\nTitle: An experience in introducing configuration management in a process-centered software engineering environment\nContent: Configuration Management (CM) is widely recognized as one of the key practices in software development, specially as far as the development of big and complex software systems is concerned. Therefore, tools supporting CM have been constantly evolving, and are now sophisticated software products, providing many features, including the possibility to model-to some extent-the work-flow concerning the configuration items. Process-Centered Software Engineering Environments (PSEEs) aim at providing development environments where production activities are carried out according to a given model of the software process. Since CM has a fundamental role in any software process, it is interesting to understand to what extent a PSEE can be used to support a process model which includes CM issues, and to compare the PSEE-based environments with state-of-the-art commercial CM tools. The work described here is a first step towards this goal, concerning the development of a specific CM environment (featuring a subset of the functionalities provided by CCC/Harvest) based on the SPADE PSEE. The results reported can however be generalized to several CM tools and PSEEs.\n\n## Task\nCompose an abstract for the title 'Applying SCRUM in an OSS Development Process: An Empirical Evaluation' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Open Source Software development often resembles Agile models. In this paper, we report about our experience in using SCRUM for the development of an Open Source Software Java tool. With this work, we aim at answering the following research questions: 1) is it possible to switch successfully to the SCRUM methodology in an ongoing Open Source Software development process? 2) is it possible to apply SCRUM when the developers are geographically distributed? 3) does SCRUM help improve the quality of the product and the productivity of the process? We answer to these questions by identifying a set of measures and by comparing the data we collected before and after the introduction of SCRUM. The results seem to show that SCRUM can be introduced and used in an ongoing geographically distributed Open Source Software process and that it helps control the development process better."
      }
    ]
  },
  {
    "tid":"117",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Monkey-in-the-browser: malware and vulnerabilities in augmented browsing script markets\nContent: With the constant migration of applications from the desktop to the web, power users have found ways of enhancing web applications, at the client-side, according to their needs. In this paper, we investigate this phenomenon by focusing on the popular Greasemonkey extension which enables users to write scripts that arbitrarily change the content of any page, allowing them to remove unwanted features from web applications, or add additional, desired features to them. The creation of script markets, on which these scripts are often shared, extends the standard web security model with two new actors, introducing novel vulnerabilities. We describe the architecture of Greasemonkey and perform a large-scale analysis of the most popular, community-driven, script market for Greasemonkey. Through our analysis, we discover not only dozens of malicious scripts waiting to be installed by users, but thousands of benign scripts with vulnerabilities that could be abused by attackers. In 58 cases, the vulnerabilities are so severe, that they can be used to bypass the Same-Origin Policy of the user's browser and steal sensitive user-data from all sites. We verify the practicality of our attacks, by developing a proof-of-concept exploit against a vulnerable user script with an installation base of 1.2 million users, equivalent to a \\\"Man-in-the-browser\\\" attack.\n\nTitle: Stranger danger: exploring the ecosystem of ad-based URL shortening services\nContent: URL shortening services facilitate the need of exchanging long URLs using limited space, by creating compact URL aliases that redirect users to the original URLs when followed. Some of these services show advertisements (ads) to link-clicking users and pay a commission of their advertising earnings to link-shortening users. In this paper, we investigate the ecosystem of these increasingly popular ad-based URL shortening services. Even though traditional URL shortening services have been thoroughly investigated in previous research, we argue that, due to the monetary incentives and the presence of third-party advertising networks, ad-based URL shortening services and their users are exposed to more hazards than traditional shortening services. By analyzing the services themselves, the advertisers involved, and their users, we uncover a series of issues that are actively exploited by malicious advertisers and endanger the users. Moreover, next to documenting the ongoing abuse, we suggest a series of defense mechanisms that services and users can adopt to protect themselves.\n\nTitle: Exploring the ecosystem of referrer-anonymizing services\nContent: The constant expansion of the World Wide Web allows users to enjoy a wide range of products and services delivered directly to their browsers. At the same time however, this expansion of functionality is usually coupled with more ways of attacking a user's security and privacy. In this arms race, certain web-services present themselves as privacy-preserving or privacy-enhancing. One type of such services is a Referrer-Anonymizing Service (RAS), a service which relays users from a source site to a destination site while scrubbing the contents of the referrer header from user requests. In this paper, we investigate the ecosystem of RASs and how they interact with web-site administrators and visiting users. We discuss their workings, what happens behind the scenes and how top Internet sites react to traffic relayed through such services. In addition, we present user statistics from our own Referrer-Anonymizing Service and show the leakage of private information by others towards advertising agencies as well as towards \u2018curious' RAS owners.\n\nTitle: XHOUND: Quantifying the Fingerprintability of Browser Extensions\nContent: In recent years, researchers have shown that unwanted web tracking is on the rise, as advertisers are trying to capitalize on users' online activity, using increasingly intrusive and sophisticated techniques. Among these, browser fingerprinting has received the most attention since it allows trackers to uniquely identify users despite the clearing of cookies and the use of a browser's private mode. In this paper, we investigate and quantify the fingerprintability of browser extensions, such as, AdBlock and Ghostery. We show that an extension's organic activity in a page's DOM can be used to infer its presence, and develop XHound, the first fully automated system for fingerprinting browser extensions. By applying XHound to the 10,000 most popular Google Chrome extensions, we find that a significant fraction of popular browser extensions are fingerprintable and could thus be used to supplement existing fingerprinting methods. Moreover, by surveying the installed extensions of 854 users, we discover that many users tend to install different sets of fingerprintable browser extensions and could thus be uniquely, or near-uniquely identifiable by extension-based fingerprinting. We use XHound's results to build a proof-of-concept extension-fingerprinting script and show that trackers can fingerprint tens of extensions in just a few seconds. Finally, we describe why the fingerprinting of extensions is more intrusive than the fingerprinting of other browser and system properties, and sketch two different approaches towards defending against extension-based fingerprinting.\n\nTitle: Extended Tracking Powers: Measuring the Privacy Diffusion Enabled by Browser Extensions.\nContent: Users have come to rely on browser extensions to realize features that are not implemented by browser vendors. Extensions offer users the ability to, among others, block ads, de-clutter websites, enrich pages with third-party content, and take screenshots. At the same time, because of their privileged position inside a user's browser, extensions have access to content and functionality that is not available to webpages, such as, the ability to conduct and read cross-origin requests, as well as get access to a browser's history and cookie jar. In this paper, we report on the first large-scale study of privacy leakage enabled by extensions. By using dynamic analysis and simulated user interactions, we investigate the leaking happening by the 10,000 most popular browser extensions of Google Chrome and find that a non-negligible fraction leaks sensitive information about the user's browsing habits, such as, their browsing history and search-engine queries. We identify common ways that extensions use to obfuscate this leakage and discover that, while some leakage happens on purpose, a large fraction of it is accidental because of the way that extensions attempt to introduce third-party content to a page's DOM. To counter the inference of a user's interests and private information enabled by this leakage, we design, implement, and evaluate BrowsingFog, a browser extension that automatically browses the web in a way that conceals a user's true interests, from a vantage point of history-stealing, third-party trackers.\n\n## Task\nCompose an abstract for the title 'Cookieless Monster: Exploring the Ecosystem of Web-Based Device Fingerprinting' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The web has become an essential part of our society and is currently the main medium of information delivery. Billions of users browse the web on a daily basis, and there are single websites that have reached over one billion user accounts. In this environment, the ability to track users and their online habits can be very lucrative for advertising companies, yet very intrusive for the privacy of users. In this paper, we examine how web-based device fingerprinting currently works on the Internet. By analyzing the code of three popular browser-fingerprinting code providers, we reveal the techniques that allow websites to track users without the need of client-side identifiers. Among these techniques, we show how current commercial fingerprinting approaches use questionable practices, such as the circumvention of HTTP proxies to discover a user's real IP address and the installation of intrusive browser plugins. At the same time, we show how fragile the browser ecosystem is against fingerprinting through the use of novel browser-identifying techniques. With so many different vendors involved in browser development, we demonstrate how one can use diversions in the browsers' implementation to distinguish successfully not only the browser-family, but also specific major and minor versions. Browser extensions that help users spoof the user-agent of their browsers are also evaluated. We show that current commercial approaches can bypass the extensions, and, in addition, take advantage of their shortcomings by using them as additional fingerprinting features."
      }
    ]
  },
  {
    "tid":"118",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Estimating Relative Camera Motion from the Antipodal-Epipolar Constraint\nContent: This paper introduces a novel antipodal-epipolar constraint on relative camera motion. By using antipodal points, which are available in large Field-of-View cameras, the translational and rotational motions of a camera are geometrically decoupled, allowing them to be separately estimated as two problems in smaller dimensions. We present a new formulation based on discrete camera motions, which works over a larger range of motions compared to previous differential techniques using antipodal points. The use of our constraints is demonstrated with two robust and practical algorithms, one based on RANSAC and the other based on Hough-like voting. As an application of the motion decoupling property, we also present a new structure-from-motion algorithm that does not require explicitly estimating rotation (it uses only the translation found with our methods). Finally, experiments involving simulations and real image sequences will demonstrate that our algorithms perform accurately and robustly, with some advantages over the state-of-the-art.\n\nTitle: Comparison of temporal filters for optical flow estimation in continuous mobile robot navigation\nContent: We present our complete study involving comparisons of three spatio-temporal filters used in the estimation of optical flow for continuous mobile robot navigation. Previous comparisons of optical flow and associated techniques have compared performance in terms of accuracy and/or efficiency, and only in isolation. These comparisons are inadequate for addressing applicability to continuous, real-time operation as part of a robot control loop. In recent work [11], we presented a comparison of optical flow techniques for corridor navigation through two biologically inspired behaviours: corridor centring and visual odometry. In that study, flow was predominantly constant. In this paper we give new results from comparisons for flow-divergence based docking, where flow is non-constant. Results for traditionally used Gaussian filters indicate that long latencies significantly impede performance for real-time tasks in the control loop.\n\nTitle: Towards log-polar fixation for mobile robots - analysis of corner tracking on the log-polar camera\nContent: Fixating on objects is fundamental to active vision tasks, such as reaching, navigation and docking. Most techniques generally have been designed for space-invariant cameras. This research proposes a new method for corner tracking to facilitate point fixation for a mobile robot using a foveated camera. When the target point is in the centre of the image, the fovea and its position can be accurately tracked at high resolution. At the same time, the periphery has a reduced pixel count thus reducing the image processing computation compared to a uniform camera with the same field of view. If the target point suddenly moves into the periphery, it still appears in the lower resolution part of the image and coarser control can bring it back into fovea. Our experiment results demonstrate the stability of the proposed method and the performance of our implementation is adequate for real-time tracking applications.\n\nTitle: Performance of optical flow techniques for indoor navigation with a mobile robot\nContent: We present a comparison of four optical flow methods and three spatio-temporal filters for mobile robot navigation in corridor-like environments. Previous comparisons of optical flow methods have evaluated performance only in terms of accuracy and/or efficiency, and typically in isolation. These comparisons are inadequate for addressing applicability to continuous, real-time operation as part of a robot control loop. We emphasise the need for comparisons that consider the context of a system, and that are confirmed by in-system results. To this end, we give results for on and off-board trials of two biologically inspired behaviours: corridor centring and visual odometry. Our results show the best in-system performances are achieved using Lucas and Kanade's gradient-based method in combination with a recursive temporal filter. Results for traditionally used Gaussian filters indicate that long latencies significantly impede performance for real-time tasks in the control loop. choice of technique for mobile robot navigation. Gradient- based methods use spatio-temporal image intensity derivatives, and thus require spatio-temporal filters to pre-smooth the image signal. Temporal filters differ in required frame support, accuracy, and computation times, highlighting the importance of their inclusion in optical flow comparisons. Previous studies have not directly addressed in-system issues or the choice of temporal filters for gradient-based methods. We emphasise the need for comparisons of vision techniques that consider the context of the system. In this paper we present a comparison of optical flow methods and temporal filters for mobile robot navigation. We focus only on behaviours involving continuous motion such as corridor centring and visual odometry. Continuous motion is characterised by optical flow that is significantly larger than the frame-to-frame changes in the flow on average. This excludes behaviours such as station keeping, where frame-by-frame changes may be larger in comparison with flow magnitude. We aim to provide insight and recommendations for the choice of flow techniques for robot navigation with continuous motion. Three gradient-based methods are included: Lucas and Kanade (8), Horn and Schunck (5), and Nagel (9), as well as Camus' correlation-based method (2). Three temporal filters are also included: Gaussian filtering, Simoncelli's pre- smoothing and derivative filters (12), and Fleet and Langley's recursive temporal filter (3). We give an overview and theo- retical comparison of these techniques before setting out our methodology for comparison. We then present results from on and off-board comparisons before giving our conclusions.\n\nTitle: Real Time Biologically-Inspired Depth Maps From Spherical Flow\nContent: We present a strategy for generating real-time relative depth maps of an environment from optical flow, under general motion. We achieve this using an insect-inspired hemispherical fish-eye sensor with 190 degree FOV, and a derotated optical flow field. The de-rotation algorithm applied is based on the theoretical work of Nelson and Aloimonos [10]. From this we obtain the translational component of motion, and construct full relative depth maps on the sphere. We examine the robustness of this strategy in both simulation and realworld experiments, for a variety of environmental scenarios. To our knowledge, this is the first demonstrated implementation of the Nelson and Aloimonos algorithm working in real-time, over real image sequences. In addition, we apply this algorithm to the real-time recovery of full relative depth maps. These preliminary results demonstrate the feasibility of this approach for closed-loop control of a robot.\n\n## Task\nCompose an abstract for the title 'Estimation of the epipole using optical flow at antipodal points' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper develops an algorithm for estimating the epipole or direction of translation of a moving monocular observer. To this end, we use constraints arising from two points that are antipodal on the image sphere. The antipodal point condition is necessary for decoupling rotation from translation. One such pair of points constrains the epipole to lie on a plane, and using two pairs of points, we have two such planes. The intersection of these two planes gives an estimate of the epipole. This means we require image motion measurements at two pairs of antipodal points to obtain an estimate. Repeating this will yield a set of possible solutions and a variety of methods could be applied to obtain a robust and refined estimate from this set. One robust and simple method is chosen for illustrative purposes and results on real images are shown. With real sequences, results of below 2deg error in the estimate of the epipole can be obtained. Since antipodal points on an image sphere are required, this algorithm must use some kind of omnidirectional or large field-of-view (FOV) sensor."
      }
    ]
  },
  {
    "tid":"119",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Decision tree based hardware power monitoring for run time dynamic power management in FPGA\nContent: Fine-grained runtime power management techniques could be promising solutions for power reduction. Therefore, it is essential to establish accurate power monitoring schemes to obtain dynamic power variation in a short period (i.e., tens or hundreds of clock cycles). In this paper, we leverage a decision-tree-based power modeling approach to establish fine-grained hardware power monitoring on FPGA platforms. A generic and complete design flow is developed to implement the decision tree power model which is capable of precisely estimating dynamic power in a fine-grained manner. A flexible architecture of the hardware power monitoring is proposed, which can be instrumented in any RTL design for runtime power estimation, dispensing with the need for extra power measurement devices. Experimental results of applying the proposed model to benchmarks with different resource types reveal an average error up to 4% for dynamic power estimation. Moreover, the overheads of area, power and performance incurred by the power monitoring circuitry are extremely low. Finally, we apply our power monitoring technique to the power management using phase shedding with an on-chip multi-phase regulator as a proof of concept and the results demonstrate 14% efficiency enhancement for the power supply of the FPGA internal logic.\n\nTitle: FDR 2.0: A Low-Power Dynamically Reconfigurable Architecture and Its FinFET Implementation\nContent: Large area/delay/power overheads are required to support the reconfigurability of field-programmable gate arrays (FPGAs). We proposed a hybrid CMOS/nanotechnology dynamically reconfigurable architecture, called NATURE, earlier to address this challenge. It uses the concept of temporal logic folding and fine-grain (i.e., cycle-level) dynamic reconfiguration to increase logic density and save area. Because logic folding reduces area significantly, most of the on-chip communications become localized. To take full advantage of localized communications, we then presented a new CMOS-based fine-grain dynamically reconfigurable (FDR) architecture. It consists of an array of homogeneous logic elements (LEs), which can be configured into logic or interconnect or a combination of both. FDR eliminates most of the long-distance and global wires, which occupy a large amount of area in conventional FPGAs. FDR improves the area-delay product by an order of magnitude relative to conventional architectures. In this paper, we present an augmented FDR 2.0 architecture, where: 1) the LE is augmented with dedicated carry logic to facilitate arithmetic operations; 2) diagonal direct links are incorporated to improve the flexibility of local communication; and 3) coarse-grain blocks, including embedded memories and digital signal processing (DSP) blocks, are added to support fast data-intensive computations. Experimental results show that the coarse-grain design can improve circuit performance by compared with the fine-grain FDR architecture. Incorporation of the DSP blocks in FDR 2.0 also enables more effective area-delay and power-delay tradeoffs, allowing the users to trade performance for smaller area or power consumption. We have implemented the design in the 22-nm FinFET technology, which enables more flexible and effective power management. Finally, different types of FinFETs and power management techniques have been explored in FDR 2.0 - o optimize power.\n\nTitle: A hybrid Nano/CMOS dynamically reconfigurable system\u2014Part II: Design optimization flow\nContent: In Part I of this work, a hybrid nano/CMOS reconfigurable architecture, called NATURE, was described. It is composed of CMOS reconfigurable logic and interconnect fabric, and nonvolatile nano on-chip memory. Through its support for cycle-by-cycle runtime reconfiguration and a highly-efficient computation model, temporal logic folding, NATURE improves logic density and area-delay product by more than an order of magnitude compared to existing CMOS-based field-programmable gate arrays (FPGAs). NATURE can be fabricated using mainstream photo-lithography fabrication techniques. Thus, it offers a currently commercially feasible architecture with high performance, superior logic density, and excellent runtime design flexibility. In Part II of this work, we present an integrated design and optimization flow for NATURE, called NanoMap. Given an input design specified in register-transfer level (RTL) and/or gate-level VHDL, NanoMap optimizes and implements the design on NATURE through logic mapping, temporal clustering, temporal placement, and routing. As opposed to other design tools for traditional FPGAs, NanoMap supports and leverages temporal logic folding by integrating novel mapping techniques. It can automatically explore and identify the best temporal logic folding configuration, targeting area, delay or area-delay product optimization. A force-directed scheduling technique is used to optimize and balance resource usage across different folding cycles. By supporting logic folding, NanoMap can provide significant design flexibility in performing area-delay trade-offs under various user-specified constraints. We present details of the mapping procedure and results for different architectural instances. Experimental results demonstrate that NanoMap can judiciously trade off area and delay targeting different optimization goals, and effectively exploit the advantages of NATURE. Part I of this work will appear in JETC Vol. 5, No. 4.\n\nTitle: Low-power 3D nano/CMOS hybrid dynamically reconfigurable architecture\nContent: In order to continue technology scaling beyond CMOS, diverse nanoarchitectures have been proposed in recent years based on emerging nanodevices, such as nanotubes, nanowires, etc. Among them, some hybrid nano/CMOS reconfigurable architectures enjoy the advantage that they can be fabricated using photolithography. NATURE is one such architecture that we have proposed recently. It comprises CMOS reconfigurable logic and CMOS fabrication-compatible nano RAMs. It uses distributed high-density and fast nano RAMs as on-chip storage for storing multiple reconfiguration copies, enabling fine-grain cycle-by-cycle reconfiguration. It supports a highly efficient computational model, called temporal logic folding, which makes possible more than an order of magnitude improvement in logic density and area-delay product, significant power reduction, and significant design flexibility in performing area-delay trade-offs. In this article, we extend NATURE in various dimensions, evaluating various FPGA approaches in the context of today's emerging technologies. First, we explore the introduction of embedded coarse-grain modules in the fine-grain NATURE architecture and present a unified dynamically reconfigurable architecture, which can significantly enhance NATURE's computation power for data-dominated applications. Second, we explore a 3D architecture for NATURE in which the nano RAM for reconfiguration storage is on one layer and the rest of the CMOS logic on another layer. This leads to further improvements in logic density and performance. Finally, we explore the possibility of using FinFETs, an emerging double-gate CMOS technology, to implement NATURE. Since power consumption is an important consideration in the deep nanometer regime, especially for FPGAs, we present a back-gate biasing methodology for flexible threshold voltage adjustment in FinFETs to significantly reduce NATURE's power consumption. Simulation results demonstrate the efficacy of the proposed methods.\n\nTitle: An Efficient Technique for Chip Temperature Optimization of Multiprocessor Systems in the Dark Silicon Era\nContent: In the dark silicon era, a fundamental problem is: given a real-time computation demand represented by a set of independent applications with their own power consumption, how to determine if an on-chip multiprocessor system is able to respond to this demand and maintain its reliability by keeping every core within the safe temperature range. In this paper, we first present a novel thermal model for the prediction of chip peak temperature assuming the application-to-core mapping is determined. The mathematical model combines linearized steady-state thermal model with empirical scaling factors to achieve significantly improved accuracy and running efficiency. Based on it, a MILP-based approach is presented to find the optimal application-to-core assignment such that the computation demand is met and the chip temperature is minimized. At last, if the minimized temperature still exceeds the safe temperature threshold, a novel heuristic algorithm, called temperature threshold-aware result handling (TTRH), is proposed to drop certain applications selectively from immediate execution, and lower the chip peak temperature to the safety threshold. Extensive performance evaluation shows that the MILP-based approach can reduce the chip peak temperature by 9.1 C on average compared to traditional techniques. TTRH algorithm can further lower the chip peak temperature by 1.38\u00b0 C on average with the application dropping rate of less than 4.35%.\n\n## Task\nCompose an abstract for the title 'Fine-grained dynamic voltage scaling on OLED display' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Organic Light Emitting Diode (OLED) has emerged as the new generation display technique for mobile multimedia devices. Compared to existing technologies OLEDs are thinner, brighter, lighter, and cheaper. However, OLED panels are still the biggest contributor to the total power consumption of mobile devices. In this work, we proposed a fine-grained dynamic voltage scaling (FDVS) technique to reduce the OLED power. An OLED panel is partitioned into multiple display areas of which the supply voltage is adaptively adjusted based on the displayed content. A DVS-friendly OLED driver design is also proposed to enhance the color accuracy of the OLED pixels at the scaled supply voltage. Our experimental results show that compared to the existing global DVS technique, FDVS technique can achieve 25.9%~43.1% more OLED power saving while maintaining a high image quality measured by Structural Similarity Index (SSIM=0.98). The further analysis shows shat FDVS technology can also effectively reduce the color remapping cost when color compensation is required to improve the image quality of an OLED panel working at a scaled voltage."
      }
    ]
  },
  {
    "tid":"120",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Optimal annuity portfolio under inflation risk\nContent: The paper investigates the importance of inflation-linked annuities in retirement planning. Given nominal, inflation-linked, and variable annuities, as well as bonds and stocks, we search for optimal consumption and investment decisions under two different objective functions: (1) maximization of expected utility of real consumption, and (2) minimization of expected deviations from an inflation-adjusted target. When optimizing the objective, we allow for rebalancing the portfolio during retirement by buying additional annuities and by trading bonds and stocks. To find the optimal solution, we apply a multi-stage stochastic programming approach. Our findings indicate that independently of the considered objective function, real annuities are a crucial asset in every portfolio. In addition, without investing in real annuities, the retiree has to rebalance the portfolio more frequently, and still obtains a lower and more volatile real consumption.\n\nTitle: Separation and Extension of Cover Inequalities for Conic Quadratic Knapsack Constraints with Generalized Upper Bounds\nContent: Motivated by addressing probabilistic 0--1 programs we study the conic quadratic knapsack polytope with generalized upper bound GUB constraints. In particular, we investigate separating and extending GUB cover inequalities. We show that, unlike in the linear case, determining whether a cover can be extended with a single variable is NP-hard. We describe and compare a number of exact and heuristic separation and extension algorithms which make use of the structure of the constraints. Computational experiments are performed for comparing the proposed separation and extension algorithms. These experiments show that a judicious application of the extended GUB cover cuts can reduce the solution time of conic quadratic 0--1 programs with GUB constraints substantially.\n\nTitle: Budgeting with bounded multiple-choice constraints\nContent: We consider a budgeting problem where a specified number of projects from some disjoint classes has to be selected such that the overall gain is largest possible, and such that the costs of the chosen projects do not exceed a fixed upper limit. The problem has several application in government budgeting, planning, and as relaxation from other combinatorial problems. It is demonstrated that the problem can be transformed to an equivalent multiple-choice knapsack problem through dynamic programming. A naive transformation however leads to a drastic increase in the number of variables, thus we propose an algorithm for the continuous problem based on Dantzig\u2013Wolfe decomposition. A master problem solves a continuous multiple-choice knapsack problem knowing only some extreme points in each of the transformed classes. The individual subproblems find extreme points for each given direction, using a median search algorithm. An integer optimal solution is then derived by using the dynamic programming transformation to a multiple-choice knapsack problem for an expanding core. The individual classes are considered in an order given by their gradients, and the transformation to a multiple-choice knapsack problem is performed when needed. In this way, only a dozen of classes need to be transformed for standard instances from the literature. Computational experiments are presented, showing that the developed algorithm is orders of magnitude faster than a general LP/MIP algorithm.\n\nTitle: Interactive cost configuration over decision diagrams\nContent: In many AI domains such as product configuration, a user should interactively specify a solution that must satisfy a set of constraints. In such scenarios, offline compilation of feasible solutions into a tractable representation is an important approach to delivering efficient backtrack-free user interaction online. In particular, binary decision diagrams (BDDs) have been successfully used as a compilation target for product and service configuration. In this paper we discuss how to extend BDD-based configuration to scenarios involving cost functions which express user preferences. We first show that an efficient, robust and easy to implement extension is possible if the cost function is additive, and feasible solutions are represented using multi-valued decision diagrams (MDDs). We also discuss the effect on MDD size if the cost function is non-additive or if it is encoded explicitly into MDD. We then discuss interactive configuration in the presence of multiple cost functions. We prove that even in its simplest form, multiple-cost configuration is NP-hard in the input MDD. However, for solving two-cost configuration we develop a pseudo-polynomial scheme and a fully polynomial approximation scheme. The applicability of our approach is demonstrated through experiments over real-world configuration models and product-catalogue datasets. Response times are generally within a fraction of a second even for very large instances.\n\nTitle: A Base Integer Programming Model and Benchmark Suite for Liner-Shipping Network Design\nContent: AbstractThe liner-shipping network design problem is to create a set of nonsimple cyclic sailing routes for a designated fleet of container vessels that jointly transports multiple commodities. The objective is to maximize the revenue of cargo transport while minimizing the costs of operation. The potential for making cost-effective and energy-efficient liner-shipping networks using operations research OR is huge and neglected. The implementation of logistic planning tools based upon OR has enhanced performance of airlines, railways, and general transportation companies, but within the field of liner shipping, applications of OR are scarce. We believe that access to domain knowledge and data is a barrier for researchers to approach the important liner-shipping network design problem. The purpose of the benchmark suite and the paper at hand is to provide easy access to the domain and the data sources of liner shipping for OR researchers in general. We describe and analyze the liner-shipping domain applied to network design and present a rich integer programming model based on services that constitute the fixed schedule of a liner shipping company. We prove the liner-shipping network design problem to be strongly NP-hard. A benchmark suite of data instances to reflect the business structure of a global liner shipping network is presented. The design of the benchmark suite is discussed in relation to industry standards, business rules, and mathematical programming. The data are based on real-life data from the largest global liner-shipping company, Maersk Line, and supplemented by data from several industry and public stakeholders. Computational results yielding the first best known solutions for six of the seven benchmark instances is provided using a heuristic combining tabu search and heuristic column generation.\n\n## Task\nCompose an abstract for the title 'A combined stochastic programming and optimal control approach to personal finance and pensions' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We combine a dynamic programming approach (stochastic optimal control) with a multi-stage stochastic programming approach (MSP) in order to solve various problems in personal finance and pensions. Both optimization methods are integrated into one MSP formulation, making it possible to achieve a solution within a short computational time. The solution takes into account the entire lifetime of an individual, while focusing on practical constraints, such as limits on portfolio composition, limits on the sum insured, inclusion of transaction costs, and taxes on capital gains, during the first years of a contract. Two applications are considered: (A) optimal investment, consumption and sum insured for an individual maximizing the expected utility of consumption and bequest, and (B) optimal investment for a pension saver who wishes to maximize the expected utility of retirement benefits. Numerical results show that among the considered practical constraints, the presence of taxes affects the optimal controls the most. Furthermore, the individual\u2019s preferences, such as impatience level and risk aversion, have even a higher impact on the controlled processes than the taxes on capital gains."
      }
    ]
  },
  {
    "tid":"121",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Improving TCP performance in multipath packet forwarding networks.\nContent: This paper investigates schemes to improve TCP performance in multipath forwarding networks. In multipath routing, packets to the same destination are sent to multiple next-hops in either packet-level or flow-level forwarding mode. Effective bandwidth is increased since we can utilize unused capacity of multiple paths to the destination. In packet-level multipath forwarding networks, TCP performance may not be enhanced due to frequent out-of-order segment arrivals at the receiver because of different delays among paths. To overcome this problem, we propose simple TCP modifications. At the sender, the fast retransmission threshold is adjusted taking the number of paths into consideration. At the receiver, the delayed acknowledgment scheme is modified such that an acknowledgment for an out-of-order segment arrival is delayed in the same way for the in-order one. The number of unnecessary retransmissions and congestion window reductions is diminished, which is verified by extensive simulations. In flow-level multipath forwarding networks, hashing is used at routers to select outgoing link of a packet. Here, we show by simulations that TCP performance is increased in proportion to the number of paths regardless of delay differences.\n\nTitle: Optimizing Aggregate Throughput of Upstream TCP Flows over IEEE 802.11 Wireless LANs\nContent: This paper via analysis and simulation revisits the interaction between MAC contention and TCP congestion control over IEEE 802.11 WLANs, misled in the previous efforts. The results reveal that the effective number of contending wireless stations is not proportional to the number of wireless stations with an upstream TCP flow in IEEE 802.11 wireless LANs. Thus, we propose a new scheme called TCP ACK priority (TAP) in which, by allowing an access point to transmit TCP ACKs at the highest priority, the optimal number of competing stations are allowed to contend for media access to utilize link bandwidth efficiently. We use an ns-2 simulator to evaluate the performance of TAP with the IEEE 802.11 DCF. The results show that there is an improvement in network performance without the loss of fairness between upstream TCP flows. The extensions for IEEE 802.11e/n are also considered.\n\nTitle: A framework of distributed dynamic multi-radio multi-channel multi-path routing protocol in Wireless Mesh Networks\nContent: Wireless mesh networks (WMNs) have gained a lot of attention recently. Many efforts are made to design proper routing protocols for WMNs. Existing multi-radio multi-channel routing protocols utilize only one path for transmission, and some multi-path routing protocols consider only single channel situation, in which multi-path routing won't improve end-to-end throughput efficiently. In our paper, we propose a framework for distributed reactive routing protocol in WMNs, which utilizes multi-radio multi-channel technique, as well as multi-path transmission strategy. Dynamic channel assignment is used to avoid the inter-flow and intra-flow channel competition and interference. Our protocol establishes and maintains two or more channel-dimensional disjoint paths, and then every data flow is splitted into multiple paths, in order to increase the total end-to-end transmission throughput. Demo and NS2 simulations are carried out for the evaluation of the performance of our proposed protocol comparing with AODV and other related routing protocols. It is shown our proposal can increase end-to-end throughput significantly.\n\nTitle: Distributed SINR based scheduling algorithm for multi-hop wireless networks\nContent: The problem of developing high-performance distributed scheduling algorithms for multi-hop wireless networks has seen enormous interest in recent years. The problem is especially challenging when studied under a physical interference model, which requires the SINR at the receiver to be above a certain threshold for decoding success. Under such an SINR model, transmission failure may be caused by interference due to simultaneous transmissions from far away nodes, which exacerbates the difficulty in developing a distributed algorithm. In this paper, we propose a scheduling algorithm that exploits carrier sensing and show that the algorithm is not only amenable to distributed implementation, but also results in throughput optimality. Our algorithm has a feature called the \"dual-state\" approach, which separates the transmission schedules from the system state and can be shown to improve delay performance.\n\nTitle: A robust flooding algorithm in multi-radio multi-channel wireless mesh networks\nContent: Flooding, that is to deliver a packet from one node to all other nodes in the network, is an integral part of many wireless protocols. Flooding is often implemented by a series of broadcasts of each node and this causes some problems such as the broadcast storm and low reliability, by being engaged with the effects of radio signal propagation, e.g., multipath fading and interference. Many researchers have studied these problems over the years, however, most of these studies have been carried out assuming that all nodes in the network are equipped with a single radio interface and utilize only a single channel. This implies that most of the existing mechanisms that enhance the performance of flooding in wireless networks will not work in multi-radio multi-channel wireless mesh networks (MR-MC WMNs). Motivated by this, in this paper, we propose a flooding mechanism that works well in MR-MC WMNs. Our flooding mechanism, which can operate with an arbitrary number of radio interfaces and channels, increases the reliability of flooding while alleviates the broadcast storm problem, using only local information. Through a detailed simulation study, we demonstrate that our flooding mechanism improves both the reliability and the efficiency of flooding in MR-MC WMNs.\n\n## Task\nCompose an abstract for the title 'TCP improvement in multi-radio multi-channel multi-hop networks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, we seek to enhance the poor performance of original TCP in wireless multi-hop environments due to the intra-flow contention between TCP-DATA and TCP-ACK packets. Assuming multi-radio multi-channel networks, where each station is equipped with multiple radios and the same number of orthogonal channels, we propose to use virtually different paths with different channel assignments for TCP-DATA and TCP-ACK transmissions. Simulations show that TCP performance can be improved significantly since TCP-DATA and TCP-ACK paths hardly interfere with each other."
      }
    ]
  },
  {
    "tid":"122",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: An Exact Solution to Track-to-Track-Fusion at Arbitrary Communication Rates\nContent: Track-to-track fusion (T2TF) aims at combining locally preprocessed information of individual trackers at a fusion center. Particularly, such schemes are obligatory in many applications of distributed sensors because of limited communication resources. If T2TF yields equivalent results compared with a Kalman filter (KF) processing all measurements from every sensor, it might be called optimal T2TF. It is well known that this can be achieved for deterministically moving targets, or if the local sensor tracks produced at all individual scan times are available in the fusion center. However, if such a full-rate communication is not available, achieving optimal performance is a great challenge due to cross-correlations between the local tracks. In this paper, we show that track decorrelation and therefore optimal T2TF can be achieved if sensor properties such as the measurement error covariance and measurement function are known at the each tracking system. To this end, local tracks have to be modified and local optimality has to be given up. As a result, we provide a distributed Kalman-type processing scheme for maneuvering targets, which yields optimal T2TF results at arbitrarily chosen instants of time by communicating and combining the local sensor tracks. Applications can be found in tracking scenarios with dynamically changing bandwidth constraints.\n\nTitle: An Exact Solution To Track-To-Track Fusion Using Accumulated State Densities\nContent: Originally, the Accumulated State Density (ASD) has been proposed to provide an exact solution to the out-of-sequence measurement problem. To this end, the posterior of the joint density of all states accumulated over time was derived for a single sensor scenario. An exact solution for T2TF has been published as the Distributed Kalman Filter (DKF). However, the DKF is exact only if global knowledge in terms of the measurement models for all sensors are available at a local processor. This paper demonstrates that an exact solution for T2TF can also be achieved as a convex combination of local ASDs generated at each node in a distributed sensor system. This method crucially differs from the DKF, in that an exact solution is achieved without each processing platform being required to have knowledge of the global information. Therefore, this theoretical development has significant potential for achieving exact T2TF in practical problems. The resulting algorithm is called the Distributed Accumulated State Density (DASD) filter.\n\nTitle: Cluster tracking under kinematical constraints using random matrices\nContent: Collectively moving object clusters are of particular interest in certain applications and have to be tracked as separate aggregated entities consisting of an unknown number of individuals. Group targets often will not cause as many detections since there are individual targets in the group due to limited sensor resolution capabilities. In this case, tracking and data association under the 'one target, one detection' assumption are no longer applicable. In this paper, ellipsoidal object extensions are modeled by random matrices, which are treated as additional state variables to be estimated. An important aspect of GMTI tracking is the incorporation of context information into the Bayesian data processing formalism. Here we consider kinematical constraints such as road maps and sensor specific characteristics such as Doppler blindness.\n\nTitle: Multi hypothesis parameter tracking in relative time of arrival\nContent: The passive non-cooperative localization and tracking of mobile terminals in urban scenarios, called blind mobile localization (BML), is a highly demanding task which occurs for instance in safety, emergency and security applications. In BML the measurement set consists out of several multipaths which are usually parametrized by their direction of arrival (DoA) and their relative time of arrival (RToA). Clutter multipaths can occur due to obstacles like pedestrians, cars, etc. near the receiver side. If a clutter multipath is received before the first measurement of a target, i.e., if it possesses a negative RToA compared to the target related measurements, the computation of the particular likelihood function is deteriorated and thus the accuracy of any BML fusion algorithm decreases. In this paper a pre-processing of the measurement set by an application of a multi-hypothesis tracking (MOT) in the parameter space is proposed. Therefore, two extensions of the MOT-approach processing additional global clutter hypothesis are derived. Finally, a ray-tracing simulation is used to numerically assess the proposed methods for different clutter levels in terms of the optimal subpattern assignment (OSPA) metric.\n\nTitle: Low Complexity Block Processing Algorithms for Adaptive Channel Estimation in OFDM Systems\nContent: For pilot-aided channel estimation in orthogonal frequency-division multiplexing (OFDM) systems, adaptive algorithms enable tracking of a time-varying channel. Using several subcarriers for the update step in the adaptive algorithm, i.e., performing block processing, improves the robustness and convergence speed of the adaptation. To decrease the complexity of these algorithms, simplified versions are deduced as well as versions with real-valued filters. In the latter case the number of required arithmetic operations can be reduced significantly not only in the adaptive algorithm but also in the filtering process itself, which represents a dominating part regarding the complexity due to the huge amount of subcarriers of practical OFDM transmission schemes. Simulation results show that gains w.r.t. the convergence behavior can be achieved by real-valued filters compared to complex-valued filters - even in scenarios where real-valued filters are theoretically suboptimum.\n\n## Task\nCompose an abstract for the title 'Low complexity track initialization from a small set of non-invertible measurements' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Target tracking from non-invertible measurement sets, for example, incomplete spherical coordinates measured by asynchronous sensors in a sensor network, is a task of data fusion present in a lot of applications. Difficulties in tracking using extended Kalman filters lead to unstable behavior, mainly caused by poor initialization. Instead of using high complexity numerical batch-estimators, we offer an analytical approach to initialize the filter from a minimum number of observations. This directly pertains to multihypothesis tracking (MHT), where in the presence of clutter and/or multiple targets (i) low complexity algorithms are desirable and (ii) using a small set of measurements avoids the combinatorial explosion. Our approach uses no numerical optimization, simply evaluating several equations to find the state estimates. This is possible since we avoid an over-determined setup by initializing only from the minimum necessary subset of measurements. Loss in accuracy is minimized by choosing the best subset using an optimality criterion and incorporating the leftover measurements afterwards. Additionally, we provide the possibility to estimate only sub-sets of parameters, and to reliably model the resulting added uncertainties by the covariance matrix. We compare two different implementations, differing in the approximation of the posterior: linearizing the measurement equation as in the extended Kalman filter (EKF) or employing the unscented transform (UT). The approach will be studied in two practical examples: 3D track initialization using bearingsonly measurements or using slant-range and azimuth only."
      }
    ]
  },
  {
    "tid":"123",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Towards optimal nonlinearities for sparse recovery using higher-order statistics\nContent: We consider machine learning techniques to develop low-latency approximate solutions for a class of inverse problems. More precisely, we use a probabilistic approach to the problem of recovering sparse stochastic signals that are members of the l\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">p</sub>\n-balls. In this context, we analyze the Bayesian mean-square-error (MSE) for two types of estimators: (i) a linear estimator and (ii) a structured estimator composed of a linear operator followed by a Cartesian product of univariate nonlinear mappings. By construction, the complexity of the proposed nonlinear estimator is comparable to that of its linear counterpart since the nonlinear mapping can be implemented efficiently in hardware by means of look-up tables (LUTs). The proposed structure lends itself to neural networks and iterative shrinkage/thresholding-type algorithms restricted to a single iteration (e.g. due to imposed hardware or latency constraints). By resorting to an alternating minimization technique, we obtain a sequence of optimized linear operators and nonlinear mappings that converge in the MSE objective. The result is attractive for real-time applications where general iterative and convex optimization methods are infeasible.\n\nTitle: Optimal deep neural networks for sparse recovery via Laplace techniques.\nContent: This paper introduces Laplace techniques for designing a neural network, with the goal of estimating simplex-constraint sparse vectors from compressed measurements. To this end, we recast the problem of MMSE estimation (w.r.t. a pre-defined uniform input distribution) as the problem of computing the centroid of some polytope that results from the intersection of the simplex and an affine subspace determined by the measurements. Owing to the specific structure, it is shown that the centroid can be computed analytically by extending a recent result that facilitates the volume computation of polytopes via Laplace transformations. A main insight of this paper is that the desired volume and centroid computations can be performed by a classical deep neural network comprising threshold functions, rectified linear (ReLU) and rectified polynomial (ReP) activation functions. The proposed construction of a deep neural network for sparse recovery is completely analytic so that time-consuming training procedures are not necessary. Furthermore, we show that the number of layers in our construction is equal to the number of measurements which might enable novel low-latency sparse recovery algorithms for a larger class of signals than that assumed in this paper. To assess the applicability of the proposed uniform input distribution, we showcase the recovery performance on samples that are soft-classification vectors generated by two standard datasets. As both volume and centroid computation are known to be computationally hard, the network width grows exponentially in the worst-case. It can be, however, decreased by inducing sparse connectivity in the neural network via a well-suited basis of the affine subspace. Finally, the presented analytical construction may serve as a viable initialization to be further optimized and trained using particular input datasets at hand.\n\nTitle: Decentralized largest eigenvalue test for multi-sensor signal detection\nContent: Multi-sensor signal detection based on the the largest eigenvalue of the received sample covariance matrix is known to be optimal (asymptotically in the sample size and under Gaussian assumption) in the Neyman-Pearson sense. In this paper we propose two decentralized algorithms to implement this type of signal detector in distributed wireless networks without fusion center. The proposed solutions are based on iterative numerical algorithms (power method and Lanczos algorithm), implemented in a decentralized manner with matrix and vector products computed via average consensus. Numerical results show that such methods, in particular the decentralized Lanczos method, outperform the recently proposed decentralized energy detector after a very small number of iterations.\n\nTitle: Effect of anti-sparse prior on PAPR performance analysis.\nContent: The dynamic range of a signal is a critical parameter in many practical applications. Especially in communication engineering high dynamic range mostly is considered as an important problem for technical reasons. l(infinity)-norm minimization, or in other words an anti-sparse penalty, naturally spreads the signal evenly. The advantage of spreading is the optimally reduced dynamic range of transformed signals which is a pleasant feature for many application, e.g. peak to average power ratio (PAPR) reduction for orthogonal frequency-division multiplexing (OFDM) systems. In this study, some of the main proximal splitting algorithms are deployed for l(infinity)-norm minimization. The stochastic model of anti-sparsity is investigated with the empirical results of proximal methods and already existing l(infinity)-norm minimization methods. A flexible prior is proposed to model anti-sparsity and it is used for more realistic PAPR performance analysis.\n\nTitle: Robust set-theoretic distributed detection in diffusion networks\nContent: We propose novel set-theoretic distributed adaptive filters for cooperative signal detection in diffusion networks, a problem that has been gaining attention owing to its application to cooperative cognitive radio networks. In the proposed method, nodes in a network detect the presence of a signal of interest by means of an inner product between the current term of a series and a known reference vector. Each term of the series is computed from information fusion among neighboring nodes and projections onto closed convex sets, which are constructed with a priori knowledge of the signal of interest and measurements obtained by nodes. In particular, we show that sets based on a priori knowledge are useful to decrease the communication overhead and to provide good detection performance. Our results are rigorous in the sense that no approximations are used to prove convergence properties. In particular, we show conditions to guarantee that the series converge to a point that can reliably identify the signal of interest. Furthermore, we also show that recent results in distributed optimization for dynamic systems can be used to derive algorithms where nodes exchange not only the current vectors of their sequences (as in previous distributed set-theoretic filters), but also side information that influences the above-mentioned sets.\n\n## Task\nCompose an abstract for the title 'A new outlier detection method based on anti-sparse representations.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Detecting outliers plays a significant role in many areas such as statistics, machine learning, data-mining, etc. Depending on the importance of accurate outlier detection, they have been investigated extensively in the mentioned fields with many different type of algorithms and approaches. One of the popular one is distance based outlier detection approaches, mostly based on nearest neighbor search. In this study, anti-sparse representation is used for approximate nearest neighbor search. In order to deal with some mathematical difficulties of obtaining anti-sparse vector, a new smooth objective function is presented for l(infinity) (or maximum)-norm minimization problem. The obtained anti-sparse vector is binarized and used for embedded approximate nearest neighbor search for outlier detection. Performance of the new anti-sparse representation based outlier detection method is investigated on a real network data set by detecting denial-of-service (DoS) attacks."
      }
    ]
  },
  {
    "tid":"124",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A novel estimator based learning automata algorithm\nContent: Reinforcement learning is one of the subjects of Artificial Intelligence and learning automata have been considered as one of the most powerful tools in this research area. On the evolution of learning automata, the rate of convergence is the most primary goal of designing a learning algorithm. In this paper, we propose a deterministic-estimator based learning automata (LA) of which the estimate of each action is the upper bound of a confidence interval, rather than the Maximum Likelihood Estimate (MLE) that has been widely used in current schemes of Estimator LA. The philosophy here is to assign more confidence on actions that are selected only for a few times, so that the automaton is encouraged to explore the uncertain actions. When all the actions have been fully explored, the automaton behaves just like the Generalized Pursuit Algorithm. A refined analysis is presented to show the \ud835\udf16-optimality of the proposed algorithm. It has been demonstrated by extensive simulations that the presented learning automaton (LA) is faster than any deterministic estimator learning automata that have been reported to date. Moreover, we extend our algorithm to the stochastic estimator schemes. It is also shown that the extended LA has achieved a significant performance improvement, comparing with the current state of the art algorithm of learning automata, especially in complex and confusing environments.\n\nTitle: A set of novel continuous action-set reinforcement learning automata models to optimize continuous functions.\nContent: Learning automata (LA) as a powerful tool for reinforcement learning which belongs to the subject of Artificial Intelligence, could search for the optimal state adaptively in a random environment. In the past decades quite a few FALA algorithms are maturely developed but exposing critical defects, when they are applied to optimize continuous functions. In order to overcome their shortcomings and explore a higher-performance LA, we propose a novel CALA algorithm to solve the function optimization problems via one kind of LA prototypes, i.e, the continuous action-set reinforcement learning automata, which is abbreviated as CARLA. The key mechanism of the proposed algorithm lies in a combination of equidistant discretization and linear interpolation. Specifically, four categories of application models are constructed. Two of them are created to obtain continuous actions when the priori information is finite ones, thus avoiding the drawbacks of FALA. The realization of this functionality recourses to the so-called cumulative distribution function (CDF) and a new concept of area surrounded by curves (AsbC) respectively. The other two models are modified versions to balance the trade-off between accuracy and speed. Moreover, these models are expanded to their generalized versions so that multidimensional function optimization problems can be handled as well. A massive amount of experiments including four benchmarks and three scenarios are designed to demonstrate the effectiveness and efficiency of the proposed application models. The proposed algorithm outperforms the state of the arts of LA as well as optimization algorithms, with a high accuracy rate, a fast convergence speed, and a competitive time consumption, especially in noised environments.\n\nTitle: A Non-Monte-Carlo Parameter-Free Learning Automata Scheme Based on Two Categories of Statistics.\nContent: Learning automata (LA), which intellectually explores its optimal state by interacting with an external environment continuously, is encountered widely in artificial intelligence. In the evaluation of LA, it has always been a key issue how to tradeoff between ``accuracy'' and ``speed,'' which substantially touches on parameter tuning. A latest issue in the design of LA methodology involves bearing a parameter-free property, thus removing the tremendous expenses brought by parameter tuning. Nevertheless, the currently existing parameter-free LA schemes generally maintain a Monte-Carlo technique, which helps avoid the tuning process at the cost of more computations. This paper examines a new measurement of parameter-free LA schemes based on statistics which overcome the difficulties found in other counterparts. Specifically, it has innovatively disengaged from the dependance on Monte-Carlo methods. Of greater significance, the learning mechanisms operating in the common stationary environments are likewise extended to the nonstationary environments. Simulations confirm the effectiveness and efficiency of the proposed algorithm, especially its low computation consumption as well as the strong tracking capability to abrupt environmental changes.\n\nTitle: A new Learning Automata based approach for online tracking of event patterns.\nContent: Detecting spatiotemporal pattern from noisy sequences of events plays a very important role in presence sharing, Internet of Things (IoT) and many other fields. As pointed out in existing literature, the core activities of these applications involve event notifications. However, excessive number of event notifications will lead to user\u05f3s intolerability. Existing literature proposed a Spatiotemporal Pattern Learning Automata (STPLA) to solve this problem effectively in both stationary and non-stationary environments. However, one limitation of the STPLA is that it cannot be both memory balanced and bias toward any of the two actions, i.e., \u201csuppress\u201d or \u201cnotify\u201d. To solve this problem, this paper proposed a new Learning Automata based approach, named as Spatiotemporal Tunable Fixed Structured Learning Automata (STP-TFSLA), for online tracking of event pattern. Furthermore, we also show that the STP-TFSLA is with small memory footprint and is able to cope with non-stationary environment.\n\nTitle: Estimator Goore Game based quality of service control with incomplete information for wireless sensor networks\nContent: Quality of Service (QoS) control is one of the key mechanisms to ensure the effective operation of wireless sensor networks (WSNs). In many specific applications such as battlefield communications, the optimum number of sensors sending information at any given time is an important QoS requirement. For such requirement, some Goore Game based QoS control approaches have been exploited to dynamically adjust the number of active sensors to the optimal one in the literatures, but they assume that the optimum number is known to the base station explicitly, and each method has its own shortcomings.In this paper, a novel approach is proposed to solve the QoS control problem. A framework for solving such problem is firstly proposed, in which the optimum sensor number is not known in advance but learnt by interacting with upper level applications in a reinforcement manner. On this basis, an algorithm named Estimator Goore Game (EGG) is designed to adjust the number of active sensors effectively. The proposed algorithm cannot only relax the above assumption, but also has a competitive performance. Furthermore, the algorithm is also applicable for both stationary and non-stationary environments. Simulation results on the convergence time, the network life and the ability of tracking dynamic QoS have demonstrated the effectiveness of the proposed algorithm. HighlightsAn estimator based Goore Game scheme for QoS control in WSNs is proposed.The scheme does not rely on the awareness of the optimum number of active sensors.The scheme can provide a guaranteed QoS and a long network life maintenance.The convergence is theoretically proved in any stationary environments.The scheme can cope with both stationary and non-stationary environments.\n\n## Task\nCompose an abstract for the title 'A novel reduced parameter s-model of estimator learning automata in the switching non-stationary environment' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Learning automata (LA), a powerful tool for reinforcement learning in the field of machine learning, could explore its optimal state by continuously interacting with an external environment. Generally, the traditional LA algorithms, especially estimator LA algorithms, can be ultimately abstracted out as P- or Q-models, which are simply located in the stationary environments. A more comprehensive consideration would be S-model operating in the non-stationary environment. For this specific LA, presently the most popular achievement belongs to stochastic estimator LA (SELA). However, synchronously handing four parameters involved in SELA is an intractable job, as these parameters may vary dramatically in values under different environments, making it essential to develop a strategy for parameter tuning. In this paper, we first propose a scheme to determine the parameter searching scope and subsequently present a series of parameter searching methods, including a four-dimensional method and a two-dimensional method, making SELA applicable for any environment with switching non-stationary characteristics. Furthermore, to decrease the tuning cost, a reduced parameter SELA supported by the new two-dimensional parameter searching method emerges. And to break the traditional limit that the environmental reward probability must be symmetrically distributed, the S-model is constructed from a new perspective, thus forming a novel reduced parameter S-model of SELA (rpS-SELA). A detailed mathematical proof theoretically reveals the absolute expediency of rpS-SELA. In addition, it is demonstrated by experimental simulations that rpS-SELA outperforms others with a reduced tuning cost, a minor time consumption, a higher accuracy rate, and above all, a stronger tracking ability to the environmental switches."
      }
    ]
  },
  {
    "tid":"125",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: IBM infosphere streams for scalable, real-time, intelligent transportation services\nContent: With the widespread adoption of location tracking technologies like GPS, the domain of intelligent transportation services has seen growing interest in the last few years. Services in this domain make use of real-time location-based data from a variety of sources, combine this data with static location-based data such as maps and points of interest databases, and provide useful information to end-users. Some of the major challenges in this domain include i) scalability, in terms of processing large volumes of real-time and static data; ii) extensibility, in terms of being able to add new kinds of analyses on the data rapidly, and iii) user interaction, in terms of being able to support different kinds of one-time and continuous queries from the end-user. In this paper, we demonstrate the use of IBM InfoSphere Streams, a scalable stream processing platform, for tackling these challenges. We describe a prototype system that generates dynamic, multi-faceted views of transportation information for the city of Stockholm, using real vehicle GPS and road-network data. The system also continuously derives current traffic statistics, and provides useful value-added information such as shortest-time routes from real-time observed and inferred traffic conditions. Our performance experiments illustrate the scalability of the system. For instance, our system can process over 120000 incoming GPS points per second, combine it with a map containing over 600,000 links, continuously generate different kinds of traffic statistics and answer user queries.\n\nTitle: Real-time, scalable route planning using a stream-processing infrastructure\nContent: A key transportation service is traffic-dependent route planning, i.e., finding a path from a source to a destination on a road network that incurs the minimum travel time delay and that considers current and future traffic conditions on all road links. This kind of route planning requires dynamic time-dependent shortest path computations, since the cost of each link is different at different intervals of time, and these time-dependent costs get updated as new traffic information is obtained. In this paper, we describe a stream-processing infrastructure that can perform scalable, real-time, time-dependent shortest path computations with high throughput and low latency on large road networks. In our performance experiments, we show that it can perform hundreds of such shortest path computations per second, with an average delay of less than 1 second on a road network with tens of thousands of nodes and links. Our experiments were carried out on the Stockholm road network, and the traffic data for purposes of getting the travel time delay on each link was derived from a real GPS data-set obtained from vehicles on this network. In this paper, we also highlight the benefits of performing dynamic, time-dependent shortest path computations. For example, our data from Stockholm suggests that computing traffic-dependent shortest paths can help decrease travel time by up to 62% compared to using a less dynamic method based on fixed road speed limits. Also the shortest path for various origin-destination pairs changes frequently during the course of a day. These findings motivate the need for highly scalable, real-time time-dependent shortest-path computations, as performed by our stream processing infrastructure.\n\nTitle: Supporting Location-Based Services through Composable Stream-Processing Flows\nContent: A stream processing flow can be represented as a directed graph that describes how streaming data is obtained from one or more sources, processed by one or more components, and finally sent to one or more sinks. In this paper, we show how stream processing flows can be used to support location based services for a variety of end-users. In particular, we show how these flows can be customized for specific end-user requests from a set of reusable software components. For this, we use a language called Cascade that we had proposed in previous work for describing commonly used patterns of flows. The patterns cover different possible variations of the flows, including variations in the structure of the flow, the components in the flow and the possible parameterizations of these components. We provide examples of flows composed in the domain of intelligent transportation services.\n\nTitle: Stream as You Go: The Case for Incremental Data Access and Processing in the Cloud\nContent: Cloud infrastructures promise to provide high-performance and cost-effective solutions to large-scale data processing problems. In this paper, we identify a common class of data-intensive applications for which data transfer latency for uploading data into the cloud in advance of its processing may hinder the linear scalability advantage of the cloud. For such applications, we propose a \"stream-as-you-go\" approach for incrementally accessing and processing data based on a stream data management architecture. We describe our approach in the context of a DNA sequence analysis use case and compare it against the state of the art in MapReduce-based DNA sequence analysis and incremental MapReduce frameworks. We provide experimental results over an implementation of our approach based on the IBM InfoSphere Streams computing platform deployed on Amazon EC2, showing an order of magnitude improvement in total processing time over the state of the art.\n\nTitle: Scalable, Real-Time Map-Matching Using IBM's System S\nContent: Vehicle GPS data is an essential \"raw\" material for a broad range of applications such as traffic management and control, routing, and navigation. To become useful, the data has to be related to the underlying road network by means of map matching algorithms, which are often computationally expensive. In addition, GPS data is not accurate and often needs to be cleaned to remove erroneous observations. In this paper, we describe how map matching can be run on IBM's System S, which provides a platform to run stream processing applications in a scalable manner. We show how various features of System S, including a component based programming model, data pipelining and parallelization of computation, help us to scale the map-matching and data cleaning processes, both as the rate of incoming GPS data increases and as the size of the underlying road network increases. We provide results of performance evaluations, where we show our system can match GPS data arriving at a rate of 1 million points per second onto a map with 1 billion links.\n\n## Task\nCompose an abstract for the title 'Data Stream Processing Infrastructure For Intelligent Transport Systems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Intelligence Transportation Systems are critical to improve the efficiency of modern transportation. A system that is flexible and powerful enough to handle diverse demands from a large user base, is still elusive. Studies have shown that developing and integrating the various components constitute a significant portion of the capital cost and complexity of such systems.In this paper, we present a stream processing infrastructure we call System S. System S enables the deployment of large scale applications. It supports a mechanism for sharing data sources, software components, and even intermediate results allowing a reduction in the cost of software integration, and ownership.We experiment the stream processing infrastructure with a Fleet Management Center, and demonstrate how the infrastructure can be used to address unique issues in traffic management."
      }
    ]
  },
  {
    "tid":"126",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Transparent Resource Allocation to Exploit Idle Cluster Nodes in Computational Grids\nContent: Clusters of workstations are one of the most suitable resources to assist e-scientists in the execution of large-scale experiments that demand processing power. The utilization rate of these machines is usually far from 100%, and hence this should motivate administrators to share their clusters to Grid communities. However, exploiting these resources in Computational Grids is challenging and brings several problems. This paper presents a transparent resource allocation strategy to harness idle cluster resources aimed at executing grid applications. This novel approach does not make use of a formal allocation request to cluster resource managers. Moreover, it does not interfere with local cluster users, being non-intrusive, and hence motivating cluster administrators to publish their resources to grid communities. We present experimental results regarding the effects of the proposed strategy on the attendance time of both cluster and grid requests and we also analyze its effectiveness in clusters with different utilization rates.\n\nTitle: Promoting performance and separation of concerns for data mining applications on the grid\nContent: Grid Computing brought the promise of making high-performance computing cheaper and more easily available than traditional supercomputing platforms. Such a promise was very well received by the data mining (DM) community, as DM applications typically process very large datasets and are thus very resource intensive. However, since the Grid is very dynamic and parallel data mining is prone to load unbalancing, obtaining good data mining performance on the Grid is hard. It typically requires the scheduler to understand the inner workings of the application, bringing two related problems. First, good Grid schedulers tend to be very specialized in the application they target. Second, changing the application may require changing the scheduler, which may be especially challenging when there is no clear separation between the application and the scheduler code. We here propose and evaluate a knowledge-based approach that provides abstractions to the DM developer and optimizes at runtime the DM application on the Grid.\n\nTitle: Trading Cycles for Information: Using Replication to Schedule Bag-of-Tasks Applications on Computational Grids\nContent: Scheduling independent tasks on heterogeneous environments, like grids, is not trivial. To make a good scheduling plan on this kind of environments, the scheduler usually needs some information such as host speed, host load, and task size. This kind of information is not always available and is often difficult to obtain. In this paper we propose a scheduling approach that does not use any kind of information but still delivers good performance. Our approach uses task replication to cope with the dynamic and heterogeneous nature of grids without depending on any information about machines or tasks. Our results show that task replication can deliver good and stable performance at the expense of additional resource consumption. By limiting replication, however, additional resource consumption can be controlled with little effect on performance.\n\nTitle: Allocation strategies for utilization of space-shared resources in Bag of Tasks grids\nContent: As the adoption of grid computing in organizations expands, the need for wise utilization of different types of resource also increases. A volatile resource, such as a desktop computer, is a common type of resource found in grids. However, using efficiently other types of resource, such as space-shared resources, represented by parallel supercomputers and clusters of workstations, is extremely important, since they can provide a great amount of computation power. Using space-shared resources in grids is not straightforward since they require jobs a priori to specify some parameters, such as allocation time and amount of processors. Current solutions (e.g. Grid Resource and Allocation Management (GRAM)) are based on the explicit definition of these parameters by the user. On the other hand, good progress has been made in supporting Bag-of-Tasks (BoT) applications on grids. This is a restricted model of parallelism on which tasks do not communicate among themselves, making recovering from failures a simple matter of reexecuting tasks. As such, there is no need to specify a maximum number of resources, or a period of time that resources must be executing the application, such as required by space-shared resources. Besides, this state of affairs makes leverage from space-shared resources hard for BoT applications running on grid. This paper presents an Explicit Allocation Strategy, in which an adaptor automatically fits grid requests to the resource in order to decrease the turn-around time of the application. We compare it with another strategy described in our previous work, called Transparent Allocation Strategy, in which idle nodes of the space-shared resource are donated to the grid. As we shall see, both strategies provide good results. Moreover, they are complementary in the sense that they fulfill different usage roles. The Transparent Allocation Strategy enables a resource owner to raise its utilization by offering cycles that would otherwise go wasted, while protecting the local workload from increased contention. The Explicit Allocation Strategy, conversely, allows a user to benefit from the accesses she has to space-shared resources in the grid, enabling her natively to submit tasks without having to craft (time, processors) requests.\n\nTitle: AnthillSched: a scheduling strategy for irregular and iterative I/O-intensive parallel jobs\nContent: Irregular and iterative I/O-intensive jobs need a different approach from parallel job schedulers. The focus in this case is not only the processing requirements anymore: memory, network and storage capacity must all be considered in making a scheduling decision. Job executions are irregular and data dependent, alternating between CPU-bound and I/O-bound phases. In this paper, we propose and implement a parallel job scheduling strategy for such jobs, called AnthillSched, based on a simple heuristic: we map the behavior of a parallel application with minimal resources as we vary its input parameters. From that mapping we infer the best scheduling for a certain set of input parameters given the available resources. To test and verify AnthillSched we used logs obtained from a real system executing data mining jobs. Our main contributions are the implementation of a parallel job scheduling strategy in a real system and the performance analysis of AnthillSched, which allowed us to discard some other scheduling alternatives considered previously.\n\n## Task\nCompose an abstract for the title 'Adaptive Selection of Partition Size for Supercomputer Requests' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, we show how application scheduling can be used to reduce the turn-around time of supercomputer jobs. Our approach focuses on the use of SA, an AppLeS application scheduler, to adaptively craft the request to be submitted to the supercomputer based on the current state of the system. We demonstrate that SA significantly improves a job's turn-around time in a variety of scenarios. We also identify how the state of the system, the characteristics of the job, and the quality of the information made available to SA influence its performance."
      }
    ]
  },
  {
    "tid":"127",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Profit optimization in multi-service cognitive mesh network using machine learning\nContent: Cognitive technology enables licensed users (primary users, PUs) to trade the surplus spectrum and to transfer temporarily\n spectrum usage right to the unlicensed users (secondary users, SUs) to get some reward. The rented spectrum is used to establish\n secondary network. However, the rented spectrum size influences the quality of service (QoS) for the PU and the gained rewards.\n Therefore, the PU needs a resource management scheme that helps it to allocate optimally a given amount of the offered spectrum\n among multiple service classes and to adapt to changes in the network conditions. The PU should support different classes\n of SUs that pay different prices for their usage of spectrum. We propose a novel approach to maximize a PU reward and to maintain\n QoS for the PUs and for the different classes of SUs. These complex contradicting objectives are embedded in our reinforcement\n learning (RL) model that is developed to derive resource adaptations to changing network conditions, so that PUs' profit can\n continuously be maximized. Available spectrum is managed by the PU that executes the optimal control policy, which is extracted\n using RL. Performance evaluation of the proposed RL solution shows that the scheme is able to adapt to different conditions\n and to guarantee the required QoS for PUs and to maintain the QoS for a multiple classes of SUs, while maximizing PUs profits.\n The results have shown that cognitive mesh network can support additional SUs traffic while still ensuring PUs QoS. In our\n model, PUs exchange channels based on the spectrum demand and traffic load. The solution is extended to the case in which\n there are multiple PUs in the network where a new distributed algorithm is proposed to dynamically manage spectrum allocation\n among PUs.\n\nTitle: Intelligent Power Management for QoS-Aware Multimedia in Cognitive Mesh Networks Using Reinforcement Learning\nContent: In cognitive multimedia mesh networks (CMMNs), unlicensed users (secondary users, SUs) can share the spectrum with licensed users (primary users, PUs) on a non-interfering basis. The main challenge in CMMN is how to implement an efficient optimal control policy that can allocate spectrum and transmission powers for the SUs efficiently and how to adapt these resources to the changing network conditions. The power management scheme should control the SUs powers in such a way that their data communication rate is not affected. SUs power cannot be dropped indefinitely but it should be bounded such that the quality of service (QoS) for the SUs is still supported. Our objective is to serve the maximum number of users and support their QoS while protecting the PUs' rights of using the spectrum exclusively. Due to the direct relationship between the data rate and the quality of the communication channel, we propose a new spectrum allocation scheme that exploits the physical properties of the channel to achieve better performance. In addition to consider QoS for users, our scheme uses several heuristics for selecting channels that meet users requirements. These heuristics include channel error rates, PUs activities, channel capacity and channel adaptation time. Performance evaluation of the proposed scheme shows that the scheme is able to support additional SUs traffic while still ensuring PUs QoS.\n\nTitle: Cluster-Based Spectrum Management Using Cognitive Radios in Wireless Mesh Network\nContent: Wireless mesh networks (WMNs) have emerged recently to extend Internet access and other networking services. WMNs routers provide network access to the clients and other networking functions such as routing, and packet forwarding. Bandwidth scarcity is the main challenge that limits the performance of WMNs. Although considerable research has been conducted on spectrum allocation, spectrum management is still considered an important open problem. This problem can be solved using cognitive radio technology that allows radios to intelligently locate free frequencies and use them efficiently. In this work, we propose a new spectrum management scheme that supports local and global management for a wireless network. Our scheme is based on clusters where the coordinator for each cluster manages spectrum information by keeping the required information at cluster level and for the whole network. Our scheme provides robust operation against any cluster head failure, as well as clients mobility.\n\nTitle: Routing Management Algorithm Based on Spectrum Trading and Spectrum Competition in Cognitive Radio Networks\nContent: Traditionally in routing approaches, each node allows a maximum load through the selected route. The existing routing approaches in cognitive radio networks (CRN) do not take into account spectrum trading as well as spectrum competition among licensed users (PUs). This paper introduces a novel routing algorithm that is based on spectrum trading and spectrum competition for cognitive radio networks while supporting different QoS levels for unlicensed users (SUs). The proposed path selection algorithm among different paths is based on user profiles which contain parameters such as SU identification, number of hops, channel identification, neighbor identification, probabilities of idle slots and PU presence. Each node shares its profile with the neighbor PU, which then exchanges its profile with other PUs and decides based on the information received. In spectrum trading phase a PU calculates its price based on the SU requirements. In spectrum competition phase a new coefficient \u00ce\u5364 is defined that controls the price because of competition among PUs and depends on many factors such as the number of primary users, available channels, and duration of the usage. All possible paths are managed and categorized based on the level of QoS requested by SUs and the price offered by the PU.\n\nTitle: Load Balancing For Spectrum Management In A Cluster-Based Cognitive Network\nContent: Cognitive radio technology has received increasing attention recently as a solution for the spectrum scarcity problem in wireless technology. It allows radios to intelligently locate free frequencies and use them efficiently. In our work, the unused spectrum is used to serve secondary users who do not have a license to access the spectrum. We propose a cluster-based architecture to form a secondary network. This network uses the licensed spectrum to overlay secondary users' traffic. Cluster-heads group nodes to form distinct clusters and they manage spectrum in their clusters. In this paper, we investigate the performance of our spectrum management scheme. Simulation results show that load balanced clustering improve spectrum utilization and increase the throughput of the secondary network.\n\n## Task\nCompose an abstract for the title 'Optimizing spectrum trading in cognitive mesh network using machine learning' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In a cognitive wireless mesh network, licensed users (primary users, PUs) may rent surplus spectrum to unlicensed users (secondary users, SUs) for getting some revenue. For such spectrumsharing paradigm, maximizing the revenue is the key objective of the PUs while that of the SUs is to meet their requirements. These complex contradicting objectives are embedded in our reinforcement learning (RL) model that is developed and implemented as shown in this paper. The objective function is defined as the net revenue gained by PUs from renting some of their spectrum. RL is used to extract the optimal control policy that maximizes the PUs' profit continuously over time. The extracted policy is used by PUs to manage renting the spectrum to SUs and it helps PUs to adapt to the changing network conditions. Performance evaluation of the proposed spectrum trading approach shows that it is able to find the optimal size and price of spectrum for each primary user under different conditions. Moreover, the approach constitutes a framework for studying, synthesizing and optimizing other schemes. Another contribution is proposing a new distributed algorithm to manage spectrum sharing among PUs. In our scheme, PUs exchange channels dynamically based on the availability of neighbor's idle channels. In our cooperative scheme, the objective of spectrum sharing is to maximize the total revenue and utilize spectrum efficiently. Compared to the poverty-line heuristic that does not consider the availability of unused spectrum, our scheme has the advantage of utilizing spectrum efficiently."
      }
    ]
  },
  {
    "tid":"128",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Flexible rule-based inference exploiting taxonomies\nContent: Rule-based systems are widely used to implement knowledge-based systems. They are usually intuitive to use, have good performance and can be easily integrated with other software components. However, a critical problem is that the behavior of a rule-based system tends to degrade abruptly whenever the knowledge base is incomplete or not detailed enough or when operating at the borders of its expertise. Various forms of approximate reasoning have been introduced but they solve the problem only in a partial way. In the paper we propose new forms of rule inference that tackle this problem, introducing a form of flexible or common sense reasoning that can support a softer degradation of problem solving ability when knowledge is partial or incomplete. The solution we propose relies on the exploitation of semantic information associated with the concepts involved in the rules. In particular, we show how taxonomical information can be exploited to define flexible forms of match between rule antecedents and the working memory, and flexible forms of conflict resolution. In this way, even when no rule perfectly matches the working memory, the inference engine can select rules that apply to more general or to similar cases and provide some approximate solution. The approach has been motivated by work on context aware (recommender) systems where the problem of incomplete descriptions and brittle degradation of problem solving ability are particularly relevant.\n\nTitle: Local reasoning and knowledge compilation for efficient temporal abduction\nContent: Generating abductive explanations is the basis of several problem solving activities such as diagnosis, planning, and interpretation. Temporal abduction means generating explanations that do not only account for the presence of observations, but also for temporal information on them, based on temporal knowledge in the domain theory. We focus on the case where such a theory contains temporal constraints that are required to be consistent with temporal information on observations. The aim of this paper is to propose efficient algorithms for computing temporal abductive explanations. Temporal constraints in the theory and in the observations can be used actively by an abductive reasoner in order to prune inconsistent candidate explanations at an early stage during their generation. However, checking temporal constraint satisfaction frequently generates some overhead. In the paper, we analyze two incremental ways of making this process efficient. First we show how, using a specific class of temporal constraints (which is expressive enough for many applications), such an overhead can be reduced significantly, yet preserving a full pruning power. In general, the approach does not affect the asymptotic complexity of the problem, but it provides significant advantages in practical cases. We also show that, for some special classes of theories, the asymptotic complexity is also reduced. We then show how, compiled knowledge based on temporal information, can be used to further improve the computation, thus, extending to the temporal framework previous results in the case of atemporal abduction. The paper provides both analytic and experimental evaluations of the computational advantages provided by our approaches.\n\nTitle: Adcuctive Reasoning with Abstraction Axioms\nContent: This paper deals with abductive reasoning on knowledge bases that are expressed at different levels of abstraction, but are not necessarily organized as a set of increasingly more abstract models, each one giving a complete (even if abstracted) description of a domain. We claim that the search for abductive explanations in such a context and, in particular, the choice of the right level at which explanations have to be determined, should be driven by the available observations in such a way that explanations involving low-level phenomena are allowed only if there are specific observations related to them, or higher-level explanations cannot be found. We present formal definitions following this principle and we discuss how explanations can be computed according to the definition.\n\nTitle: A logical approach to deal with incomplete causal models in diagnostic problem solving\nContent: Different approaches have been proposed in the last few years to the formalization of deep diagnostic problem solving. In this paper we briefly analyze the two main approaches proposed so far for modeling diagnostic problem solving, i.e. the logical one (a la Reiter or a la de Kleer) and the probabilistic one (a la Pearl). Starting from such an analysis, we propose a different approach to the formalization of deep reasoning, based on the adoption of peculiar forms of non-monotonic logics. The approach allows us to perform sophisticated and qualitative forms of \"approximate\" (common-sense) reasoning. More specifically, we have designed a formalism for deep modeling which permits the description (representation) of incomplete and/or uncertain causal knowledge. A formal logical semantics has been defined for the modeling scheme, so that all the deep reasoning process can be logically characterized and defined. In particular a complex form of hypothetical (assumption-based) reasoning scheme has been defined to deal with incomplete models.\n\nTitle: Using Compiled Knowledge to Guide and Focus Abductive Diagnosis\nContent: Several artificial intelligence architectures and systems based on \"deep\" models of a domain have been proposed, in particular for the diagnostic task. These systems have several advantages over traditional knowledge based systems, but they have a main limitation in their computational complexity. One of the ways to face this problem is to rely on a knowledge compilation phase, which produces knowledge that can be used more effectively with respect to the original one.In this paper we show how a specific knowledge compilation approach can focus reasoning in abductive diagnosis, and, in particular, can improve the performances of AID, an abductive diagnosis system. The approach aims at focusing the overall diagnostic cycle in two interdependent ways: avoiding the generation of candidate solutions to be discarded a posteriori and integrating the generation of candidate solutions with discrimination among different candidates. Knowledge compilation is used off-line to produce operational (i.e., easily evaluated) conditions that embed the abductive reasoning strategy and are used in addition to the original model, with the goal of ruling out parts of the search space or focusing on parts of it. The conditions are useful to solve most cases using less time for computing the same solutions, yet preserving all the power of the model-based system for dealing with multiple faults and explaining the solutions. Experimental results showing the advantages of the approach are presented.\n\n## Task\nCompose an abstract for the title 'Common-Sense Rule Inference' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In the paper we show how rule-based inference can be made more flexible by exploiting semantic information associated with the concepts involved in the rules. We introduce flexible forms of common sense reasoning in which whenever no rule applies to a given situation, the inference engine can fire rules that apply to more general or to similar situations. This can be obtained by defining new forms of match between rules and the facts in the working memory and new forms of conflict resolution. We claim that in this way we can overcome some of the brittleness problems that are common in rule-based systems."
      }
    ]
  },
  {
    "tid":"129",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: JPMiner: Mining Frequent Jump Patterns from Graph Databases\nContent: A major challenge in frequent subgraph mining is the sheer size of its mining results. In many cases, allow minimum support may generate an explosive number of frequent subgraphs, which severely restricts the usage of frequent sub graph mining. In this paper, we study anew problem of mining frequent jump patterns from graph databases. Mining frequent jump patterns can dramatically reduce the number of output graph patterns, and still capture interesting graph patterns. By integrating the operation of checking jump patterns into the well-known DFS code tree enumeration framework, we present an efficient algorithm JPMiner for this new problem. We experimentally evaluate various aspects of Jupiter using both real and synthetic datasets. Experimental results demonstrate that the number of frequent jump patterns is much smaller than that of closed frequent graph patterns, and JPMiner is efficient and scalable in mining frequent jump patterns.\n\nTitle: Mining Frequent Subgraph Patterns from Uncertain Graph Data\nContent: In many real applications, graph data is subject to uncertainties due to incompleteness and imprecision of data. Mining such uncertain graph data is semantically different from and computationally more challenging than mining conventional exact graph data. This paper investigates the problem of mining uncertain graph data and especially focuses on mining frequent subgraph patterns on an uncertain graph database. A novel model of uncertain graphs is presented, and the frequent subgraph pattern mining problem is formalized by introducing a new measure, called expected support. This problem is proved to be NP-hard. An approximate mining algorithm is proposed to find a set of approximately frequent subgraph patterns by allowing an error tolerance on expected supports of discovered subgraph patterns. The algorithm uses efficient methods to determine whether a subgraph pattern can be output or not and a new pruning method to reduce the complexity of examining subgraph patterns. Analytical and experimental results show that the algorithm is very efficient, accurate, and scalable for large uncertain graph databases. To the best of our knowledge, this paper is the first one to investigate the problem of mining frequent subgraph patterns from uncertain graph data.\n\nTitle: Mining frequent subgraphs over uncertain graph databases under probabilistic semantics\nContent: Frequent subgraph mining has been extensively studied on certain graph data. However, uncertainty is intrinsic in graph data in practice, but there is very few work on mining uncertain graph data. This paper focuses on mining frequent subgraphs over uncertain graph data under the probabilistic semantics. Specifically, a measure called $${\\varphi}$$ -frequent probability is introduced to evaluate the degree of recurrence of subgraphs. Given a set of uncertain graphs and two real numbers $${0 , the goal is to quickly find all subgraphs with $${\\varphi}$$ -frequent probability at least \u9a74. Due to the NP-hardness of the problem and to the #P-hardness of computing the $${\\varphi}$$ -frequent probability of a subgraph, an approximate mining algorithm is proposed to produce an $${(\\varepsilon, \\delta)}$$ -approximate set \u9a74 of \"frequent subgraphs\", where $${0 is error tolerance, and 0 \u9a74 S is contained in \u9a74 with probability at least ((1 \u9a74 \u9a74) /2) s , where s is the number of edges in S; (2) any infrequent subgraph with $${\\varphi}$$ -frequent probability less than $${\\tau - \\varepsilon}$$ is contained in \u9a74 with probability at most \u9a74/2. The theoretical analysis shows that to obtain any frequent subgraph with probability at least 1 \u9a74 \u0394, the input parameter \u9a74 of the algorithm must be set to at most $${1 - 2 (1 - \\Delta)^{1 / \\ell_{\\max}}}$$ , where 0 \u0394 \u9a74 max is the maximum number of edges in frequent subgraphs. Extensive experiments on real uncertain graph data verify that the proposed algorithm is practically efficient and has very high approximation quality. Moreover, the difference between the probabilistic semantics and the expected semantics on mining frequent subgraphs over uncertain graph data has been discussed in this paper for the first time.\n\nTitle: Frequent subgraph pattern mining on uncertain graph data\nContent: Graph data are subject to uncertainties in many applications due to incompleteness and imprecision of data. Mining uncertain graph data is semantically different from and computationally more challenging than mining exact graph data. This paper investigates the problem of mining frequent subgraph patterns from uncertain graph data. The frequent subgraph pattern mining problem is formalized by designing a new measure called expected support. An approximate mining algorithm is proposed to find an approximate set of frequent subgraph patterns by allowing an error tolerance on the expected supports of the discovered subgraph patterns. The algorithm uses an efficient approximation algorithm to determine whether a subgraph pattern can be output or not. The analytical and experimental results show that the algorithm is very efficient, accurate and scalable for large uncertain graph databases.\n\nTitle: Efficient Algorithms for Summarizing Graph Patterns\nContent: We investigate the problem of summarizing frequent subgraphs by a smaller set of representative patterns. We show that some special graph patterns, called \\delta\\hbox{-}jumppatterns in this paper, must be representative patterns. Based on the fact, we devise two algorithms, RP-FP and RP-GD, to mine a representative set that summarizes frequent subgraphs. RP-FP derives a representative set from frequent closed subgraphs, whereas RP-GD mines a representative set from graph databases directly. Three novel heuristic strategies, Last-Succeed-First-Check, Reverse-Path-Trace, and Nephew-Representative-Based-Cover, are proposed to further improve the efficiency of RP-GD. RP-FP can provide a tight ratio bound but has heavy computation cost. RP-GD cannot provide a ratio bound guarantee but is more efficient than RP-FP. We also make use of the similarity between sibling branches in the graph pattern space to devise another much more efficient algorithm, RP-Leap, for mining a representative set that can approximately summarize frequent subgraphs. Our extensive experiments on both real and synthetic data sets verify the summarization quality and efficiency of our algorithms. To further demonstrate the interestingness of representative patterns, we study an application of representative patterns to classification. We demonstrate that the classification accuracy achieved by representative pattern-based model is no less than that achieved by closed graph pattern-based model.\n\n## Task\nCompose an abstract for the title 'Mining Top-K Graph Patterns That Jointly Maximize Some Significance Measure' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Most of graph pattern mining algorithms focus on finding frequent subgraphs and its compact representations, such as closed frequent subgraphs and maximal frequent subgraphs. However, little attention has been paid to mining graph patterns with user-specified significance measure. In this paper, we study a new problem of mining top-k graph patterns that jointly maximize some significance measure from graph databases. Exploiting entropy and information gain, we give two problem formulations, EM and IGM. We first prove them to be NP-hard and then propose two efficient algorithms, PP-TopK and DM-TopK, to solve them. PP-TopK greedily selects top-k graph patterns among frequent subgraphs. DM-TopK integrates the pruning techniques into the mining framework, and directly mines top-k graph patterns from graph databases. Empirical results demonstrate the quality of our top-k graph patterns, and validate the efficiency and scalability of our algorithms."
      }
    ]
  },
  {
    "tid":"130",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Composing enterprise mashup components and services using architecture integration patterns\nContent: Enterprise mashups leverage various source of information to compose new situational applications. The architecture of such applications must address integration issues: it needs to deal with heterogeneous local and/or public data sources, and build value-added applications on existing corporate IT systems. In this paper, we leverage enterprise architecture integration patterns to compose reusable mashup components. We present a service oriented architecture that addresses reusability and integration needs for building enterprise mashup applications. Key techniques to customize this architecture are developed for mashups with themed data on location maps. The usage of this architecture is illustrated by a property valuation application derived from a real-world scenario. We demonstrate and discuss how this state-of-the-art architecture design method can be applied to enhance the design and development of emerging enterprise mashups.\n\nTitle: An architectural approach to composing reputation-based distributed services\nContent: Reputation-Based Trust (RBT) model with embedded incentive mechanisms provides an accurate quantitative measurement for services choosing their partners based on fair ratings accumulated from users. These mechanisms stimulate services to offer ratings truthfully, otherwise they lose their gains or even receive penalties. However, leveraging such mechanisms in distributed environments is a challenging task by its centralized nature. In this paper, we propose a new architecture development that combines relevant architectural components to make trust systems highly scalable with the auction mechanisms' capability to prevent lie. In this architecture we define an auction-based trust negotiation protocol that guides the interactions of distributed services and realize it in the distributed trust framework. Our architecture scales efficiently for increasing numbers of services interacting with the system, while still achieving protection against untruthful incentives even when a majority of ratings are unfair. An example of a supply chain is devised with empirical evidence collected.\n\nTitle: Distilling Scenarios from Patterns for Software Architecture Evaluation - A Position Paper\nContent: Software architecture (SA) evaluation is a quality assurance technique that is increasingly attracting significant research and commercial interests. A number of SA evaluation methods have been developed. Most of these methods are scenario-based, which relies on the quality of the scenarios used for the evaluation. Most of the existing techniques for developing scenarios use stakeholders and requirements documents as main sources of collecting scenarios. Recently, architectures of large software systems are usually composed of patterns and styles. One of the purposes of using patterns is to develop systems with predictable quality attributes. Since patterns are documented in a format that requires the inclusion of problem, solution and quality consequences, we observed that scenarios are, though as informal text, pervasive in patterns description, which can be extracted and documented for the SA evaluation. Thus, we claim that the patterns can be another source of collecting quality attributes sensitive scenarios. This position paper presents arguments and examples to support our claim.\n\nTitle: A Taxonomy of Blockchain-Based Systems for Architecture Design\nContent: Blockchain is an emerging technology for decentralised and transactional data sharing across a large network of untrusted participants. It enables new forms of distributed software architectures, where agreement on shared states can be established without trusting a central integration point. A major difficulty for architects designing applications based on blockchain is that the technology has many configurations and variants. Since blockchains are at an early stage, there is little product data or reliable technology evaluation available to compare different blockchains. In this paper, we propose how to classify and compare blockchains and blockchain-based systems to assist with the design and assessment of their impact on software architectures. Our taxonomy captures major architectural characteristics of blockchains and the impact of their principal design decisions. This taxonomy is intended to help with important architectural considerations about the performance and quality attributes of blockchain-based systems.\n\nTitle: Resource-Oriented Architecture for Business Processes\nContent: REpresentational State Transfer(REST) is the set of design principles behind the World Wide Web (WWW). REST treats all entities in the world as link-connected resources, and supports a Resource-Oriented Architecture (ROA) for the design of applications. REST and ROA are responsible for many of the desirable quality attributes achieved in the WWW, such as loose-coupling (better adaptability) and interoperability. However, many exiting Web-based or service-oriented applications (WSDL/SOAP-based) only use WWW/HTTP as a tunneling protocol or abuse URL and POX (Plain Old XML) by encoding method semantics in them. These applications use fine-grained Remote Procedure Calls (RPC), breaking REST/ROA principles. We observe two kinds of challenges: 1) conceptually modelling process-intensive applications using a ROA promoted by the REST principles; and 2) practically decomposing a workflow-based business process into distributed, dynamic and RESTful process fragments. In this paper, we propose a ROA for business processes following the RESTful principles. We evaluate our approach by comparing it with current SOAP/WSDL/BPEL-driven approaches in terms of feasibility, process visibility, interoperability, and adaptability.\n\n## Task\nCompose an abstract for the title 'On Creating Industry-Wide Reference Architectures' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The discontent over the limited capability of Web services to reflect the organization\u2019s business model, and efficiently adhere to business strategies and requirements, caused a shift in business thinking. As a result, the enterprise has been recognizing ..."
      }
    ]
  },
  {
    "tid":"131",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Monitoring and fault tolerance for real-time online interactive applications\nContent: The edutain@grid European project [1] is developing a support platform for deployment, management and execution of Real-Time Online Interactive Applications (ROIA) on Grid. In this paper we present a monitoring system we developed which collects data from all the resources in a distributed environment and from the ROIA managed by our platform. We also describe a fault tolerance service which addresses not only the faults commonly encountered in distributed systems, but also faults manifesting at service level, within the platform's management services. Finally, a use-case consisting of the platform running a massively multiplayer online game as a concrete ROIA, is presented in order to demonstrate the roles of the monitoring and fault tolerance services.\n\nTitle: Edutain@Grid: A Business Grid Infrastructure for Real-Time On-Line Interactive Applications\nContent: Grid infrastructures are maturing to a point where they are attracting the interest of businesses in many application domains. While many large-scale on-line gaming platforms exist, they fail to take into consideration the potential business to business relationships when it comes to dynamic on-line game hosting. This work presents an initial implementation of the edutain@grid architecture to support business value chains identified for on-line gaming and e-learning application hosting. An analysis of business actors and value chains is presented briefly before a detailed description of the edutain@grid implementation. We also consider first results concerning how best to construct appropriate value chains using bipartite and bi-directional Service Level Agreements.\n\nTitle: Dynamic Real-Time Resource Provisioning for Massively Multiplayer Online Games\nContent: Massively Multiplayer Online Games (MMOG) are a class of computationally-intensive client-server applications with severe real-time Quality of Service (QoS) requirements, such the number of updates per second each client needs to receive from the servers for a fluent and realistic experience. To guarantee the QoS requirements, game providers currently over-provision a large amount of their resources, which makes the overall efficiency of provisioning and utilisation of resources rather low and prohibits any but the largest providers from joining the market. To address this deficiency, we propose a new prediction-based method for dynamic resource provisioning and scaling of MMOGs in distributed Grid environments. Firstly, a load prediction service anticipates the future game world entity distribution from historical trace data using a fast and flexible neural network-based method. On top of it, we developed generic analytical game load models used to foresee future hot-spots that congest the game servers and make the overall environment fragmented and unplayable. Finally, a resource allocation service performs dynamic load distribution, balancing, and migration of entities that keep the game servers reasonably loaded such that the real-time QoS requirements are maintained.\n\nTitle: Prediction-based real-time resource provisioning for massively multiplayer online games\nContent: Massively Multiplayer Online Games (MMOGs) are a class of computationally intensive client-server applications with severe real-time Quality of Service (QoS) requirements, such as the number of updates per second each client needs to receive from the servers for a fluent and realistic experience. To guarantee the QoS requirements, game providers currently over-provision a large amount of their resources, which makes the overall efficiency of provisioning and utilization of resources rather low and prohibits any but the largest providers from joining the market. To address this deficiency, we propose a new prediction-based method for dynamic resource provisioning and scaling of MMOGs in distributed Grid environments. Firstly, a load prediction service anticipates the future game world entity distribution from historical trace data using a fast and flexible neural network-based method. On top of it, we developed generic analytical game load models used to foresee future hot-spots that congest the game servers and make the overall environment fragmented and unplayable. Finally, a resource allocation service performs dynamic load distribution, balancing, and migration of entities that keep the game servers reasonably loaded such that the real-time QoS requirements are maintained. Experimental results based on a realistic simulation environment demonstrate the advantages of our prediction service compared to other conventional methods, especially due to its ability to adapt to different user load patterns, and a reduction of the average over-allocation from 250% (in the case of static over-provisioning) to around 25% using our dynamic provisioning method.\n\nTitle: A Web Service-Based Experiment Management System for the Grid\nContent: We have developed the ZENTURIO experiment management system for performance studies, parameter studies, and software testing of parallel applications on cluster and Grid architectures. In this paper we describe the design and implementation of ZENTURIO as a Web service-based distributed architecture. We highlight limitations of Web services for deployment on the Grid and propose solutions. Experiment Generator and Experiment Executor services are used to generate, respectively submit and manage experiments on remote Grid sites. A UDDI-based Service Repository publishes information about existing Web service implementations. Factory and Registry services are employed to create and register Web services on the Grid. A GMA-compliant event infrastructure supports high-level notifications about important events. We compare our design with the Open Grid Service Architecture (OGSA) and highlight similarities and differences. We report results of using ZENTURIO to conduct performance analysis of a material science code executed on the Grid by using GRAM.\n\n## Task\nCompose an abstract for the title 'An Information System for Real-Time Online Interactive Applications' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The edutain@grid European project [1] is developing a support platform for deployment, management and execution of Real-Time Online Interactive Applications (ROIA) on Grid. In this paper we present an information system designed by the edutain@grid project which provides support for ROIA deployment and monitoring, and offers a generic frontend for ROIA-specific optimisations. We conduct a variety of experiments that justify various decisions of our design, and investigate the performance and scalability of our system with respect to various types of queries."
      }
    ]
  },
  {
    "tid":"132",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Variation-Aware Layer Assignment With Hierarchical Stochastic Optimization on a Multicore Platform\nContent: As the very large scale integration (VLSI) technology enters the nanoscale regime, VLSI design is increasingly sensitive to variations on process, voltage, and temperature. Layer assignment technology plays a crucial role in industrial VLSI design flow. However, existing layer assignment approaches have largely ignored these variations, which can lead to significant timing violations. To address this issue, a variation-aware layer assignment approach for cost minimization is proposed in this paper. The proposed layer assignment approach is a single-stage stochastic program that directly controls the timing yield via a single parameter, and it is solved using Monte Carlo simulations and the Latin hypercube sampling technique. A hierarchical design is also adopted to enable the optimization process on a multicore platform. Experiments have been performed on 5000 industrial nets, and the results demonstrate that the proposed approach: 1) can significantly improve the timing yield by 64% in comparison with the nominal design and 2) can reduce the wire cost by 15.7% in comparison with the worst case design.\n\nTitle: IK-SVD: Dictionary Learning for Spatial Big Data via Incremental Atom Update\nContent: A large group of dictionary learning algorithms focus on adaptive sparse representation of data. Almost all of them fix the number of atoms in iterations and use unfeasible schemes to update atoms in the dictionary learning process. It's difficult, therefore, for them to train a dictionary from Big Data. A new dictionary learning algorithm is proposed here by extending the classical K-SVD method. In the proposed method, when each new batch of data samples is added to the training process, a number of new atoms are selectively introduced into the dictionary. Furthermore, only a small group of new atoms as subspace controls the current orthogonal matching pursuit, construction of error matrix, and SVD decomposition process in every training cycle. The information, from both old and new samples, is explored in the proposed incremental K-SVD (IK-SVD) algorithm, but only the current atoms are adaptively updated. This makes the dictionary better represent all the samples without the influence of redundant information from old samples.\n\nTitle: Task-Tree Based Large-Scale Mosaicking for Massive Remote Sensed Imageries with Dynamic DAG Scheduling\nContent: Remote sensed imagery mosaicking at large scale has been receiving increasing attentions in regional to global research. However, when scaling to large areas, image mosaicking becomes extremely challenging for the dependency relationships among a large collection of tasks which give rise to ordering constraint, the demand of significant processing capabilities and also the difficulties inherent in organizing these enormous tasks and RS image data. We propose a task-tree based mosaicking for remote sensed imageries at large scale with dynamic DAG scheduling. It expresses large scale mosaicking as a data-driven task tree with minimal height. And also a critical path based dynamical DAG scheduling solution with status queue named CPDS-SQ is provided to offer an optimized schedule on multi-core cluster with minimal completion time. All the individual dependent tasks are run by a core parallel mosaicking program implemented with MPI to perform mosaicking on different pairs of images. Eventually, an effective but easier approach is offered to improve the large-scale processing capability by decoupling the dependence relationships among tasks from the complex parallel processing procedure. Through experiments on large-scale mosaicking, we confirmed that our approach were efficient and scalable.\n\nTitle: An Iterative Hierarchical Key Exchange Scheme for Secure Scheduling of Big Data Applications in Cloud Computing\nContent: As the new-generation distributed computing platform, cloud computing environments offer high efficiency and low cost for data-intensive computation in big data applications. Cloud resources and services are available in pay-as-you-go mode, which brings extraordinary flexibility and cost-effectiveness as well as zero investment in their own computing infrastructure. However, these advantages come at a price-people no longer have direct control over their own data. Based on this view, data security becomes a major concern in the adoption of cloud computing. Authenticated Key Exchange (AKE) is essential to a security system that is based on high efficiency symmetric-key encryption. With virtualization technology being applied, existing key exchange schemes such as Internet Key Exchange (IKE) becomes time-consuming when directly deployed into cloud computing environment. In this paper we propose a novel hierarchical key exchange scheme, namely Cloud Background Hierarchical Key Exchange (CBHKE). Based on our previous work, CBHKE aims at providing secure and efficient scheduling for cloud computing environment. In our new scheme, we design a two-phase layer-by-layer iterative key exchange strategy to achieve more efficient AKE without sacrificing the level of data security. Both theoretical analysis and experimental results demonstrate that when deployed in cloud computing environment, efficiency of the proposed scheme is dramatically superior to its predecessors CCBKE and IKE schemes.\n\nTitle: Particle Swarm Optimization based dictionary learning for remote sensing big data.\nContent: Dictionary learning, which is based on sparse coding, has been frequently applied to many tasks related to remote sensing processes. Recently, many new non-analytic dictionary-learning algorithms have been proposed. Some are based on online learning. In online learning, data can be sequentially incorporated into the computation process. Therefore, these algorithms can train dictionaries using large-scale remote sensing images. However, their accuracy is decreased for two reasons. On one hand, it is a strategy of updating all atoms at once; on the other, the direction of optimization, such as the gradient, is not well estimated because of the complexity of the data and the model. In this paper, we propose a method of improved online dictionary learning based on Particle Swarm Optimization (PSO). In our iterations, we reasonably selected special atoms within the dictionary and then introduced the PSO into the atom-updating stage of the dictionary-learning model. Furthermore, to guide the direction of the optimization, the prior reference data were introduced into the PSO model. As a result, the movement dimension of the particles is reasonably limited and the accuracy and effectiveness of the dictionary are promoted, but without heavy computational burdens. Experiments confirm that our proposed algorithm improves the performance of the algorithm for large-scale remote sensing images, and our method also has a better effect on noise suppression.\n\n## Task\nCompose an abstract for the title 'Hierarchical extreme learning machine based image denoising network for visual Internet of Things.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In the visual Internet of Things (VIoT), imaging sensors must achieve a balance between limited bandwidth and useful information when images contain heavy noise. In this paper, we address the problem of removing heavy noise and propose a novel hierarchical extreme learning machine-based image denoising network, which comprises a sparse auto-encoder and a supervised regression. Due to the fast training of a hierarchical extreme learning machine, an effective image denoising system that is robust for various noise levels can be trained more efficiently than other denoising methods, using a deep neural network. Our proposed framework also contains a non-local aggregation procedure that aims to fine-tune noise reduction according to structural similarity. Compared to the compression ratio in noisy images, the compression ratio of denoised images can be dramatically improved. Therefore, the method can achieve a low communication cost for data interactions in the VIoT. Experimental studies on images, including both hand-written digits and natural scenes, have demonstrated that the proposed technique achieves excellent performance in suppressing heavy noise. Further, it greatly reduces the training time, and outperforms other state-of-the-art approaches in terms of denoising indexes for the peak signal-to-noise ratio (PSNR) or the structural similarity index (SSIM)."
      }
    ]
  },
  {
    "tid":"133",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Improving IT Change Management Processes with Automated Risk Assessment\nContent: The rational management of IT infrastructures is a goal of modern organizations that aim to deliver high quality services to their customers in an affordable way. Since changes are imminent in such a dynamic environment, failures during this process may directly affect business continuity. Hence, risk assessment is a key process in IT change management. Despite its importance, risks are usually assessed by humans based on empirical knowledge, leading to inaccurate basis for decision making. In this paper, we present a solution for automating the risk assessment process, which combines historical data from previous changes and analyzes impact of changes over affected elements. A prototypical system was developed to evaluate the solution on an emulated IT infrastructure. The results achieved show how the automated solution is capable of raising the quality of changes, therefore reducing service disruption caused by changes.\n\nTitle: A rule-based approach to prioritization of IT work requests maximizing net benefit to the business\nContent: With the growth of IT outsourcing opportunities, providers of managed IT services, such as HP, are compelled to exploit any possible economy of scale in dealing with emerging requirements of their customers. In this paper we present a methodology and a set of tools for enhanced executive decision support capability for the best allocation of scarce development resources through timelier and more accurate delivery of forecast indicators relative to the net benefit. Examples of such indicators are total forecast revenue for a bundle of work requests and total forecast cost reductions for a bundle of work requests. The tools deliver on reduced development cost and shorter time to value through the identification of synergies, duplication and commonality in work requests. For example, our approach will be able to identify in a reliable manner work that falls into these discrete categories pertaining to potential duplication thereby highlighting areas of potential cost reduction. Moreover they guarantee a reduced turn around time of delivery to trade customers through prioritization driven by net benefit and optimized release.\n\nTitle: A solution to support risk analysis on IT change management\nContent: The growing necessity of organizations in using technologies to support to their operations implies that managing IT resources became a mission-critical issue for the health of the primary companies' businesses. Thus, in order to minimize problems in the IT infrastructure, possibly affecting the daily business operations, risks intrinsic to the change process have to be analyzed and assessed. Risk Management is a widely discussed subject in several areas, although for IT Change Management it is quite a new discipline. The Information Technology Infrastructure Library (ITIL) introduces a set of best practices to conduct the management of IT infrastructures. According to ITIL, risks should be investigated, measured, and mitigated before any change is approved. Even with these guidelines, there is no default automatic method for risk assessment in IT Change Management. In this paper we introduce a risk analysis method based on the execution history of past changes. In addition, we propose a failure representation model to capture the feedback of the execution of changes over IT infrastructures.\n\nTitle: A Runtime Constraint-Aware Solution for Automated Refinement of IT Change Plans\nContent: Change design is one of the key steps within the IT change management process and involves defining the set of activities required for the implementation of a change. Despite its importance, existing approaches for automating this step disregard the impact that actions will cause on the affected elements of the IT infrastructure. As a consequence, activities that compose the change plan may not be executable, for example, due to runtime constraints that emerge during the change plan execution (e.g., lack of disk space and memory exhaustion). In order to address this issue, we propose a solution for the automated refinement of runtime constraint-aware change plans, built upon the concept of incremental change snapshots of the target IT environment. The potential benefits of our approach are (i) the generation of accurate, workable change plans, composed of activities that do not hinder the execution of subsequent ones, and (ii) a decrease in the occurrence of service-delivery disruptions caused by failed changes. The experimental evaluation carried out in our investigation shows the feasibility of the proposed solution, being able to generate plans less prone to be prematurely aborted due to resource constraints.\n\nTitle: Modeling IT support organizations using multiple-priority queues\nContent: As IT services grow more and more complicated, and their management becomes increasingly challenging, IT support organizations assume an essential role to ensure the delivery of Service Level Objectives. What-if scenario analysis represents a very effective tool for the performance optimization of IT support organizations, as it enables an iterative and customized performance optimization process. The problem of accurately modeling IT support organizations requires the development of sophisticated models as well as dedicated parameter inference techniques and tools. This paper presents a multiple-priority queuing model suited for the reenactment of IT support groups developed from the analysis of empirical evidence, as well as a powerful method to infer the model parameters. We applied our model to reenact the behavior of a real life IT support group with our Symian simulator. The results demonstrate that multiple-priority queuing models can reproduce real life IT support groups with a high degree of accuracy.\n\n## Task\nCompose an abstract for the title 'Change Priority Determination in IT Service Management Based on Risk Exposure' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In the Change Management process within IT Service Management, some activities need to evaluate the risk exposure associated with changes to be made to the infrastructure and services. The paper presents a method to evaluate risk exposure associated with a change. Further, we show how to use the risk exposure metric to automatically assign priorities to changes. The formal model developed for this purpose captures the business perspective by using financial metrics in the evaluation of risk. Thus the method is an example of Business-Driven IT Management. A case study, performed in conjunction with a large IT service provider, is reported and provides good results when compared to decisions made by human managers."
      }
    ]
  },
  {
    "tid":"134",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Multi-dimensional Scheduling Scheme for QoS-Aware Real-Time Applications on Heterogeneous Clusters\nContent: Multiple performance guarantees are increasingly needed in QoS-aware real-time applications. Unfortunately, most traditional scheduling algorithms only consider one or two dimensions of them. In this paper, we build a new scheduler model that is suitable for providing multiple services. Based on the scheduler mode, we propose a novel multi-dimensional scheduling scheme MDSS, which tasks timing constraints, QoS, throughput, fairness and load balancing into account. The MDSS includes three steps. Step 1 is used to provide real-time scheduling and the existing DASAP algorithm is employed in step 1. In step 2, we present a novel algorithm MQFQ that strives to enhance the QoS levels of accepted tasks and makes these tasks have fair QoS services, as well. Another new algorithm ITLB is proposed and used in step 3. The ITLB algorithms is capable of balancing load of nodes and improving throughput of the system. We have performed extensive simulation experiments to compare MDSS scheme with MDSR scheme, DASAP and DALAP algorithms. Experimental results indicate that MDSS improves QoS benefit, has better load balancing and has higher throughput over MDSR, DASAP and DALAP while guaranteeing high schedulability.\n\nTitle: Adaptive energy-efficient scheduling for real-time tasks on DVS-enabled heterogeneous clusters\nContent: Developing energy-efficient clusters not only can reduce power electricity cost but also can improve system reliability. Existing scheduling strategies developed for energy-efficient clusters conserve energy at the cost of performance. The performance problem becomes especially apparent when cluster computing systems are heavily loaded. To address this issue, we propose in this paper a novel scheduling strategy-adaptive energy-efficient scheduling or AEES-for aperiodic and independent real-time tasks on heterogeneous clusters with dynamic voltage scaling. The AEES scheme aims to adaptively adjust voltages according to the workload conditions of a cluster, thereby making the best trade-offs between energy conservation and schedulability. When the cluster is heavily loaded, AEES considers voltage levels of both new tasks and running tasks to meet tasks' deadlines. Under light load, AEES aggressively reduces the voltage levels to conserve energy while maintaining higher guarantee ratios. We conducted extensive experiments to compare AEES with an existing algorithm-MEG, as well as two baseline algorithms-MELV, MEHV. Experimental results show that AEES significantly improves the scheduling quality of MELV, MEHV and MEG.\n\nTitle: Towards Adaptive Power-Aware Scheduling for Real-Time Tasks on DVS-Enabled Heterogeneous Clusters\nContent: Nowadays, power reducing has become a critical issue in clusters for minimizing electricity cost, improving system reliability and protecting environment. Consequently, developing power-aware scheduling strategies for applications on clusters, especially on heterogeneous clusters is highly desirable. We propose in this paper an adaptive power-aware scheduling strategy called APAS for a periodic real-time tasks on DVS-enabled heterogeneous clusters. APAS takes the scheduliability, power consumption, and system load into consideration. While scheduling, APAS is capable of adaptively adjusting voltage levels according to the system load. When the system is in heavy load, APAS increases voltage levels to improve scheduliability. In contrast, when the system is lightly loaded, APAS degrades voltage levels to reduce power consumption while guaranteeing high scheduliability. Compared with Greedy, SLVL and SHVL, APAS shows obvious excellent scheduling quality to others by simulation experiments.\n\nTitle: Multi-Dimensional Scheduling for Real-Time Tasks on Heterogeneous Clusters.\nContent: Multiple performance requirements need to be guaranteed in some real-time applications such as multimedia data processing\n and real-time signal processing in addition to timing constraints. Unfortunately, most conventional scheduling algorithms\n only take one or two dimensions of them into account. Motivated by this fact, this paper investigates the problem of providing\n multiple performance guarantees including timeliness, QoS, throughput, QoS fairness and load balancing for a set of independent\n tasks by dynamic scheduling. We build a scheduler model that can be used for multi-dimensional scheduling. Based on the scheduler\n model, we propose a heuristic multi-dimensional scheduling strategy, MDSS, consisting of three steps. The first step can be\n of any existing real-time scheduling algorithm that determines to accept or reject a task. In step 2, we put forward a novel\n algorithm MQFQ to enhance the QoS levels of accepted tasks, and to make these tasks have fair QoS levels at the same time.\n Another new algorithm ITLB is proposed and used in step 3. The ITLB algorithm is capable of balancing load and improving throughput\n of the system. To evaluate the performance of MDSS, we perform extensive simulation experiments to compare MDSS strategy with\n MDSR strategy, DASAP and DALAP algorithms. Experimental results show that MDSS significantly outperforms MDSR, DASAP and DALAP.\n\nTitle: A parallel strategy for plane sweep algorithm in multi-core system\nContent: This paper proposes to design and realize a parallel strategy based on plane sweep algorithm that can be used in the multi-core system. The main contribution of this work is to present an efficient method to compute the large and complex data with plane sweep method. We use MPI (Message Passing Interface) as the programming model in Linux multi-core system, select shape file as the standard input and output files, and present a dynamic tree merge model to collect the result from each computing core, in which the time complexity can be reduced from O(n2) to O(n log n) and the problem of data lock and load balancing can be effectively solved. Experiment shows that in single-core system, our algorithm has a better performance than ArcGIS; in multi-core system of 8-cores, our algorithm can obtain 8\u00d7 speedups compared with ArcGIS.\n\n## Task\nCompose an abstract for the title 'Novel clock synchronization algorithm of parametric difference for parallel and distributed simulations' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Clock synchronization is crucial in distributed simulation applications (DSAs). Traditional synchronization methods are mainly based on computers' physical clocks and have network delays. Consequently, the clock jumps easily, and the stability of the algorithm is less than excellent. Furthermore, a new class of DSAs with flexible and dynamically assembled simulation components to optimize the performance of clock clusters is emerging. To remedy the problems, we propose a novel logic parametric difference clock (PDC), which is recursively constructed by an alterable parametric difference frequency (PDF) and its counter. We also develop a simple and efficient synchronization algorithm of parametric difference (SAPD), in which the indeterminate network delay is considered in the PDF, and the PDC counter is used as the synchronous symbol. Compared with existing typical algorithms, the SAPD has the predominant advantages of convergence, stability, and precision for DSAs in local area networks."
      }
    ]
  },
  {
    "tid":"135",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: LEPA: Incentivizing Long-term Privacy-preserving Data Aggregation in Crowdsensing.\nContent: In this paper, we study the incentive mechanism design for real-time data aggregation, which holds a large spectrum of crowdsensing applications. Despite extensive studies on static incentive mechanisms, none of these are applicable to real-time data aggregation due to their incapability of maintaining PUsu0027 long-term participation. We emphasize that, to maintain PUsu0027 long-term participation, it is of significant importance to protect their privacy as well as to provide them a desirable cumulative compensation. Thus motivated, in this paper, we propose LEPA, an efficient incentive mechanism to stimulate long-term participation in real-time data aggregation. Specifically, we allow PUs to preserve their privacy by reporting noisy data, the impact of which on the aggregation accuracy is quantified with proper privacy and accuracy measures. Then, we provide a framework that jointly optimizes the incentive schemes in different time slots to ensure desirable cumulative compensation for PUs and thereby prevent PUs from leaving the system halfway. Considering PUsu0027 strategic behaviors and combinatorial nature of the sensing tasks, we propose a computationally efficient on-line auction with close-to-optimal performance in presence of NP-hardness of winner user selection. We further show that the proposed on-line auction satisfies desirable properties of truthfulness and individual rationality. The performance of LEPA is validated by both theoretical analysis and extensive simulations.\n\nTitle: Achieving Bilateral Utility Maximization and Location Privacy Preservation in Database-Driven Cognitive Radio Networks\nContent: Database-driven cognitive radio has been well recognized as an efficient way to reduce interference between Primary Users (PUs) and Secondary Users (SUs). In database-driven cognitive radio, PUs and SUs must provide their locations to enable dynamic channel allocation, which raises location privacy breach concern. Previous studies only focus on unilateral privacy preservation, i.e., Only PUs' or SUs' privacy is preserved. In this paper, we propose to protect bilateral location privacy of a PU and an SU. The main challenge lies in how to coordinate the PU and SU to maximize their utility provided that their location privacy is protected. We first introduce a quantitative method to calculate both PU's and SU's location privacy, and then design a novel privacy preserving Utility Maximization protocol (UMax). UMax allows for both PU and SU to adjust their privacy preserving levels and optimize transmit power iteratively to achieve the maximum utility. Through extensive evaluations, we demonstrate that our proposed mechanism can efficiently increase the utility of both PU and SU while preserving their location privacy.\n\nTitle: Toward optimal allocation of location dependent tasks in crowdsensing\nContent: Crowdsensing offers an efficient approach to meet the demand in large scale sensing applications. In crowdsensing, it is of great interest to find the optimal task allocation, which is challenging since sensing tasks with different requirements of quality of sensing are typically associated with specific locations and mobile users are constrained by time budgets. We show that the allocation problem is NP hard. We then focus on approximation algorithms, and devise an efficient local ratio based algorithm (LRBA). Our analysis shows that the approximation ratio of the aggregate rewards obtained by the optimal allocation to those by LRBA is 5. This reveals that LRBA is efficient, since a lower (but not tight) bound on the approximation ratio is 4. We also discuss about how to decide the fair prices of sensing tasks to provide incentives since mobile users tend to decline the tasks with low incentives. We design a pricing mechanism based on bargaining theory, in which the price of each task is determined by the performing cost and market demand (i.e., the number of mobile users who intend to perform the task). Extensive simulation results are provided to demonstrate the advantages of our proposed scheme.\n\nTitle: Promoting Cooperation by the Social Incentive Mechanism in Mobile Crowdsensing.\nContent: An incentive mechanism is important for mobile crowdsensing to recruit sufficient participants to complete large-scale sensing tasks with high quality. Previous incentive mechanisms have focused on quantifying participants' contribution to the quality of sensing and provide incentives directly to them. In this article, we introduce a novel approach, called the social incentive mechanism, which, surprisingly, incentivizes the social friends of the participants who perform the sensing tasks. The basic idea is to leverage the social ties among participants to promote global cooperation. Since the incentive that a participant receives largely relies on the behaviors of his/her social friends, participants have the motivation to impact their friends' behaviors through their social relationships in order to gain a higher payoff. This approach is applicable to many scenarios where the contributions to the quality of sensing among participants are interdependent, such as data aggregation. We have provided a case study which shows that the social incentive mechanism is more cost-effective than traditional incentive mechanisms.\n\nTitle: Re-DPoctor: Real-time health data releasing with w-day differential privacy.\nContent: Wearable devices enable users to collect health data and share them with healthcare providers for improved health service. Since health data contain privacy-sensitive information, unprotected data release system may result in privacy leakage problem. Most of the existing work use differential privacy for private data release. However, they have limitations in healthcare scenarios because they do not consider the unique features of health data being collected from wearables, such as continuous real-time collection and pattern preservation. In this paper, we propose Re-DPoctor, a real-time health data releasing scheme with w-day differential privacy where the privacy of health data collected from any consecutive w days is preserved. We improve utility by using a specially-designed partition algorithm to protect the health data patterns. Meanwhile, we improve privacy preservation by applying newly proposed adaptive sampling technique and budget allocation method. We prove that Re-DPoctor satisfies w-day differential privacy. Experiments on real health data demonstrates that our method achieves better utility with strong privacy guarantee than existing state-of-the-art methods.\n\n## Task\nCompose an abstract for the title 'Crowd-Empowered Privacy-Preserving Data Aggregation for Mobile Crowdsensing.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We develop an auction framework for privacy-preserving data aggregation in mobile crowdsensing, where the platform plays the role as an auctioneer to recruit workers for a sensing task. In this framework, the workers are allowed to report privacy-preserving versions of their data to protect their data privacy; and the platform selects workers based on their sensing capabilities, which aims to address the drawbacks of game-theoretic models that cannot ensure the accuracy level of the aggregated result, due to the existence of multiple Nash Equilibria. Observe that in this auction based framework, there exists externalities among workers' data privacy, because the data privacy of each worker depends on both her injected noise and the total noise in the aggregated result that is intimately related to which workers are selected to fulfill the task. To achieve a desirable accuracy level of the data aggregation in a cost-effective manner, we explicitly characterize the externalities, i.e., the impact of the noise added by each worker on both the data privacy and the accuracy of the aggregated result. Further, we explore the problem structure, characterize the hidden monotonicity property of the problem, and determine the critical bid of workers, which makes it possible to design a truthful, individually rational and computationally efficient incentive mechanism. The proposed incentive mechanism can recruit a set of workers to approximately minimize the cost of purchasing private sensing data from workers subject to the accuracy requirement of the aggregated result. We validate the proposed scheme through theoretical analysis as well as extensive simulations.\n\n"
      }
    ]
  },
  {
    "tid":"136",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Artificial Noise Aided Energy Efficiency Optimization in MIMOME System With SWIPT.\nContent: This letter studies energy efficient optimization problem for a general multiple-input multiple-output multiple-eavesdropper system, in which the legitimate receiver exploits a power splitting (PS) scheme for decoding information and harvesting energy simultaneously. In order to cripple the eavesdroppers' reception and provide energy for the legitimate receiver to harvest, the transmitter also emits artificial noise (AN) in the transmission. Our interest in this letter lies in the joint design of the transmit covariance matrices and PS ratio, such that the secrecy energy efficiency (SEE) is maximized. The SEE maximization problem is a nonconvex fractional problem. To handle it, we resort to the fractional programming to transform its objective function into a more tractable subtractive form. Furthermore, we show that the resultant problem is primal decomposable by recasting it as an equivalent form amenable to alternating optimization. Numerical results indicate that incorporating AN and PS is beneficial to the SEE enhancement.\n\nTitle: Sum Secrecy Rate Optimization For Mimome Wiretap Channel With Artificial Noise And D2d Underlay Communication\nContent: This paper considers a cellular multiple-input multiple-output multiple-eavesdropper (MIMOME) channel, with a pair of single-antenna device-to-device (D2D) nodes working as an underlay. A novel eavesdropping scenario is studied in this paper, where the eavesdroppers intend to simultaneously overhear the cellular communication and the D2D communication. Our goal is to jointly optimize the covariance of confidential message and artificial noise, as well as the transmit power at the D2D transmitter, such that the sum secrecy rate is maximized, while satisfying the quality of service constraint on the D2D communication. This sum secrecy rate maximization (SSRM) problem is non-convex by nature. To handle it, an equivalent reformulation of this SSRM problem is introduced, wherein the resulting problem becomes primal decomposable and thus can be iteratively solved using an alternating optimization (AO) algorithm. Also, we prove that the AO algorithm is bound to converge to a stationary point of the primal SSRM problem. Furthermore, we extend the SSRM problem to a more general case with multiple pairs of D2D nodes. Again, the resulting problem is shown to be solvable by the AO algorithm, with provable convergence to the stationary point. Finally, numerical results are presented to verify the efficacy of our proposed method.\n\nTitle: Optimal Precoding Design and Power Allocation for Decentralized Detection of Deterministic Signals\nContent: We consider a decentralized detection problem in a power-constrained wireless sensor network (WSN), in which a number of sensor nodes collaborate to detect the presence of a deterministic vector signal. The signal to be detected is assumed known a priori. Each sensor conducts a local linear processing to convert its observations into one or multiple messages. The messages are conveyed to the fusion center (FC) by an uncoded amplify-and-forward scheme, where a global decision is made. Given a total network transmit power constraint, we investigate the optimal linear processing strategy for each sensor. Our study finds that the optimal linear precoder has the form of a matched filter. Depending on the channel characteristics, one or multiple versions of the filtered/compressed message should be reported to the FC. In addition, assuming a fixed total transmit power, we examine how the detection performance behaves with the number of sensors in the network. Analysis shows that increasing the number of sensors can substantially improve the system detection reliability. Finally, decentralized detection with unknown signals is studied and a heuristic precoding design is proposed. Numerical results are conducted to corroborate our theoretical analysis and to illustrate the performance of the proposed algorithm.\n\nTitle: Optimal precoding design for decentralized detection of deterministic signals\nContent: We consider a decentralized detection problem in a power-constrained wireless sensor networks (WSNs), in which a number of sensor nodes collaborate to detect the presence of a deterministic vector signal. The signal to be detected is assumed known a priori. Each sensor conducts a local linear processing to convert its observations into one or multiple messages. The messages are conveyed to the fusion center (FC) by an uncoded amplify-and-forward scheme, where a global decision is made. Given a total network transmit power constraint, we investigate the optimal linear processing strategy for each sensor. Our study finds that the optimal linear precoder has the form of a matched filter. Depending on the channel characteristics, one or multiple versions of the filtered/compressed message should be reported to the FC.\n\nTitle: Distributed compression and estimation for wireless sensor networks with noisy channels\nContent: The problem of distributed estimation in wireless sensor networks (WSNs) has attracted much attention. As bandwidth and power are severely limited in WSNs, one of the main objectives in current WSN research is to design bandwidth and power efficient algorithms. A multitude of studies along this line have appeared. Among them, some previous works (e.g., (A. Ribeiro and G.B. Giannakis, 2006), (Z. Luo, 2005), (J.-J. Xiao, 2006), (J. Li and G. AlRegib, 2007), (H. Li and J. Fang, 2007)) consider distributed estimation using aggressively quantized versions of the original observations. In this setup, quantization becomes an integral part of the estimation process and is critical to the estimation performance. Another category of methods (e.g., (K. Zhang et al., 2003), (Z-Q Luo et al., 2005), (Y. Zhu et al., 2005), (E. Song et al., 2005), (I.D. Schizas et al., 2007), (J. Fang and H. Li)), not relying on the above low-rate quantization strategy, follow an optimal decentralized compression-estimation approach to reduce the transmission requirement. In these methods, the data dimensionality is reduced before each sensor sends its data to a fusion center (FC). Upon receiving the compressed data, the FC combines them according to some fusion criterion to obtain a final estimate. The crux of these techniques is to design the compression matrix so as to minimize the estimation mean-square error (MSE), which has been extensively investigated by (K. Zhang et al., 2003), (Z-Q Luo et al., 2005), (Y. Zhu et al., 2005), (E. Song et al., 2005), (I.D. Schizas et al., 2007), (J. Fang and H. Li) under different fusion criterions and observation correlation scenarios. These works (K. Zhang et al., 2003), (Z-Q Luo et al., 2005), (Y. Zhu et al., 2005), (E. Song et al., 2005), (I.D. Schizas et al., 2007), (J. Fang and H. Li), however, mostly assume perfect wireless channels through which the signals can be sent from the sensors to the fusion center (FC) without any distortion. This assumption,- - clearly, is unrealistic in practice because the wireless links inevitably suffer from the channel noise and adverse channel effects such as fading and attenuation. In this paper, we study the problem of decentralized compression-estimation in the presence of channel noise and fading. By utilizing a series of matrix transformations and established properties, we present a scheme to design the compression matrices. Specifically, for the single sensor case, an analytic solution of the optimal compression is derived.\n\n## Task\nCompose an abstract for the title 'Artificial noise-aided biobjective transmitter optimization for service integration in multi-user MIMO broadcast channel.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper considers an artificial noise (AN)-aided transmit design for multi-user MIMO systems with integrated services. Specifically, two sorts of service messages are combined and served simultaneously: one multicast message intended for all receivers and one confidential message intended for only one receiver and required to be perfectly secure from other unauthorized receivers. Our interest lies in the joint design of input covariances of the multicast message, confidential message, and artificial noise (AN), such that the achievable secrecy rate and multicast rate are simultaneously maximized. This problem is identified as a secrecy rate region maximization (SRRM) problem in the context of physical-layer service integration. Since this biobjective optimization problem is inherently complex to solve, we put forward two different scalarization methods to convert it into a scalar optimization problem. First, we propose to prefix the multicast rate as a constant, and accordingly, the primal biobjective problem is converted into a secrecy rate maximization (SRM) problem with quality of multicast service (QoMS) constraint. By varying the constant, we can obtain different Pareto optimal points. The resulting SRM problem can be iteratively solved via a provably convergent difference-of-concave (DC) algorithm. In the second method, we aim to maximize the weighted sum of the secrecy rate and the multicast rate. Through varying the weighted vector, one can also obtain different Pareto optimal points. We show that this weighted sum rate maximization (WSRM) problem can be recast into a primal decomposable form, which is amenable to alternating optimization (AO). Then, we compare these two scalarization methods in terms of their overall performance and computational complexity via theoretical analysis as well as numerical simulation, based on which new insights can be drawn."
      }
    ]
  },
  {
    "tid":"137",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Circuit-aware design of energy-efficient massive MIMO systems\nContent: Densification is a key to greater throughput in cellular networks. The full potential of coordinated multipoint (CoMP) can be realized by massive multiple-input multiple-output (MIMO) systems, where each base station (BS) has very many antennas. However, the improved throughput comes at the price of more infrastructure; hardware cost and circuit power consumption scale linearly/affinely with the number of antennas. In this paper, we show that one can make the circuit power increase with only the square root of the number of antennas by circuit-aware system design. To this end, we derive achievable user rates for a system model with hardware imperfections and show how the level of imperfections can be gradually increased while maintaining high throughput. The connection between this scaling law and the circuit power consumption is established for different circuits at the BS.\n\nTitle: Massive MIMO With Optimal Power and Training Duration Allocation\nContent: We consider the uplink of multicell multiple-input-multiple-output systems, where the base stations (BSs), which are equipped with massive arrays, simultaneously serve several terminals in the same frequency band. We assume that the BS estimates the channel from uplink training and then uses the maximum ratio combining technique to detect the signals transmitted from all terminals in its own cell. We propose an optimal resource allocation scheme that jointly selects the training duration, the training signal power, and the data signal power in order to maximize the sum spectral efficiency for a given total energy budget spent in a coherence interval. Numerical results verify the benefits of the optimal resource allocation scheme. Furthermore, we show that more training signal power should be used at low signal-to-noise ratios (SNRs) and vice versa at high SNRs. Interestingly, for the entire SNR regime, the optimal training duration is equal to the number of terminals.\n\nTitle: Massive MIMO with Non-Ideal Arbitrary Arrays: Hardware Scaling Laws and Circuit-Aware Design\nContent: Massive multiple-input multiple-output (MIMO) systems are cellular networks where the base stations (BSs) are equipped with unconventionally many antennas, deployed on colocated or distributed arrays. Huge spatial degrees-of-freedom are achieved by coherent processing over these massive arrays, which provide strong signal gains, resilience to imperfect channel knowledge, and low interference. This comes at the price of more infrastructure; the hardware cost and circuit power consumption scale linearly/affinely with the number of BS antennas N. Hence, the key to cost-efficient deployment of large arrays is low-cost antenna branches with low circuit power, in contrast to today\u2019s conventional expensive and power-hungry BS antenna branches. Such low-cost transceivers are prone to hardware imperfections, but it has been conjectured that the huge degrees-of-freedom would bring robustness to such imperfections. We prove this claim for a generalized uplink system with multiplicative phasedrifts, additive distortion noise, and noise amplification. Specifically, we derive closed-form expressions for the user rates and a scaling law that shows how fast the hardware imperfections can increase with N while maintaining high rates. The connection between this scaling law and the power consumption of different transceiver circuits is rigorously exemplified. This reveals that one can make the circuit power increase as p N, instead of linearly, by careful circuit-aware system design.\n\nTitle: Massive MIMO systems with hardware-constrained base stations\nContent: Massive multiple-input multiple-output (MIMO) systems are cellular networks where the base stations (BSs) are equipped with unconventionally many antennas. Such large antenna arrays offer huge spatial degrees-of-freedom for transmission optimization; in particular, great signal gains, resilience to imperfect channel knowledge, and small inter-user interference are all achievable without extensive inter-cell coordination. The key to cost-efficient deployment of large arrays is the use of hardware-constrained base stations with low-cost antenna elements, as compared to today's expensive and power-hungry BSs. Low-cost transceivers are prone to hardware imperfections, but it has been conjectured that the excessive degrees-of-freedom of massive MIMO would bring robustness to such imperfections. We herein prove this claim for an uplink channel with multiplicative phase-drift, additive distortion noise, and noise amplification. Specifically, we derive a closed-form scaling law that shows how fast the imperfections increase with the number of antennas.\n\nTitle: Hardware design and optimal ADC resolution for uplink massive MIMO systems\nContent: This work focuses on the hardware design for the efficient operation of Massive multiple-input multiple-output (MIMO) systems. A closed-form uplink achievable data rate expression is derived considering imperfect channel state information (CSI) and hardware impairments. We formulate an optimization problem to maximize the sum data rate subject to a constraint on the total power consumption. A general power consumption model accounting for the level of hardware impairments is utilized. The optimization variables are the number of base station (BS) antennas and the level of impairments per BS antenna. The resolution of the analog-to-digital converter (ADC) is a primary source of such impairments. The results show the trade-off between the number of BS antennas and the level of hardware impairments, which is important for practical hardware design. Moreover, the maximum power consumption can be tuned to achieve maximum energy efficiency (EE). Numerical results suggest that the optimal level of hardware impairments yields ADCs of 4 to 5 quantization bits.\n\n## Task\nCompose an abstract for the title 'Energy efficiency optimization in hardware-constrained large-scale MIMO systems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Large-scale multiple-input multiple-output (MIMO) communication systems can bring substantial improvement in spectral efficiency and/or energy efficiency, due to the excessive degrees-of-freedom and huge array gain. However, large-scale MIMO is expected to deploy lower-cost radio frequency (RF) components, which are particularly prone to hardware impairments. Unfortunately, compensation schemes are not able to remove the impact of hardware impairments completely, such that a certain amount of residual impairments always exists. In this paper, we investigate the impact of residual transmit RF impairments (RTRI) on the spectral and energy efficiency of training-based point-to-point large-scale MIMO systems, and seek to determine the optimal training length and number of antennas which maximize the energy efficiency. We derive deterministic equivalents of the signal-to-noise-and-interference ratio (SINR) with zero-forcing (ZF) receivers, as well as the corresponding spectral and energy efficiency, which are shown to be accurate even for small number of antennas. Through an iterative sequential optimization, we find that the optimal training length of systems with RTRI can be smaller compared to ideal hardware systems in the moderate SNR regime, while larger in the high SNR regime. Moreover, it is observed that RTRI can significantly decrease the optimal number of transmit and receive antennas."
      }
    ]
  },
  {
    "tid":"138",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Identifying epileptic source location and extent: An iterative sparse electromagnetic source imaging algorithm\nContent: In this paper we have introduced a novel electromagnetic source imaging (ESI) technique and demonstrated its validity and excellent performance in imaging the location and extent of underlying epileptic sources in patients suffering from focal epilepsy. The proposed algorithm employs ideas from sparse signal processing literature and convex optimization theories to improve source imaging results obtained from scalp-recorded electroencephalogram (EEG). EEG source imaging results generally use subjective methods to determine the extent of the underlying brain activity. The proposed technique provides significant improvement in dealing with such shortcomings and eliminates the need for thresholding. The results of our computer simulations and clinical validation study demonstrate the excellent performance of the proposed algorithm and suggest it may become a useful tool for objectively determining the location and extent of focal epileptic activity in a noninvasive fashion.\n\nTitle: Dynamic imaging of ictal oscillations using non-invasive high-resolution EEG.\nContent: Scalp electroencephalography (EEG) has been established as a major component of the pre-surgical evaluation for epilepsy surgery. However, its ability to localize seizure onset zones (SOZ) has been significantly restricted by its low spatial resolution and indirect correlation with underlying brain activities. Here we report a novel non-invasive dynamic seizure imaging (DSI) approach based upon high-density EEG recordings. This novel approach was particularly designed to image the dynamic changes of ictal rhythmic discharges that evolve through time, space and frequency. This method was evaluated in a group of 8 epilepsy patients and results were rigorously validated using intracranial EEG (iEEG) (n=3) and surgical outcome (n=7). The DSI localized the ictal activity in concordance with surgically resected zones and ictal iEEG recordings in the cohort of patients. The present promising results support the ability to precisely and accurately image dynamic seizure activity from non-invasive measurements. The successful establishment of such a non-invasive seizure imaging modality for surgical evaluation will have a significant impact in the management of medically intractable epilepsy.\n\nTitle: Noninvasive cortical imaging of epileptiform activities from interictal spikes in pediatric patients.\nContent: Improved non-invasive localization of the epileptogenic foci prior to epilepsy surgery would improve surgical outcome in patients with partial seizure disorders. A critical component for the identification of the epileptogenic brain is the analysis of electrophysiological data obtained during ictal activity from prolonged intracranial recordings. The development of a noninvasive means to identify the seizure onset zone (SOZ) would thus play an important role in treating patients with intractable epilepsy. In the present study, we have investigated non-invasive imaging of epileptiform activity in patients with medically intractable epilepsy by means of a cortical potential imaging (CPI) technique. Eight pediatric patients (1M/7F, ages 4\u201314years) with intractable partial epilepsy were studied. Each patient had multiple (6 to 14) interictal spikes (IIS) subjected to the CPI analysis. Realistic geometry boundary element head models were built using each individual's MRI in order to maximize the imaging precision. CPI analysis was performed on the IISs, and extrema in the estimated CPI images were compared with SOZs as determined from the ictal electrocorticogram (ECoG) recordings, as well as the resected areas in the patients and surgical outcomes. The distances between the maximum cortical activities of the IISs reflected by the estimated cortical potential distributions and the SOZs were determined to quantitatively evaluate the performance of the CPI in localizing the epileptogenic zone. Ictal ECoG recordings revealed that six patients exhibited a single epileptogenic focus while two patients had multiple foci. In each patient, the CPI results revealed an area of activity overlapping with the SOZs as identified by ictal ECoG. The distance from the extreme of the CPI images at the peak of IIS to the nearest intracranial electrode associated with the onset of the ictal activity was evaluated for each patient and the averaged distance was 4.6mm. In the group of patients studied, the CPI imaged epileptogenic foci were within the resected areas. According to the follow-up of the eight patients included, two were seizure free and six had substantial reduction in seizure frequency. These promising results demonstrate the potential for noninvasive localization of the epileptogenic focus from interictal scalp EEG recordings. Confirmation of our results may have a significant impact on the process of presurgical planning in pediatric patients with intractable epilepsy by dramatically reducing or potentially eliminating the use of intracranial recording.\n\nTitle: Dynamic imaging of ictal rhythmic activity using dense-array EEG.\nContent: Electroencephalogram (EEG) is an important component of the pre-surgical evaluation in the treatment of medically intractable epilepsy. However, clinical EEG uses 19 to 32 electrodes that significantly limits its localization ability. Recent development of dense-array recording techniques has suggested that increased spatial sampling rate improves the accuracy of source localization. In the current study, we proposed a 76-channel EEG system for the long-term monitoring of epilepsy patients, and proposed a dynamic seizure imaging (DSI) technique to image the ictal rhythmic activity that may evolve through time, space and frequency. We tested the system in a cohort of 8 patients and our results show that the DSI estimated the seizure activity in good correlation with intracranial recordings, successful surgery outcomes and other clinical evidence. The proposed dense-array recording and DSI imaging approach enable a non-invasive but quantitative imaging of continuous seizure activity. The results suggest that DSI may potentially be useful to assist the pre-surgical evaluation in patients with intractable epilepsy.\n\nTitle: Noninvasive imaging of the high frequency brain activity in focal epilepsy patients.\nContent: High-frequency (HF) activity represents a potential biomarker of the epileptogenic zone in epilepsy patients, the removal of which is considered to be crucial for seizure-free surgical outcome. We proposed a high frequency source imaging (HFSI) approach to noninvasively image the brain sources of the scalp-recorded HF EEG activity. Both computer simulation and clinical patient data analysis were performed to investigate the feasibility of using the HFSI approach to image the sources of HF activity from noninvasive scalp EEG recordings. The HF activity was identified from high-density scalp recordings after high-pass filtering the EEG data and the EEG segments with HF activity were concatenated together to form repetitive HF activity. Independent component analysis was utilized to extract the components corresponding to the HF activity. Noninvasive EEG source imaging using realistic geometric boundary element head modeling was then applied to image the sources of the pathological HF brain activity. Five medically intractable focal epilepsy patients were studied and the estimated sources were found to be concordant with the surgical resection or intracranial recordings of the patients. The present study demonstrates, for the first time, that source imaging from the scalp HF activity could help to localize the seizure onset zone and provide a novel noninvasive way of studying the epileptic brain in humans. This study also indicates the potential application of studying HF activity in the presurgical planning of medically intractable epilepsy patients.\n\n## Task\nCompose an abstract for the title 'Seizure Prediction In Intracranial Eeg: A Patient-Specific Rule-Based Approach' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this study, we report our development of a patient-specific rule-based seizure prediction system. Five univariate and one bivariate nonlinear measures were extracted from non-overlapping 10-second segments of intracranial EEG (iEEG) data recorded using both depth electrodes in the brain and subdural electrodes over the cortical surface. Nonlinear features representing the specific characteristic properties of EEG signal were then integrated spatio-temporally in a way to predict to predict seizure with high sensitivity. The present system was tested on 58 hours of iEEG data containing ten seizures recorded in two patients with medically intractable focal epilepsy. Within a prediction horizon of 30 and 60 minutes, our method showed an average sensitivity of 90% and 96.5% with an average false prediction rate of 0.06/h and 0.055/h, respectively. The present results suggest that such a rule-based system can become potentially a useful approach for predicting seizures prior to onset."
      }
    ]
  },
  {
    "tid":"139",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: The MOLEN Polymorphic Processor\nContent: In this paper, we present a polymorphic processor paradigm incorporating both general purpose and custom computing processing. The proposal incorporates an arbitrary number of programmable units, exposes the hardware to the programmers/designers, and allows them to modify and extend the processor functionality at will. To achieve the previously stated attributes, we present a new programming paradigm, a new instruction set architecture, a microcode-based microarchitecture, and a compiler methodology. The programming paradigm, in contrast with the conventional programming paradigms, allows general-purpose conventional code and hardware descriptions to coexist in a program. In our proposal, for a given instruction set architecture, a one-time instruction set extension of eight instructions is sufficient to implement the reconfigurable functionality of the processor. We propose a microarchitecture based on reconfigurable hardware emulation to allow high-speed reconfiguration and execution. To prove the viability of the proposal, we experimented with the MPEG-2 encoder and decoder and a Xilinx Virtex II Pro FPGA. We have implemented three operations, SAD, DCT, and IDCT. The overall attainable application speedup for the MPEG-2 encoder and decoder is between 2.64-3.18 and between 1.56-1.94, respectively, representing between 93 percent and 98 percent of the theoretically obtainable speedups.\n\nTitle: The Virtex II ProTM MOLEN Processor\nContent: We use the Xilinx Virtex II Pro\u2122 technology as prototyping plat- form to design a MOLEN polymorphic processor, a custom computing machine based on the co-processor architectural paradigm. The PowerPC embedded in the FPGA is operating as a general purpose (core) processor and the reconfigurable fabric is used as a reconfigurable co-processor. The paper focuses on hardware synthesis results and experimental performance evaluation, proving the viability of the MOLEN concept. More precisely, the MPEG-2 application is accelerated very closely to its theoretical limits by implementing SAD, DCT and IDCT as re- configurable co-processors. For a set of popular test video sequences the MPEG-2 encoder overall speedup is in the range between 2.64 and 3.18. The speedup of the MPEG-2 decoder varies between 1.65 and 1.94.\n\nTitle: ImpEDE: A multidimensional design-space exploration framework for biomedical-implant processors\nContent: The demand for biomedical implants keeps increasing. However, most of the current implant design methodologies involve custom-ASIC design. The SiMS project aims to change this process and make implant design more modular, flexible, faster and extensible. The most recent work within the SiMS context provides ImpEDE, a framework based on a multiobjective genetic algorithm, for automatic exploration of the design space of implant processors. The framework provides the processor designer with a Pareto front through which informed decisions can be made about specific implant families after analyzing their particular tradeoffs and requirements. A highly efficient, parallelized version of the genetic algorithm is also used to evolve the front and has as its objectives the optimization of power, performance and area. In addition, we illustrate the extensibility of our framework by modifying it to include a case study of a synthetic implant application with hard realtime deadlines.\n\nTitle: A reconfigurable audio beamforming multi-core processor\nContent: Over the last years, the Beamforming technique has been adopted by the audio engineering society to amplify the signal of an acoustic source, while attenuating any ambient noise. Existing software implementations provide a flexible customizing environment, however they introduce performance limitations and excessive power consumption overheads. On the other hand, hardware approaches achieve significantly better performance and lower power consumption compared to the software ones, but they lack the flexibility of a high-level versatile programming environment. To address these drawbacks, we have already proposed a minimalistic processor architecture tailoring audio Beamforming applications to configurable hardware. In this paper, we present its application as a multi-core reconfigurable Beamforming processor and describe our hardware prototype, which is mapped onto a Virtex4FX60 FPGA. Our approach combines software programming flexibility with improved hardware performance, low power consumption and compact program-executable memory footprint. Experimental results suggest that our FPGA-based processor, running at 100 MHz, can extract in real-time up to 14 acoustic sources 2.6 times faster than a 3.0 GHz Core2 Duo OpenMP-based implementation. Furthermore, it dissipates an order of magnitude less energy, compared to the general purpose processor software implementation.\n\nTitle: Implementation Study of FFT on Multi-lane Vector Processors\nContent: In this paper we extend a custom FFT vector architecture by adding multiple lane capabilities and study its hardware implementation. We use the six step algorithm to segment a long Fourier transform of size $N=Z\\times L$ into $L$ smaller transforms of size $Z$. We split the data into pairs of vector registers (for the real and imaginary part), each containing $Z$ elements. A vector register pair with its corresponding functional unit form a single lane replicated $L$ times. While smaller transforms proceed iteratively all of them are computed in parallel. The shorter FFT transforms along the X dimension are computed using previously proposed vector permutations while the transforms along the Y dimension are performed using a simple butterfly network that handles inter-lane communication. All data patterns required by the FFT computation are generated implicitly in hardware by a simple control unit. No data transposition is required and the twiddle factors are stored locally inside the functional units. We validated our design through simulation and ASIC synthesis targeting 90nm CMOS technology. We compare three possible configurations for computing a 256point FFT, all running at 217 MHz with $Z\\times L$ equal to: a) $32\\times8$, b) $16\\times16$ and c) $8\\times32$. Configuration a) is the smallest and the slowest; configuration b) requires 1.43 times fewer cycles but 1.64 more area while configuration c) requires 1.76 times fewer cycles and is 3.16 times larger. Unlike other high performance FFT implementations, our design offers the possibility to trade-off execution speed for two resource types: vector register size and number of lanes. Another important contribution is the possibility to execute 2D FFT without any HW modifications or special provisions.\n\n## Task\nCompose an abstract for the title 'The MOLEN Processor Prototype' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We present a prototype design of the MOLEN polymorphic processor, a CCM based on the co-processor architectural paradigm. The Xilinx Virtex II Pro technology is used as a prototyping platform. Experimental results prove the viability of the MOLEN concept. More precisely, the MPEG-2 application is accelerated very closely to its theoretical limits by implementing SAD, DCT and IDCT in reconfigurable hardware. The MPEG-2 encoder overall speedup is in the range between 2.80 and 2.96. The speedup of the MPEG-2 decoder varies between 1.56 and 1.63."
      }
    ]
  },
  {
    "tid":"140",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Learning templates for artistic portrait lighting analysis.\nContent: Lighting is a key factor in creating impressive artistic portraits. In this paper, we propose to analyze portrait lighting by learning templates of lighting styles. Inspired by the experience of artists, we first define several novel features that describe the local contrasts in various face regions. The most informative features are then selected with a stepwise feature pursuit algorithm to derive the templates of various lighting styles. After that, the matching scores that measure the similarity between a testing portrait and those templates are calculated for lighting style classification. Furthermore, we train a regression model by the subjective scores and the feature responses of a template to predict the score of a portrait lighting quality. Based on the templates, a novel face illumination descriptor is defined to measure the difference between two portrait lightings. Experimental results show that the learned templates can well describe the lighting styles, whereas the proposed approach can assess the lighting quality of artistic portraits as human being does.\n\nTitle: Face Illumination Manipulation Using a Single Reference Image by Adaptive Layer Decomposition\nContent: This paper proposes a novel image-based framework to manipulate the illumination of human face through adaptive layer decomposition. According to our framework, only a single reference image, without any knowledge of the 3D geometry or material information of the input face, is needed. To transfer the illumination effects of a reference face image to a normal lighting face, we first decompose the lightness layers of the reference and the input images into large-scale and detail layers through weighted least squares (WLS) filter with adaptive smoothing parameters according to the gradient values of the face images. The large-scale layer of the reference image is filtered with the guidance of the input image by guided filter with adaptive smoothing parameters according to the face structures. The relit result is obtained by replacing the largescale layer of the input image with that of the reference image. To normalize the illumination effects of a non-normal lighting face (i.e., face delighting), we introduce similar reflectance prior to the layer decomposition stage by WLS filter, which make the normalized result less affected by the high contrast light and shadow effects of the input face. Through these two procedures, we can change the illumination effects of a non-normal lighting face by first normalizing the illumination and then transferring the illumination of another reference face to it. We acquire convincing relit results of both face relighting and delighting on numerous input and reference face images with various illumination effects and genders. Comparisons with previous papers show that our framework is less affected by geometry differences and can preserve better the identification structure and skin color of the input face.\n\nTitle: Artistic Illumination Transfer for Portraits\nContent: Relighting a portrait in a single image is still a challenging problem, particularly when only a single artistic reference photograph or painting is provided. In this paper, we propose an artistic illumination transfer system for portraits based on a database of portrait images (photographs and paintings) associated with hand-drawn illumination templates (276) by artists. Users can select a reference portrait image in the database, and the corresponding illumination template is transferred to an input portrait using image warping. Users can also provide reference portrait images those are not in the database. Based on the Face Illumination Descriptor (FID), the system selects from the database the reference image with the closest illumination to that of the user-provided reference image and adjusts the corresponding illumination template to match the contrast of the user-provided reference image. Experiments on not only paintings but also photographs, paper-cuts and sketches demonstrate that convincing illumination transferred results can be rendered by our system. \u00a9 2012 Wiley Periodicals, Inc.\n\nTitle: Face illumination transfer through edge-preserving filters\nContent: This article proposes a novel image-based method to transfer illumination from a reference face image to a target face image through edge-preserving filters. According to our method, only a single reference image, without any knowledge of the 3D geometry or material information of the target face, is needed. We first decompose the lightness layers of the reference and the target images into large-scale and detail layers through weighted least square (WLS) filter after face alignment. The large-scale layer of the reference image is filtered with the guidance of the target image. Adaptive parameter selection schemes for the edge-preserving filters is proposed in the above two filtering steps. The final relit result is obtained by replacing the large-scale layer of the target image with that of the reference image. We acquire convincing relit result on numerous target and reference face images with different lighting effects and genders. Comparisons with previous work show that our method is less affected by geometry differences and can preserve better the identification structure and skin color of the target face.\n\nTitle: Light field projection for lighting reproduction\nContent: We propose a novel approach to generate 4D light field in the physical world for lighting reproduction. The light field is generated by projecting lighting images on a lens array. The lens array turns the projected images into a controlled anisotropic point light source array which can simulate the light field of a real scene. In terms of acquisition, we capture an array of light probe images from a real scene, based on which an incident light field is generated. The lens array and the projectors are geometric and photometrically calibrated, and an efficient resampling algorithm is developed to turn the incident light field into the images projected onto the lens array. The reproduced illumination, which allows per-ray lighting control, can produce realistic lighting result on real objects, avoiding the complex process of geometric and material modeling. We demonstrate the effectiveness of our approach with a prototype setup.\n\n## Task\nCompose an abstract for the title 'Learning artistic lighting template from portrait photographs' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper presents a method for learning artistic portrait lighting template from a dataset of artistic and daily portrait photographs. The learned template can be used for (1) classification of artistic and daily portrait photographs, and (2) numerical aesthetic quality assessment of these photographs in lighting usage. For learning the template, we adopt Haar-like local lighting contrast features, which are then extracted from pre-defined areas on frontal faces, and selected to form a log-linear model using a stepwise feature pursuit algorithm. Our learned template corresponds well to some typical studio styles of portrait photography. With the template, the classification and assessment tasks are achieved under probability ratio test formulations. On our dataset composed of 350 artistic and 500 daily photographs, we achieve a 89.5% classification accuracy in cross-validated tests, and the assessment model assigns reasonable numerical scores based on portraits' aesthetic quality in lighting."
      }
    ]
  },
  {
    "tid":"141",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: The acoustic emotion gaussians model for emotion-based music annotation and retrieval\nContent: One of the most exciting but challenging endeavors in music research is to develop a computational model that comprehends the affective content of music signals and organizes a music collection according to emotion. In this paper, we propose a novel acoustic emotion Gaussians (AEG) model that defines a proper generative process of emotion perception in music. As a generative model, AEG permits easy and straightforward interpretations of the model learning processes. To bridge the acoustic feature space and music emotion space, a set of latent feature classes, which are learned from data, is introduced to perform the end-to-end semantic mappings between the two spaces. Based on the space of latent feature classes, the AEG model is applicable to both automatic music emotion annotation and emotion-based music retrieval. To gain insights into the AEG model, we also provide illustrations of the model learning process. A comprehensive performance study is conducted to demonstrate the superior accuracy of AEG over its predecessors, using two emotion annotated music corpora MER60 and MTurk. Our results show that the AEG model outperforms the state-of-the-art methods in automatic music emotion annotation. Moreover, for the first time a quantitative evaluation of emotion-based music retrieval is reported.\n\nTitle: A Histogram Density Modeling Approach To Music Emotion Recognition\nContent: Music emotion recognition is concerned with developing predictive models that comprehend the affective content of musical signals. Recently, a growing number of attempts has been made to model the music emotion as a probability distribution in the valence-arousal (VA) space to better account for the subjectivity. In this paper, we present a novel histogram density modeling approach that models the emotion distribution by a 2-D histogram over the quantized VA space and learns a set of latent histograms to predict the emotion probability density of a song from audio. The proposed model is free from parametric distribution assumptions over the VA space, easy to implement, and extremely fast to train. We also extend our model to deal with the temporal dynamics of time-varying emotion labels. Comprehensive performance study on two larger-scale datasets demonstrates that our approach achieves comparable performance to the state-of-the-art ones, but with much better training and testing efficiency.\n\nTitle: Modeling the Affective Content of Music with a Gaussian Mixture Model\nContent: Modeling the association between music and emotion has been considered important for music information retrieval and affective human computer interaction. This paper presents a novel generative model called acoustic emotion Gaussians (AEG) for computational modeling of emotion. Instead of assigning a music excerpt with a deterministic (hard) emotion label, AEG treats the affective content of music as a (soft) probability distribution in the valence-arousal space and parameterizes it with a Gaussian mixture model (GMM). In this way, the subjective nature of emotion perception is explicitly modeled. Specifically, AEG employs two GMMs to characterize the audio and emotion data. The fitting algorithm of the GMM parameters makes the model learning process transparent and interpretable. Based on AEG, a probabilistic graphical structure for predicting the emotion distribution from music audio data is also developed. A comprehensive performance study over two emotion-labeled datasets demonstrates that AEG offers new insights into the relationship between music and emotion (e.g., to assess the \u201caffective diversity\u201d of a corpus) and represents an effective means of emotion modeling. Readers can easily implement AEG via the publicly available codes. As the AEG model is generic, it holds the promise of analyzing any signal that carries affective or other highly subjective information.\n\nTitle: Automatic Music Video Generation Based on Emotion-Oriented Pseudo Song Prediction and Matching.\nContent: The main difficulty in automatic music video (MV) generation lies in how to match two different media (i.e., video and music). This paper proposes a novel content-based MV generation system based on emotion-oriented pseudo song prediction and matching. We use a multi-task deep neural network (MDNN) to jointly learn the relationship among music, video, and emotion from an emotion-annotated MV corpus. Given a queried video, the MDNN is applied to predict the acoustic (music) features from the visual (video) features, i.e., the pseudo song corresponding to the video. Then, the pseudo acoustic (music) features are matched with the acoustic (music) features of each music track in the music collection according to a pseudo-song-based deep similarity matching (PDSM) metric given by another deep neural network (DNN) trained on the acoustic and pseudo acoustic features of the positive (official), less-positive (artificial), and negative (artificial) MV examples. The results of objective and subjective experiments demonstrate that the proposed pseudo-song-based framework performs well and can generate appealing MVs with better viewing and listening experiences.\n\nTitle: EMV-matchmaker: Emotional Temporal Course Modeling and Matching for Automatic Music Video Generation\nContent: This paper presents a novel content-based emotion-oriented music video (MV) generation system, called EMV-matchmaker, which utilizes the emotional temporal phase sequence of the multimedia content as a bridge to connect music and video. Specifically, we adopt an emotional temporal course model (ETCM) to respectively learn the relationship between music and its emotional temporal phase sequence and the relationship between video and its emotional temporal phase sequence from an emotion-annotated MV corpus. Then, given a video clip (or a music clip), the visual (or acoustic) ETCM is applied to predict its emotional temporal phase sequence in a valence-arousal (VA) emotional space from the corresponding low-level visual (or acoustic) features. For MV generation, string matching is applied to measure the similarity between the emotional temporal phase sequences of video and music. The results of objective and subjective experiments demonstrate that EMV-matchmaker performs well and can generate appealing music videos that can enhance the viewing and listening experience.\n\n## Task\nCompose an abstract for the title 'Personalized music emotion recognition via model adaptation.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In the music information retrieval (MIR) research, developing a computational model that comprehends the affective content of music signal and utilizes such a model to organize music collections have been an essential topic. Emotion perception in music is in nature subjective. Consequently, building a general emotion recognition system that performs equally well for every user could be insufficient. In contrast, it would be more desirable for one's personal computer/device being able to understand his/her perception of music emotion. In our previous work, we have developed the acoustic emotion Gaussians (AEG) model, which can learn the broad emotion perception of music from general users. Such a general music emotion model, called the background AEG model in this paper, can recognize the perceived emotion of unseen music from a general point of view. In this paper, we go one step further to realize the personalized music emotion modeling by adapting the background AEG model with a limited number of emotion annotations provided by a target user in an online and dynamic fashion. A novel maximum a posteriori (MAP)-based algorithm is proposed to achieve this in a probabilistic framework. We carry out quantitative evaluations on a well-known emotion annotated corpus, MER60, to validate the effectiveness of the proposed method for personalized music emotion recognition."
      }
    ]
  },
  {
    "tid":"142",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: An Approach to Combine Predicated/Speculative Execution for Programs with Unpredictable Branches\nContent: Predicated and speculative execution have been separately proposed in the past to increase the amount of instruction-level parallelism (ILP) present in a program. However, little work has been done to combine the merits of both. Excessive speculative execution can negatively affect the performance, especially when unguided by suitable profiler estimates. Similarly, predicated execution always results in unnecessary execution of operations present in the untaken branch. Conventional techniques use expensive profiler analysis to reduce the overhead costs of predicated execution. In this paper we first study the effects of individual as well as combined effects of speculative/predicated execution features. We then present an algorithm which attempts to combine the merits of the two. We show that for programs with unpredictable branch outcomes, the proposed technique improves the program performance as compared to previous techniques. We also show that separating the two (predication and speculation) results in less efficient schedules.\n\nTitle: Integrated I-cache Way Predictor and Branch Target Buffer to Reduce Energy Consumption\nContent: In this paper, we present a Branch Target Buffer (BTB) design for energy savings in set-associative instruction caches. We extend the functionality of a BTB by caching way predictions in addition to branch target addresses. Way prediction and branch target prediction are done in parallel. Instruction cache energy savings are achieved by accessing one cache way if the way prediction for a fetch is available.To increase the number of way predictions for higher energy savings, we modify the BTB management policy to allocate entries for nonbranch instructions. Furthermore, we propose to partition a BTB into ways for branch instructions and ways for non-branch instructions to reduce the BTB energy as well.We evaluate the effectiveness of our BTB design and management policies with SPEC95 benchmarks. The best BTB configuration shows a 74% energy savings on average in a 4-way set-associative instruction cache and the performance degradation is only 0.1%. When the instruction cache energy and the BTB energy are considered together, the average energy-delay product reduction is 65%.\n\nTitle: An Efficient Global Resource-Directed Approach to Exploiting Instruction-Level Parallelism\nContent: Many techniques have been proposed for exploiting instruction level parallelism, ranging from the optimal and expensive but ignoring resource constraints, to various forms of introducing resource constraints. One of the most aggressive of these techniques is Resource-Constrained Software Pipelining (RCSP) [Aiken, Nicolau, and Novack TOPADS 6(12) 1995]. RCSP works by repeatedly scheduling successive iterations of a loop in parallel until the data and resource dependence structure of the loop causes the process to converge on a repeating scheduling pattern. This repeating pattern is then used as the new loop body. In principle, this process can be made optimal with respect to full unrolling and scheduling of the loop. Of course, this is not the same as absolute optimality; however, given the NP-hard nature of the problem and the results of [Schwiegelshohn, Gasperoni, and Ebcioglu JPDC 11(2) 1991], this may be the strongest form possible for general loop pipelining. The main drawback of RCSP is that, in practice, its space/time overhead can be fairly expensive. In this paper, we present Resource-Directed Loop Pipelining (RDLP), a new approach that attempts to retain much of the advantages of RCSP while minimizing the expense. It does so by allowing the availability of target resources to in some sense guide the application of parallelism exposing and parallelizing transformations. One of the key features of RDLP is the separation of control heuristics from transformations that allows the loop pipelining to be as general as the underlying system of code motion transformations. Results are presented that show that even with very unsophisticated heuristics, RDLP achieves roughly the same performance as RCSP, while providing a 4-fold decrease in space/time cost.\n\nTitle: A Systematic Approach to Branch Speculation\nContent: A general theoretical framework is developed for the study of branch speculation. The framework yields a systematic way to select the schedule in a given set that, for any (estimated) bias of the branch, minimizes the expected execution time. Among other things, it is shown that in some cases the optimal schedule is neither of those resulting from aggressively speculating on any given outcome of the conditional. Our results can be useful in either static or dynamic approaches. We propose a simple run-time estimator for the bias and discuss how to combine it with schedule selection. A number of examples motivate and illustrate the techniques, and show that our approach yields better performance in the case of highly unpredictable branches.\n\nTitle: Analyzing the Individual/Combined Effects of Speculative and Guarded Execution on a Superscalar Architecture\nContent: Speculative execution is a technique by which instructions are executed before the condition that controls it is evaluated. This can increase the performance if some of the idle CPU cycles are now used to execute speculated instructions. Guarded execution is a technique in which the branch instruction is eliminated and control dependences are converted to data dependences. This can help reduce some of the side-effects involved with branch instructions besides creating larger compilation units. However, excessive application of either one of them can result in dismal performance. Conventional approaches have used a one-time feedback metric and made all decisions based on it. We present a new way of designing feedback metria and show how it can be used to regulate the effects of dynamic speculation and the side-effects of applying guarded execution statically. The proposed method presents a 0.3-0.6 fold improvements over a conventional scheme using SPEC benchmarks.\n\n## Task\nCompose an abstract for the title 'Using profiling to reduce branch misprediction costs on a dynamically scheduled processor' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Modern dynamically scheduled processors use branch prediction hardware to speculatively fetch and execute most likely executed paths in a program. Complex branch predictors have been proposed which attempt to identify these paths accurately such that the hardware can benefit from out-of-order (OOO) execution. Recent studies have shown that inspite of such complex prediction schemes, there still exist many frequently executed branches which are difficult to predict. Predicated execution has been proposed as an alternative technique to eliminate some of these branches in various forms ranging from a restrictive support to a full-blown support. We call the restrictive form of predicated execution as guarded execution.In this paper, we propose a new algorithm which uses profiling and selectively performs if-conversion for architectures with guarded execution support. Branch profiling is used to gather the taken, non-taken and misprediction counts for every branch. This combined with block profiling is used to select paths which suffer from heavy mispredictions and are profitable to if-convert. Effects of three different selection criterias, namely size-based, predictability-based and profiled-based, on net cycle improvements, branch mispredictions and mis-speculated instructions are then studied. We also propose new mechanisms to convert unsafe instructions to safe form to enhance the applicability of the technique. Finally, we explain numerous adjustments that were made to the selection criterias to better reflect the OOO processor behavior."
      }
    ]
  },
  {
    "tid":"143",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Robust and efficient saliency modeling from image co-occurrence histograms.\nContent: This paper presents a visual saliency modeling technique that is efficient and tolerant to the image scale variation. Different from existing approaches that rely on a large number of filters or complicated learning processes, the proposed technique computes saliency from image histograms. Several two-dimensional image co-occurrence histograms are used, which encode not only \"how many\" (occurrence) but also \"where and how\" (co-occurrence) image pixels are composed into a visual image, hence capturing the \"unusualness\" of an object or image region that is often perceived by either global \"uncommonness\" (i.e., low occurrence frequency) or local \"discontinuity\" with respect to the surrounding (i.e., low co-occurrence frequency). The proposed technique has a number of advantageous characteristics. It is fast and very easy to implement. At the same time, it involves minimal parameter tuning, requires no training, and is robust to image scale variation. Experiments on the AIM dataset show that a superior shuffled AUC (sAUC) of 0.7221 is obtained, which is higher than the state-of-the-art sAUC of 0.7187.\n\nTitle: Image indexing and retrieval using visual keyword histograms\nContent: We propose a novel image representation called visual keyword histogram (VKH) for content-based indexing and retrieval. Visual keywords are domain-relevant visual prototypes (e.g. faces, foliage, buildings etc) with both perceptual appearance and textual semantics. Collectively, VKHs axe computed over spatial tessellation to represent the distribution of visual keywords in various parts of an image. To construct a vocabulary of visual keywords, an incremental neural network is deployed to learn visual keywords from examples. This allows us to build domain-specific visual vocabularies rapidly and incrementally. Last but not least, we propose a new visual query language called Query by Spatial Icons (QBSI) that allows a user to specify a query in terms of \"what\" and \"where\". A visual query term constrains whether a visual keyword should be present and a query formals chains these terms into a disjunctive normal form via logical operators. We show our approach on real and complex home photos with very promising results.\n\nTitle: A Latent Model for Visual Disambiguation of Keyword-based Image Search\nContent: The problem of polysemy in keyword-based image search arises mainly from the inherent ambiguity in user queries. We propose a latent model based approach that re- solves user search ambiguity by allowing sense specific diversity in search results. Given a query keyword and the images retrieved by issuing the query to an image search en- gine, we first learn a latent visual sense model of these polysemous images. Next, we use Wikipedia to disambiguate the word sense of the original query, and issue these Wiki-senses as new queries to retrieve sense specific images. A sense-specific image classifier is then learnt by combining information from the latent visual sense model, and used to cluster and re-rank the polysemous images from the original query keyword into its specific senses. Results on a ground truth of 17K image set returned by 10 keyword searches and their 62 word senses provides empirical indications that our method can im- prove upon existing keyword based search engines. Our method learns the visual word sense models in a totally unsupervised manner, effectively filters out irrelevant images, and is able to mine the long tail of image search.\n\nTitle: A non-parametric visual-sense model of images--extending the cluster hypothesis beyond text\nContent: The main challenge of a search engine is to find information that are relevant and appropriate. However, this can become difficult when queries are issued using ambiguous words. Rijsbergen first hypothesized a clustering approach for web pages wherein closely associated pages are treated as a semantic group with the same relevance to the query (Rijsbergen 1979). In this paper, we extend Rijsbergen's cluster hypothesis to multimedia content such as images. Given a user query, the polysemy in the return image set is related to the many possible meanings of the query. We develop a method to cluster the polysemous images into their semantic categories. The resulting clusters can be seen as the visual senses of the query, which collectively embody the visual interpretations of the query. At the heart of our method is a non-parametric Bayesian approach that exploits the complementary text and visual information of images for semantic clustering. Latent structures of polysemous images are mined using the Hierarchical Dirichlet Process (HDP). HDP is a non-parametric Bayesian model that represents images using a mixture of components. The main advantage of our model is that the number of mixture components is not fixed a priori, but is determined during the posterior inference process. This allows our model to grow with the level of polysemy (and visual diversity) of images. The same set of components is used to model all images, with only the mixture weights varying amongst images. Evaluation results on a large collection of web images show the efficacy of our approach.\n\nTitle: Adaptive picture-in-picture technology based on visual saliency\nContent: Picture-in-picture (PiP) is a feature of some television receivers and video devices, which allows one main program to be displayed on the full screen while one or more subprogram displayed in inset windows. Currently most TV/video devices require users to specify where and how large to place the sub-program over the main program display. This process is however not user-friendly as it involves a manual process and once specified, the size and the location of the sub-program will be fixed even when they block some key visual information from the main program. We propose an automatic and adaptive PiP technology that makes use of computational modeling of visual saliency. For each frame of the main program, a saliency map is computed efficiently which quantifies how probable a display region of the main program contains useful information and will attract humans' attention/eyes. The sub-program can thus be adaptively resized and placed to the display region that contains the least useful information. Preliminary experiments show the effectiveness of the proposed technology.\n\n## Task\nCompose an abstract for the title 'Saliency modeling from image histograms' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We proposed a computational visual saliency modeling technique. The proposed technique makes use of a color co-occurrence histogram (CCH) that captures not only \"how many\" but also \"where and how\" image pixels are composed into a visually perceivable image. Hence the CCH encodes image saliency information that is usually perceived as the discontinuity between an image region or object and its surrounding. The proposed technique has a number of distinctive characteristics: It is fast, discriminative, tolerant to image scale variation, and involves minimal parameter tuning. Experiments over benchmarking datasets show that it predicts fixational eye tracking points accurately and a superior AUC of 71.25 is obtained."
      }
    ]
  },
  {
    "tid":"144",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Cyber-physical integration in programmable microfluidic biochips\nContent: Microfluidic biochip technology integrates miniaturized components into a chip that can perform traditional biochemical laboratory procedures. Commercial impact is highlighted by the recent acquisition of Advanced Liquid Logic by Illumina Inc., a leader in DNA sequencing and biomolecular analysis. Due to the inherent variability involved in many biochemical processes, uncertainties manifest themselves in many ways in microluidics. Cyber-physical integration of on-chip sensors permits feedback-driven monitoring in-real time to detect and correct errors, along with other benefits such as adaptive control and dynamic re-synthesis. This paper overviews low-based and digital (droplet-based) microluidic biopchips, and discusses the state-of-the-art in microluidic device fabrication, the interplay between sensor feedback and adaptive control software, and practical experiences relating to biochip cyber-physical integration. It demonstrates the connections between the many fundamental principles of chip design and engineering, and the needs of the biochip community.\n\nTitle: Reliability-Driven Energy-Efficient Task Scheduling for Multiprocessor Real-Time Systems\nContent: This paper proposes a reliability-driven task scheduling scheme for multiprocessor real-time embedded systems that optimizes system energy consumption under stochastic fault occurrences. The task scheduling problem is formulated as an integer linear program where a novel fault adaptation variable is introduced to model the uncertainties of fault occurrences. The proposed scheme, which considers both the dynamic power and the leakage power, is able to handle the scheduling of independent tasks and tasks with precedence constraints, and is capable of scheduling tasks with varying deadlines. Experimental results have demonstrated that the proposed reliability-driven parallel scheduling scheme achieves energy savings of more than 15% when compared to the approach of designing for the corner case of fault occurrences.\n\nTitle: Variation-Aware Layer Assignment With Hierarchical Stochastic Optimization on a Multicore Platform\nContent: As the very large scale integration (VLSI) technology enters the nanoscale regime, VLSI design is increasingly sensitive to variations on process, voltage, and temperature. Layer assignment technology plays a crucial role in industrial VLSI design flow. However, existing layer assignment approaches have largely ignored these variations, which can lead to significant timing violations. To address this issue, a variation-aware layer assignment approach for cost minimization is proposed in this paper. The proposed layer assignment approach is a single-stage stochastic program that directly controls the timing yield via a single parameter, and it is solved using Monte Carlo simulations and the Latin hypercube sampling technique. A hierarchical design is also adopted to enable the optimization process on a multicore platform. Experiments have been performed on 5000 industrial nets, and the results demonstrate that the proposed approach: 1) can significantly improve the timing yield by 64% in comparison with the nominal design and 2) can reduce the wire cost by 15.7% in comparison with the worst case design.\n\nTitle: Discrete Wavelet Transform Based Circuit Layout Fingerprinting using Chaotic System.\nContent: Tight time-to-market pressure requires CAD tools to heavily involve component reuse, or intellectual property (IP) reuse, which imposes intense security concerns on IP protection. For the IP providers, it is critical to protect their products against unauthorized reproduction. Thus, circuit layout fingerprinting becomes quite important which helps the IP providers to detect which user distributes the illegal IPs. However, previous works addressing the circuit fingerprinting all have large area and runtime overhead. In this paper, a novel efficient discrete wavelet transform (DWT)-based key sensitive circuit layout fingerprinting technique using chaotic system is proposed. The new circuit layout fingerprinting technique targets to be applied after placement and routing, while before fabrication. Thus, it only slightly impacts the original design and introduces small runtime overhead as well. To further enhance the security, the chaotic system based on Fibonacci transformation is integrated. The experimental results demonstrate that our chaotic DWT-based technique largely outperforms a median-based technique for various attacks. The chaotic DWT-based technique improves the detection error rate by 68.5% for the gate swapping attack, by 65.8% for the random perturbation attack, by 54.8% for the partition-based perturbation attack and by 54.5% for the combined perturbation attack. In addition, the average wirelength overhead is only 0.0149% compared to the original design and the average runtime overhead is only 6.73 s. These demonstrate the er effectiveness of our circuit layout fingerprinting technique.\n\nTitle: Hierarchical Cross-Entropy Optimization for Fast On-Chip Decap Budgeting\nContent: Decoupling capacitor (decap) has been widely used to effectively reduce dynamic power supply noise. Traditional decap budgeting algorithms usually explore the sensitivity-based nonlinear optimizations or conjugate gradient (CG) methods, which can be prohibitively expensive for large-scale decap budgeting problems and cannot be easily parallelized. In this paper, we propose a hierarchical cross-entropy based optimization technique which is more efficient and parallel-friendly. Cross-entropy (CE) is an advanced optimization framework which explores the power of rare event probability theory and importance sampling. To achieve the high efficiency, a sensitivity-guided cross-entropy (SCE) algorithm is introduced which integrates CE with a partitioning-based sampling strategy to effectively reduce the solution space in solving the large-scale decap budgeting problems. Compared to improved CG method and conventional CE method, SCE with Latin hypercube sampling method (SCE-LHS) can provide 2\u00d7 speedups, while achieving up to 25% improvement on power supply noise. To further improve decap optimization solution quality, SCE with sequential importance sampling (SCE-SIS) method is also studied and implemented. Compared to SCE-LHS, in similar runtime, SCE-SIS can lead to 16.8% further reduction on the total power supply noise.\n\n## Task\nCompose an abstract for the title 'Design Automation of Cyber-Physical Systems: Challenges, Advances, and Opportunities.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"A cyber-physical system (CPS) is an integration of computation with physical processes whose behavior is defined by both computational and physical parts of the system. In this paper, we present a view of the challenges and opportunities for design automation of CPS. We identify a combination of characteristics that define the challenges unique to the design automation of CPS. We then present sele..."
      }
    ]
  },
  {
    "tid":"145",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A New Approach For Automatic Analysis Of 3d Low Dose Ct Images For Accurate Monitoring The Detected Lung Nodules\nContent: Our long term research goal is to develop a fully automated, image-based diagnostic system for early diagnosis of pulmonary nodules that may lead to lung cancer. This paper focuses on monitoring the development of lung nodules detected in successive chest low dose (LD) CT scans of a patient. We propose a new methodology for 3D LDCT data registration which is non-rigid and involves two steps: (i) global alignment of one scan (target) to another scan (reference or prototype) using the learned prior appearance model followed by (ii) local alignment in order to correct for intricate deformations. After equalizing signals for two subsequent chest scans, visual appearance of these chest images is modeled with a Markov-Gibbs random field with pairwise interaction. We estimate the affine transformation that globally register the target to the prototype by gradient descent maximization of a special Gibbs energy function. To handle local deformations, we deform each voxel of the target over evolving closed equi-spaced surfaces (iso-surfaces) to closely match the prototype. The evolution of the iso-surfaces is guided by an exponential speed function in the directions that minimize distances between the corresponding voxel pairs on the iso-surfaces in both the data sets. Preliminary results on the 135 LDCT data sets from 27 patients show that our proper registration could lead to precise diagnosis and identification of the development of the detected pulmonary nodules.\n\nTitle: Automatic Detection and Recognition of Lung Abnormalities in Helical CT Images Using Deformable Templates\nContent: Automatic detection of lung nodules is an important problem in computer analysis of chest radiographs. In this paper we propose a novel algorithm for isolating lung nodules from spiral CT scans. The proposed algorithm is based on using four different types of deformable templates describing typical geometry and gray level distribution of lung nodules. These four types are (i) solid spherical model of large-size calcified and non-calcified nodules appearing in several successive slices; (ii) hollow spherical model of large lung cavity nodules; (iii) circular model of small nodules appearing in only a single slice, and (iv) semicircular model of lung wall nodules. Each template has a specific gray level pattern which is analytically estimated in order to fit the available empirical data. The detection combines the normalized cross-correlation template matching by genetic optimization and Bayesian post-classification. This approach allows for isolating abnormalities which spread over several adjacent CT slices. Experiments with 200 patients' CT scans show that the developed techniques detect lung nodules more accurately than other known algorithms.\n\nTitle: A Framework for Unsupervised Segmentation of Lung Tissues from Low Dose Computed Tomography Images\nContent: New techniques for more accurate unsupervised segmentation of lung tis- sues from Low Dose Computed Tomography (LDCT) are proposed. In this paper we describe LDCT images and desired maps of regions (lung and the other chest tissues) by a joint Markov-Gibbs random field model (MGRF) of independent image signals and interdependent region labels but focus on most accurate model identification. To better specify region borders, each empirical distribution of signals is precisely approximated by a Linear Com- bination of Discrete Gaussians (LCDG) with positive and negative compo- nents. We modify a conventional Expectation-Maximization (EM) algorithm to deal with the LCDG and develop a sequential EM-based technique to get an initial LCDG-approximation for the modified EM algorithm. The initial segmentation based on the LCDG-models is then iteratively refined using a MGRF model with analytically estimated potentials. Experiments on real data sets confirm high accuracy of the proposed approach.\n\nTitle: A novel Gaussian Scale Space-based joint MGRF framework for precise lung segmentation.\nContent: A new framework for the precise segmentation of lung tissues from Computed Tomography (CT) is proposed. The CT images, Gaussian Scale Space (GSS) data generation using Gaussian Kernels (GKs), and desired maps of regions (lung and the other chest tissues) are described by a joint Markov-Gibbs Random Field Model (MGRF) of independent image signals and interdependent region labels. We focus on the most accurate model identification of the joint MGRF models. To better specify region borders, each empirical distribution of signals is rigorously approximated by a Linear Combination of Discrete Gaussians (LCDG) with positive and negative components. The classical Expectation-Maximization (EM) algorithm has been adapted for the LCDG model. The initial segmentations from the original and the generated GSS CT images are based on the LCDG-models; then they are iteratively refined using an MGRF model with analytically estimated potentials. Finally, these initial segmentations are fused together using a Bayesian fusion approach to get the final segmentation of the lung region. Experiments on eleven real data sets based on Dice Similarity Coefficient (DSC) metric confirms the high accuracy of the proposed approach.\n\nTitle: Image-based CAD system for accurate identification of lung injury\nContent: This paper proposes a novel framework for the identification of the radiation-induced lung injury (RILI) after radiation therapy (RT) using 4D computed tomography (CT) scans. The proposed methodology consists of four components: (i) elastic image registration; (ii) segmentation of the lung fields; (iii) extraction of functional and texture features; and (iv) classification of the lung tissues. The registration step locally aligns the consecutive phases of the respiratory cycle using an elastic image registration approach based on descent minimization of the sum of squared difference similarity metric. Secondly, lung fields are segmented using a hybrid framework that integrates an adaptive shape prior model, a first-order intensity model, and a second order homogeneity descriptor of the lung tissues. Next, regional features that describe both the texture features using the novel 7\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">th</sup>\n-order Markov-Gibbs random field (MGRF) model in addition to the lung functionality features (e.g., ventilation and elasticity) are estimated from a segmented lungs. Finally, a random forest classifier (RF) is applied to distinguish between injured and normal lung tissues. To evaluate the proposed framework, we used data sets that have been collected from 13 patients who had underwent RT treatment. Experimental results demonstrate the promise of the proposed framework for the identification of the injured lung region, and thus hold the promise as a valuable tool for early detection of RILI.\n\n## Task\nCompose an abstract for the title 'Automatic Detection Of 2d And 3d Lung Nodules In Chest Spiral Ct Scans' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Automatic detection of lung nodules is an important problem in computer analysis of chest radiographs. In this paper, we propose a novel algorithm for isolating lung abnormalities (nodules) from spiral chest low-dose CT (LDCT) scans. The proposed algorithm consists of three main steps. The first step isolates the lung nodules, arteries, veins, bronchi, and bronchioles from the surrounding anatomical structures. The second step detects lung nodules using deformable 3D and 2D templates describing typical geometry and gray-level distribution within the nodules of the same type. The detection combines the normalized cross-correlation template matching and a genetic optimization algorithm. The final step eliminates the false positive nodules (FPNs) using three features that robustly define the true lung nodules. Experiments with 200 CT data sets show that the proposed approach provided comparable results with respect to the experts."
      }
    ]
  },
  {
    "tid":"146",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Average rate and outage probability of cyclic prefixed single-carrier opportunistic cooperative diversity systems\nContent: In this paper, several performance analysis metrics for the cyclic prefix-based single-carrier (CP-SC) opportunistic cooperative diversity systems are presented. After the statistical evaluation of the end-to-end signal-to-noise-ratio of a two-hop relaying transmission, we derive tight upper bounds for the maximum achievable average rate and lower bounds for the outage probability in closed form. Further, asymptotic analysis on the outage probability reveals that the diversity gain is determined by both the number of relay nodes in the system and the number of channel taps being supported by the CP length. Simulation results verify the derived closed-form analytical expressions and also the diversity gains. Asymptotic performance is also verified via Monte Carlo simulations.\n\nTitle: Average Spectral Efficiency of Opportunistic QRD-Based Cyclic Prefixed Single-Carrier Cooperative Diversity Systems with Power Allocation\nContent: Due to higher peak-to-average power ratio and higher power backing-off in OFDM relaying systems, a cyclic prefixed single-carrier cooperative diversity system is considered in this paper. Under the transmission power constraint, the joint optimal power allocation to the source and relay node is investigated. For a two-hop decode-and-forward relaying protocol, the optimal power allocation is first obtained in the considered system, and then the opportunistic destination terminal selection is applied to improve the achievable average spectral efficiency (ASE). Based on the proposed QR decomposition (QRD)-based receiver in the destination terminal, closed-form expressions for the maximum achievable ASE are derived. Simulation results verify the derived closed-form analytical expressions.\n\nTitle: Performance Analysis of Cognitive Spectrum-Sharing Single-Carrier Systems With Relay Selection\nContent: In this paper, we analyze the performance of cooperative spectrum sharing single-carrier (SC) relay systems. Taking into account the peak interference power at the primary user (PU) and the maximum transmit power at the secondary user (SU) network, two separate power allocation constraints are formed. For a two-hop decode-and-forward (DF) relaying protocol and two power allocation constraints, two relay selection schemes, namely, a full-channel state information (CSI)-based best relay selection (BRS) and a partial CSI-based best relay selection (PBRS), are proposed. The distributions of the end-to-end signal-to-noise ratios (e2e-SNRs) for the four cases are derived first, and then their outage probabilities and asymptotic outage probabilities are derived in closed-form. The derived asymptotic outage probabilities are utilized to see different diversity gains. Monte Carlo simulations have verified the derived diversity gains for the four different cases. We also present upper bounds on the ergodic capacities for two particular cases.\n\nTitle: On the Performance of Cyclic Prefix-Based Single-Carrier Cooperative Diversity Systems with Best Relay Selection\nContent: In this paper, several important performance metrics for the cyclic prefix-based single-carrier (CP-SC) cooperative diversity systems with best relay node selection, are presented and quantitatively compared with Monte-Carlo simulations. The statistical evaluation of the upper bound of the average end-to-end signal-to-noise-ratio (SNR) of a two-hop amplify-and-forward (AF) relaying CP-SC transmission is presented. By using this upper bound, closed-form bounds for the maximum achievable average rate, the outage probability and the average symbol error rate (SER), are derived. Further, asymptotic analysis on the outage probability and the average SER reveal that the diversity gain is determined by both the number of relay nodes in the system and the number of channel taps being supported by the CP length. Simulation results verify the derived closed-form analytical expressions and also the diversity gains. Asymptotic performance is also verified in practically high SNR region via Monte Carlo simulations.\n\nTitle: Performance Analysis of Cyclic Prefixed Single-Carrier Spectrum Sharing Relay Systems in Primary User Interference\nContent: For cyclic prefixed single-carrier (CP-SC) spectrum sharing relay systems, the joint impact of the primary transmitter and primary receiver on the performance of secondary networks is investigated. A two-hop amplify-and-forward (AF) relay protocol is employed, and then its end-to-end signal-to-interference-ratio (e2e-SIR) is defined. An upper bound on the cumulative distribution function (CDF) of this e2e-SIR is derived for the CP-SC system. Having investigated an asymptotic expression for the CDF, the asymptotic outage diversity and coding gain can be obtained. Also, under a total transmission constraint, a suboptimal power allocation (sub-OPA) is derived for approximately minimizing the asymptotic outage probability. Monte Carlo simulations verify the derived outage probability and its asymptotic diversity gain with and without power allocation. It is shown that the primary transmitter has a major impact on the secondary network performance. Importantly, it is observed that when the interference from the primary transmitter proportionally increases with the interference power constraint at the primary receiver, the spectrum-sharing system results in no diversity gain for the full range of the SIR.\n\n## Task\nCompose an abstract for the title 'Outage Probability of Single-Carrier Cooperative Spectrum Sharing Systems with Decode-and-Forward Relaying and Selection Combining' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"For cyclic prefixed single-carrier (CP-SC) spectrum sharing relaying systems, a two-hop decode-and-forward (DF) relaying protocol with a direct link and selection combining are employed in the secondary user relay network. For this cooperative CP-SC spectrum sharing system, the end-to-end signal-to-noise ratio (e2e-SNR) is first derived, and then the outage probability performance of the secondary user relaying system is investigated. Having derived an asymptotic expression for the cumulative distribution function of the e2e-SNR, the asymptotic outage diversity is obtained under a limited maximum transmit power at the secondary source and relay while satisfying the maximum allowable interference at the primary user. Notably, when the maximum allowable interference is independent of a limited transmission power, an outage probability floor is observed. Moreover, under the assumption of an unlimited transmit power at the secondary nodes, the asymptotic outage diversity is derived as a function of the interference. It can be seen that the same outage diversity gain can be achieved as in the non-spectrum-sharing CP-SC relaying system. Analytically derived asymptotic outage diversity gains for limited and unlimited transmission power cases are verified by Monte Carlo simulations."
      }
    ]
  },
  {
    "tid":"147",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Mobile Element Based Differentiated Message Delivery in Wireless Sensor Networks\nContent: In recent years, mobile elements (MEs) have been proposed as mechanical carriers of data to prolong the lifetime of sensor networks and to overcome network partitioning problem. A scheduling approach is proposed in [1] for MEs to collect periodically generated data, also called regular messages (RMs), from nearby sensor nodes with no buffer overflow. However, increased delay in message delivery with ME-based communication compared to multi-hop communication may not be tolerated in some cases. Some messages can be more urgent than others due to critical values of the sensed data. Such messages maybe required to be delivered to the ME within a specified deadline. In this paper, this new problem of Differentiated Message Delivery (DMD) considering both regular and urgent message collection is addressed. The proposed solution incorporates multi-hop communication into the ME scheduling problem. The investigated performance metrics are the minimum required ME speed to prevent data loss and guarantee the maximum tolerated urgent message delay, as well as urgent and regular message loss rates for a given ME speed. The proposed solution is shown to perform well in terms of these metrics in various network scenarios. Furthermore, comparisons with existing ME scheduling algorithms show that the proposed solution meets the urgent message delivery requirement with a reasonable increase in ME speed.\n\nTitle: Cluster-based information processing in wireless sensor networks: an energy-aware approach\nContent: Emerging Wireless Sensor Network (WSN) applications demand considerable computation capacity for in-network processing in resource limited WSN environments. To achieve the required processing capacity under energy consumption constraints, collaboration among sensors through parallel processing methods emerges as a promising solution. Although such methods have been extensively studied in wired networks of processors, its counterpart for WSNs remains largely unexplored. In this paper, a localized task mapping and scheduling solution for energy-constrained applications in WSNs, Energy-constrained Task Mapping and Scheduling (EcoMapS), is presented. EcoMapS guarantees energy consumption constraints while minimizing schedule length. EcoMapS incorporates channel modeling, concurrent task mapping, communication and computation scheduling, and sensor failure handling algorithms. Simulation results show significant performance improvements of EcoMapS over existing mechanisms in terms of minimizing schedule lengths subject to energy consumption constrains. Copyright (c) 2007 John Wiley & Sons, Ltd.\n\nTitle: Cross-Layer Collaborative In-Network Processing in Multihop Wireless Sensor Networks\nContent: Emerging Wireless Sensor Network (WSN) applications demand considerable computation capacity for in-network processing. To achieve the required processing capacity, cross-layer collaborative in-network processing among sensors emerges as a promising solution: Sensors do not only process information at the application layer, but also synchronize their communication activities to exchange partially processed data for parallel processing. However, scheduling computation and communication events is a challenging problem in WSNs due to limited resource availability and shared communication medium. In this work, an application-independent task mapping and scheduling solution in multihop homogeneous WSNs, Multihop Task Mapping and Scheduling (MTMS), is presented that provides real-time guarantees. Using our proposed application model, the multihop channel model, and the communication scheduling algorithm, computation tasks and associated communication events are scheduled simultaneously. The Dynamic Voltage Scaling (DVS) algorithm is presented to further optimize energy consumption. Simulation results show significant performance improvements compared with existing mechanisms in terms of minimizing energy consumption subject to delay constraints.\n\nTitle: Dynamic critical-path task mapping and scheduling for collaborative in-network processing in multi-hop wireless sensor networks\nContent: Although task mapping and scheduling in wired networks of processors has been extensively studied, their application to WSNs remains largely unexplored. Existing algorithms cannot be directly implemented in WSNs due to limited resource availability and shared communication medium. In this work, an application-independent task mapping and scheduling solution in multi-hop WSNs is presented that provides realtime guarantees. Using a novel multi-hop channel model and a communication scheduling algorithm, computation tasks and associated communication events are scheduled simultaneously with a dynamic critical-path scheduling algorithm. Dynamic voltage scaling (DVS) mechanism is implemented to further optimize energy consumption. According to the simulation results, the proposed solution outperforms existing mechanisms in terms of guaranteeing application deadlines with minimum energy consumption\n\nTitle: Real-time task mapping and scheduling for collaborative in-network processing in DVS-enabled wireless sensor networks\nContent: With the increasing importance of energy consumption considerations and new requirements of emerging applications, in-network processing of information gains recognition as a viable solution for Wireless Sensor Networks (WSNs). The required processing capability can be achieved through locally collaborative information processing among sensors. Task mapping and scheduling plays an important role in efficient collaborative information processing. Although task mapping and scheduling in wired networks of processors has been well studied in the past, its counterpart for WSNs remains largely unexplored. In this paper, a task mapping and scheduling solution for real-time applications in WSNs, Real-time Task Mapping and Scheduling (RT-MapS), is presented. RT-MapS incorporates wireless channel modeling, Hyper-DAG extension, concurrent task mapping, communication and computation scheduling, and Dynamic Voltage Scaling (DVS) methods. Simulation results show significant performance improvements compared with existing mechanisms in terms of providing deadline guarantee with minimum energy consumption.\n\n## Task\nCompose an abstract for the title 'Data harvesting with mobile elements in wireless sensor networks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In recent studies, using mobile elements (MEs) as mechanical carriers to relay data has been shown to be an effective way of prolonging sensor network life time and relaying information in partitioned networks. As the data generation rates of sensors may vary, some sensors need to be visited more frequently than others. In this paper, a partitioning-based algorithm is presented that schedules the movements of MEs in a sensor network such that there is no data loss due to buffer overflow. Simulation results show that the proposed Partitioning-Based Scheduling (PBS) algorithm performs well in terms of reducing the minimum required ME speed to prevent data loss, providing high predictability in inter-visit durations, and minimizing the data loss rate for the cases when the ME is constrained to move slower than the minimum required ME speed."
      }
    ]
  },
  {
    "tid":"148",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Novel Method for Block Size Forensics Based on Morphological Operations\nContent: Passive forensics analysis aims to find out how multimedia data is acquired and processed without relying on pre-embedded or pre-registered information. Since most existing compression schemes for digital images are based on block processing, one of the fundamental steps for subsequent forensics analysis is to detect the presence of block artifacts and estimate the block size for a given image. In this paper, we propose a novel method for blind block size estimation. A 2\u81332 cross-differential filter is first applied to detect all possible block artifact boundaries, morphological operations are then used to remove the boundary effects caused by the edges of the actual image contents, and finally maximum-likelihood estimation (MLE) is employed to estimate the block size. The experimental results evaluated on over 1300 nature images show the effectiveness of our proposed method. Compared with existing gradient-based detection method, our method achieves over 39% accuracy improvement on average.\n\nTitle: Identifying shifted double JPEG compression artifacts for non-intrusive digital image forensics\nContent: Non-intrusive digital image forensics (NIDIF) aims at authenticating the validity of digital images utilizing their intrinsic characteristics when the active forensic methods, such as digital watermarking or digital signatures, fail or are not present. The NIDIF for lossy JPEG compressed images are of special importance due to its pervasively use in many applications. Recently, researchers showed that certain types of tampering manipulations can be revealed when JPEG re-compress artifacts (JRCA) is found in a suspicious JPEG image. Up to now, most existing works mainly focus on the detection of doubly JPEG compressed images without block shifting. However, they cannot identify another JRCA --- the shifted double JPEG (SD-JPEG) compression artifacts which are commonly present in composite JPEG images. In this paper, the SD-JPEG artifacts are modeled as a noisy 2-D convolutive mixing model. A symmetry verification based method and a first digit histogram based remedy method are proposed to form an integral identification framework. It can reliably detect the SD-JPEG artifacts when a critical state is not reached. The experimental results have shown the effectiveness of the proposed framework.\n\nTitle: Multi-Scale Fusion for Improved Localization of Malicious Tampering in Digital Images.\nContent: Sliding window-based analysis is a prevailing mechanism for tampering localization in passive image authentication. It uses existing forensic detectors, originally designed for fullframe analysis, to obtain the detection scores for individual image regions. One of the main problems with window-based analysis is its impractically low localization resolution stemming from the need to use relatively large analysis windows. While decreasing the window size can improve the localization resolution, the classification results tend to become unreliable due to insufficient statistics about the relevant forensic features. In this study, we investigate a multi-scale analysis approach which fuses multiple candidate tampering maps, resulting from the analysis with different windows, to obtain a single, more reliable tampering map with better localization resolution. We propose three different techniques for multi-scale fusion, and verify their feasibility against various reference strategies.We consider a popular tampering scenario with mode-based first digit features to distinguish between singly and doubly-compressed regions. Our results clearly indicate that the proposed fusion strategies can successfully combine the benefits of small-scale and large-scale analysis and improve the tampering localization performance.\n\nTitle: Multi-Scale Analysis Strategies in PRNU-Based Tampering Localization.\nContent: Accurate unsupervised tampering localization is one of the most challenging problems in digital image forensics. In this paper, we consider a photo response non-uniformity analysis and focus on the detection of small forgeries. For this purpose, we adopt a recently proposed paradigm of multi-scale analysis and discuss various strategies for its implementation. First, we consider a multi-scale fusion approach, which involves combination of multiple candidate tampering probability maps into a single, more reliable decision map. The candidate maps are obtained with sliding windows of various sizes and thus allow to exploit the benefits of both the small- and large-scale analyses. We extend this approach by introducing modulated threshold drift and content-dependent neighborhood interactions, leading to improved localization performance with superior shape representation and easier detection of small forgeries. We also discuss two novel alternative strategies: a segmentation-guided approach, which contracts the decision statistic to a central segment within each analysis window and an adaptive-window approach, which dynamically chooses analysis window size for each location in the image. We perform extensive experimental evaluation on both synthetic and realistic forgeries and discuss in detail practical aspects of parameter selection. Our evaluation shows that the multi-scale analysis leads to significant performance improvement compared with the commonly used single-scale approach. The proposed multi-scale fusion strategy delivers stable results with consistent improvement in various test scenarios.\n\nTitle: JPEG Error Analysis and Its Applications to Digital Image Forensics\nContent: JPEG is one of the most extensively used image formats. Understanding the inherent characteristics of JPEG may play a useful role in digital image forensics. In this paper, we introduce JPEG error analysis to the study of image forensics. The main errors of JPEG include quantization, rounding, and truncation errors. Through theoretically analyzing the effects of these errors on single and double JPEG compression, we have developed three novel schemes for image forensics including identifying whether a bitmap image has previously been JPEG compressed, estimating the quantization steps of a JPEG image, and detecting the quantization table of a JPEG image. Extensive experimental results show that our new methods significantly outperform existing techniques especially for the images of small sizes. We also show that the new method can reliably detect JPEG image blocks which are as small as 8 \u00d7 8 pixels and compressed with quality factors as high as 98. This performance is important for analyzing and locating small tampered regions within a composite image.\n\n## Task\nCompose an abstract for the title 'Forensic sensor pattern noise extraction from large image data set' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The sensor pattern noise (SPN) can be regarded as the unique identity of a digital camera which is highly useful in digital image forensics [1, 2]. Existing methods [1, 2] which works by denoising each individual natural image often took an investigator a long time and great efforts to collect sufficient photos of diversified enough natural scenes. These processes are hard to repeat or standardized for officially using by an authority. In this work, we create noise image data set by taking photos of random noises displayed on a high definition monitor and propose a homomorphic based SPN extraction method. It offers the forensic researcher a fast way to create a large image data set in a few minutes. And the extraction method only needs to denoise once, which is highly efficient to deal with large numbers of photos. We compared the source camera identification performance of the proposed SPN extraction method to a prior state-of-art with identical experimental settings. The experimental results confirm the effectiveness of the proposed method."
      }
    ]
  },
  {
    "tid":"149",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Goal-Based Modeling Approach to Develop Requirements of an Adaptive System with Environmental Uncertainty\nContent: Dynamically adaptive systems (DASs) are intended to monitor the execution environment and then dynamically adapt their behavior in response to changing environmental conditions. The uncertainty of the execution environment is a major motivation for dynamic adaptation; it is impossible to know at development time all of the possible combinations of environmental conditions that will be encountered. To date, the work performed in requirements engineering for a DAS includes requirements monitoring and reasoning about the correctness of adaptations, where the DAS requirements are assumed to exist. This paper introduces a goal-based modeling approach to develop the requirements for a DAS, while explicitly factoring uncertainty into the process and resulting requirements. We introduce a variation of threat modeling to identify sources of uncertainty and demonstrate how the RELAX specification language can be used to specify more flexible requirements within a goal model to handle the uncertainty.\n\nTitle: Supporting decision-making for self-adaptive systems: from goal models to dynamic decision networks\nContent: [Context/Motivation] Different modeling techniques have been used to model requirements and decision-making of self-adaptive systems (SASs). Specifically, goal models have been prolific in supporting decision-making depending on partial and total fulfilment of functional (goals) and non-functional requirements (softgoals). Different goalrealization strategies can have different effects on softgoals which are specified with weighted contribution-links. The final decision about what strategy to use is based, among other reasons, on a utility function that takes into account the weighted sum of the different effects on softgoals. [Questions/Problems] One of the main challenges about decisionmaking in self-adaptive systems is to deal with uncertainty during runtime. New techniques are needed to systematically revise the current model when empirical evidence becomes available from the deployment. [Principal ideas/results] In this paper we enrich the decision-making supported by goal models by using Dynamic Decision Networks (DDNs). Goal realization strategies and their impact on softgoals have a correspondence with decision alternatives and conditional probabilities and expected utilities in the DDNs respectively. Our novel approach allows the specification of preferences over the softgoals and supports reasoning about partial satisfaction of softgoals using probabilities. We report results of the application of the approach on two different cases. Our early results suggest the decision-making process of SASs can be improved by using DDNs.\n\nTitle: Revisiting Goal-Oriented Models for Self-Aware Systems-of-Systems\nContent: Systems-of-systems (SoS) are systems resulted from the interaction among other independent constituent systems that collaborate to offer new functionalities towards accomplishing global missions. Each of these constituent systems accomplishes its individual missions and is able to contribute to the achievement of the global missions of the SoS, both being viewed as a set of associated goals. In the perspective of self-aware systems, SoS need to exhibit goal-awareness, i.e., They need to be aware of their own goals and of how their constituent systems contribute to their accomplishment. In this paper, we revisit goal-oriented concepts aiming at identifying and modeling goals at both SoS level and the constituent systems level. Moreover, we take advantage of such goal-oriented models to express the relationship among goals at these levels as well as to define how each constituent system can contribute to the accomplishment of global goals of an SoS. In addition, we shed light on important issues related to goal modeling in self-aware SoS to be addressed in future research.\n\nTitle: Dynamically Adaptive Systems are Product Lines too: Using Model-Driven Techniques to Capture Dynamic Variability of Adaptive Systems\nContent: In this paper we propose an approach to support the de- sign and operation of dynamically adaptive systems. We ap- ply the concept of variability modeling from software prod- uct lines to define how systems adapt at runtime to changes in their environment. Our approach models two dynamic variability dimensions; environment variability, which de- fines the conditions under which a system must adapt, and structural variability which defines the resulting architec- tural configurations. The variability dimensions identified are modeled using domain-specific languages (DSMLs) tai- lored to adaptive middleware technologies that provide sup- port for runtime variability according to reconfiguration policies. We describe our experience with applying this ap- proach through a case study; the design of a flood warning system. EP/C010345/1 The Divergent Grid. ceptualized as a dynamic software product line (SPL) in which variabilities are bound at run-time. Each component- based configuration can be considered as a product or vari- ant of the DAS. This means that variants of the DAS product line can be produced at runtime. The DAS product line de- fines a core reference architecture that constrains each prod- uct variant's component configuration. In this context, the developer of a middleware-based DAS must deal with many variability decisions. Currently, developers of such systems have to make these decisions by reasoning at low levels of abstraction. To decide which components are required and how the components need to be configured and reconfigured in response to data about the state of the environment, DAS developers need detailed knowledge of the supporting middleware infrastructure. By contrast, architects and domain experts work at a much\n\nTitle: Using Architecture Models to Support the Generation and Operation of Component-Based Adaptive Systems\nContent: Modelling architectural information is particularly important because of the acknowledged crucial role of software architecture in raising the level of abstraction during development. In the MDE area, the level of abstraction of models has frequently been related to low-level design concepts. However, model-driven techniques can be further exploited to model software artefacts that take into account the architecture of the system and its changes according to variations of the environment. In this paper, we propose model-driven techniques and dynamic variability as concepts useful for modelling the dynamic fluctuation of the environment and its impact on the architecture. Using the mappings from the models to implementation, generative techniques allow the (semi) automatic generation of artefacts making the process more efficient and promoting software reuse. The automatic generation of configurations and reconfigurations from models provides the basis for safer execution. The architectural perspective offered by the models shift focus away from implementation details to the whole view of the system and its runtime change promoting high-level analysis.\n\n## Task\nCompose an abstract for the title 'Goal-Based Modeling of Dynamically Adaptive System Requirements' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Self-adaptation is emerging as an increasingly important capability for many applications, particularly those deployed in dynamically changing environments, such as ecosystem monitoring and disaster management. One key challenge posed by Dynamically Adaptive Systems (DASs) is the need to handle changes to the requirements and corresponding behavior of a DAS in response to varying environmental conditions. Berry et al. previously identified four levels of RE that should be performed for a DAS. In this paper, we propose the Levels of RE for Modeling that reify the original levels to describe RE modeling work done by DAS developers. Specifically, we identify four types of developers: the system developer, the adaptation scenario developer, the adaptation infrastructure developer, and the DAS research community. Each level corresponds to the work of a different type of developer to construct goal model(s) specifying their requirements. We then leverage the Levels of RE for Modeling to propose two complementary processes for performing RE for a DAS. We describe our experiences with applying this approach to GridStix, an adaptive flood warning system, deployed to monitor the River Ribble in Yorkshire, England."
      }
    ]
  },
  {
    "tid":"150",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Discriminative segmentation-based evaluation through shape dissimilarity.\nContent: Segmentation-based scores play an important role in the evaluation of computational tools in medical image analysis. These scores evaluate the quality of various tasks, such as image registration and segmentation, by measuring the similarity between two binary label maps. Commonly these measurements blend two aspects of the similarity: pose misalignments and shape discrepancies. Not being able to distinguish between these two aspects, these scores often yield similar results to a widely varying range of different segmentation pairs. Consequently, the comparisons and analysis achieved by interpreting these scores become questionable. In this paper, we address this problem by exploring a new segmentation-based score, called normalized Weighted Spectral Distance (nWSD), that measures only shape discrepancies using the spectrum of the Laplace operator. Through experiments on synthetic and real data we demonstrate that nWSD provides additional information for evaluating differences between segmentations, which is not captured by other commonly used scores. Our results demonstrate that when jointly used with other scores, such as Dice's similarity coefficient, the additional information provided by nWSD allows richer, more discriminative evaluations. We show for the task of registration that through this addition we can distinguish different types of registration errors. This allows us to identify the source of errors and discriminate registration results which so far had to be treated as being of similar quality in previous evaluation studies.\n\nTitle: Metric learning with spectral graph convolutions on brain connectivity networks.\nContent: Graph representations are often used to model structured data at an individual or population level and have numerous applications in pattern recognition problems. In the field of neuroscience, where such representations are commonly used to model structural or functional connectivity between a set of brain regions, graphs have proven to be of great importance. This is mainly due to the capability of revealing patterns related to brain development and disease, which were previously unknown. Evaluating similarity between these brain connectivity networks in a manner that accounts for the graph structure and is tailored for a particular application is, however, non-trivial. Most existing methods fail to accommodate the graph structure, discarding information that could be beneficial for further classification or regression analyses based on these similarities. We propose to learn a graph similarity metric using a siamese graph convolutional neural network (s-GCN) in a supervised setting. The proposed framework takes into consideration the graph structure for the evaluation of similarity between a pair of graphs, by employing spectral graph convolutions that allow the generalisation of traditional convolutions to irregular graphs and operates in the graph spectral domain. We apply the proposed model on two datasets: the challenging ABIDE database, which comprises functional MRI data of 403 patients with autism spectrum disorder (ASD) and 468 healthy controls aggregated from multiple acquisition sites, and a set of 2500 subjects from UK Biobank. We demonstrate the performance of the method for the tasks of classification between matching and non-matching graphs, as well as individual subject classification and manifold learning, showing that it leads to significantly improved results compared to traditional methods.\n\nTitle: Graph Saliency Maps through Spectral Convolutional Networks: Application to Sex Classification with Brain Connectivity.\nContent: Graph convolutional networks (GCNs) allow to apply traditional convolution operations in non-Euclidean domains, where data are commonly modelled as irregular graphs. Medical imaging and, in particular, neuroscience studies often rely on such graph representations, with brain connectivity networks being a characteristic example, while ultimately seeking the locus of phenotypic or disease-related differences in the brain. These regions of interest (ROIs) are, then, considered to be closely associated with function and/or behaviour. Driven by this, we explore GCNs for the task of ROI identification and propose a visual attribution method based on class activation mapping. By undertaking a sex classification task as proof of concept, we show that this method can be used to identify salient nodes (brain regions) without prior node labels. Based on experiments conducted on neuroimaging data of more than 5000 participants from UK Biobank, we demonstrate the robustness of the proposed method in highlighting reproducible regions across individuals. We further evaluate the neurobiological relevance of the identified regions based on evidence from large-scale UK Biobank studies.\n\nTitle: Classifier-Based Multi-atlas Label Propagation with Test-Specific Atlas Weighting for Correspondence-Free Scenarios.\nContent: We propose a segmentation method which transfers the advantages of multi-atlas label propagation (MALP) to correspondence-free scenarios. MALP is a branch of segmentation approaches with attractive properties, which is currently applicable only in correspondence-based regimes such as brain labeling, which assume correspondence between atlases and test image. This precludes its use for the large class of tasks without this property, such as tumor segmentation. In this work, we propose a method which circumvents the correspondence assumption by using a classifier-based atlas representation in the spirit of the recently proposed Atlas Forests (AF). To counteract the negative effects of the over-training property of AF for applications with highly heterogeneous examples, we employ test-specific atlas weighting by the STAPLE approach. The main idea is that over-training ceases to be a problem if the prediction is based only on training atlases which are \"similar\" to the test image. Here, the \"similarity\" is based on the estimated ability of an atlas-based classifier to perform a correct labeling. We show a successful use of the proposed method for segmentation of brain tumors on data from the BraTS 2013 Challenge, which presents a correspondence-free scenario in which standard MALP cannot be expected to operate.\n\nTitle: Joint classification-regression forests for spatially structured multi-object segmentation\nContent: In many segmentation scenarios, labeled images contain rich structural information about spatial arrangement and shapes of the objects. Integrating this rich information into supervised learning techniques is promising as it generates models which go beyond learning class association, only. This paper proposes a new supervised forest model for joint classification-regression which exploits both class and structural information. Training our model is achieved by optimizing a joint objective function of pixel classification and shape regression. Shapes are represented implicitly via signed distance maps obtained directly from ground truth label maps. Thus, we can associate each image point not only with its class label, but also with its distances to object boundaries, and this at no additional cost regarding annotations. The regression component acts as spatial regularization learned from data and yields a predictor with both class and spatial consistency. In the challenging context of simultaneous multi-organ segmentation, we demonstrate the potential of our approach through experimental validation on a large dataset of 80 three-dimensional CT scans.\n\n## Task\nCompose an abstract for the title 'WESD--Weighted Spectral Distance for Measuring Shape Dissimilarity' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper presents a new distance for measuring shape dissimilarity between objects. Recent publications introduced the use of eigenvalues of the Laplace operator as compact shape descriptors. Here, we revisit the eigenvalues to define a proper distance, called Weighted Spectral Distance (WESD), for quantifying shape dissimilarity. The definition of WESD is derived through analyzing the heat trace. This analysis provides the proposed distance with an intuitive meaning and mathematically links it to the intrinsic geometry of objects. We analyze the resulting distance definition, present and prove its important theoretical properties. Some of these properties include: 1) WESD is defined over the entire sequence of eigenvalues yet it is guaranteed to converge, 2) it is a pseudometric, 3) it is accurately approximated with a finite number of eigenvalues, and 4) it can be mapped to the ([0,1)) interval. Last, experiments conducted on synthetic and real objects are presented. These experiments highlight the practical benefits of WESD for applications in vision and medical image analysis."
      }
    ]
  },
  {
    "tid":"151",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Generalized Spatial Modulation With Transmit Antenna Grouping For Massive Mimo\nContent: In this paper, an effective low complexity generalized spatial modulation (GenSM) scheme with transmit antenna grouping is proposed for massive multi-input multi-output (MIMO) system to deal with the channel correlation among transmit antennas. In the proposed scheme, all transmit antennas are divided into several equal-sized groups, and spatial modulation (SM) is carried out to select one active antenna in each group independently. Two different grouping methods, i.e., block grouping and interleaved grouping, are introduced to optimize the error performance in low and high signal-to-noise ratio (SNR) region, respectively. In consideration of the large amount of transmit antennas in a massive MIMO system, both linear and 2-dimensional transmit antenna arrays are considered in our design. To evaluate the performance, a closed-form expression of the average bit error probability (ABEP) upper bound is derived for all proposed grouping methods and Monte-Carlo simulations are conducted to verify the analysis and reveal the performance gain of the proposed scheme in terms of bit error rate (BER) in comparison with conventional GenSM.\n\nTitle: Bipartite Matching Based User Grouping for Grouped OFDM-IDMA\nContent: In this paper, we present a novel user grouping method for grouped OFDM-IDMA systems. Aiming at maximizing the system capacity, we adaptively distribute users among the pre-allocated subcarrier groups according to their respective channel conditions. Using the notion of SNR evolution function, we first analyze the achievable capacity of the system and formulate the optimization problem as a weighted bipartite matching problem. Due to the analytically intractability of the SNR evolution function, we opt to use either the worst case or best case of the achievable capacity to approximate the original problem. Then Kuhn-Munkres method is employed to solve the approximated problems. The performance of the proposed scheme is evaluated by both theoretical analyses and simulations. Results show that with our proposed algorithm, the system capacity is markedly improved and is very close to the theoretical capacity upper bound.\n\nTitle: Index modulated OFDM with interleaved grouping for V2X communications\nContent: The rapid penetration of intelligent transportation systems (ITS) into the conventional transportation infrastructure urgently calls for high spectral efficiency high reliability communication technology for vehicle-to-vehicle and vehicle-to-infrastructure (V2X) applications. Since orthogonal frequency division multiplexing (OFDM) is widely considered as a promising candidate for such applications, in this paper we propose a novel variation of OFDM for improved spectral efficiency as well as enhanced reliability in V2X channels with correlated frequency-selective fading and inevitable Doppler effects. Our proposed scheme is built upon a recently emerging technique termed as index modulated (IM-)OFDM. However, different from the existing localized subcarrier grouping, we propose interleaved subcarrier grouping. We then carry out analytical and simulated comparisons to demonstrate the merits of this new scheme in terms of both the bit error rate (BER) performance and the maximum achievable rate (MAR) of the overall system, in V2X channels.\n\nTitle: Multistage block-spreading for impulse radio multiple access through ISI channels\nContent: Transmitting digital information using ultra-short pulses, impulse radio (IR) has received increasing interest for multiple access (MA). When IRMA systems have to operate in dense multipath environments, the multiple user interference (MUI) and intersymbol interference (ISI) induced, adversely affect system capacity and performance. Analog IRMA utilizes pulse position modulation (PPM) and random time-hopping codes to mitigate ISI and suppress MUI statistically. We develop an all-digital IRMA scheme that relies on multistage block-spreading (MS-BS), and judiciously designed transceiver pairs to eliminate MUI deterministically, and regardless of ISI multipath effects. Our proposed MS-BS-IRMA system can accommodate a large number of users and is capable of providing different users with variable transmission rates, which is important for multimedia applications. Unlike conventional IRMA systems, MS-BS-IRMA exhibits no degradation in bit-error rate performance, as the number of users increases.\n\nTitle: OFDM-IDMA with User Grouping\nContent: A generalized version of orthogonal frequency division multiplexing interleave division multiple access (OFDM-IDMA) referred to as grouped OFDM-IDMA (G-OFDM-IDMA) is introduced in this paper. By dividing users into groups and transmitting each group's data only on some (as opposed to all) subcarriers, G-OFDM-IDMA can have much lower decoding complexity compared with conventional OFDM-IDMA while preserving the bit error probability (BEP) performance and the bandwidth efficiency. The user grouping problem is formulated into an integer linear programming problem whose suboptimal solution is proposed and compared with the lower bound. The optimization complexity issue is also addressed. Simulations are carried out to test the performance of G-OFDM-IDMA under various system configurations. It is observed that up to 80% complexity could be saved when the conventional OFDM-IDMA is substituted by G-OFDM-IDMA configured according to the suboptimal grouping solution.\n\n## Task\nCompose an abstract for the title 'Generalized Spatial Modulation With Transmit Antenna Grouping For Correlated Channels' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, an effective generalized spatial modulation (GenSM) scheme with transmit antenna grouping is proposed to overcome the performance degradation caused by correlated channels. In the proposed scheme, the transmit antennas are divided into several equal-sized groups, and spatial modulation (SM) is carried out to select one active antenna in each group independently. It is quite different from the conventional GenSM which jointly selects active antenna set. Apart from the straightforward block grouping method, which collects the adjacent antennas to the same group, interleaved grouping is also introduced. It can maximize the average distance between the antennas in the same group, since the channel correlation depends on it. To evaluate the performance, a closed-form expression of the average bit error probability (ABEP) upper bound is derived for all proposed grouping methods and Monte-Carlo simulations are conducted to verify the analysis and reveal the performance gain of the proposed scheme in terms of bit error rate (BER) in comparison with conventional GenSM and SM."
      }
    ]
  },
  {
    "tid":"152",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Deriving a multi-subject functional-connectivity atlas to inform connectome estimation.\nContent: The estimation of functional connectivity structure from functional neuroimaging data is an important step toward understanding the mechanisms of various brain diseases and building relevant biomarkers. Yet, such inferences have to deal with the low signal-to-noise ratio and the paucity of the data. With at our disposal a steadily growing volume of publicly available neuroimaging data, it is however possible to improve the estimation procedures involved in connectome mapping. In this work, we propose a novel learning scheme for functional connectivity based on sparse Gaussian graphical models that aims at minimizing the bias induced by the regularization used in the estimation, by carefully separating the estimation of the model support from the coefficients. Moreover, our strategy makes it possible to include new data with a limited computational cost. We illustrate the physiological relevance of the learned prior, that can be identified as a functional connectivity atlas, based on an experiment on 46 subjects of the Human Connectome Dataset.\n\nTitle: Enhancing the Reproducibility of Group Analysis with Randomized Brain Parcellations.\nContent: Neuroimaging group analyses are used to compare the inter-subject variability observed in brain organization with behavioural or genetic variables and to assess risks factors of brain diseases. The lack of stability and of sensitivity of current voxel-based analysis schemes may however lead to non-reproducible results. A new approach is introduced to overcome the limitations of standard methods, in which active voxels are detected according to a consensus on several random parcellations of the brain images, while a permutation test controls the false positive risk. Both on syntetic and real data, this approach shows higher sensitivity, better recovery and higher reproducibility than standard methods and succeeds in detecting a significant association in an imaging-genetic study between a genetic variant next to the COMT gene and a region in the left thalamus on a functional Magnetic Resonance Imaging contrast.\n\nTitle: Detection of signal synchronizations in resting-state fMRI datasets.\nContent: In this paper, we propose a generic framework for the analysis of steady-state fMRI datasets, applied here to resting-state datasets. Our approach avoids the introduction of user-defined seed regions for the study of spontaneous activity. Unlike existing techniques, it yields a sparse representation of resting-state activity networks which can be characterized and investigated fairly easily in a semi-interactive fashion. We proceed in several steps, based on the idea that spectral coherence of the fMRI time courses in the low frequency band carries the information of interest. In particular, we address the question of building adapted representations of the data from the spectral coherence matrix. We analyze nine datasets taken from three subjects and show resting-state networks validated by EEG-fMRI simultaneous acquisition literature, with low intra-subject variability; we also discuss the merits of different (rapid/slow) fMRI acquisition schemes.\n\nTitle: Bootstrapped Permutation Test for Multiresponse Inference on Brain Behavior Associations\nContent: Despite that diagnosis of neurological disorders commonly involves a collection of behavioral assessments, most neuroimaging studies investigating the associations between brain and behavior largely analyze each behavioral measure in isolation. To jointly model multiple behavioral scores, sparse multiresponse regression (SMR) is often used. However, directly applying SMR without statistically controlling for false positives could result in many spurious findings. For models, such as SMR, where the distribution of the model parameters is unknown, permutation test and stability selection are typically used to control for false positives. In this paper, we present another technique for inferring statistically significant features from models with unknown parameter distribution. We refer to this technique as bootstrapped permutation test (BPT), which uses Studentized statistics to exploit the intuition that the variability in parameter estimates associated with relevant features would likely be higher with responses permuted. On synthetic data, we show that BPT provides higher sensitivity in identifying relevant features from the SMR model than permutation test and stability selection, while retaining strong control on the false positive rate. We further apply BPT to study the associations between brain connectivity estimated from pseudo-rest fMRI data of 1139 fourteen year olds and behavioral measures related to ADHD. Significant connections are found between brain networks known to be implicated in the behavioral tasks involved. Moreover, we validate the identified connections by fitting a regression model on pseudo-rest data with only those connections and applying this model on resting state fMRI data of 337 left out subjects to predict their behavioral scores. The predicted scores significantly correlate with the actual scores, hence verifying the behavioral relevance of the found connections.\n\nTitle: Triangulating Cortical Functional Networks With Anatomical Landmarks\nContent: Defining precisely the position of active regions obtained from functional neuromiaging studies is challenging due to the functional and anatomical variability across subjects. Traditional volumetric normalization techniques ignore the geometry of the cortex and use a relatively imprecise three-dimensional coordinate system. In this study we propose an alternative method that relates the position of functional regions on the cortical surface to the positions of the main macro-anatomical structures, the sulci. Our approach consists of using the nearest sulci to build a local referential in which the position of a region is defined. This triangulation approach improves the localization of brain regions involved in various cognitive tasks.\n\n## Task\nCompose an abstract for the title 'Deriving reproducible biomarkers from multi-site resting-state data: An Autism-based example.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Resting-state functional Magnetic Resonance Imaging (R-fMRI) holds the promise to reveal functional biomarkers of neuropsychiatric disorders. However, extracting such biomarkers is challenging for complex multi-faceted neuropathologies, such as autism spectrum disorders. Large multi-site datasets increase sample sizes to compensate for this complexity, at the cost of uncontrolled heterogeneity. This heterogeneity raises new challenges, akin to those face in realistic diagnostic applications. Here, we demonstrate the feasibility of inter-site classification of neuropsychiatric status, with an application to the Autism Brain Imaging Data Exchange (ABIDE) database, a large (N=871) multi-site autism dataset. For this purpose, we investigate pipelines that extract the most predictive biomarkers from the data. These R-fMRI pipelines build participant-specific connectomes from functionally-defined brain areas. Connectomes are then compared across participants to learn patterns of connectivity that differentiate typical controls from individuals with autism. We predict this neuropsychiatric status for participants from the same acquisition sites or different, unseen, ones. Good choices of methods for the various steps of the pipeline lead to 67% prediction accuracy on the full ABIDE data, which is significantly better than previously reported results. We perform extensive validation on multiple subsets of the data defined by different inclusion criteria. These enables detailed analysis of the factors contributing to successful connectome-based prediction. First, prediction accuracy improves as we include more subjects, up to the maximum amount of subjects available. Second, the definition of functional brain areas is of paramount importance for biomarker discovery: brain areas extracted from large R-fMRI datasets outperform reference atlases in the classification tasks."
      }
    ]
  },
  {
    "tid":"153",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Fuzzy object shape model for newborn brain MR image segmentation\nContent: In magnetic resonance (MR) images, finding a small change of parenchyma in newborn babies' brain significantly helps physicians to diagnose suspicious hypoxic-ischemic encephalopathy patients. However, there are no computer-aided methods because an automated segmentation algorithm has not been established yet. This paper proposes a new image segmentation method for parenchyma segmentation in T2-weighted MR images. The proposed method introduces a fuzzy object model, which has a fuzzy boundary and MR signal learned from training data. It segments the parenchyma by maximizing a fuzzy degree of deformable surface model. The fuzzy degree is estimated by using the fuzzy object model. To validate the proposed method, we recruited 12 newborn babies whose revised ages were -1 month to 1 month. 9 subjects were used to generate the fuzzy object model, and the remained subjects were used to evaluate the segmentation accuracy. The segmentation accuracy has been evaluated by using sensitivity and false-positive ratio, which were calculated by comparing delineation result (ground truth).\n\nTitle: Fuzzy thick rubber model for cerebral surface extraction in neonatal brain MR images\nContent: Cerebral surface extraction plays a fundamental role of computer aided diagnosis (CAD) for neonatal brain magnetic resonance (MR) images. However, cerebral sulci of the neonatal brains is complexity folded, and it is difficult to extract complete cerebral contour from MR images due to the limitation of spatial resolution and partial volume effect (PVE). This paper proposes a novel method to extract the cerebral contour based on fuzzy thick rubber model (TRM). The TRM is deformed by using fuzzy control schemes so that the digitally synthesized MR images from the deforming TRM are identical to the given MR images. By synthesizing the MR images with respect to PVE, the proposed method is able to extract the cerebral contour with sub-voxel accuracy. The proposed method was applied to 7 subjects whose revised ages were from -17 days to 34 days. The root-mean-squared-error between the extracted contour and the manually delineated contour by two physicians was 1.09 \u00b1 0.48 mm from the truth contour. And, to demonstrate the clinical effective, gyral index was calculated using the extracted cerebral contour.\n\nTitle: Fuzzy Logic Assisted Quantification of Gyral Deformation Index Using Magnetic Resonance Images for the Infantile Brain\nContent: There are various cerebral diseases that deform the cerebral shape with region specificity. So it is effective to quantify the deformation change of cerebral gyri. This study introduces new index called gyral deformation index (GDI) that is defined as a ratio of area of gyrus of interest to area of cerebrum in the defined projection plane. To calculate the gyral areas, this paper proposes a gyral labeling method in the projection plane using magnetic resonance images. The new method finds the boundaries between the gyri by optimizing deformable boundary models aided by fuzzy logic. The proposed method was applied to quantify the cerebral deformation of infants on a plane which is perpendicular to the longitudinal fissure. The comparison results with the manual delineation showed that the method labels gyri with a mean sensitivity of 92.8% and a mean false positive rate of 0.1% for 14 infantile subjects (3 weeks \u2013 4 years 3 months old).\n\nTitle: Fuzzy-ASM Based Automated Skull Stripping Method from Infantile Brain MR Images\nContent: Automated stripping of skulls from infantile brain MR Images is the fundamental work to visualize cerebral surface and to measure cerebral volumes. They are important to evaluate cerebral diseases because most cerebral diseases cause morphometric changes in cerebrum. This study proposes a novel image segmentation method based on fuzzy rule-based active surface model. The proposed method was validated by applying it to two neonatal (3W and 4W) and six infantile (5W to 4Y2M) subjects. The mean sensitivity was 98.84 %, and false-positive rate was 1.21 %, and the cerebral surface was visualized well.\n\nTitle: Cerebral Contour Extraction With Particle Method In Neonatal Mr Images\nContent: Cerebral surface extraction from neonatal MR images is the basic work of quantifying the deformation of the cerebrum. Although there are many conventional methods of segmenting the cerebral region, only the rough area is given by counting the number of surface voxels in the segmented region. This article proposes a new method of extraction that is based on the particle method. The method introduces three kinds of particles that correspond to cerebrospinal fluid, gray matter, and white matter; it converts the brain MR images into the set of particles. The proposed method was applied to neonatal magnetic resonance images, and the experimental results showed that the cerebral contour was extracted with a root-mean-square-error of 0.51 mm compared with the ground truth contour given by a physician.\n\n## Task\nCompose an abstract for the title 'Fuzzy connectedness image segmentation for newborn brain extraction in MR images.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Newborn's brain has a various shape, and easily changes with not only brain developing and cerebral diseases. Although the brain segmentation in MR images is an effective way to quantify the brain shape and size, there are few studies in neonatal brain MR image analysis. This paper introduces a novel method based on fuzzy connectedness (FC) with fuzzy object model (FOM). FOM is built from a training dataset, and gives fuzzy degree belonging to parenchyma with respect to location and intensity. FC is calculated from object affinity and homogeneous affinity, and the object affinity is given by the FOM. The method first segments the white matter, and then segments the surrounding cortex. The propose method has been applied to 10 newborn subjects whose revised age was between -1 month and +2 month. Leave-on-out cross-validation (LOOCV) was conducted, and the mean false-positive volume fraction was 1.33%, the mean false-negative volume fraction was 2.90%, and geometric-mean was 1.42%."
      }
    ]
  },
  {
    "tid":"154",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A scalable and energy-efficient hybrid-based MAC protocol for wireless sensor networks\nContent: The problem of MAC protocol design for wireless sensor networks poses many challenges such as energy constraint resources, strict wireless bandwidth, channel utilization & etc. These challenges make handling changes in network topology and network scalability a major issue. In this paper we present a new scalable and energy efficient hybrid-based MAC protocol for wireless sensor networks, abbreviated as SEHM. The protocol saves energy by reducing energy consumption due to idle listening and signal interference. Idle listening is reduced by making idle nodes and all nodes that have no data to send to switch early to sleep state. Signal interference is limited through using time division based medium access. Scalability of our approach is achieved through dividing the sensor network into clusters. Clusters are dynamically formed as all nodes in the sensor network are allowed to content for the position of a cluster head, to finally elect suitable cluster heads. The performance of our protocol is studied and analyzed by means of computer simulations, and we show that our approach outperforms S-MAC protocol in terms of energy consumption and packet delivery rate.\n\nTitle: An Energy Efficient Priority-Based QoS MAC Protocol for Wireless Sensor Networks.\nContent: In this paper we present an energy efficient priority based MAC protocol for Wireless Sensor Networks (WSNs). We abbreviate it as PRIMA. The PRIMA protocol consists of two phases; a clustering phase and a channel access phase. Clustering the sensor network makes the MAC protocol to handle well network scalability issues. The channel access is composed of a hybrid mode of TDMA and CSMA. CSMA mode is used to communicate control messages, while data messages are assigned TDMA slots. Doing so, minimizes packet collisions and consequently minimizing energy consumption. Our PRIMA protocol forces the nodes that have no data to send to go early into a sleep state to save energy, this minimizes the idle listening periods which is considered as a main source of the energy consumption in sensor networks. The PRIMA protocol provides Quality of Service (QoS) by employing a queueing model to classify the traffic depending on its importance into four different queues. Higher priority queues have absolute preferential treatment over low priority queues.Through simulations and analytical analysis, we evaluate the performance of our proposed MAC protocol and compare it against the Q-MAC protocol. Results have shown that our protocol outperforms Q-MAC in terms of energy consumption, packet delivery ratio and average packet delay.\n\nTitle: Energy efficient and QoS aware medium access control for wireless sensor networks\nContent: Enabling quality of service (QoS) applications over battery-constrained wireless sensor networks (WSNs) poses significant challenges, because of the special characteristics of such networks. Developing real-time applications for WSNs necessitates the development of new communication protocols capable of achieving application-specific QoS and at the same time save energy. In this paper, we continue our previous work on developing and improving our EQ-MAC protocol, which is designed specifically for WSNs to reduce energy consumption and provides QoS guarantees through the use of the service differentiation concept. The proposed protocol consists of two subprotocols: Classifier MAC (C-MAC) and Channel Access MAC (CA-MAC). C-MAC is responsible of classifying gathered data at sensor nodes based on its importance. The CA-MAC is an energy conserving medium access mechanism which uses a hybrid scheduling technique. CA-MAC saves energy by differentiating between control and data messages. Data messages are assigned scheduled slots with no contention (here we have to emphasize that slots are assigned only to those nodes that have data to send, this allows an efficient energy use of the Time Division Multiple Access (TDMA) slots), whereas short periodic control messages are assigned random access slots. To study the performance of the EQ-MAC protocol, we have conducted several simulation experiments on Network Simulator (NS-2) to evaluate and compare the performance of our protocol against S-MAC and Q-MAC protocols. Simulation results demonstrate that EQ-MAC outperforms the other two protocols and achieves a significant improvement in terms of the energy expenditure and delivery ratio. Under prioritized traffic, EQ-MAC is comparable to Q-MAC in average latency, while achieving less energy consumption. Copyright \u00a9 2010 John Wiley & Sons, Ltd.\n\nTitle: An energy efficient and QoS aware multipath routing protocol for wireless sensor networks\nContent: Enabling real time applications in Wireless Sensor Networks (WSNs) demands certain delay and bandwidth requirements which pose more challenges in the design of networking protocols. Therefore, enabling such applications in this type of networks requires energy and Quality of Service (QoS) awareness in different layers of the protocol stack. In many of these applications (such as multimedia applications, or real time and mission critical applications), the network traffic is mixed of delay sensitive and reliability demanding data. Hence, QoS routing becomes an important issue. In this paper, we propose an Energy Efficient and QoS aware multipath routing protocol (we name it shortly as EQSR) that maximizes the network lifetime through balancing energy consumption across multiple nodes, uses the concept of service differentiation to allow high important traffic (or delay sensitive traffic) to reach the sink node within an acceptable delay, reduces the end to end delay through spreading out the traffic across multiple paths, and increases the throughput through introducing data redundancy. EQSR uses the residual energy, node available buffer size, and Signal-to-Noise Ratio (SNR) to predict the best next hop through the paths construction phase. Based on the concept of service differentiation the EQSR protocol employs a queuing model to handle both real time and non-real time traffic. By means of computer simulations, we evaluated and studied the performance of our routing protocol and compared it with another protocol. Simulation results have shown that our protocol achieves lower average delay and higher packet delivery ratio than the other protocol.\n\nTitle: Performance evaluation of a hybrid MAC protocol for wireless sensor networks\nContent: One of the classical methods to study the performance of networking protocols and algorithms is to use simulation tools like OPNET, NS-2, OMNet++, ...etc. An alternative to simulation tools is to use formal modeling techniques. The advantage of using formal modeling techniques over simulation tools is that the user can do both performance evaluation and model verification. One of these methods is Petri Nets. A Petri Net is a mathematical and graphical tool, invented by Carl Adam Petri, used to verify systems and protocols. Petri Nets have been used to analyze and verify systems in different areas of computer science such as parallel processing, distributed systems and computer networks, artificial intelligence...etc. In this paper, we use the Colored Petri Nets (CPN), an extension for Petri Nets, to model and evaluate the performance of our EQ-MAC protocol [1]. EQ-MAC is quality of service and energy aware hybrid MAC protocol specifically designed for wireless sensor networks. After developing the Petri models for the EQ-MAC protocol, the GreatSPN software system is used to validate the correctness of the developed models and evaluate the performance. Two important performance measures are defined, which are average delay and packets delivery ratio; The results demonstrate the efficiency of our protocol. Furthermore, this work demonstrates the ability of Petri-Nets to model and evaluate MAC protocol for wireless sensor networks.\n\n## Task\nCompose an abstract for the title 'Towards a classification of energy aware MAC protocols for wireless sensor networks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Power management is an important issue in wireless sensor networks (WSNs) because wireless sensor nodes are usually battery powered, and an efficient use of the available battery power becomes an important concern specially for those applications where the system is expected to operate for long durations. This necessity for energy efficient operation of a WSN has prompted the development of new protocols in all layers of the communication stack. Provided that, the radio transceiver is the most power consuming component of a typical sensor node, large gains can be achieved at the link layer where the medium access control (MAC) protocol controls the usage of the radio transceiver unit. MAC protocols for sensor networks differ greatly from typical wireless networks access protocols in many issues. MAC protocols for sensor networks must have built-in power conservation, mobility management, and failure recovery strategies. Furthermore, sensor MAC protocols should make performance trade-off between latency and throughput for a reduction in energy consumption to maximize the lifetime of the network. This is in general achieved through duty cycling the radio transceiver. Many MAC protocols with different objectives were proposed for wireless sensor networks in the literature. Most of these protocols take into account the energy efficiency as a main objective. There is much more innovative work should be done at the MAC layer to address the hard unsolved problems. In this paper, we first outline and discuss the specific requirements and design trade-offs of a typical wireless sensor MAC protocol by describing the properties of WSN that affect the design of MAC layer protocols. Then, a typical collection of wireless sensor MAC protocols presented in the literature are surveyed, classified, and described emphasizing their advantages and disadvantages whenever possible. Finally, we present research directions and identify open issues for future medium access research. Copyright \u00a9 2009 John Wiley & Sons, Ltd."
      }
    ]
  },
  {
    "tid":"155",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Joint Orchestration of Cloud-Based Microservices and Virtual Network Functions.\nContent: Recent studies show the increasing popularity of distributed cloud applications, which are composed of multiple microservices. Besides their known benefits, microservice architecture also enables to mix and match cloud applications and Network Function Virtualization (NFV) services (service chains), which are composed of Virtual Network Functions (VNFs). Provisioning complex services containing VNFs and microservices in a combined NFV/cloud platform can enhance service quality and optimise cost. Such a platform can be based on the multi-cloud concept. However, current multi-cloud solutions do not support NFV requirements, making them inadequate to support complex services. In this paper, we investigate these challenges and propose a solution for jointly managing and orchestrating microservices and virtual network functions.\n\nTitle: Specification, Composition, And Placement Of Network Services With Flexible Structures\nContent: Network function virtualization and software-defined networking allow services consisting of virtual network functions to be designed and implemented with great flexibility by facilitating automatic deployments, migrations, and reconfigurations for services and their components. For extended flexibility, we go beyond seeing services as a fixed chain of functions. We define the service structure in a flexible way that enables changing the order of functions in case the functionality of the service is not influenced by this and propose a YANG data model for expressing this flexibility. Flexible structures allow the network orchestration system to choose the optimal composition of service components that, for example, gives the best results for placement of services in the network. When number of flexible services and number of components in each service increase, combinatorial explosion limits the practical use of this flexibility. In this paper, we describe a selection heuristic that gives a Pareto set of the possible compositions of a service as well as possible combinations of different services, with respect to different optimization objectives. Moreover, we present a heuristic algorithm for placement of a combination of services, which aims at placing service components along shortest paths that have enough capacity for accommodating the services. By applying these solutions, we show that allowing flexibility in the service structure is feasible.\n\nTitle: Specifying and placing chains of virtual network functions\nContent: Network appliances perform different functions on network flows and constitute an important part of an operator's network. Normally, a set of chained network functions process network flows. Following the trend of virtualization of networks, virtualization of the network functions has also become a topic of interest. We define a model for formalizing the chaining of network functions using a context-free language. We process deployment requests and construct virtual network function graphs that can be mapped to the network. We describe the mapping as a Mixed Integer Quadratically Constrained Program (MIQCP) for finding the placement of the network functions and chaining them together considering the limited network resources and requirements of the functions. We have performed a Pareto set analysis to investigate the possible trade-offs between different optimization objectives.\n\nTitle: Efficient flow processing-aware controller placement in future wireless networks\nContent: Future wireless access networks include a broad variety of applications. Examples for these range from network applications meant to optimize the usage of resources, e.g. Coordinated Multi-Point (CoMP) transmission, over coordinating network mechanisms for Inter-Cell Interference Coordination (ICIC) to virtualized applications to gain flexibility and save hardware costs such as Virtual Network Functions (VNFs). Still, there is one common thread for all these functions: the necessity for data flow processing. Data flow processing imposes many requirements on network resources and a good allocation of the locations for data processing in a network is a challenging task and crucial for network performance. Hence, data flow processing is a significant point to be considered when searching for efficient control architectures for future wireless networks. We propose a flow processing-aware controller placement framework, which introduces data flow processing to the research domain of future wireless network control and enhances the classic Software-Defined Networking (SDN) view of reducing network control to just packet forwarding. We have developed an optimization problem and a heuristic algorithm that verify the practical feasibility of flow processing-aware controller placement. Our evaluations reveal that the heuristic algorithm performs close to optimal, while reducing the runtime by about four orders of magnitude compared to the optimization problem and thus providing acceptable runtime for real-world application.\n\nTitle: Flow Processing-Aware Controller Placement In Wireless Densenets\nContent: The traffic demand in wireless access networks has grown substantially in recent years and is expected to continue doing so, both in terms of total volume and data rate required by individual users. To overcome the current limitations of wireless access networks, operators already push for very dense and heterogeneous wireless networks, commonly known as DenseNets. Efficiently controlling DenseNets is a task for current research, as simply scaling existing networks by orders of magnitude brings along several problems, e.g. increased energy consumption and the explosion of signaling. Further, new innovations such as Network Functions Virtualization (NFV) which target the operation expenses for networks with a large variety of proprietary hardware appliances, reveal that data flow processing should not be neglected in this research domain. Software-Defined Networking (SDN), which separates the control plane and the data forwarding plane, has been identified as a promising approach for an efficient control of DenseNets. However, the controllers of classic SDN approaches are confined to the routing of data flows, while the support of sophisticated processing mechanisms, e.g. NFV or Coordinated Multi-Point transmission (CoMP), is not considered. Data flow processing imposes many requirements on data rate, latency and processing capacity. Hence, an adequate allocation of the locations for data processing in a network is a challenging task and crucial for network performance. In this paper we propose our flow processing-aware controller placement framework that takes into account data flow processing and control applications, both enhancing the SDN architecture specifically for DenseNets.\n\n## Task\nCompose an abstract for the title 'Joint Optimization of Scaling and Placement of Virtual Network Services.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The management of complex network services requires flexible and efficient service provisioning as well as optimized handling of continuous changes in the workload of the services. To adapt to changes in the demand, service components need to be replicated (scaling) and allocated to physical resources (placement) dynamically. In this paper, we propose a fully automated approach to the joint optimization problem of scaling and placement, enabling quick reaction to changes. We formalize the problem, analyze its complexity, and develop two algorithms to solve it. Empirical results show the applicability and effectiveness of the proposed approach."
      }
    ]
  },
  {
    "tid":"156",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Distributed Top-K Query Processing in Wireless Sensor Networks\nContent: Wireless Sensor Networks create an innovative technology that enables users to monitor and study the physical world at an extremely high resolution. Query processing in such ad-hoc environments is a challenging task due to the complexities imposed by the inherent energy and communication constraints. To this end, the research community has proposed to take into account user-defined parameters in order to derive the K most relevant (or Top-K) answers quickly and efficiently. A Top-K query returns the subset of most relevant answers, in place of all answers, for two reasons: i) to minimize the cost metric that is associated with the retrieval of all answers; and ii) to improve the recall and the precision of the answer set, such that the user is not overwhelmed with irrelevant results.This tutorial presents the fundamental concepts behind distributed Top-K query processing and the adaptations of these algorithms to distributed and wireless sensor networks. It additionally provides a gentle overview of rudimentary and advanced techniques covering a significant body of research in this domain. The tutorial will start out with an overview of the most influential centralized and middleware Top-K query processing algorithms and then proceed with an elaborate description of distributed Top-K ranking algorithms for One-time Top-K Queries, Continuous Top-K Queries and Approximate Top-K Queries. Finally, it will provide an outlook to compelling future applications that can be constructed on the foundation of these algorithms. Although the tutorial is specifically geared towards Wireless Sensor Networks, many of the presented ideas find extensions in other mobile environments such as Adhoc Networks, Vehicular Networks and the Mobile Web.\n\nTitle: pFusion: A P2P Architecture for Internet-Scale Content-Based Search and Retrieval\nContent: The emerging Peer-to-Peer (P2P) model has become a very powerful and attractive paradigm for developing Internet-scale systems for sharing resources, including files and documents. The distributed nature of these systems, where nodes are typically located across different networks and domains, inherently hinders the efficient retrieval of information. In this paper, we consider the effects of topologically aware overlay construction techniques on efficient P2P keyword search algorithms. We present the Peer Fusion (pFusion) architecture that aims to efficiently integrate heterogeneous information that is geographically scattered on peers of different networks. Our approach builds on work in unstructured P2P systems and uses only local knowledge. Our empirical results, using the pFusion middleware architecture and data sets from Akamai's Internet mapping infrastructure (AKAMAI), the Active Measurement Project (NLANR), and the Text REtrieval Conference (TREC) show that the architecture we propose is both efficient and practical.\n\nTitle: CLODA: A Crowdsourced Linked Open Data Architecture\nContent: In this paper we present our Crowdsourced Linked Open Data Architecture (CLODA), a first attempt to combine crowdsourcing, localization and location-based services to generate, collect, validate and relate real-world, geo-spatial and multi-dimensional information using smartphones and other mobile devices. CLODA focuses on the construction of URI addressable, interlinked and semi-structured data following the Linked-Open Data (LOD) paradigm. The validity of the constructed data is then contributed by a participating crowd. We present our prototype implementation on top of Google Maps and a blend of in-house technologies, particularly our indoor positioning framework, coined Airplace, our trajectory similarity framework, coined SmartTrace, our neighborhood detection framework, coined Proximity and our smartphone testing platform coined SmartLab.\n\nTitle: Rayzit: An Anonymous and Dynamic Crowd Messaging Architecture\nContent: The smartphone revolution has introduced a new era of social networks where users communicate over anonymous messaging platforms to exchange opinions, ideas and even carry out commerce. These platforms enable individuals to establish social interactions between strangers based on a common interest or attribute. In this paper we present Rayzit1, a novel anonymous crowd messaging architecture, which utilizes the location of each user to connect them instantly to their k Nearest Neighbors (kNN) as they move in space. Contrary to the very large body of location-based social networks that suffer from bootstrapping issues, our architecture enables a user to always interact with the geographically closest possible users around. We establish this communication using a fast computation of an All kNN query that generates a dynamic global social graph every few seconds. We present motivating application scenarios and the detailed back-end architecture that allows Rayzit to scale. We have collected and analyzed data from the interactions of thousands of active users and confirm our claims.\n\nTitle: Multi-objective Query Optimization in Smartphone Social Networks\nContent: The bulk of social network applications for smart phones (e.g., Twitter, Face book, Foursquare, etc.) currently rely on centralized or cloud-like architectures in order to carry out their data sharing and searching tasks. Unfortunately, the given model introduces both data-disclosure concerns (e.g., disclosing all captured media to a central entity) and performance concerns (e.g., consuming precious smart phone battery and bandwidth during content uploads). In this paper, we present a novel framework, coined Smart Opt, for searching objects (e.g., images, videos, etc.) captured by the users in a mobile social community. Our framework, is founded on an in-situ data storage model, where captured objects remain local on their owner's smart phones and searches then take place over a novel lookup structure we compute dynamically, coined the Multi-Objective Query Routing Tree (MO-QRT). Our structure concurrently optimizes several conflicting objectives (i.e., it minimizes energy consumption, minimizes search delay and maximizes query recall), using a Multi-objective Evolutionary Algorithm based on Decomposition (MOEA/D) that calculates a diverse set of high quality non-dominated solutions in a single run. We assess our ideas with mobility patterns derived by Microsoft's Geolife project and social patterns derived by DBLP. Our study reveals that Smart Opt can yield query recall rates of 95%, with one order of magnitude less time and two orders of magnitude less energy than its competitors.\n\n## Task\nCompose an abstract for the title 'Design and Implementation of a Distributed Crawler and Filtering Processor' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Web crawlers are the key component of services running on Internet and providing searching and indexing support for the entire Web, for corporate Intranets and large portal sites. More recently, crawlers have also been used as tools to conduct focused Web searches and to gather data about the characteristics of the WWW. In this paper, we study the employment of crawlers as a programmable, scalable, and distributed component in future Internet middleware infrastructures and proxy services. In particular, we present the architecture and implementation of, and experimentation withWebRACE, a high-performance, distributedWeb crawler, filtering server and object cache. We address the challenge of designing and implementing modular, open, distributed, and scalable crawlers, using Java. We describe our design and implementation decisions, and various optimizations. We discuss the advantages and disadvantages of using Java to implement the WebRACE-crawler, and present an evaluation of its performance. WebRACE is designed in the context of eRACE, an extensible Retrieval Annotation Caching Engine, which collects, annotates and disseminates information from heterogeneous Internet sources and protocols, according to XML-encoded user profiles that determine the urgency and relevance of collected information."
      }
    ]
  },
  {
    "tid":"157",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Pattern reversal visual evoked responses of V1/V2 and V5/MT as revealed by MEG combined with probabilistic cytoarchitectonic maps.\nContent: Pattern reversal stimulation provides an established tool for assessing the integrity of the visual pathway and for studying early visual processing. Numerous magnetoencephalographic (MEG) and electroencephalographic (EEG) studies have revealed a three-phasic waveform of the averaged pattern reversal visual evoked potential/magnetic field, with components N75(m), P100(m), and N145(m). However, the anatomical assignment of these components to distinct cortical generators is still a matter of debate, which has inter alia connected with considerable interindividual variations of the human striate and extrastriate cortex. The anatomical variability can be compensated for by means of probabilistic cytoarchitectonic maps, which are three-dimensional maps obtained by an observer-independent statistical mapping in a sample of ten postmortem brains. Transformed onto a subject's brain under consideration, these maps provide the probability with which a given voxel of the subject's brain belongs to a particular cytoarchitectonic area. We optimize the spatial selectivity of the probability maps for V1 and V2 with a probability threshold which optimizes the self- vs. cross-overlap in the population of postmortem brains used for deriving the probabilistic cytoarchitectonic maps. For the first time, we use probabilistic cytoarchitectonic maps of visual cortical areas in order to anatomically identify active cortical generators underlying pattern reversal visual evoked magnetic fields as revealed by MEG. The generators are determined with magnetic field tomography (MFT), which reconstructs the current source density in each voxel. In all seven subjects, our approach reveals generators in V1/V2 (with a greater overlap with V1) and in V5 unilaterally (right V5 in three subjects, left V5 in four subjects) and consistent time courses of their stimulus-locked activations, with three peak activations in V1/V2 (contributing to C1m/N75m, P100m, and N145m) and two peak activations in V5 (contributing to P100m and N145m). The reverberating V1/V2 and V5 activations demonstrate the effect of recurrent activation mechanisms including V1 and extrastriate areas and/or corticofugal feedback loops. Our results demonstrate that the combined investigation of MEG signals with MFT and probabilistic cytoarchitectonic maps significantly improves the anatomical identification of active brain areas.\n\nTitle: Functional anatomy and differential time courses of neural processing for explicit, inferred, and illusory contours\nContent: The perception of shape does not necessarily require viewing an explicit outline figure. Using event-related functional magnetic resonance imaging we examined the time courses of neural activations provoked by shapes defined by (1) lines, (2) illusory contour inducers, and (3) reversed inducers. SPM99 was used to analyze the common and differential neural responses associated with the stimuli and their temporal derivatives. Illusory figures versus reversed inducers activated extrastriate cortex. Reversed inducers versus illusory figures activated the right parietal cortex. For both illusory and line contours versus reversed inducers, analysis of the temporal derivatives showed earlier activations in extrastriate and left parietal cortex and for line contours also in the extrastriate cortex bilaterally and in the right parietal cortex; these earlier activations were mirrored by differences in reaction times with subjects responding more slowly to shapes defined by reversed inducers. The results show substantial bottom-up effects (in occipital cortex) in the recognition of illusory and explicit shapes. By contrast, in stimuli where the shape must be inferred, there is greater reliance on right parietal cortex, consistent with increased attentional demands and top-down processing. The temporal derivatives provide useful information on the differential timing of the associated hemodynamic responses in occipital, parietal, and motor cortex.\n\nTitle: Locating the functional and anatomical boundaries of human primary visual cortex.\nContent: The primary visual cortex (V1) can be delineated both functionally by its topographic map of the visual field and anatomically by its distinct pattern of laminar myelination. Although it is commonly assumed that the specialized anatomy V1 exhibits corresponds in location with functionally defined V1, demonstrating this in human has not been possible thus far due to the difficulty of determining the location of V1 both functionally and anatomically in the same individual. In this study we use MRI to measure the anatomical and functional V1 boundaries in the same individual and demonstrate close agreement between them. Functional V1 location was measured by parcellating occipital cortex of 10 living humans into visual cortical areas based on the topographic map of the visual field measured using functional MRI. Anatomical V1 location was estimated for these same subjects using a surface-based probabilistic atlas derived from high-resolution structural MRI of the stria of Gennari in 10 intact ex vivo human hemispheres. To ensure that the atlas prediction was correct, it was validated against V1 location measured using an observer-independent cortical parcellation based on the laminar pattern of cell density in serial brain sections from 10 separate individuals. The close agreement between the independent anatomically and functionally derived V1 boundaries indicates that the whole extent of V1 can be accurately predicted based on cortical surface reconstructions computed from structural MRI scans, eliminating the need for functional localizers of V1. In addition, that the primary cortical folds predict the location of functional V1 suggests that the mechanism giving rise to V1 location is tied to the development of the cortical folds.\n\nTitle: Transmitter receptors reveal segregation of cortical areas in the human superior parietal cortex: relations to visual and somatosensory regions.\nContent: Regional distributions of ligand binding sites of 12 different neurotransmitter receptors (glutamatergic: AMPA, kainate, NMDA; GABAergic: GABAA, GABAB; cholinergic: muscarinic M2, nicotinic; adrenergic: \u03b11, \u03b12; serotonergic: 5-HT1A, 5-HT2; dopaminergic: D1) were studied in human postmortem brains by means of quantitative receptor autoradiography. Binding site densities were measured in the superior parietal lobule (SPL) (areas 5L, 5M, 5Ci, and different locations within Brodmann's area (BA) 7), somatosensory (BA 2), and visual cortical areas (BA 17, and different locations within BAs 18 and 19). Similarities of receptor distribution between cortical areas were analyzed by cluster analysis, uni- and multivariate statistics of mean receptor densities (averaged over all cortical layers), and profiles representing the laminar distribution patterns of receptors. A considerable heterogeneity of regional receptor densities and laminar patterns between the sites was found in the SPL and the visual cortex. The most prominent regional differences were found for M2 receptors. In the SPL, rostrocaudally oriented changes of receptor densities were more pronounced than those in mediolateral direction. The receptor distribution in the rostral SPL was more similar to that of the somatosensory cortex, whereas caudal SPL resembled the receptor patterns of the dorsolateral extrastriate visual areas. These results suggest a segregation of the different SPL areas based on receptor distribution features typical for somatosensory or visual areas, which fits to the dual functional role of this cortical region, i.e., the involvement of the human SPL in visuomotor and somatosensory motor transformations.\n\nTitle: Segregation of visceral and somatosensory afferents: An fMRI and cytoarchitectonic mapping study\nContent: Ano-rectal stimulation provides an important model for the processing of somatosensory and visceral sensations in the human nervous system. In spite of their anatomical proximity, the anal canal is innervated by somatosensory afferents whereas the rectum is innervated by the visceral nervous system. In a functional magnetic resonance (fMRI) experiment, we examined the cerebral responses to pneumatic balloon distension of these two structures to test whether somatosensory and visceral stimulation elicited distinct brain activations in spite of their spinal convergence. The specificity of the identified activations was analyzed by Bayesian mixed effects modeling. Activations in the parietal operculum were also compared to the location of cytoarchitectonically defined areas OP 1\u20134, which are part of the secondary somatosensory cortex (SII), to analyze whether the SII region was activated by anal and/or rectal stimulation.\n\n## Task\nCompose an abstract for the title 'Neural mechanism underlying impaired visual judgement in the dysmetabolic brain: an fMRI study.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"An altered brain metabolism in the parietal and prefrontal regions of the cerebral cortex as well as cognitive alterations have been found in patients suffering from hepatic encephalopathy. The neural mechanisms underlying these metabolically induced cognitive alterations, however, are not known. Since patients with liver cirrhosis without clinically overt encephalopathy already show an impaired performance in a flicker light test, the aim of this study was to analyze the normal and pathologically impaired neural mechanisms of these patients using functional magnetic resonance imaging (fMRI). Nine subjects at the early stage of encephalopathy [nonmanifest hepatic encephalopathy (nmHE)] and ten controls underwent scanning, while they indicated the apparent transition from a steady light to the onset of a flicker light, that is, the critical flicker frequency (CFF). Judgement-related blood oxygenation level-dependent (BOLD) activation was decreased in nmHE compared to controls in the right inferior parietal cortex (IPL). Furthermore, the analysis of psychophysiologic interaction suggests impaired neural interaction in patients with nmHE, especially between the IPL and the parietooccipital cortex (Poc), the intraparietal sulcus, the anterior cingulate cortex (ACC), the right prefrontal cortex (PFC), the medial temporal lobe, and the extrastriate cortex V5. In contrast, nonmanifest patients revealed an enhanced coupling between IPL and the postcentral cortex. Our findings provide evidence of an early-impaired and compensatory neural mechanism during visual judgement already in the earliest stages of hepatic encephalopathy and suggest an aberrant coupling between cerebral regions in the dysmetabolic brain."
      }
    ]
  },
  {
    "tid":"158",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Localization in wireless networks based on jointly compressed sensing\nContent: Location sensing is fundamental for supporting wireless communications services. This paper exploits the signal correlation structure observed in an indoor localization environment in order to provide accurate position estimation by means of a limited amount of signal-strength measurements. Because the mobile devices have limited processing power and battery capacity, the proposed received signal-strength localization protocol avoids putting on extra computational overhead on the mobile device by performing the position estimation at the Access Points (APs). Since the APs observe correlated signals from the mobile devices, the introduced method exploits the common structure of the received measurements in order to jointly estimate the positions precisely. The evaluation of the proposed protocol is performed on real laboratory data through experiments that quantify the impact of the system parameters on the location error.\n\nTitle: Decentralized indoor wireless localization using compressed sensing of signal-strength fingerprints\nContent: This paper combines recent developments in sparse approximation and distributed consensus theory to efficiently perform decentralized localization in wireless networks. To this goal, we exploit the Compressed Sensing (CS) framework, which provides a new paradigm for recovering signals being sparse in some basis by means of a limited amount of random incoherent projections. In particular, we propose a novel decentralized technique that considers the spatial correlations among the received measurements at the base stations (BSs) to provide global accurate position estimation, while reducing significantly the amount of measurements exchanged among the BSs and required for accurate positioning. We exploit the common structure of the received measurements to design a gossip-based algorithm in order to alleviate the effects of radio channel-induced signal variations on the estimation accuracy. Experimental evaluation with real data demonstrates the superiority of the proposed decentralized CS-based localization technique over traditional fingerprinting methods in terms of the achieved positioning accuracy.\n\nTitle: Indoor positioning in Wireless LANS using compressive sensing signal-strength fingerprints\nContent: Accurate indoor localization is a significant task for many ubiquitous and pervasive computing applications, with numerous solutions based on IEEE802.11, Bluetooth, ultrasound and infrared technologies being proposed. The inherent sparsity present in the problem of location estimation motivates in a natural fashion the use of the recently introduced theory of compressive sensing (CS), which states that a signal having a sparse representation in an appropriate basis can be reconstructed with high accuracy from a small number of random linear projections. In the present work, we exploit the framework of CS to perform accurate indoor localization based on signal-strength measurements, while reducing significantly the amount of information transmitted from a wireless device with limited power, storage, and processing capabilities to a central server. Equally importantly, the inherent property of CS acting as a weak encryption process is demonstrated by showing that the proposed approach presents an increased robustness to potential intrusions of an unauthorized entity. The experimental evaluation reveals that the proposed CS-based localization technique is superior in terms of an increased localization accuracy in conjunction with a low computational complexity when compared with previous statistical fingerprint-based methods.\n\nTitle: Localization in Wireless networks via Laser scanning and Bayesian compressed sensing\nContent: WiFi indoor localization has seen a renaissance with the introduction of RSSI-based approaches. However, manual fingerprinting techniques that split the indoor environment into predefined grids are implicitly bounding the maximum achievable localization accuracy. WoLF, our proposed Wireless localization and Laser-scanner assisted Fingerprinting system, solves this problem by automating the way indoor fingerprint maps are generated. We furthermore show that WiFi localization on the generated high resolution maps can be performed by sparse reconstruction which exploits the peculiarities imposed by the physical characteristics of indoor environments. Particularly, we propose a Bayesian Compressed Sensing (BCS) approach in order to find the position of the mobile user and dynamically determine the sufficient number of APs required for accurate positioning. BCS employs a Bayesian formalism in order to reconstruct a sparse signal using an undetermined system of equations. Experimental results with data collected in a university building validate WoLF in terms of localization accuracy under actual environmental conditions.\n\nTitle: Joint sparse signal ensemble reconstruction in a WSN using decentralized Bayesian matching pursuit\nContent: Wireless networks comprised of low-cost sensory devices have been increasingly used in surveillance both at the civilian and military levels. Limited power, processing, and bandwidth resources is a major issue for abandoned sensors, which should be addressed to increase the network's performance and lifetime. In this work, the framework of compressive sensing is exploited, which allows accurate recovery of signals being sparse in some basis using only a small number of random incoherent projections. In particular, a recently introduced Bayesian Matching Pursuit method is modified in a decentralized way to reconstruct a multi-signal ensemble acquired by the nodes of a wireless sensor network, by exploiting a joint sparsity structure among the signals of the ensemble. The proposed approach requires a minimal amount of data transmissions among the sensors and a central node thus preserving the sensors' limited resources. At the same time, it achieves a reconstruction performance comparable to other distributed compressive sensing methods, which require the communication of a whole set of measurements to the central node.\n\n## Task\nCompose an abstract for the title 'Physical-layer intrusion detection for wireless networks using compressed sensing' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The broadcast nature of wireless networks has been widely exploited by adversaries in order to cause severe denial-of-service attacks. Several algorithms are proposed in the literature for the detection and mitigation of such attacks at the physical and medium access layers. In this work, we combine recent advances in compressed sensing theory, along with a cumulative-sum anomaly-based algorithm, for the detection of physical-layer attacks. The algorithm considers a metric based on the Signal-to-Interference-plus-Noise-Ratio (SINR). Compressed sensing makes feasible the use of far fewer SINR measurements for effective intrusion detection. The performance evaluation based on real experimental data shows that attacks are detected with high accuracy using a small number of measurements."
      }
    ]
  },
  {
    "tid":"159",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: CoJava: A Unified Language for Simulation and Optimization\nContent: We have proposed and implemented, and will demonstrate, the language CoJava, which offers both the advantages of simulation-like process modeling, and the capabilities of true decision optimization.By design, the syntax of CoJava is identical to the programming language Java, extended with special constructs to (1) make a non-deterministic choice of a numeric value, (2) assert a constraint, and (3) designate a program variable as the objective to be optmized.A CoJava program thus defines a set of nondeterministic execution paths, each being a program run with specific selection of values in the choice statements. The semantics of CoJava interprets a program as an optimal nondeterministic execution path, namely, a path that (1) satisfies the range conditions in the choice statements, (2) satisfies the assert-constraint statements, and (3) produces the optimal value in a designated program variable, among all execution paths that satisfy (1) and (2). Thus, to run a CoJava program amounts to first finding an optimal execution path, and then procedurally executing it.To find an optimal non-deterministic execution path, we have developed a reduction to a standard constraint optimization formulation. Constraint variables represent values in program variables that can be created at any state of a non-deterministic execution. Constraints encode transitions, triggered by CoJava statements, from one program state to the next.Based on the reduction, we have developed a CoJava constraint compiler. The compiler operates by first translating the Java program into a similar Java program in which the primitive numeric operators and data types are replaced by symbolic constraint operators and data types. This intermediate java program functions as a constraint generator. This program is compiled and executed to produce a symbolic decision problem. The decision problem is then submitted to an external optimization solver.\n\nTitle: An Optimal Regression Algorithm for Piecewise Functions Expressed as Object-Oriented Programs\nContent: Core Java is a framework which extends the programming language Java with built-in regression analysis, i.e., the capability to do parameter estimation for a function. Core Java is unique in that functional forms for regression analysis are expressed as first-class citizens, i.e., as Java programs, in which some parameters are not a priori known, but need to be learned from training sets provided as input. Typical applications of Core Java include calibration of parameters of computational processes, described as OO programs. If-then-else statements of Java language are naturally adopted to create piecewise functional forms of regression. Thus, minimization of the sum of least squared errors involves an optimization problem with a search space that is exponential to the size of learning set. In this paper, we propose a combinatorial restructuring algorithm which guarantees learning optimality and furthermore reduces the search space to be polynomial in the size of learning set, but exponential to the number of piece-wise bounds.\n\nTitle: CoJava: optimization modeling by nondeterministic simulation\nContent: We have proposed and implemented the language CoJava, which offers both the advantages of simulation-like process modeling in Java, and the capabilities of true decision optimization. By design, the syntax of CoJava is identical to the programming language Java, extended with special constructs to (1) make a nondeterministic choice of a numeric value, (2) assert a constraint, and (3) designate a program variable as the objective to be optimized. A sequence of specific selections in nondeterministic choice statements corresponds to an execution path. We define an optimal execution path as one that (1) satisfies the range conditions in the choice statements, (2) satisfies the assert-constraint statements, and (3) produces the optimal value in a designated program variable, among all execution paths that satisfy (1) and (2). The semantics of a CoJava program amounts to first finding an optimal execution path, and then procedurally executing it. To find an optimal execution path, the implemented CoJava compiler reduces the problem to a standard optimization formulation, and then solves it on an external solver. Then, the CoJava program is run as a Java program, where the choice statements select the found optimal values, and the assert and optimization statements are ignored. We illustrate the usage and semantics of CoJava using a simple supply-chain example, in which elastic demand, a manufacturer and a supplier are modeled as Java classes.\n\nTitle: COD: Iterative Utility Elicitation for Diversified Composite Recommendations\nContent: This paper studies and proposes methods for providing recommendations on composite bundles of products and services that are dynamically defined using database views extended with decision optimization based on mathematical programming. A framework is proposed for finding a diverse recommendation set when no prior knowledge on user preference is given. To support this framework, a method is developed for utility function elicitation, which is based on iteratively refining a set of axes in the n-dimensional utility space. The notion of a diverse recommendation set is refined and formalized by partitioning the recommendation space into layers that correspond to their distance to the maximal utility. In each layer, the method selects recommendations that maximize each dimension of the utility space. A preliminary experimental study is conducted, which shows that the proposed framework significantly outperforms a popular commercial system in terms of precision and recall.\n\nTitle: Reusing Relational Queries for Intuitive Decision Optimization\nContent: Decision optimization is used in many applications such as those for finding the best course of action in emergencies. However, optimization solutions require considerable mathematical expertise and effort to generate effective models. On the other hand, reporting applications over databases are more intuitive and have long been established using the mature database query technology. A decision optimization problem can be viewed as an \"inverse\" of the reporting problem. For example, a report may tell the total cost of a certain supply chain given the various sourcing and transportation options used; the corresponding optimization problem can be to select among all possible sourcing and transportation options to minimize the total cost. Reusing existing reporting queries for decision optimization will achieve the dual goals of taking advantage of past investments and of making decision optimization more intuitive. To realize these goals, this paper addresses two related technical issues with a decision guidance query language (DGQL) framework. The first is to annotate existing queries to precisely express the optimization semantics, and the second is to translate the annotated queries into equivalent mathematical programming (MP) formulation that can be solved efficiently. This paper presents the decision queries with examples, provides formal syntax and semantics to DGQL, describes an implementation method through a reduction to MP formulation. Finally, the paper illustrates via experiments on a prototype system that the optimization tasks done with DGQL compete squarely with expertly generated MP models.\n\n## Task\nCompose an abstract for the title 'CoReJava: Learning Functions Expressed as Object-Oriented Programs' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Proposed and implemented is the language CoReJava (constraint optimization regression in Java), which extends the programming language Java with regression analysis, i.e., the capability to do parameter estimation for a function. CoReJava is unique in that functional forms for regression analysis are expressed as first-class citizens, i.e., as Java programs, in which some parameters are not a priori known, but need to be learned from training sets provided as input. Typical applications of CoReJava include calibration of parameters of computational processes, described as OO programs. To implement regression learning, the CoReJava compiler (1) analyses the structure of the parameterized Java program that represent a functional form, (2) automatically generates a constraint optimization problem, in which constraint variables are the unknown parameters, and the objective function to be minimized is the sum of squares of errors w.r.t. the training set, and (3) solves the optimization problem using an external non-linear optimization solver. CoReJava then executes as a regular Java program, in which the initially unknown parameters are replaced with the found optimal values. CoReJava syntax and semantics are formally defined and exemplified using a simple supply chain example."
      }
    ]
  },
  {
    "tid":"160",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Ubiquitous laboratory: a research support environment for ubiquitous learning based on sensor networks\nContent: With the great progress of technologies, computers are embedded into everywhere to make our daily life convenient, efficient and comfortable. In the learning field, ubiquitous technologies make it possible to provide services to learners anytime and anywhere in the real world. In this paper, we present an ubiquitous environment to support college students, professors, and visitors in a laboratory to encourage positive research activities. Actions of people in the Ubiquitous Laboratory (U-Lab) are individually detected by sensor networks and analyzed to provide supports. Based on these collected information, U-Lab provides services such as to grasp a precise research progress and share research information among students and professors. We also propose a method to advise students to improve their research activities. As a case study, we implemented a Ubiquitous Corner (U-Corner) to prove our proposal is useful and practical.\n\nTitle: CAMF: Context-Aware Message Forwarding in Mobile Social Networks\nContent: AbstractIn mobile social networks (MSN), with the aim of conserving limited resources, egotistic nodes might refuse to forward messages for other nodes. Different from previous work which mainly focuses on promoting cooperation between selfish nodes, we consider it from a more pragmatic perspective in this paper. Be specific, we regard selfishness as a native attribute of a system and allow nodes to exhibit selfish behavior in the process of message forwarding. Apparently, selfishness has a profound influence on routing efficiency, and thus novel mechanisms are necessary to improve routing performance when self-centered nodes are considered. We first put forward a stateless approach to measure encounter opportunities between nodes, and represent forwarding capabilities of nodes by combining the acquired encounter opportunities with node selfishness. We then quantify receiving capabilities of nodes based on their available buffer size and energy. Taking both forwarding and receiving capabilities into account, we finally present a forwarding set mechanism, which could be deduced to a multiple knapsack problem to maximize the forwarding profit. Consequently, we take all the above studies into the design of a context-aware message forwarding algorithm (CAMF). Extensive trace-driven simulations show that CAMF outperforms other existing algorithms greatly. In fact, it achieves a surprisingly high routing performance while consumes low transmission cost and resource in MSN.\n\nTitle: Dynamic Itinerary Planning for Mobile Agents with a Content-Specific Approach in Wireless Sensor Networks\nContent: We study data fusion in sensor networks using mobile agents (MAs), which are capable of saving energy of sensor nodes and performing advanced computation functions based on the requests of various applications. Research on MAs still remains unfledged in development of application-oriented data fusion, which is highly desired in wireless sensor networks (WSNs) deployed in recent days for environmental and disaster monitoring. In this paper, we propose a dynamic itinerary planning for MAs (DIPMA) to collect data from sensor networks with an application-oriented approach. In particular, the DIPMA algorithm is applied to the data collection for frost prediction which is a real-world application in agriculture using next-generation sensor networks. The performance of the DIPMA is evaluated by simulations and the experimental results show that the total execution time of MA can be reduced significantly with our approach while sound prediction accuracy is maintained.\n\nTitle: An Improved Design of the Ubiquitous Learning System Based on Sensor Networks\nContent: With the great progress of information technologies, computers are embedded into everywhere to make our daily life convenient, efficient and invisible. In order to utilize this progress for learning, a scheduler system, a Ubiquitous Learning System (ULS) was proposed. ULS contributes a lot to improve studentpsilas learning efficiency. Yet improvement can still be made to the system by using sensor networks. In our new design, we embed various kinds of sensors, like temperature sensor, oxygen sensor, light sensor, into the environment. Based on the information collected by such sensors, decisions can be made by the control center to adjust the environment automatically for learner. Our research result reveals that the improved ULS not only benefited learners to acquire their learning habits but also improve their learning efficiency.\n\nTitle: Improved Resource Allocation Algorithms For Practical Image Encoding In A Ubiquitous Computing Environment\nContent: As a case study of the ubiquitous computing system, we have implemented a prototype for the JPEG encoding application. In order to achieve this eventual development in the real world, we studied resource allocation policies that can improve the overall performance of the system. In this paper, we consider those static and dynamic allocation approaches and then propose four different allocation algorithms. In particular, we extensively studied the dynamic allocation algorithms by exploring various cache policies which include disabled cache, unrestricted cache and restricted cache. Performance of these algorithms in large scale application scenario is also evaluated based on both the improved prototype and a simulation environment. The experimental results show a significant performance improvement achieved by the new proposed algorithms in terms of load balance, execution time, waiting time and execution efficiency.\n\n## Task\nCompose an abstract for the title 'Towards Context-Aware Workflow Management for Ubiquitous Computing' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Ubiquitous computing is a user-centric distributed computing paradigm, allowing users to access to their preferred services even while moving around. To make such a vision a reality, context-aware workflow management is one of key issues because the context of ubiquitous applications is highly varying. In this paper, we propose a context model for intelligent campus navigation applications, then present a context-adaptive workflow management algorithm which can dynamically adjust workflow execution policies in terms of current context information. Moreover, we model the workflow management algorithm in Petri nets. Our context model and workflow management algorithm may be easily extended to other ubiquitous applications with a little change on context structure."
      }
    ]
  }
]