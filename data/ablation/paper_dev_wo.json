[
  {
    "tid":"0",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'PIST: An Efficient and Practical Indexing Technique for Historical Spatio-Temporal Point Data' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Despite pressing need, current relational database management systems (RDBMS) support for spatio-temporal data is limited and inadequate, and most existing spatio-temporal indices cannot be readily integrated into existing RDBMSs. This paper proposes a practical index for spatio-temporal (PIST) data, an indexing technique, rather than a new indexing structure, for historical spatio-temporal data points that can be fully integrated within existing RDBMSs. PIST separates the spatial and temporal components of the data. For the spatial component, we develop a formal cost model and a partitioning strategy that leads to an optimal space partitioning for uniformly distributed data and an efficient heuristic partitioning for arbitrary data distributions. For the temporal component of the data a B\u9a74+\u9a74-tree is used. We show that this layer's performance can be maximized if an optimal maximal temporal range is enforced, and we present a procedure to determine such an optimal value. Being fully mapped onto a RDBMS, desirable and important properties, such as concurrency control, are immediately inherited by PIST. Using ORACLE as our implementation platform we perform extensive experiments with both real and synthetic datasets comparing its performance against other RDBMS-based options, as well as the MV3R-tree. PIST outperforms the former by at least one order of magnitude, and is competitive or better with respect to the latter, with the unarguable advantage that it can readily used on top of virtually any existing RDBMS."
      }
    ]
  },
  {
    "tid":"1",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Ensemble of Anchor Adapters for Transfer Learning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In the past decade, there have been a large number of transfer learning algorithms proposed for various real-world applications. However, most of them are vulnerable to \\emph{negative transfer}\\footnote{Here we say the ``negative transfer\" occurs when the accuracy from transfer learning algorithm is lower than the one of supervised learning algorithm, i.e., Logistic Regression (LR) in this paper.} when their performances are even worse than traditional supervised models. Aiming at more robust transfer learning models, we propose an ENsemble framework of \\emph{anCHOR adapters} (ENCHOR for short), in which an \\emph{anchor adapter} adapts the features of instances (from both source and target domains) based on their similarities to a specific \\emph{anchor} (an instance selected from source or target domains). Specifically, the more similar to the anchor instance, the higher degree of the original feature of an instance remains unchanged in the adapted representation, and vice versa. This adapted representation for the data actually expresses the local structure around the corresponding anchor, and then any transfer learning method can be applied to this adapted representation for a prediction model, which focuses more on the neighborhood of the anchor. Next, based on multiple anchors, multiple anchor adapters can be built and combined into an ensemble for final output. Additionally, we develop an effective measure to select the anchors for ensemble building to achieve further performance improvement. Extensive experiments on hundreds of text classification tasks are conducted to demonstrate the effectiveness of ENCHOR. The results show that: when traditional supervised models perform poorly, ENCHOR (based on only 8 selected anchors) achieves $6\\%-13\\%$ increase in terms of average accuracy compared with the state-of-the-art methods, and it greatly alleviates negative transfer."
      }
    ]
  },
  {
    "tid":"2",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimal set of video representations in adaptive streaming' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Adaptive streaming addresses the increasing and heterogenous demand of multimedia content over the Internet by offering several streams for each video. Each stream has a different resolution and bit rate, aimed at a specific set of users, e.g., TV, mobile phone. While most existing works on adaptive streaming deal with optimal playout-control strategies at the client side, in this paper we concentrate on the providers' side, showing how to improve user satisfaction by optimizing the encoding parameters. We formulate an integer linear program that maximizes users' average satisfaction, taking into account the network characteristics, the type of video content, and the user population. The solution of the optimization is a set of encoding parameters that outperforms commonly used vendor recommendations, in terms of user satisfaction and total delivery cost. Results show that video content information as well as network constraints and users' statistics play a crucial role in selecting proper encoding parameters to provide fairness among users and reduce network usage. By combining patterns common to several representative cases, we propose a few practical guidelines that can be used to choose the encoding parameters based on the user base characteristics, the network capacity and the type of video content."
      }
    ]
  },
  {
    "tid":"3",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Live-structure dataflow analysis for Prolog' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"For the class of applicative programming languages, efficient methods for reclaiming the memory occupied by released data structures constitute an important aspect of current implementations. The present article addresses the problem of memory reuse for logic programs through program analysis rather than by run-time garbage collection. The aim is to derive run-time properties that can be used at compile time to specialize the target code for a program according to a given set of queries and to automatically introduce destructive assignments in a safe and transparent way so that fewer garbage cells are created.The dataflow analysis is constructed as an application of abstract interpretation for logic programs. An abstract domain for describing structure-sharing and liveness properties is developed as are primitive operations that guarantee a sound and terminating global analysis. We explain our motivation for the design of the abstract domain, make explicit the underlying implementation assumptions, and discuss the precision of the results obtained by a prototype analyzer."
      }
    ]
  },
  {
    "tid":"4",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Feature Generation by Simple-FLDA for Pattern Recognition' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a new feature generation method for pattern recognition is proposed, which is approximately derived from geometrical interpretation of the Fisher linear discriminant analysis (FLDA). In a field of pattern recognition or signal processing, the principal component analysis (PCA) is popular for data compression and feature extraction. Furthermore, iterative learning algorithms for obtaining eigenvectors in PCA have been presented in such fields, including neural networks. Their effectiveness has been demonstrated in many applications. However, recently the FLDA has been used in many fields, especially face image analysis. The drawback of FLDA is a long computational time based on a large-sized covariance matrix and the issue that the within-class covariance matrix is usually singular. Generally FLDA has to carry out minimization of a within-class variance. However in this case the inverse matrix of the within-class covariance matrix cannot be obtained, since data dimension is generally higher than the number of data and then it includes many zero eigenvalues. In order to overcome this difficulty, a new iterative feature generation method, a simple FLDA is introduced and its effectiveness is demonstrated for pattern recognition problems."
      }
    ]
  },
  {
    "tid":"5",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Variability in software architecture: the road ahead' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Variability in a software system is reflected in and facilitated through the architecture of that system. The Third International Workshop on Variability in Software Architecture (VARSA) was held in conjunction with the 11th Working IEEE/IFIP Conference on Software Architecture 2014 in Sydney, Australia. Based on the findings from previous editions of VARSA, this edition aimed at exploring methods, technologies and tools to handle variability at the software architecture level. The workshop featured one industrial keynote talk, one academic keynote talk and five research paper presentations."
      }
    ]
  },
  {
    "tid":"6",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Leveraging MPI's one-sided communication interface for shared-memory programming' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Hybrid parallel programming with MPI for internode communication in conjunction with a shared-memory programming model to manage intranode parallelism has become a dominant approach to scalable parallel programming. While this model provides a great deal of flexibility and performance potential, it saddles programmers with the complexity of utilizing two parallel programming systems in the same application. We introduce an MPI-integrated shared-memory programming model that is incorporated into MPI through a small extension to the one-sided communication interface. We discuss the integration of this interface with the upcoming MPI 3.0 one-sided semantics and describe solutions for providing portable and efficient data sharing, atomic operations, and memory consistency. We describe an implementation of the new interface in the MPICH2 and Open MPI implementations and demonstrate an average performance improvement of 40% to the communication component of a five-point stencil solver."
      }
    ]
  },
  {
    "tid":"7",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Novel Low Power Time-Mode Comparator For Successive Approximation Register Adc' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A novel low power time-mode comparator with enhanced resolution and speed is proposed in this paper. The comparator incorporates a symmetrical input time-to-digital converter (TDC) and a highly dynamic voltage-to-time converter (VTC). Energy reduction is achieved mainly through the use of capacitor discharge automatic switch-off and inverter clocking. The combined effect of the low timing requirement and capacitor voltage presetting enables significant precision and speed improvements. Simulations in a 0.18 um process show that the comparator can be clocked at 38 MHz, draws less than 0.4 pJ energy from supply and can resolve voltages as low as 10 mu V."
      }
    ]
  },
  {
    "tid":"8",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Open Proctor: An Academic Integrity Tool for the Open Learning Environment.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Managing integrity of continuous and authentic assessments in an open and distance environments is a complex process. The core challenge is to map student identities with their academic work in effective and efficient manners, while preserving privacy, ensuring minimal disruption, minimizing impacts on accessibility and convenience. To address this issue, we have developed a prototype cloud-based application entitled OpenProctor that employs machine-learning techniques to analyze patterns in the learner-produced textual content in order to provide identity and authorship assurance. In contrast to the traditional academic integrity approaches that seek to control the remote learning environment, our method takes advantage of the readily available learner-generated data to analyze how the students go about doing their academic work. We present the framework, the architecture and main functions of the OpenProctor system and discuss the future research directions and plans for the system's future development."
      }
    ]
  },
  {
    "tid":"9",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Extraction of prostatic lumina and automated recognition for prostatic calculus image using PCA-SVM.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Identification of prostatic calculi is an important basis for determining the tissue origin. Computation-assistant diagnosis of prostatic calculi may have promising potential but is currently still less studied. We studied the extraction of prostatic lumina and automated recognition for calculus images. Extraction of lumina from prostate histology images was based on local entropy and Otsu threshold recognition using PCA-SVM and based on the texture features of prostatic calculus. The SVM classifier showed an average time 0.1432 second, an average training accuracy of 100%, an average test accuracy of 93.12%, a sensitivity of 87.74%, and a specificity of 94.82%. We concluded that the algorithm, based on texture features and PCA-SVM, can recognize the concentric structure and visualized features easily. Therefore, this method is effective for the automated recognition of prostatic calculi."
      }
    ]
  },
  {
    "tid":"10",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Atlas-Guided Multi-channel Forest Learning for Human Brain Labeling.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Labeling MR brain images into anatomically meaningful regions is important in quantitative brain researches. Previous works can be roughly categorized into two classes: multi-atlas and learning based labeling methods. These methods all suffer from their own limitations. For multi-atlas based methods, the label fusion step is often handcrafted based on the predefined similarity metrics between voxels in the target and atlas images. For learning based methods, the spatial correspondence information encoded in the atlases is lost since they often use only the target image appearance for classification. In this paper, we propose a novel atlas-guided multi-channel forest learning, which could effectively address the aforementioned limitations. Instead of handcrafting the label fusion step, we learn a non-linear classification forest for automatically fusing both image appearance and label information of the atlas with the image appearance of the target image. Validated on LONI-LBPA40 dataset, our method outperforms several traditional labeling approaches."
      }
    ]
  },
  {
    "tid":"11",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Incorporating Great Deluge with Harmony Search for Global Optimization Problems.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Harmony search (HS) algorithm is relatively a recent metaheuristic optimization method inspired by natural phenomenon of musical improvisation process. despite its success, the main drawback of harmony search are contained in its tendency to converge prematurely due to its greedy selection method. This probably leads the harmony search algorithm to get stuck in local optima and unsought solutions owing to the limited exploration of the search space. The great deluge algorithm is a local search-based approach that has an efficient capability of increasing diversity and avoiding the local optima. This capability comes from its flexible method of accepting the new constructed solution. The aim of this research is to propose and evaluate a new variant of HS. To do so, the acceptance method of the great deluge algorithm is incorporated in the harmony search to enhance its convergence properties by maintaining a higher rate of diversification at the initial stage of the search process. The proposed method is called Harmony Search Great Deluge (HS-GD) algorithm The performance of HS-GD and the classical harmony search algorithm was evaluated using a set of ten benchmark global optimization functions. In addition, five benchmark functions of the former set were employed to compare the results of the proposed method with three previous harmony search variations including the classical harmony search. The results show that HS-GD often outperforms the other comparative approaches."
      }
    ]
  },
  {
    "tid":"12",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Development, Long-Term Operation and Portability of a Real-Environment Speech-Oriented Guidance System' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, the development, long-term operation and portability of a practical ASR application in a real environment is investigated. The target application is a speech-oriented guidance system installed at the local community center. The system has been exposed to ordinary people since November 2002. More than 300 hours or more than 700,000 inputs have been collected during four years. The outcome is a rare example of a large scale real-environment speech database. A simulation experiment is carried out with this database to investigate how the system's performance improves during the first two years of operation. The purpose is to determine empirically the amount of real-environment data which has to be prepared to build a system with reasonable speech recognition performance and response accuracy. Furthermore, the relative importance of developing the main system components, i.e. speech recognizer and the response generation module, is assessed. Although depending on the system's modeling capacities and domain complexity, experimental results show that overall performance stagnates after employing about 10-15 k utterances for training the acoustic model, 40\u201350 k utterances for training the language model and 40 k\u201350 k utterances for compiling the question and answer database. The Q&A database was most important for improving the system's response accuracy. Finally, the portability of the well-trained first system prototype for a different environment, a local subway station, is investigated. Since collection and preparation of large amounts of real data is impractical in general, only one month of data from the new environment is employed for system adaptation. While the speech recognition component of the first prototype has a high degree of portability, the response accuracy is lower than in the first environment. The main reason is a domain difference between the two systems, since they are installed in different environments. This implicates that it is imperative to take the behavior of users under real conditions into account to build a system with high user satisfaction."
      }
    ]
  },
  {
    "tid":"13",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Advances in Computer-Based Autoantibodies Analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Indirect Immunofluorescence (BF) imaging is the recommended method to detect autoantibodies in patient serum, whose common markers are antinuclear autoantibodies (ANA) and autoantibodies directed against double strand DNA (anti-dsDNA). Since the availability of accurately performed and correctly reported laboratory determinations is crucial for the clinicians, an evident medical demand is the development of Computer Aided Diagnosis (CAD) tools supporting physicians' decisions. In this paper we present a comprehensive system that helps in recognising the presence of ANA and anti-dsDNA autoantibodies. The analysis of CAD performance shows its potential in lowering the method variability, in increasing the level of standardization and in serving as a second reader reducing the physicians' workload. The system has been successfully tested on annotated datasets."
      }
    ]
  },
  {
    "tid":"14",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Alternating Optimization Approach For Phase Retrieval' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we address the problem of phase retrieval to recover a signal from the magnitude of its Fourier transform. In many applications of phase retrieval, the signals encountered are naturally sparse. In this work, we consider the case where the signal is sparse under the assumption that few components are nonzero. We exploit further the sparse nature of the signals and propose a two stage sparse phase retrieval algorithm. A simple iterative minimization algorithm recovers a sparse signal from measurements of its Fourier transform (or other linear transform) magnitude based on the minimization of a block l(1) norm. We show in the experiments that the proposed algorithm achieves a competitive performance. It is robust to noise and scalable in practical implementation. The proposed method converges to a more accurate and stable solution than other existing techniques for synthetic signals. For speech signals, experiments show that the voice quality of reconstructed speech signals is almost as good as the original signals."
      }
    ]
  },
  {
    "tid":"15",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An integrated geometrical and stochastic approach for accurate infant brain extraction' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a novel approach for extracting the brain from 3D T1-weighted MR images. The proposed approach combines a stochastic two-level Markov-Gibbs random field (MGRF) image model with a geometric model that parcels the brain into a set of nested iso-surfaces using a fast marching level setmethod. The classification of each brain voxel found on the iso-surfaces is performed based on the first-order (a linear combination of discrete gaussian (LCDG) model) and second-order (an MGRF model with analytically estimated parameters) visual appearance features of the brain structures. Our approach is tested on 280 infant 3D MR brain scans and evaluated on 9 data sets using the Dice coefficient, the 95-percentile modified Hausdorff distance, and absolute brain volume difference. Experimental results showed that the fusion of the stochastic and geometric models of brain MRI data has led to more accurate brain extraction, when compared with other widely-used brain extraction tools, such as BET, BET2, and brain surface extractor (BSE)."
      }
    ]
  },
  {
    "tid":"16",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Using Continuous Queries for Event Filtering and Routing in Sparse MANETs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In our project, we design middleware services for emergency and rescue scenarios in sparse Mobile Ad- Hoc Networks (SMANETs). One of these services is a distributed event notification service (DENS) for asynchronous communication. The DENS architecture allows several subscription languages for different kinds of subscriptions. To support complex subscriptions, we use a Data Stream Management System (DSMS) for filtering of the data streams and matching of filtered events and subscriptions. We have designed a simple rescue scenario to investigate the possibilities of using a DSMS together with the DENS middleware layer. In this paper we discuss the design of our implementation and first experimental results of our ongoing work."
      }
    ]
  },
  {
    "tid":"17",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'ML-based sensor network localization and tracking: Batch and time-recursive approaches' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Simultaneous localization and tracking (SLAT) in sensor networks aims to determine the positions of sensor nodes and a moving target in a network, given incomplete and inaccurate range measurements. One of the established methods for achieving this goal is to maximize a likelihood function (ML), which requires initialization with an ap- proximate solution to avoid convergence towards local ex- trema. In this paper a Euclidean Distance Matrix (EDM) completion problem is solved to obtain initial sensor/target positions. The likelihood function is then iteratively opti- mized through either a Majorization-Minimization (MM) or Newton method. To reduce the computational load, an in- cremental scheme is proposed whereby each new target po- sition is estimated from range measurements, providing ad- ditional initialization for ML without the need for solving an expanded EDM completion problem. The performance of these methods is assessed through simulation."
      }
    ]
  },
  {
    "tid":"18",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Cluster-PSO Based Resource Orchestration for Multi-task Applications in Vehicular Cloud.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Vehicular cloud computing (VCC) provides a vehicular user attaching several resources with different types at the same time. Additionally, the vehicular applications especially for big data processing are always complicated and may be decomposed into several fine-grained tasks. When offloading the complicated multi-task application to the vehicular clouds, the task executes individually in terms of its own computation, storage and bandwidth requirement. Different from the task offloading in mobile cloud computing which aims to optimize the energy consumption, the important metric for vehicular users is the application delay. Moreover, the moving vehicles always have the similar resource properties and may form the solution clusters when finding the resource orchestration policy, which brings an opportunity of improving resource orchestration performance. In this paper, we formulate the VCC resource orchestration as an optimization problem, and propose a cluster-particle swarm optimization (PSO) algorithm to obtain the resource orchestration policy. A fast cluster algorithm is used to divide the solution space and generate sub-swarms for better exploring the orchestration solutions. The experiment results show that the cluster-PSO algorithm can achieve a higher resource orchestration accuracy in an acceptable time comparing to the other PSO algorithms. Especially, when there are more tasks in an application and the vehicle has more optional VCC resources, the performance of the cluster-PSO based resource orchestration is outstanding."
      }
    ]
  },
  {
    "tid":"19",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Remote Sensing Imagery for Soil Characterization: a Wavelet Neural Data Fusion Approach' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Technological advances in remote sensing imagery allows to obtain high resolution images, helpful in soil characterizing, monitoring and predicting natural hazards. On the other hand, the different kind of in-service sensors allows inferences on a large frequency band. In spite of this, due to the increasing requirements of industrial and civil entities, the academic research is actually involved in improving the quality of imageries. The aim is to implement automatic tools able to work in real-time applications, above all in order to solve pattern identification problem in remote sensing. Within this framework, our work proposes a data fusion methodology, based on the Multiscale Kalman Filter, in order to improve soil characterization in Ikonos surveys."
      }
    ]
  },
  {
    "tid":"20",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Survey on Intelligent Wheelchair Prototypes and Simulators.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Nowadays more than 700 million persons around the world have some kind of disability or handicap. During the last decades the elderly population in most of the European countries and across all the most civilized countries is also growing at an increasing pace. This phenomenon is receiving increasing attention from the scientific community, during the last years, and several solutions are being proposed in order to allow a more independent life to the people belonging to those groups. In this context Intelligent Wheelchairs (IW) are instruments that are a natural development of the scientific work that has been conducted to improve the traditional Wheelchair characteristics using health informatics, assistive robotics and human computer interface technologies. Some of the most important features of the IW are their navigation capabilities and automatic adaptation of their interface to the user. This paper presents the evolution and state of art concerning IWs prototypes and simulators and intelligent human-computer interfaces in the context of this devices. Our study enabled us to conclude that although several Intelligent Wheelchair prototypes are being developed in a large number of research projects, around the world, the adaptation of their user interface to the patient is an often neglected research topic. Thus, projects aiming at developing new concepts of Intelligent Wheelchairs are needed mainly using multimodal interfaces and wheelchair interfaces adapted to the user characteristics."
      }
    ]
  },
  {
    "tid":"21",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Discrete Continuity of Information System, Knowledge System, and e-Business System' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Since information systems are pervasive in the busi-ness and non-business areas, the issue of extending researches on information systems to knowledge sys-tems and e-business systems is one of the most profit-able topics of researches. We propose a historical, discontinuous changes introducing ambiguity in explaining and interpreting innovative nature of three paradigms of systems: information systems, knowledge systems, and e-business systems. Resorting to the historical perspective in developing ideas into meaningful themes, we proposed a discrete continuity in interpreting changes of paradigms of sys-tems. Discrete continuity may be explained by ambigu-ously-shared meaningful perspectives applied to differ-ent paradigms of systems and interpretive elements of each system. The discrete continuity has been adopted to make ambiguity utilized may have instrumental contribution in researches. The engrafted ambiguity in systems design, development, and use could have enduring instrumental value in interpreting the types or variants of systems in each paradigm of systems."
      }
    ]
  },
  {
    "tid":"22",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Compression Performance Of High Efficiency Video Coding (Hevc) Working Draft 4' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents the results of compression comparison tests between the current state of the emerging High Efficiency Video Coding (HEVC) draft standard and the current dominant standard H.264/MPEG-4 AVC (High Profile) as an anchor reference. The conditions used for the comparison tests were designed to reflect relevant application scenarios and to enable a fair comparison to the maximum extent feasible, i.e. using comparable quantization settings, reference frame buffering, etc. The testing was generally configured in favour of using a relatively strong H.264/MPEG-4 AVC anchor reference. Several of the encoder optimizations currently found in the HEVC software are tested and shown to be helpful to improve the H.264/MPEG-4 AVC anchor performance. When compared to the improved anchor encoder configurations, the HEVC draft design currently provides a bit rate savings for equal PSNR of about 39% for random access applications, 44% for low-delay use, and 25% for all-intra use."
      }
    ]
  },
  {
    "tid":"23",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Are your sites down? Requirements-driven self-tuning for the survivability of Web systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Running in a highly uncertain and greatly complex environment, Web systems cannot always provide full set of services with optimal quality, especially when work loads are high or subsystem failures are frequent. Hence, it is significant to continuously maintain a high satisfaction level of survivability, hereafter survivability assurance, while relaxing or sacrificing certain quality or functional requirements that are not crucial to the survival of the entire system. After giving a value-based interpretation to survivability assurance to facilitate a quantitative analysis, we propose a requirements-driven self-tuning method for the survivability assurance of Web systems. Maintaining an enriched and live goal model, our method adapts to runtime tradeoff decisions made by our PID (proportional-integral-derivative) controller and goal-oriented reasoner for both quality and functional requirements. The goal-based configuration plans produced by the reasoner is carried out on the live goal model, and then mapped into system architectural configurations. Experiments on an online shopping system are conducted to validate the effectiveness of the proposed method."
      }
    ]
  },
  {
    "tid":"24",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'CDMA-based MAC protocol for wireless ad hoc networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose a CDMA-based power controlled medium access protocol for mobile ad hoc networks (MANETs). Unlike previously proposed protocols, ours accounts for the multiple access interference (MAI), thereby addressing the notorious near-far problem that undermines the throughput performance in MANETs. Channel-gain information obtained from overheard RTS and CTS packets over an out-of-band control channel is used to dynamically bound the transmission power of mobile terminals in the vicinity of a receiver. By properly estimating the required transmission power for data packets, the proposed protocol allows for interference-limited simultaneous transmissions to take place in the neighborhood of a receiving terminal. Simulation results indicate that compared to the IEEE 802.11 approach, the proposed protocol achieves a significant increase in network throughput at no additional cost in energy consumption."
      }
    ]
  },
  {
    "tid":"25",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Double enhancement learning for explicit internal representations: unifying self-enhancement and information enhancement to incorporate information on input variables' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a new type of information-theoretic method called \"double enhancement learning,\" in which two types of enhancement, namely, self-enhancement and information enhancement, are unified. Self-enhancement learning has been developed to create targets spontaneously within a network, and its performance has proven to be comparable with that of conventional competitive learning and self-organizing maps. To improve the performance of the self-enhancement learning, we try to include information on input variables in the framework of self-enhancement learning. The information on input variables is computed by information enhancement in which a specific input variable is used to enhance competitive unit outputs. This information is again used to train a network with the self-enhancement learning. We applied the method to three problems, namely, an artificial data, a student survey and the voting attitude problem. In all three problems, quantization errors were significantly decreased with the double enhancement learning. The topographic errors were relatively higher, but the smallest number of topographic errors was also obtained by the double enhancement learning. In addition, we saw that U-matrices for all problems showed explicit boundaries reflecting the importance of input variables."
      }
    ]
  },
  {
    "tid":"26",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An efficient implementation of GPU virtualization in high performance clusters' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Current high performance clusters are equipped with high bandwidth/low latency networks, lots of processors and nodes, very fast storage systems, etc. However, due to economical and/or power related constraints, in general it is not feasible to provide an accelerating coprocessor -such as a graphics processor (GPU)- per node. To overcome this, in this paper we present a GPU virtualization middleware, which makes remote CUDA-compatible GPUs available to all the cluster nodes. The software is implemented on top of the sockets application programming interface, ensuring portability over commodity networks, but it can also be easily adapted to high performance networks."
      }
    ]
  },
  {
    "tid":"27",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Background modeling by exploring multi-scale fusion of texture and intensity in complex scenes' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Background modeling is a fundamental yet challenging issue in video surveillance. Traditional methods usually adopt single feature type to solve the problem, while the performance is usually unsatisfactory when handling complex scenes. In this paper, we propose a multi-scale framework, which combines both texture and intensity feature, to achieve a robust and accurate solution. Our contributions are three folds: first, we provide a multi-scale analysis for the issue; second, for texture feature we propose a novel texture operator named Scale-invariant Center-symmetric Local Ternary Pattern, and a corresponding Pattern Adaptive Kernel Density Estimation technique for its probability estimation; third, we design a Simplified Gaussian Mixture Models for intensity feature. Our method is tested on several complex real world videos with illumination variation, soft shadows and dynamic backgrounds. The experimental results clearly demonstrate that our method is superior to the previous methods."
      }
    ]
  },
  {
    "tid":"28",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Trust management towards service-oriented applications' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In service-oriented computing (SOC) environments, service clients interact with service providers for services or transactions.\n From the point view of service clients, the trust status of a service provider is a critical issue to consider, particularly\n when the service provider is unknown to them. Typically, the trust evaluation is based on the feedback on the service quality\n provided by service clients. In this paper, we first present a trust management framework that is event-driven and rule-based.\n In this framework, trust computation is based on formulae. But rules are defined to determine which formula to use and what\n arguments to use, according to the event occurred during the transaction or service. In addition, we propose some trust evaluation\n metrics and a formula for trust computation. The formula is designed to be adaptable to different application domains by setting\n suitable arguments. Particularly, the proposed model addresses the incremental characteristics of trust establishment process.\n Furthermore, we propose a fuzzy logic based approach for determining reputation ranks that particularly differentiates new\n service providers and old (long-existing) ones. This is further incentive to new service providers and penalize poor quality\n services from service providers. Finally, a set of empirical studies has been conducted to study the properties of the proposed\n approaches, and the method to control the trust changes in both trust increment and decrement cases. The proposed framework\n is adaptable for different domains and complex trust evaluation systems."
      }
    ]
  },
  {
    "tid":"29",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'SKIPFLOW: Incorporating Neural Coherence Features for End-to-End Automatic Text Scoring' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Deep learning has demonstrated tremendous potential for Automatic Text Scoring (ATS) tasks. In this paper, we describe a new neural architecture that enhances vanilla neural network models with auxiliary neural coherence features. Our new method proposes a new SKIPFLOW mechanism that models relationships between snapshots of the hidden representations of a long short-term memory (LSTM) network as it reads. Subsequently, the semantic relationships between multiple snapshots are used as auxiliary features for prediction. This has two main benefits. Firstly, essays are typically long sequences and therefore the memorization capability of the LSTM network may be insufficient. Implicit access to multiple snapshots can alleviate this problem by acting as a protection against vanishing gradients. The parameters of the SKIPFLOW mechanism also acts as an auxiliary memory. Secondly, modeling relationships between multiple positions allows our model to learn features that represent and approximate textual coherence. In our model, we call this neural coherence features. Overall, we present a unified deep learning architecture that generates neural coherence features as it reads in an end-to-end fashion. Our approach demonstrates state-of-the-art performance on the benchmark ASAP dataset, outperforming not only feature engineering baselines but also other deep learning models."
      }
    ]
  },
  {
    "tid":"30",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Comparison of eigenvector methods with classical and model-based methods in analysis of internal carotid arterial Doppler signals' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Doppler ultrasound is known as a reliable technique, which demonstrates the flow characteristics and resistance of arteries in various vascular disease. In this study, internal carotid arterial Doppler signals recorded from 105 subjects were processed by PC-computer using classical, model-based, and eigenvector methods. The classical method (fast Fourier transform), two model-based methods (Burg autoregressive, least-squares modified Yule-Walker autoregressive moving average methods), and three eigenvector methods (Pisarenko, multiple signal classification, and Minimum-Norm methods) were selected for processing internal carotid arterial Doppler signals. Doppler power spectra of internal carotid arterial Doppler signals were obtained using these spectrum analysis techniques. The variations in the shape of the Doppler power spectra were examined in order to obtain medical information. These power spectra were then used to compare the applied methods in terms of their frequency resolution and the effects in determination of stenosis and occlusion in internal carotid arteries."
      }
    ]
  },
  {
    "tid":"31",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Collaborative Filtering Based on User Trends' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recommender systems base their operation on past user ratings over a collection of items, for instance, books, CDs, etc. Collaborative Filtering (CF) is a succesful recommendation technique. User ratings are not expected to be independent, as users follow trends of similar rating behavior. In terms of Text Mining, this is analogous to the formation of higher-level concepts from plain terms. In this paper, we propose a novel CF algorithm which uses Latent Semantic Indexing (LSI) to detect rating trends and performs recommendations according to them. Our results indicate its superiority over existing CF algorithms."
      }
    ]
  },
  {
    "tid":"32",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mammoth: Gearing Hadoop Towards Memory-Intensive MapReduce Applications' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The MapReduce platform has been widely used for large-scale data processing and analysis recently. It works well if the hardware of a cluster is well configured. However, our survey has indicated that common hardware configurations in smalland medium-size enterprises may not be suitable for such tasks. This situation is more challenging for memory-constrained systems, in which the memory is a bottleneck resource compared with the CPU power and thus does not meet the needs of large-scale data processing. The traditional high performance computing (HPC) system is an example of the memory-constrained system according to our survey. In this paper, we have developed Mammoth, a new MapReduce system, which aims to improve MapReduce performance using global memory management. In Mammoth, we design a novel rule-based heuristic to prioritize memory allocation and revocation among execution units (mapper, shuffler, reducer, etc.), to maximize the holistic benefits of the Map/Reduce job when scheduling each memory unit. We have also developed a multi-threaded execution engine, which is based on Hadoop but runs in a single JVM on a node. In the execution engine, we have implemented the algorithm of memory scheduling to realize global memory management, based on which we further developed the techniques such as sequential disk accessing, multi-cache and shuffling from memory, and solved the problem of full garbage collection in the JVM. We have conducted extensive experiments with comparison against the native Hadoop platform. The results show that the Mammoth system can reduce the job execution time by more than 40% in typical cases, without requiring any modifications of the Hadoop programs. When a system is short of memory, Mammoth can improve the performance by up to 5.19 times, as observed for I/O intensive applications, such as PageRank. Given the growing importance of supporting large-scale data processing and analysis and the proven success of the MapReduce platform, the- Mammoth system can have a promising potential and impact."
      }
    ]
  },
  {
    "tid":"33",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Regularized Fitted Q-Iteration: Application to Planning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We consider planning in a Markovian decision problem, i.e., the problem of finding a good policy given access to a generative model of the environment. We propose to use fitted Q-iteration with penalized (or regularized) least-squares regression as the regression subroutine to address the problem of controlling model-complexity. The algorithm is presented in detail for the case when the function space is a reproducing-kernel Hilbert space underlying a user-chosen kernel function. We derive bounds on the quality of the solution and argue that data-dependent penalties can lead to almost optimal performance. A simple example is used to illustrate the benefits of using a penalized procedure."
      }
    ]
  },
  {
    "tid":"34",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Incorporating vertical results into search click models' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In modern search engines, an increasing number of search result pages (SERPs) are federated from multiple specialized search engines (called verticals, such as Image or Video). As an effective approach to interpret users' click-through behavior as feedback information, most click models were designed to reduce the position bias and improve ranking performance of ordinary search results, which have homogeneous appearances. However, when vertical results are combined with ordinary ones, significant differences in presentation may lead to user behavior biases and thus failure of state-of-the-art click models. With the help of a popular commercial search engine in China, we collected a large scale log data set which contains behavior information on both vertical and ordinary results. We also performed eye-tracking analysis to study user's real-world examining behavior. According these analysis, we found that different result appearances may cause different behavior biases both for vertical results (local effect) and for the whole result lists (global effect). These biases include: examine bias for vertical results (especially those with multimedia components), trust bias for result lists with vertical results, and a higher probability of result revisitation for vertical results. Based on these findings, a novel click model considering these biases besides position bias was constructed to describe interaction with SERPs containing verticals. Experimental results show that the new Vertical-aware Click Model (VCM) is better at interpreting user click behavior on federated searches in terms of both log-likelihood and perplexity than existing models."
      }
    ]
  },
  {
    "tid":"35",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The QoS Indicators Analysis of Integrated EUHT Wireless Communication System Based on Urban Rail Transit in High-Speed Scenario.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Nowadays, in urban rail transit systems, train wayside communication system uses Wireless Local Area Network (WLAN) as wireless technologies to achieve safety-related information exchange between trains and wayside equipment. However, according to the high speed mobility of trains and the limitations of frequency band, WLAN is unable to meet the demands of future intracity and intercity rail transit. And although the Time Division-Long Term Evolution (TD-LTE) technology has high performance compared with WLAN, only 20 MHz bandwidth can be used at most. Moreover, in high-speed scenario over 300 km/h, TD-LTE can hardly meet the future requirement as well. The equipment based on Enhanced Ultra High Throughput (EUHT) technology can achieve a better performance in high-speed scenario compared with WLAN and TD-LTE. Furthermore, it allows using the frequency resource flexibly based on 5.8 GHz, such as 20 MHz, 40 MHz, and 80 MHz. In this paper, we set up an EUHT wireless communication system for urban rail transit in high-speed scenario integrated all the traffics of it. An outdoor testing environment in Beijing-Tianjin High-speed Railway is set up to measure the performance of integrated EUHT wireless communication system based on urban rail transit. The communication delay, handoff latency, and throughput of this system are analyzed. Extensive testing results show that the Quality of Service (QoS) of the designed integrated EUHT wireless communication system satisfies the requirements of urban rail transit system in high-speed scenario. Moreover, compared with testing results of TD-LTE which we got before, the maximum handoff latency of safety-critical traffics can be decreased from 225 ms to 150 ms. The performance of throughput-critical traffics can achieve 2-way 2 Mbps CCTV and 1-way 8 Mbps PIS which are much better than 2-way 1 Mbps CCTV and 1-way 2 Mbps PIS in TD-LTE."
      }
    ]
  },
  {
    "tid":"36",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Speed modulation in energy-aware real-time systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a general framework for analyzing and designing embedded systems with energy and timing requirements. A set of realistic assumptions is considered in the model in order to apply the results in practical realtime applications. For example, the processor is assumed to have as a set of discrete operating modes, each characterized by speed, power consumption. The transition delay between modes is considered. To take I/O operations into account, task computation times are modeled with a part that scales with the speed and a part having a fixed duration. Given a set of real-time tasks, the proposed method allows to compute the optimal sequence of voltage/speed changes that approximates the minimum continuous speed which guarantees the feasibility of the system. The analysis is performed both under fixed and dynamic priority assignments."
      }
    ]
  },
  {
    "tid":"37",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Boosted scene categorization approach by adjusting inner structures and outer weights of weak classifiers' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Scene categorization plays an important role in computer vision and image content understanding. It is a multi-class pattern classification problem. Usually, multi-class pattern classification can be completed by using several component classifiers. Each component classifier carries out discrimination of some patterns from the others. Due to the biases of training data, and local optimal of weak classifiers, some weak classifiers may not be well trained. Usually, some component classifiers of a weak classifier may be not act well as the others. This will make the performances of the weak classifier not as good as it should be. In this paper, the inner structures of weak classifiers are adjusted before their outer weights determination. Experimental results on three AdaBoost algorithms show the effectiveness of the proposed approach."
      }
    ]
  },
  {
    "tid":"38",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Analytical characterization of CSI quantization effects on broadband multipath channels.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The knowledge of channel state information (CSI) at transmitter is crucial in precoding techniques to efficiently explore wireless multiuser multi-antenna capabilities namely to mitigate the interference and/or to separate spatial streams. For frequency division duplex systems, the CSI available is a quantized version fed back from the receivers, and the nonlinear effects associated to CSI quantization can lead to significant performance degradation. The knowledge of the statistical characteristics of those nonlinear distortion effects is important for performance evaluation purposes, as well as to design robust precoding techniques."
      }
    ]
  },
  {
    "tid":"39",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Applications Acceleration through Adaptive Hardware Components' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"High Level Synthesis (HLS) provides automatic flows for the generation of hardware accelerators starting from their behavioral description. HLS guarantees results comparable to hand-written design for some applications domains such as Digital Signal Processing. However, it is not yet able to cope with performance requirements when scaling the application complexity. One of the biggest limitation is an execution paradigm still based on the construction of a centralized Finite State Machine (FSM). Parallelism exploitation is thus bound to Instruction Level Parallelism within a single execution flow. This is in contrast to the current trends for hardware architectures and programming languages, which are progressively moving towards execution paradigms dominated other type of parallelisms, such as Task or Thread Level Parallelism. This work proposes a novel adaptive accelerator design, not based on the FSM execution paradigm, which provides support to dynamic parallel execution. Execution is parallel, because no pre-computed scheduled is considered. Operations are directly managed by dedicated lightweight hardware modules, which directly communicate to notify execution completion and to start other dependent operations. Execution is parallel, because several execution flows may run concurrently. The proposed design targets different application domains, from Embedded Systems accelerators to hybrid high-performance architectures."
      }
    ]
  },
  {
    "tid":"40",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Scalable and Sound Low-Rank Tensor Learning.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many real-world data arise naturally as tensors. Equipped with a low rank prior, learning algorithms can benefit from exploiting the rich dependency encoded in a tensor. Despite its prevalence in low-rank matrix learning, trace norm ceases to be tractable in tensors and therefore most existing works resort to matrix unfolding. Although some theoretical guarantees are available, these approaches may lose valuable structure information and are not scalable in general. To address this problem, we propose directly optimizing the tensor trace norm by approximating its dual spectral norm, and we show that the approximation bounds can be efficiently converted to the original problem via the generalized conditional gradient algorithm. The resulting approach is scalable to large datasets, and matches state-of-the-art recovery guarantees. Experimental results on tensor completion and multitask learning confirm the superiority of the proposed method."
      }
    ]
  },
  {
    "tid":"41",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'LPS-annotate: complete annotation of compositionally biased regions in the protein knowledgebase.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Compositional bias (i.e. a skew in the composition of a biological sequence towards a subset of residue types) can occur at a wide variety of scales, from compositional biases of whole genomes, down to short regions in individual protein and gene-DNA sequences that are compositionally biased (CB regions). Such CB regions are made from a subset of residue types that are strewn along the length of the region in an irregular way. Here, we have developed the database server LPS-annotate, for the analysis of such CB regions, and protein disorder in protein sequences. The algorithm defines compositional bias through a thorough search for lowest-probability subsequences (LPSs) (i.e., the least likely sequence regions in terms of composition). Users can (i) initially annotate CB regions in input protein or nucleotide sequences of interest, and then (ii) query a database of greater than 1 500 000 pre-calculated protein-CB regions, for investigation of further functional hypotheses and inferences, about the specific CB regions that were discovered, and their protein disorder propensities. We demonstrate how a user can search for CB regions of similar compositional bias and protein disorder, with a worked example. We show that our annotations substantially augment the CB-region annotations that already exist in the UniProt database, with more comprehensive annotation of more complex CB regions. Our analysis indicates tens of thousands of CB regions that do not comprise globular domains or transmembrane domains, and that do not have a propensity to protein disorder, indicating a large cohort of protein-CB regions of biophysically uncharacterized types. This server and database is a conceptually novel addition to the workbench of tools now available to molecular biologists to generate hypotheses and inferences about the proteins that they are investigating. It can be accessed at http://libaio.biol.mcgill.ca/lps-annotate.html."
      }
    ]
  },
  {
    "tid":"42",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The 2014 International Planning Competition: Progress and Trends.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This article reviews the 2014 International Planning Competition (IPC-2014), the eighth in a series of competitions starting in 1998. IPC-2014 was held in three separate parts to assess the state of the art in three prominent areas of planning research: the deterministic (classical) part (IPCD), the learning part (IPCL), and the probabilistic part (IPPC). Each part evaluated planning systems in ways that pushed the edge of existing planner performance by introducing new challenges, novel tasks, or both. The competition surpassed again the number of competitors that participated in its predecessor, highlighting the competition's central role in shaping the landscape of ongoing developments in evaluating planning systems."
      }
    ]
  },
  {
    "tid":"43",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Search advertising using web relevance feedback' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The business of Web search, a $10 billion industry, relies heavily on sponsored search, whereas a few carefully-selected paid advertisements are displayed alongside algorithmic search results. A key technical challenge in sponsored search is to select ads that are relevant for the user's query. Identifying relevant ads is challenging because queries are usually very short, and because users, consciously or not, choose terms intended to lead to optimal Web search results and not to optimal ads. Furthermore, the ads themselves are short and usually formulated to capture the reader's attention rather than to facilitate query matching. Traditionally, matching of ads to queries employed standard information retrieval techniques using the bag of words approach. Here we propose to go beyond the bag of words, and augment both queries and ads with additional knowledge-rich features. We use Web search results initially returned for the query to create a pool of relevant documents. Classifying these documents with respect to an external taxonomy and identifying salient named entities give rise to two new feature types. Empirical evaluation based on over 9,000 query-ad pairwise judgments confirms that using augmented queries produces highly relevant ads. Our methodology also relaxes the requirement for each ad to explicitly specify the exhaustive list of queries (\"bid phrases\") that can trigger it."
      }
    ]
  },
  {
    "tid":"44",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Exploring mechanisms of spontaneous functional connectivity in MEG: How delayed network interactions lead to structured amplitude envelopes of band-pass filtered oscillations.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Spontaneous (or resting-state) brain activity has attracted a growing body of neuroimaging research over the last decades. Whole-brain network models have proved helpful to investigate the source of slow (<0.1Hz) correlated hemodynamic fluctuations revealed in fMRI during rest. However, the mechanisms mediating resting-state long-distance correlations and the relationship with the faster neural activity remain unclear. Novel insights coming from MEG studies have shown that the amplitude envelopes of alpha- and beta-frequency oscillations (~8\u201330Hz) display similar correlation patterns as the fMRI signals."
      }
    ]
  },
  {
    "tid":"45",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Towards interactive indexing for large Chinese calligraphic character databases' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, based on a novel shape-similarity-based retrieval method, we propose an interactive partial-distance-map (PDM)- based high-dimensional indexing scheme to speed up the retrieval performance of the large Chinese calligraphic character databases. Specifically, we use the approximate minimal bounding hyper- sphere of query character to search the PDM and utilize the users' relevance feedback to refine the search process. We conduct comprehensive experiments to testify the efficiency and effectiveness of the proposed method."
      }
    ]
  },
  {
    "tid":"46",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Minimum bit error rate multiuser detection for OFDM-SDMA using particle swarm optimization' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The Minimum Bit Error Rate (MBER) detectors outperform the conventional Minimum Mean Squared Error (MMSE) detector by minimizing the Bit Error Rate (BER) directly. In this paper an MBER multiuser detector for Orthogonal Frequency Division Multiplexing-Space Division Multiple Access (OFDM-SDMA) system is proposed employing Particle Swarm Optimization (PSO) for finding the optimum weight vectors. Simulation results show that the proposed system achieves faster convergence with lower complexity as compared to Genetic Algorithms (GA) with same Bit Error Rate (BER) performance."
      }
    ]
  },
  {
    "tid":"47",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Proactive replication for data durability' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many wide\u203aarea storage systems replicate data for durability. A common way of maintaining the replicas is to detect node failures and respond by creating additional copies of objects that were stored on failed nodes and hence suffered a loss of redundancy. Reactive techniques can minimize total bytes sent since they only create replicas as needed; however, they can create spikes in network use after a failure. These spikes may overwhelm application trafc and can make it difcult to provision bandwidth. This paper explores a proactive approach that creates additional copies not in response to failures, but periodically at a x ed low rate. We introduce Tempo, a distributed hash table that allows each user to specify a maximum maintenance bandwidth and uses it to perform proactive replication. Results from a simulation study suggest that Tempo can deliver high durability despite only using several kilobytes per second of bandwidth, comparable to state\u203aof\u203a the\u203aart reactive systems."
      }
    ]
  },
  {
    "tid":"48",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Automatic Synthesis of NF Models by Program Analysis.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Network functions (NFs), like firewall, NAT, IDS, have been widely deployed in today\u2019s modern networks. However, currently there is no standard specification or modeling language that can accurately describe the complexity and diversity of different NFs. Recently there have been research efforts to propose NF models. However, they are often generated manually and thus error-prone. This paper proposes a method to automatically synthesize NF models via program analysis. We develop a tool called NFactor, which conducts code refactoring and program slicing on NF source code, in order to generate its forwarding model. We demonstrate its usefulness on two NFs and evaluate its correctness. A few applications of NFactor are described, including network verification."
      }
    ]
  },
  {
    "tid":"49",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Portable Situation-Reporting System by a Palmtop Humanoid Robot for Daily Life' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose the portable situation reporting sys- tem by a small robot for daily life, which demands the rapidly system reconstitution for daily life support. In this paper, we established a system by an attachment mechanism assisted the robot system based on device-distributed approach and the patterned processing by the tree structuring of sensor infor- mation processing. The attachment mechanism facilitates the hardware system reconstitution and the patterned processing simplifies the software system restructuring. We evaluated the reconfigurable system not to re-write programs and judged the effectiveness of the system by the experiments. I. I NTRODUCTION"
      }
    ]
  },
  {
    "tid":"50",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Context-Aware Object Connection Discovery in Large Graphs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Given a large graph and a set of objects, the task of object connection discovery is to find a subgraph that retains the best connection between the objects. Object connection discovery is useful to many important applications such as discovering the connection between different terrorist groups for counter-terrorism operations. Existing work considers only the connection between individual objects; however, in many real problems the objects usually have a context (e.g., a terrorist belongs to a terrorist group). We identify the context for the nodes in a large graph. We partition the graph into a set of communities based on the concept of modularity, where each community becomes naturally the context of the nodes within the community. By considering the context we also significantly improve the efficiency of object connection discovery, since we break down the big graph into much smaller communities. We first compute the best intra-community connection by maximizing the amount of information flow in the answer graph. Then, we extend the connection to the inter-community level by utilizing the community hierarchy relation, while the quality of the inter-community connection is also ensured by modularity. Our experiments show that our algorithm is three orders of magnitude faster than the state-of-the-art algorithm, while the quality of the query answer is comparable."
      }
    ]
  },
  {
    "tid":"51",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'HPAP: A novel authentication scheme for RFID systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The increasing demand to deploy efficient ways of identification has made Radio frequency Identification (RFID) technology ubiquitous. Due to the wireless nature of communication between the reader and the tag, this technology imposes major security and privacy threats. Massive work to design a powerful authentication protocol has been put to overcome various threats against privacy and security of the system. However, certain constraints on RFID tags such as limited computation capabilities, memory size and communication cost, has made most approaches fail to conduct fully secured RFID system. In this paper, we present a novel cryptographic scheme, Hacker Proof Authentication Protocol (HPAP), that allows mutual authentication between the reader and the tag as well as secure tags' information. We prove our protocol achieves full security by deploying tag static identifier, updated timestamp, a one way hash function and encryption keys with semi randomized nature as they are updated using Linear Feedback Shift Register (LFSR). We show that the protocol insures to maintain less storage, computation load and low-cost in comparison to various existing RFID authentication protocols."
      }
    ]
  },
  {
    "tid":"52",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A concurrency control protocol for parallel B-tree structures without latch-coupling for explosively growing digital content' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"While shared-nothing parallel infrastructures provide fast processing of explosively growing digital content, managing data efficiently across multiple nodes is important. The value-range partitioning method with parallel B-tree structures in a shared-nothing environment is an efficient approach for handling large amounts of data. To handle large amounts of data, it is also important to provide an efficient concurrency control protocol for the parallel B-tree. Many studies have proposed concurrency control protocols for B-trees, which use latch-coupling. None of these studies has considered that latch-coupling contains a performance bottleneck of sending of messages between processing elements (PEs) in distributed environments because latch-coupling is efficient for a B-tree on a single machine. The only protocol without latch-coupling is the B-link algorithm, but it is difficult to use the B-link algorithm directly on an entire parallel B-tree structure because it is necessary to guarantee the consistency of the side pointers. We propose a new concurrency control protocol named LCFB that requires no latch-coupling in optimistic processes. LCFB reduces the amount of communication between PEs during a B-tree traversal. To detect access path errors in the LCFB protocol caused by removal of latch-coupling, we assign boundary values to each index page. Because a page split may cause page deletion in a Fat-Btree, we also propose an effective method for handling page deletions without latch-coupling. We then combine LCFB with the B-link algorithm within each PE to reduce the cost of Structure Modification Operations (SMOs) in a PE, as a solution to the difficulty of consistency management for the side pointers in a parallel B-tree structure. To compare the performance of the proposed protocol with conventional protocols MARK-OPT, INC-OPT, and ARIES/IM, we implemented them on an autonomous disk system with a Fat-Btree structure. Experimental results in various environments indicate that the system throughput of the proposed protocols is always superior to those of the other protocols, especially in large-scale configurations, and LCFB with the B-link algorithm is effective at higher update ratios."
      }
    ]
  },
  {
    "tid":"53",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Conceptual Clustering Applied to Ontologies' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A clustering method is presented which can be applied to semantically annotated resources in the context of ontological knowledge\n bases. This method can be used to discover emerging groupings of resources expressed in the standard ontology languages. The\n method exploits a language-independent semi-distance measure over the space of resources, that is based on their semantics\n w.r.t. a number of dimensions corresponding to a committee of discriminating features represented by concept descriptions.\n A maximally discriminating group of features can be constructed through a feature construction method based on genetic programming.\n The evolutionary clustering algorithm proposed is based on the notion of medoids applied to relational representations. It\n is able to induce a set of clusters by means of a fitness function based on a discernibility criterion. An experimentation\n with some ontologies proves the feasibility of our method.\n "
      }
    ]
  },
  {
    "tid":"54",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'h-Hash: A Hash Index Structure for Flash-Based Solid State Drives' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Flash-based solid state drives (SSDs) have been widely used as storage medium because of their fast access speed, high reliability and low power consumption. In spite of these advantages, they inherit distinct characteristics such as no in-place updates, asymmetric operation speed and unit because they consist of NAND flash memories. Therefore, a disk-based hash index structure may result in severe performance degradation if it is directly deployed on a NAND flash memory-based storage system. In this paper, we propose a new hybrid hash index structure for flash-based SSDs. It delays and reduces the split operations which cause slow block erasure and additional read and write operations by exploiting overflow buckets according to the ratio of updates and deletions. Through various performance evaluations, we show the superiority of our proposed index structure by comparing it to other hash index structures for flash-based SSDs."
      }
    ]
  },
  {
    "tid":"55",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Tracking multiple maneuvering point targets using multiple filter bank in infrared image sequence' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Performance of any tracking algorithm depends upon the model selected to capture the target dynamics. In real world applications, no a priori knowledge about the target motion is available. Moreover, it could be a maneuvering target. The proposed method is able to track maneuvering or nonmaneuvering multiple point targets with large motion (/spl mnplus/20 pixels) using multiple filter bank in an IR image sequence in the presence of clutter and occlusion due to clouds. The use of multiple filters is not new, but the novel idea here is that it uses single-step decision logic to switch over between filters. Our approach does not use any a priori knowledge about maneuver parameters, nor does it exploit a parameterized nonlinear model for the target trajectories. This is in contrast to: (i) interacting multiple model (IMM) filtering which required the maneuver parameters, and (ii) extended Kalman filter (EKF) or unscented Kaiman filter (UKF), both of which require a parameterized model for the trajectories. We compared our approach for target tracking with IMM filtering using EKF and UKF for non-linear trajectory models. UKF uses the nonlinearity of the target model, where as a first order linearization is used in case of EKF. RMS for the predicted position error (RMS-PPE) obtained using our proposed methodology is significantly less in case of highly maneuvering target."
      }
    ]
  },
  {
    "tid":"56",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Jazz: a tool for demand-driven structural testing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Software testing to produce reliable and robust software has become vitally important. Testing is a process by which quality can be assured through the collection of information about software. While testing can improve software quality, current tools typically are inflexible and have high overheads, making it a challenge to test large projects. We describe a new scalable and flexible tool, called Jazz, that uses a demand-driven structural testing approach. Jazz has a low overhead of only 17.6% for branch testing."
      }
    ]
  },
  {
    "tid":"57",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Privacy-preserving self-helped medical diagnosis scheme based on secure two-party computation in wireless sensor networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With the continuing growth of wireless sensor networks in pervasive medical care, people pay more and more attention to privacy in medical monitoring, diagnosis, treatment, and patient care. On one hand, we expect the public health institutions to provide us with better service. On the other hand, we would not like to leak our personal health information to them. In order to balance this contradiction, in this paper we design a privacy-preserving self-helped medical diagnosis scheme based on secure two-party computation in wireless sensor networks so that patients can privately diagnose themselves by inputting a health card into a self-helped medical diagnosis ATM to obtain a diagnostic report just like drawing money from a bank ATM without revealing patients' health information and doctors' diagnostic skill. It makes secure self-helped disease diagnosis feasible and greatly benefits patients as well as relieving the heavy pressure of public health institutions."
      }
    ]
  },
  {
    "tid":"58",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Adaptive tracking control for uncertain switched stochastic nonlinear pure-feedback systems with unknown backlash-like hysteresis.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, an adaptive tracking control problem is studied for a class of switched stochastic nonlinear pure-feedback systems with unknown backlash-like hysteresis under arbitrary switching. The mean-value theorem is used to overcome the difficulty arising from the pure-feedback structure. Based on neural networks\u2019 approximation capability, an adaptive tracking control approach is developed via the adaptive backstepping technique and common Lyapunov function method. It is proved that the proposed control scheme can guarantee that all signals in the closed-loop system are semi-globally uniformly ultimately bounded in probability and the tracking error converges to an adjustable neighborhood of the origin. Finally, a simulation example further shows the effectiveness of the presented control scheme."
      }
    ]
  },
  {
    "tid":"59",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Scheduling Impact on the Performance of Relay-Enhanced LTE-A Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper studies the performance of two traditional schedulers, namely, proportional fair (PF) and round robin (RR), in the context of relay-enhanced Long-Term Evolution Advanced (LTE-A) networks. These two schedulers are natural candidates for implementation in relay nodes (RNs), and following the results obtained in single-hop networks, mobile operators could be tempted to adopt PF because of the good tradeoff it offers between cell capacity and fairness. Based on a statistical throughput evaluation model, we show that this is not necessarily the right option. The number of RNs, their locations in the cell, and backhaul link quality have a decisive influence on scheduler choice. In some scenarios, it is even not desirable to deploy relays as they degrade the network performance compared with the no-relay case. For the purpose of performance evaluation, we develop a realistic and computationally tractable statistical network model that takes into account fast fading, multiple interferers, cell range expansion bias, backhaul link quality, and traffic load. We also propose optimization of the radio frame structure and a suboptimal RN placement scheme to fairly compare RR and PF."
      }
    ]
  },
  {
    "tid":"60",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Reflectance and Shape Estimation with a Light Field Camera under Natural Illumination.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Reflectance and shape are two important components in visually perceiving the real world. Inferring the reflectance and shape of an object through cameras is a fundamental research topic in the field of computer vision. While three-dimensional shape recovery is pervasive with varieties of approaches and practical applications, reflectance recovery has only emerged recently. Reflectance recovery is a challenging task that is usually conducted in controlled environments, such as a laboratory environment with a special apparatus. However, it is desirable that the reflectance be recovered in the field with a handy camera so that reflectance can be jointly recovered with the shape. To that end, we present a solution that simultaneously recovers the reflectance and shape (i.e., dense depth and normal maps) of an object under natural illumination with commercially available handy cameras. We employ a light field camera to capture one light field image of the object, and a 360-degree camera to capture the illumination. The proposed method provides positive results in both simulation and real-world experiments."
      }
    ]
  },
  {
    "tid":"61",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'CrossNavi: enabling real-time crossroad navigation for the blind with commodity phones' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Crossroad is among the most dangerous parts outside for the visually impaired people. Numerous studies have exploited navigating systems for the visually impaired community, providing services ranging from block detection, route planning to realtime localization. However, none of them have addressed the safety issue in crossroad and integrated three key factors necessary for a practical crossroad navigation system: detecting the crossroad, locating zebra patterns, and guiding the user within zebra crossing when passing the road. Our CrossNavi application responds to these needs, providing an integrated crossroad navigation service that incorporates all the essential functionalities mentioned above. The overall service is fulfilled by the collaboration of built-in sensors on commodity phones, and requires minimal human participation. We describe the technical aspects of its design, implementation, interface, and further improvements to make the system practical on a wider basis. Experimental results from three visually impaired volunteers show that the system exhibits promising behavior in both urban and rural areas."
      }
    ]
  },
  {
    "tid":"62",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'High Performance Development for High End Computing With Python Language Wrapper (PLW)' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a design and implementation of a system that leverages interactive scripting environment to the needs of scientific computing. The system allows seamless translation of high level script codes to highly optimized native language executables that can be ported to parallel systems with high performance hardware and potential lack of the scripting language interpreter. Performance results are given to show various usage scenarios that differ in terms of invested programmer's effort and resulting performance gains."
      }
    ]
  },
  {
    "tid":"63",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Deliberation Networks: Sequence Generation Beyond One-Pass Decoding.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The encoder-decoder framework has achieved promising progress for many sequence generation tasks, including machine translation, text summarization, dialog system, image captioning, etc. Such a framework adopts an one-pass forward process while decoding and generating a sequence, but lacks the deliberation process: A generated sequence is directly used as final output without further polishing. However, deliberation is a common behavior in human's daily life like reading news and writing papers/articles/books. In this work, we introduce the deliberation process into the encoder-decoder framework and propose deliberation networks for sequence generation. A deliberation network has two levels of decoders, where the first-pass decoder generates a raw sequence and the second-pass decoder polishes and refines the raw sentence with deliberation. Since the second-pass deliberation decoder has global information about what the sequence to be generated might be, it has the potential to generate a better sequence by looking into future words in the raw sentence. Experiments on neural machine translation and text summarization demonstrate the effectiveness of the proposed deliberation networks. On the WMT 2014 English-to-French translation task, our model establishes a new state-of-the-art BLEU score of 41.5."
      }
    ]
  },
  {
    "tid":"64",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Workload-Based Adaptive Load-Balancing Technique For Mobile Ad Hoc Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A novel load-balancing technique for ad hoc on-demand routing protocols is presented. Currently, ad hoc routing protocols lack load-balancing capabilities, and thus, they often fail to provide good performance especially in the presence of a large volume of traffic. We present a simple but very effective method to achieve load balance and congestion alleviation. The new scheme is motivated by the observation that ad hoc on-demand routing protocols flood route request (RREQ) messages to acquire routes, and only nodes that respond to those messages have a potential to serve as intermediate forwarding nodes. If a node ignores RREQ messages within a specific period, it can completely be excluded from the additional communications that might have occurred for that period otherwise. Thus, a node can decide not to serve a traffic flow by dropping the RREQ for that How. In the new scheme, RREQ messages are forwarded selectively according to the load status of each node so that overloaded nodes can be excluded from the requested paths. Each node begins to allow additional traffic flows again whenever its overloaded status is dissolved. The new scheme utilizes interface queue occupancy and workload to control RREQ messages adaptively. The enhanced versions of protocols with this scheme are compared to the base protocols. Simulation results reveal that the new scheme greatly reduces packet latency as well as routing overhead without adversely affecting the network throughput, and it successfully balances the network load among nodes."
      }
    ]
  },
  {
    "tid":"65",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'UPS: A United Cooperative Paradigm for Primary and Secondary Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The dominant spectrum sharing paradigm of today is the interweave paradigm. This paper advocates a new and alternative paradigm called United network of Primary and Secondary networks (UPS). UPS allows a complete cooperation between primary and secondary networks at the node level to relay each other's traffic, in addition to existing dynamic spectrum access (DSA) in time, space, and frequency domains. Such cooperation allows the primary and secondary networks to access a much richer network resources from the combined network. As a case study, we consider a problem with the goal of supporting the rate requirement of the primary network traffic while maximizing the minimum throughput of the secondary sessions. For this problem, we develop an optimization model and formulate a combinatorial optimization problem. Although this problem is in the form of mixed integer linear program (MILP), we can use CPLEX to solve it efficiently. Simulation results show that the UPS paradigm offers much better throughput performance than the interweave DSA paradigm."
      }
    ]
  },
  {
    "tid":"66",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Social Trust Prediction Using Heterogeneous Networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Along with increasing popularity of social websites, online users rely more on the trustworthiness information to make decisions, extract and filter information, and tag and build connections with other users. However, such social network data often suffer from severe data sparsity and are not able to provide users with enough information. Therefore, trust prediction has emerged as an important topic in social network research. Traditional approaches are primarily based on exploring trust graph topology itself. However, research in sociology and our life experience suggest that people who are in the same social circle often exhibit similar behaviors and tastes. To take advantage of the ancillary information for trust prediction, the challenge then becomes what to transfer and how to transfer. In this article, we address this problem by aggregating heterogeneous social networks and propose a novel joint social networks mining (JSNM) method. Our new joint learning model explores the user-group-level similarity between correlated graphs and simultaneously learns the individual graph structure; therefore, the shared structures and patterns from multiple social networks can be utilized to enhance the prediction tasks. As a result, we not only improve the trust prediction in the target graph but also facilitate other information retrieval tasks in the auxiliary graphs. To optimize the proposed objective function, we use the alternative technique to break down the objective function into several manageable subproblems. We further introduce the auxiliary function to solve the optimization problems with rigorously proved convergence. The extensive experiments have been conducted on both synthetic and real- world data. All empirical results demonstrate the effectiveness of our method."
      }
    ]
  },
  {
    "tid":"67",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Aggressive sub-channel allocation algorithm for intelligent transmission in multi-user OFDMA system' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper solves the problem of finding an intelligent sub-channel allocation method to communicate with multiple users in OFDMA wireless systems. We propose an ASA (Aggressive Sub-channel Allocation) algorithm, which is an intelligent dynamic channel allocation algorithm considering all users' channel state information conditionally to maximize throughput in an OFDMA system. We compare the ASA algorithm with Round-robin, PF (Proportional Fair) and ACG (Amplitude Craving Greedy) in the 2-tier cell environment and then analyze the performance of each algorithm through computer simulation in downlink OFDMA system. Simulation results show that the proposed ASA algorithm gets 24 %, 16 % and 14 % better ring throughput compared with the Round-robin, PF and ACG, respectively."
      }
    ]
  },
  {
    "tid":"68",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Deployment of Better Than Worst-Case Design: Solutions and Needs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The advent of nanometer feature sizes in silicon fabrication has triggered a number of new design challenges for computer designers. These challenges include design complexity and operation in the presence of environmental and device uncertainty. To make things worse, these new challenges add to the many challenges that designers already face in order to scale system performance while meeting power and reliability budgets. Current design objectives are being met by applying even more engineers and increasing overall design times, an unsustainable trend. This paper overviews a novel design strategy, called Better Than Worst-Case design, that addresses these challenges through a methodology based on separating the concerns of performance and reliability by coupling complex design components with simple reliable checker mechanisms. We present the key aspects of Better ThanWorst-Case Design and cover some recently proposed solutions that deploy this technique in application domains ranging from microprocessors to digital signal processors. We then highlight a few aspects that need to be addressed to make this approach more practical in general contexts and suggest possible solutions."
      }
    ]
  },
  {
    "tid":"69",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Base station cooperative multiuser MIMO using block-diagonalized random beamforming with online update' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes applying random (opportunistic) beamforming to base station (BS) cooperative multiuser multiple-input multiple-output (MIMO) transmission. This proposal comprises two parts. First, we propose a block-diagonalized random unitary beamforming matrix. The proposed beamforming matrix achieves better throughput distribution compared to the purely random unitary beamforming matrix when the average path loss based on distance-dependent loss and shadowing loss is largely different among transmitter antennas, which is true in BS cooperative MIMO. Second, we propose an online update algorithm for a random beamforming matrix to improve the throughput compared to the purely random and channel-independent beamforming matrix generation, especially when the number of users is low. Different from conventional approaches, the proposed online update algorithm does not increase the overhead of the reference signal transmission and control delay. Simulation results show the effectiveness of the proposed method using a block-diagonalized random unitary beamforming matrix with online updates in a BS cooperative multiuser MIMO scenario."
      }
    ]
  },
  {
    "tid":"70",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Improving Person Reidentification Using a Self-Focusing Network in Internet of Things' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Person reidentification (re-ID), which is a significant and potential application in the Internet of Things (IoT), aims to retrieve pedestrians of interest given a labeled image in a camera network. Now, it is still existing many challenges that severely influence feature representation in practical scenarios. Many methods adopt the attention mechanism in convolutional neural network (CNN) to improve the ability of feature learning. Although they only apply 1-D attention block in the popular deep learning architecture, the learned features are not discriminative for the feature representation. In this work, we investigate a self-focusing network (SFNet) that considers both the channel-dimensional attention and spatial-dimensional attention to adaptively learn more discriminative features. Namely, we embed the new attention module into the common backbone network, which can focus on the salient region by inhibiting the redundant features. Specifically, we design eight variants of the channel-dimensional attention and spatial-dimensional attention throughout the entire network and explore the most powerful feature representation. The heatmaps of different layers are visualized to intuitively present the performance of SFNet. Furthermore, we compare SFNet with the prior work on three popular person re-ID benchmarks by abundant experiments."
      }
    ]
  },
  {
    "tid":"71",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Real-time Analysis of Flow Data for Network Attack Detection' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"\u00f3 With the wide deployment of o w monitoring in IP networks, the analysis of the exported o w data has become an important research area. It has been shown that o w data can be used to detect trafc anomalies, DoS attacks, and the propagation of worms. In practice, anomalies and attacks should be detected as fast as possible in order to allow taking appropriate countermeasures. We describe the necessary steps from the raw o w data to the detection result in a systematic way. Furthermore, we present TOPAS, a system and framework for real-time analysis of o w data, that has been developed in order to meet these requirements. Performance measurements and various application examples point out the capabilities and benets of our approach. Index Terms\u00f3 network monitoring, o w analysis, anomaly and attack detection"
      }
    ]
  },
  {
    "tid":"72",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Tracing Traffic Through Intermediate Hosts That Repacketize Flows' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Tracing interactive traffic that traverses stepping stones (i.e., intermediate hosts) is challenging, as the packet headers, lengths, and contents can all be changed by the stepping stones. The traffic timing has therefore been studied as a means of tracing traffic. One such technique uses traffic timing as a side channel into which a watermark, or identifying tag, can be embedded to aid with tracing. The effectiveness of such techniques is greatly reduced when repacketization of the traffic occurs at the stepping stones. Repacketization is a natural effect of many applications, including SSH, and therefore poses a serious challenge for traffic tracing.This paper presents a new method of embedding a watermark in traffic timing, for purposes of tracing the traffic in the presence of repacketization. This method uses an invariant characteristic of two traffic flows which are part,of the same stepping stone chain, namely, elapsed time of the flows. The duration of each flow is sliced into short fixed-length intervals. Packet timing is adjusted to manipulate the packet count in specific intervals, for purposes of embedding the watermark. A statistical analysis of the method, with no assumptions or limitations concerning the distribution of packet times, proves the effectiveness of the method given a sufficient number of packets, despite natural and/or deliberate repacketization and perturbation of the traffic timing by an adversary. The method has been implemented and tested on a large number of synthetically-generated SSH traffic flows. The results demonstrate that 100% detection rates and less than 1% false positive rates are achievable under conditions of 2 seconds of maximum timing perturbation and 12% repacketization rate, using fewer than 1000 packets."
      }
    ]
  },
  {
    "tid":"73",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Time-memory trade-offs using sparse matrix methods for large-scale eigenvalue problems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Iterative methods such as Lanczos and Jacobi-Davidson are typically used to compute a small number of eigenvalues and eigenvectors of a sparse matrix. However, these methods are not effective in certain large-scale applications, for example, \"global tight binding molecular dynamics.\" Such applications require all the eigenvectors of a large sparse matrix; the eigenvectors can be computed a few at a time and discarded after a simple update step in the modeling process. We show that by using sparse matrix methods, a direct-iterative hybrid scheme can significantly reduce memory requirements while requiring less computational time than a banded direct scheme. Our method also allows a more scalable parallel formulation for eigenvector computation through spectrum slicing. We discuss our method and provide empirical results for a wide variety of sparse matrix test problems."
      }
    ]
  },
  {
    "tid":"74",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The prediction of head and eye movement for 360 degree images.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Estimating salient areas of visual stimuli which are liable to attract viewers\u2019 visual attention is a challenging task because of the high complexity of cognitive behaviors in the brain. Many researchers have been dedicated to this field and obtained many achievements. Some application areas, ranging from computer vision, computer graphics, to multimedia processing, can benefit from saliency detection, considering that the detected saliency has depicted the visual importance of different areas of the visual stimuli. As for the 360 degree visual stimuli, images and videos should record the whole scene in the 3D world, so the resolutions of panoramic images and videos are usually very high. However, when watching 360 degree stimuli, observers can only see part of the scene in the view port, which is presented to the eyes of the observers through the Head Mounted Display (HMD). So sending the whole video, or rendering the whole scene may result in the waste of resources. Thus if we can predict the current field of view, then focuses can be put to the streaming and rendering of the scene in the current field of view. Further more, if we can predict salient areas in the scene, then more fine processing can be done to the visually important areas. The prediction of salient regions for traditional images and videos have been extensively studied. However, conventional saliency prediction methods are not fully adequate for 360 degree contents, because 360 degree stimuli own some unique characteristics. Related study in this area is limited. In this paper, we study the problem of predicting head movement, head\u2013eye motion, and scanpath of viewers when they are watching 360 degree images in the commodity HMDs. Three types of data are specifically analyzed. The first is the head movement data, which can be regarded as the movement of the view port. The second is the head\u2013eye motion data which combines the motion of the head and the movement of the eye within the view port. The third is the scan-paths data of observers in the entire panorama which record the position information as well as the time information. And our model is designed to predict the saliency maps for the first two, and the scanpaths for the last one. Experimental results demonstrate the effectiveness of our model."
      }
    ]
  },
  {
    "tid":"75",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Novel Approach for Developing Web-Based Enterprise Information Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"All over the world software industry is facing the threatening failure rate of software projects. The scenario is worst in Pakistan having very immature software industry where projects are mostly over budgeted, over scheduled, and get low customer satisfaction. ..."
      }
    ]
  },
  {
    "tid":"76",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'SLAW: A New Mobility Model for Human Walks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Simulating human mobility is important in mobile networks because many mobile devices are either attached to or controlled by humans and it is very hard to deploy real mobile networks whose size is controllably scalable for performance evaluation. Lately various measurement studies of human walk traces have discovered several significant statistical patterns of human mobility. Namely these include truncated power-law distributions of flights, pause-times and inter-contact times, fractal way-points, and heterogeneously defined areas of individual mobility. Unfortunately, none of existing mobility models effectively captures all of these features. This paper presents a new mobility model called SLAW (self-similar least action walk) that can produce synthetic walk traces containing all these features. This is by far the first such model. Our performance study using using SLAW generated traces indicates that SLAW is effective in representing social contexts present among people sharing common interests or those in a single community such as university campus, companies and theme parks. The social contexts are typically common gathering places where most people visit during their daily lives such as student unions, dormitory, street malls and restaurants. SLAW expresses the mobility patterns involving these contexts by fractal way points and heavy-tail flights on top of the way points. We verify through simulation that SLAW brings out the unique performance features of various mobile network routing protocols."
      }
    ]
  },
  {
    "tid":"77",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient mining of salinity and temperature association rules from ARGO data' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents an efficient technique for analyzing ARGO ocean data comprising time series of salinity/temperature measurements where informative salinity/temperature patterns are extracted. Most traditional mining techniques focus on finding associations among items within one transaction and are therefore unable to discover rich contextual patterns related to location and time. In order to show the associated salinity/temperature variations among different locations and time intervals, for example, ''if the salinity rose from 0.15psu to 0.25psu in the area that is in the east-northeast direction and is near Taiwan, then the temperature will rise from 0^oC to 1.2^oC in the area that is in the east-northeast direction and is far away from Taiwan next month'', a quantitative inter-transaction association rules mining algorithm is proposed. The FITI and the PrefixSpan algorithms are adopted to maximize the mining efficiency. The strategy is applied to ocean salinity measurements obtained from the waters surrounding Taiwan. These experimental evaluations show that the proposed algorithm achieves better performance than other inter-transaction association rule mining algorithms."
      }
    ]
  },
  {
    "tid":"78",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Cooperative Localization in Mines Using Fingerprinting and Neural Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Localizing people in confined and underground areas is one of the topics under research in mining labs and industries. The position of personnel and equipments in areas such as mines is of high importance because it improves industrial safety and security. Due to the special nature of underground environments, signals transmitted in a mine gallery/tunnel suffer from severe multipath effects caused by reflection, refraction, diffraction and collision with humid rough surfaces. In such cases and in cases where the signals are blocked due to the non-line of sight (NLOS) regions, traditional localization techniques based on the RSS, AOA and TOA/TDOA lead to high position estimation errors. One of the proposed solutions to such challenging situations is based on extracting channel impulse response (CIR) fingerprints with reference to one wireless receiver and using an artificial neural network as a matching algorithm to localize. In this article we study this approach in a multiple access network where multiple access points are present. The diversity of the collected fingerprints will allow us to create artificial neural networks that will work separately or cooperatively using the same localization technique. The results will show that using cooperative artificial intelligence in the presence of multiple signatures from different reference points improves significantly the accuracy, precision, scalability and the overall performance of the localization system."
      }
    ]
  },
  {
    "tid":"79",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'EverCopter: continuous and adaptive over-the-air sensing with detachable wired flying objects' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The paper proposes EverCopter, which provides continuous and adaptive over-the-air sensing with detachable wired flying objects. While a major advantage of sensing systems with battery-operated MAVs is a wide sensing coverage, sensing time is limited due to its limited amount of energy. We propose dynamically rechargeable flying objects, called EverCopter. EverCopter achieves both long sensing time and wide sensing coverage by the following two characteristics. First, multiple EverCopters can be tied in a row by power supply cables. Since the root EverCopter in a row is connected to DC power supply on the ground, each EverCopter can fly without battery. This makes their sensing time forever, unless the power supply on the ground fails. Second, the leaf EverCopter can detach itself from the row in order to enjoy wider sensing coverage. An EverCopter, while it is detached, runs with its own battery-supplied energy. When the remaining energy becomes low, it flies back to the row to recharge the battery."
      }
    ]
  },
  {
    "tid":"80",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'StegErmelc: A Novel DCT-Based Steganographic Method Using Three Strategies' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes a DCT-based steganographic method named StegErmelc in the JPEG domain. Three strategies are proposed, namely (i) edge-like block selection, (ii) recursive matrix encoding, and (iii) largest coefficient serving, to form a novel steganographic method for achieving scalable carrier capacity, low detectability by universal blind steganalyzer, and high image quality, simultaneously. For a given message length, StegErmelc flexibly scales its carrier capacity to accommodate the message while trading off with stego detectability. At full capacity, StegErmelc has comparable carrier capacity relative to the existing methods. When embedding the same amount of information, StegErmelc remarkably reduces the stego detection rate to about 0.3\u20130.5 lower than that of the existing methods considered, and consequently StegErmelc can withstand blind steganalyzer when embedding up to 0.10 bpc. Under the same condition, StegErmelc produces stego image with quality higher than that of the existing methods considered. Graphical comparison with three additional evaluation metrics is also presented to show the relative performance of StegErmelc with respect to the existing methods considered."
      }
    ]
  },
  {
    "tid":"81",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Estimating the level of motion sickness based on EEG spectra' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Motion sickness (MS) is a normal response to real, perceived, or even anticipated movement. People tend to get motion sickness on a moving boat, train, airplane, car, or amusement park rides. Although many motion sicknessrelated biomarkers have been identified, but how to estimate human's motion sickness level (MSL) is a big challenge in the operational environment. Traditionally, questionnaire and physical check are the common ways to passively evaluate subject's sickness level. Our past studies had investigated the EEG activities correlated with motion sickness in a virtual-reality based driving simulator. The driving simulator comprised an actual automobile mounted on a Stewart motion platform with six degrees of freedom, providing both visual and vestibular stimulations to induce motion-sickness in a manner that is close to that in daily life. EEG data were acquired at a sampling rate of 500 Hz using a 32- channel EEG system. The acquired EEG signals were analyzed using independent component analysis (ICA) and time-frequency analysis to assess EEG correlates of motion sickness. Subject's degree of motion-sickness was simultaneously and continuously reported using an onsite joystick, providing non-stop psychophysical references to the recorded EEG changes. We found that the parietal, motor, occipital brain regions exhibited significant EEG power changes in response to vestibular and visual stimuli. Based on these findings and experimental results, this study aims to develop an EEG-based system to estimate subject's motion sickness level upon the EEG power spectra from motion-sickness related brain areas. The MS evaluation system can be applied to early detection of the subject's motion sickness and prevent its uncomfortable syndromes in our daily life. Furthermore, the experiment results could also lead to a practical human-machine interface for noninvasive monitoring of motion sickness of drivers or passengers in real-world environments."
      }
    ]
  },
  {
    "tid":"82",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimal Allocation of Time-Resources for Multihypothesis Activity-Level Detection' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The optimal allocation of samples for activity-level detection in a wireless body area network for health-monitoring applications is considered. A wireless body area network with heterogeneous sensors is deployed in a simple star topology with the fusion center receiving biometric samples from each of the sensors. The number of samples collected from each of the sensors is optimized to minimize the probability of misclassification between multiple hypotheses at the fusion center. Using experimental data from our pilot study, we find equally allocating samples amongst sensors is normally suboptimal. A lower probability of error can be achieved by allocating a greater fraction of the samples to sensors which can better discriminate between certain activity-levels. As the number of samples is an integer, prior work employed an exhaustive search to determine the optimal allocation of integer samples. However, such a search is computationally expensive. To this end, an alternate continuous-valued vector optimization is derived which yields approximately optimal allocations which can be found with significantly lower complexity."
      }
    ]
  },
  {
    "tid":"83",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Graph regularized sparse coding for image representation.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Sparse coding has received an increasing amount of interest in recent years. It is an unsupervised learning algorithm, which finds a basis set capturing high-level semantics in the data and learns sparse coordinates in terms of the basis set. Originally applied to modeling the human visual cortex, sparse coding has been shown useful for many applications. However, most of the existing approaches to sparse coding fail to consider the geometrical structure of the data space. In many real applications, the data is more likely to reside on a low-dimensional submanifold embedded in the high-dimensional ambient space. It has been shown that the geometrical information of the data is important for discrimination. In this paper, we propose a graph based algorithm, called graph regularized sparse coding, to learn the sparse representations that explicitly take into account the local manifold structure of the data. By using graph Laplacian as a smooth operator, the obtained sparse representations vary smoothly along the geodesics of the data manifold. The extensive experimental results on image classification and clustering have demonstrated the effectiveness of our proposed algorithm."
      }
    ]
  },
  {
    "tid":"84",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Intelligent student profiling for predicting e-Assessment outcomes' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The main objective of this paper is introducing intelligence in the e-Learning and e-Assessment processes. Therefore, we present an existing adaptive e-Learning and e-Assessment strategies, verify them with machine learning (ML) algorithms, build students Profile and eventually, we present our new model that will be able to estimate the final result of the overall students' work during the semester, taking into account all the learning objectives that the students have passed. Thus, our idea is creating an intelligent agent that will simulate the behavior of a real professor as much as possible."
      }
    ]
  },
  {
    "tid":"85",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Balancing ILP and TLP in SMT Architectures through Out-of-Order Instruction Dispatch' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Simultaneous Multi-threading (SMT) architectures open up new avenues for datapath optimizations due to the presence of thread-level parallelism (TLP). One recent proposal for exploiting such parallelism is the 2OP_BLOCK scheduler design, which completely avoids the dispatch of instructions with two non-ready source operands into the issue queue. This technique reduces the scheduler complexity and also provides performance benefits for workloads with sufficient TLP, as the issue queue is more efficiently utilized. In this paper we first revisit the 2OP_BLOCK scheduler and show that this design actually results in performance losses for workloads with a limited number of threads because the constraints imposed on the exploitable ILP within each thread outweigh its advantages. To balance the ILP and TLP in SMT processors supporting such schedulers, we propose out-of-order dispatch of instructions within each thread. This simple augmentation naturally allows the 2OP_BLOCK scheduler to perform well even when the number of threads is small. Furthermore, for environments with a larger number of threads, the out-of-order dispatch mechanism improves the performance of the original proposal by up to 15% on the average across simulated multithreaded mixes of SPEC 2000 benchmarks."
      }
    ]
  },
  {
    "tid":"86",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Network design for information networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We define a new class of network design problems motivated by designing information networks. In our model, the cost of transporting flow for a set of users (or servicing them by a facility) depends on the amount of information requested by the set of users. We assume that the aggregation cost follows economies of scale, that is, the incremental cost of a new user is less if the set of users already served is larger. Naturally, information requested by some sets of users might aggregate better than that of others, so our cost is now a function of the actual set of users. not just their total demand.We provide constant-factor approximation algorithms to two important problems in this general model. In the Group Facility Location problem, each user needs information about a resource. and the cost is a linear function of the number of resources involved (instead of the number of clients served). The Dependent Maybecast Problem extends the Karger-Minkoff maybecast model to probabilities with limited correlation and also contains the 2-stage stochastic optimization problem as a special case. We also give an O(ln n)-approximation algorithm for the Single Sink Information Network Design problem.We show that the Stochastic Steiner Tree problem can be approximated by dependent maybecast, and using this we obtain an O(1)-approximation algorithm for the k-stage stochastic Steiner tree problem for any fixed k. This is the first approximation algorithm for multi-stage stochastic optimization. Our algorithm allows scenarios to have different inflation factors, and works for any distribution provided that we can sample the distribution."
      }
    ]
  },
  {
    "tid":"87",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Hierarchical swapped networks: efficient low-degree alternatives to hypercubes and generalized hypercubes' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a new class of interconnection networks called hierarchical swapped networks (HSNs). We show that some subclasses of HSNs can efficiently emulate hypercubes, or generalized hypercubes, while having node degrees significantly smaller than the emulated networks. In particular, a suitably constructed HSN can emulate a hypercube or generalized hypercube with constant slowdown under the single-dimension communication model and asymptotically optimal slowdown with respect to its node degree under the all-port communication model. As a consequence, we obtain a variety of efficient algorithms on HSNs through emulation, thus proving the versatility of HSN. Some subclasses of HSNs are also shown to have asymptotically optimal diameters with respect to their node degrees. HSNs appear to be attractive low-degree alternatives to hypercubes and generalized hypercubes for general-purpose parallel computers"
      }
    ]
  },
  {
    "tid":"88",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Ad-Hoc Localization In Urban District' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we present a range-free ad-hoc localization algorithm called UPL (Urban Pedestrians Localization), for positioning mobile nodes in urban district. The design principle of UPL, is two-fold. (1) We assume that location seeds are deployed sparsely due to deployment-cost constraints. Thus most mobile nodes cannot expect to meet these location seeds frequently. Therefore, each mobile node in UPL relies on location information received from its neighboring mobile nodes in order to estimate its area of presence. The area of presence of each mobile node becomes inexact as it moves, but it is helpful to reduce the areas of presence of the other mobile nodes. (2) To predict the area of presence of mobile nodes accurately under mobility, we employ information about obstacles such as walls, and present an algorithm to calculate the movable areas of mobile nodes considering obstacles. This also helps to reduce each node's area of presence. The experimental results have shown that by the above two ideas UPL could achieve 8m positioning error in average with 10m of radio range."
      }
    ]
  },
  {
    "tid":"89",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Wikikreator: Improving Wikipedia Stubs Automatically' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Stubs on Wikipedia often lack comprehensive information. The huge cost of editing Wikipedia and the presence of only a limited number of active contributors curb the consistent growth of Wikipedia. In this work, we present WikiKreator, a system that is capable of generating content automatically to improve existing stubs on Wikipedia. The system has two components. First, a text classifier built using topic distribution vectors is used to assign content from the web to various sections on a Wikipedia article. Second, we propose a novel abstractive summarization technique based on an optimization framework that generates section-specific summaries for Wikipedia stubs. Experiments show that WikiKreator is capable of generating well-formed informative content. Further, automatically generated content from our system have been appended to Wikipedia stubs and the content has been retained successfully proving the effectiveness of our approach."
      }
    ]
  },
  {
    "tid":"90",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Understanding and Alleviating the Impact of the Flash Address Translation on Solid State Devices.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Flash-based solid state devices (SSDs) have been widely employed in consumer and enterprise storage systems. However, the increasing SSD capacity imposes great pressure on performing efficient logical to physical address translation in a page-level flash translation layer (FTL). Existing schemes usually employ a built-in RAM to store mapping information, called mapping cache, to speed up the address translation. Since only a fraction of the mapping table can be cached due to limited cache space, a large number of extra flash accesses are required for cache management and garbage collection, degrading the performance and lifetime of an SSD. In this paper, we first apply analytical models to investigate the key factors that incur extra flash accesses during address translation. Then, we propose a novel page-level FTL with an efficient translation page-level caching mechanism, named TPFTL, to minimize the extra flash accesses. TPFTL employs a two-level least recently used (LRU) list with space-efficient optimizations to organize cached mapping entries. Inspired by the models, we further design a workload-adaptive loading policy combined with an efficient replacement policy to increase the cache hit rate and reduce the writebacks of replaced dirty entries. Finally, we evaluate TPFTL using extensive trace-driven simulations. Our evaluation results show that compared to the state-of-the-art FTLs, TPFTL significantly reduces the extra operations caused by address translation, achieving reductions on system response time and write amplification by up to 27.1% and 32.2%, respectively."
      }
    ]
  },
  {
    "tid":"91",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Scalable video transcoding for mobile communications' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mobile multimedia contents have been introduced in the market and their demand is growing every day due to the increasing number of mobile devices and the possibility to watch them at any moment in any place. These multimedia contents are delivered over different networks that are visualized in mobile terminals with heterogeneous characteristics. To ensure a continuous high quality it is desirable that this multimedia content can be adapted on-the-fly to the transmission constraints and the characteristics of the mobile devices. In general, video contents are compressed to save storage capacity and to reduce the bandwidth required for its transmission. Therefore, if these compressed video streams were compressed using scalable video coding schemes, they would be able to adapt to those heterogeneous networks and a wide range of terminals. Since the majority of the multimedia contents are compressed using H.264/AVC, they cannot benefit from that scalability. This paper proposes a technique to convert an H.264/AVC bitstream without scalability to a scalable bitstream with temporal scalability as part of a scalable video transcoder for mobile communications. The results show that when our technique is applied, the complexity is reduced by 98 % while maintaining coding efficiency."
      }
    ]
  },
  {
    "tid":"92",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Predicting Outcomes of Nonsmall Cell Lung Cancer Using CT Image Features' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Nonsmall cell lung cancer is a prevalent disease. It is diagnosed and treated with the help of computed tomography (CT) scans. In this paper, we apply radiomics to select 3-D features from CT images of the lung toward providing prognostic information. Focusing on cases of the adenocarcinoma nonsmall cell lung cancer tumor subtype from a larger data set, we show that classifiers can be built to predict survival time. This is the first known result to make such predictions from CT scans of lung cancer. We compare classifiers and feature selection approaches. The best accuracy when predicting survival was 77.5% using a decision tree in a leave-one-out cross validation and was obtained after selecting five features per fold from 219."
      }
    ]
  },
  {
    "tid":"93",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Granular Space Structure on a Micrometric Scale for Industrial Robots' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We study the statistical relationship between angular position and target for industrial robots on a micrometric scale and this leads us to understand the angular position stochastic structure. The concept of granular angular space is introduced and transposed in the Cartesian space. Modeling is based on experimental work performed for a Kuka and a Samsung robot. The influence of workspace location, posture and angular granularity ratio on the Cartesian granular space are then analysed."
      }
    ]
  },
  {
    "tid":"94",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Study Of Cost Effective Scheduling Of Nurses Based On The Domain Transformation Method' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper discusses and analyses the tradeoff between the flexibility afforded with greater number of staff and the implied cost of employing extra staff in the context of the nurse-scheduling problem. If the number of staff is constant, our study allows quantification of the degree of pressure put on the staff resulting from the schedules that do not satisfy their preferences for shift allocation. We present a practical approach, based on our domain transformation methodology that achieves good quality schedules without high computational requirements."
      }
    ]
  },
  {
    "tid":"95",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'New scaling technique for direct mode coding in B pictures' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"To leave the maximum flexibility in encoder to optimize the trade-off between coding performance and complexity, in video coding standards such as H.264 AVC, H.263 and MPEG-4 etc, any number of B pictures and any arrangement of P pictures within a group of pictures (GOP) of arbitrary length are permitted. In addition, multiple reference picture prediction is also permitted in some video coding systems such as H.264/AVC to achieve efficient coding by allowing the encoder to select reference pictures among a large number of coded pictures. Both of the above cases without fixing the temporal distance between forward and backward reference pictures require the division operation for deriving the motion vectors of direct mode, which can efficiently exploit the temporal correlation among pictures and does not require any bits for coding the motion vectors. However, the division is an expensive and undesired operation in video decoder hardware design. Although the H.264/AVC video standard has provided a scaling technique to tackle this problem, unfortunately, its performance is also deteriorated. This paper presents a new scaling technique to both remove the division operation for deriving direct mode motion vectors and efficiently improve the accuracy of derived direct mode motion vectors compared with the scaling technique in H.264/AVC."
      }
    ]
  },
  {
    "tid":"96",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Latency Analysis for Dynamic Spectrum Access in Cognitive Radio: Dedicated or Embedded Control Channel?' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Dynamic spectrum access (DSA) is the key feature of cognitive radio (CR) networks, but it also poses many new challenges on the medium access control (MAC) design. One of key challenges is the fact that the secondary CR users can only borrow the licensed spectrum from the primary users for a short period of time. Hence, unlike many available multi-channel MAC protocols for ad hoc networks where throughput is the main performance issue, the DSA protocols in CR networks shall place more emphases on the access latency. Hence, one fundamental issue arises: how can the spectrum be dimensioned for control channels in order to minimize the access delay of DSA protocol in CR networks? In this paper, we provide a comparative study in an analytical manner on the latency performance of two DSA protocols: 1) dedicated control channel, and 2) embedded dedicated control channel approaches. Our results show that an optimal ratio of the control channel bandwidth over the total channel bandwidth can be found to minimize the latency of DSA with dedicated control channels. However, the delay performance of DSA with dedicated control channels is more sensitive to the variations of the data lengths than that of DSA with embedded control channels. Hence, we conclude that the way of dimensioning the spectrum for control frames for DSA in CR networks should be adaptive to the variations of the traffic characteristics and the number of users."
      }
    ]
  },
  {
    "tid":"97",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimal H\u221e synchronization of general discrete-time delayed chaotic neural networks via dynamic output feedback' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper deals with robust H\u221e synchronization of general discrete-time chaotic neural networks with external disturbance. This general discrete-time model, which is the interconnection of a linear delayed dynamic system and a bounded static nonlinear operator, covers not only several well-known discrete-time delayed neural networks, such as Hopfield neural networks, cellular neural networks (CNNs), bidirectional associative memory (BAM) networks, and recurrent multilayer perceptrons (RMLPs), but also Lur'e systems. Based on Lyapunov stability and H\u221e control theories, dynamic output feedback controllers are established to not only guarantee exponentially stable synchronization of both master and slave systems with time delays, but also reduce the effect of external disturbance to an H\u221e-norm constraint. Furthermore, two classes of optimal controllers are presented, one minimizing the H\u221e-norm bound, the other maximizing the exponential synchronization rate. The control design equations are shown to be a set of linear matrix inequality (LMI) standard problems which can be easily solved by various convex optimization algorithms to determine the optimal H\u221e control laws and the optimal exponential synchronization rates."
      }
    ]
  },
  {
    "tid":"98",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'NetFPGA: reusable router architecture for experimental research' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Our goal is to enable fast prototyping of networking hardware (e.g. modified Ethernet switches and IP routers) for teaching and research. To this end, we built and made available the NetFPGA platform. Starting from open-source reference designs, students and researchers create their designs in Verilog, and then download them to the NetFPGA board where they can process packets at line-rate for 4-ports of 1GE. The board is becoming widely used for teaching and research, and so it has become important to make it easy to re-use modules and designs. We have created a standard interface between modules, making it easier to plug modules together in pipelines, and to create new re-usable designs. In this paper we describe our modular design, and how we have used it to build several systems, including our IP router reference design and some extensions to it."
      }
    ]
  },
  {
    "tid":"99",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Game-Theory-Based Active Defense for Intrusion Detection in Cyber-Physical Embedded Systems.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cyber-Physical Embedded Systems (CPESs) are distributed embedded systems integrated with various actuators and sensors. When it comes to the issue of CPES security, the most significant problem is the security of Embedded Sensor Networks (ESNs). With the continuous growth of ESNs, the security of transferring data from sensors to their destinations has become an important research area. Due to the limitations in power, storage, and processing capabilities, existing security mechanisms for wired or wireless networks cannot apply directly to ESNs. Meanwhile, ESNs are likely to be attacked by different kinds of attacks in industrial scenarios. Therefore, there is a need to develop new techniques or modify the current security mechanisms to overcome these problems. In this article, we focus on Intrusion Detection (ID) techniques and propose a new attack-defense game model to detect malicious nodes using a repeated game approach. As a direct consequence of the game model, attackers and defenders make different strategies to achieve optimal payoffs. Importantly, error detection and missing detection are taken into consideration in Intrusion Detection Systems (IDSs), where a game tree model is introduced to solve this problem. In addition, we analyze and prove the existence of pure Nash equilibrium and mixed Nash equilibrium. Simulations show that the proposed model can both reduce energy consumption by up to 50% compared with the existing All Monitor (AM) model and improve the detection rate by up to 10% to 15% compared with the existing Cluster Head (CH) monitor model."
      }
    ]
  },
  {
    "tid":"100",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The State-of-the-Art Research Progress on Motion Segmentation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Motion segmentation is currently one of the most active research topics in the areas of computer vision and multimedia. It has a wide spectrum of potential applications, including action recognition, visual surveillance, video indexing, video compression, and many other applications. In this paper, we review the recent developments (especially for those in recent five years) of motion segmentation methods and analyze their respective pros and cons. Several valuable research directions are discussed and some suggestions are put forward in the end."
      }
    ]
  },
  {
    "tid":"101",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'RADcast: Enabling Reliability Guarantees for Content Dissemination in Ad Hoc Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper deals with the problem of reliable and fast broadcast of mission-critical data with rich content over ad hoc networks. Existing approaches to dissemination reliability often assume network size knowledge, or that receivers know about the dissemination in advance. Without making similar assumptions, we propose a distinct approach which accommodates the varying reliability needs of applications. We develop the RADcast (reliable application data broadcast) protocol as an integration of two components: (a) Peddler, which ensures that receivers obtain the dissemination metadata, and (b) Pryer, which delivers the actual data to dissemination-aware receivers. We indicate how reliability guarantees/performance tradeoffs can be achieved by a careful instantiation of Peddler and Pryer. We implement RADcast on mobile devices inside a middleware and determine its feasibility. Furthermore, through extensive simulations, we show that RADcast achieves desired reliability in all cases, and performs consistently under varying network conditions and device mobilities. As compared to existing approaches, RADcast either incurs significantly lower latency/message overhead, or reduces latency by 50% with a tradeoff in message overhead."
      }
    ]
  },
  {
    "tid":"102",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Don't worry, be happy: the geography of happiness on Facebook' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Social media sites have been used to track the expression of emotion words across nations. A question left unexplored is whether, at a similar scale, these sites can be used to collect reliable well-being data. To tackle this question, we collect Satisfaction With Life (SWL) test results from a Facebook application and show that aggregate country-level results significantly vary across twelve rich countries and strongly correlate with official well-being scores. To then show that collecting data on Facebook offers an informative look at the sociology of well-being, we study the impact of (un)happiness on the twelve countries by relating the test results on Facebook to reputable international indicators of social and health problems. We find that countries where happiness is lower have increased problems across the board: decreasing well-being is associated with increasing homicide, obesity, drug use, mental illness, and anxiety. In addition to offering these findings, this work hints at the conditions under which social media could be used for data-driven social science research."
      }
    ]
  },
  {
    "tid":"103",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Comparison of SIRT and SQS for regularized weighted least squares image reconstruction' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Tomographic image reconstruction is often formulated as a regularized weighted least squares (RWLS) problem optimized by iterative algorithms that are either inherently algebraic or derived from a statistical point of view. This paper compares a modified version of SIRT (Simultaneous Iterative Reconstruction Technique), which is of the former type, with a version of SQS (Separable Quadratic Surrogates), which is of the latter type. We show that the two algorithms minimize the same criterion function using similar forms of preconditioned gradient descent. We present near-optimal relaxation for both based on eigenvalue bounds and include a heuristic extension for use with ordered subsets. We provide empirical evidence that SIRT and SQS converge at the same rate for all intents and purposes. For context, we compare their performance with an implementation of preconditioned conjugate gradient. The illustrative application is X-ray CT of luggage for aviation security."
      }
    ]
  },
  {
    "tid":"104",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A joint source-channel decoder for H.264 SPS and PPS headers' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes a joint source-channel decoder (JSCD) to decode the Sequence Parameter Set (SPS) and Picture Parameter Set (PPS) headers for the H.264 encoded video sequence. Erroneously received headers can render the entire video sequence un-decodable. By exploiting the syntax and semantics of the headers as well as certain a priori information, our proposed JSCD is capable of reducing the probability of erroneous headers and hence improving the overall PSNR of the video by as high as 3 dB."
      }
    ]
  },
  {
    "tid":"105",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Hip-Knee Control For Gait Assistance With Powered Knee Orthosis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A Powered Knee Orthosis (PKO) was developed for the elderly and patients with disordered gait to regain normal walking. In order to enhance the PKO performance and reduce system complexity especially for people with muscle weakness in their knee joints, an algorithm named HIP-KNEE control is proposed. This algorithm is based on the analysis of kinematic gait model, and the desired knee joint angle (KNEE) is estimated from the measurements of hip joint angle (HIP). The relationship between HIP and KNEE is modeled as a polynomial, which can be easily implemented to an embedded controller for real-time control. This control method is suitable to subjects with good function in hip joint, and it can provide help in walking without special training. An Inertia Measurement Units (IMU) is used for obtaining HIP input, and integrated with a footswitch for checking the heel condition; the gait assistance performance can be further improved."
      }
    ]
  },
  {
    "tid":"106",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Large-Scale Paralleled Sparse Principal Component Analysis.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Principal component analysis (PCA) is a statistical technique commonly used in multivariate data analysis. However, PCA can be difficult to interpret and explain since the principal components (PCs) are linear combinations of the original variables. Sparse PCA (SPCA) aims to balance statistical fidelity and interpretability by approximating sparse PCs whose projections capture the maximal variance of original data. In this paper we present an efficient and paralleled method of SPCA using graphics processing units (GPUs), which can process large blocks of data in parallel. Specifically, we construct parallel implementations of the four optimization formulations of the generalized power method of SPCA (GP-SPCA), one of the most efficient and effective SPCA approaches, on a GPU. The parallel GPU implementation of GP-SPCA (using CUBLAS) is up to eleven times faster than the corresponding CPU implementation (using CBLAS), and up to 107 times faster than a MatLab implementation. Extensive comparative experiments in several real-world datasets confirm that SPCA offers a practical advantage."
      }
    ]
  },
  {
    "tid":"107",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The RNA Ontology RNAO: An ontology for integrating RNA sequence and structure data' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Biomedical Ontologies integrate diverse biomedical data and enable intelligent data-mining and help translate basic research into useful clinical knowledge. We present the RNA Ontology RNAO, an ontology for integrating diverse RNA data, including RNA sequences and sequence alignments, three-dimensional structures, and biochemical and functional data. For example, individual atomic resolution RNA structures have broader significance as representatives of classes of homologous molecules, which can differ significantly in sequence while sharing core structural features and common roles or functions. Thus, structural data gain value by being linked to homologous sequences in genomic data and databases of sequence alignments. Likewise, the value of genomic data is enhanced by annotation of shared structural features, especially when these can be linked to specific functions. Moreover, the significance of biochemical, functional and mutational analyses of RNA molecules are most fully understood when linked to molecular structures and phylogenies. To achieve these goals, RNAO provides logically rigorous definitions of the components of RNA primary, secondary and tertiary structure and the relations between these entities. RNAO is being developed to comply with the developing standards of the Open Biomedical Ontologies OBO Consortium. The RNAO can be accessed at http://code.google.com/p/rnao/."
      }
    ]
  },
  {
    "tid":"108",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Bayesian model search for mixture models based on optimizing variational bounds.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"When learning a mixture model, we suffer from the local optima and model structure determination problems. In this paper, we present a method for simultaneously solving these problems based on the variational Bayesian (VB) framework. First, in the VB framework, we derive an objective function that can simultaneously optimize both model parameter distributions and model structure. Next, focusing on mixture models, we present a deterministic algorithm to approximately optimize the objective function by using the idea of the split and merge operations which we previously proposed within the maximum likelihood framework. Then, we apply the method to mixture of expers (MoE) models to experimentally show that the proposed method can find the optimal number of experts of a MoE while avoiding local maxima."
      }
    ]
  },
  {
    "tid":"109",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'In the Face (book) of Social Learning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"  Social networks have risen to prominence over the last years as the predominant form of electronic interaction between individuals. In an attempt to harness the power of the large user base which they have managed to attract, this study proposes an e-learning prototype which integrates concepts of the social and semantic web. A selected set of services are deployed which have been scientifically proven to positively impact the learning process of users via electronic means. The integrability of these services into a social network platform application is visualized through an exploratory prototype. The Graphical User Interface (GUI) which is developed to implement these key features is in alignment with User-Centered principles. The designed prototype proves that a number of services can be integrated in a user-friendly application and can potentially serve to gain feedback regarding additional aspects that should be included. "
      }
    ]
  },
  {
    "tid":"110",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fast multi reference frame motion estimation for high efficiency video coding' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a fast multi reference frame motion estimation algorithm is proposed to reduce the computational complexity for high efficiency video coding (HEVC). Firstly, according to the reference frame distribution, we define the motion complexity for a frame. Based on the motion complexity, an initial reference frame set (RFS) is constructed to reduce the number of the reference frame for motion estimation. Then for each prediction unit (PU), the average distortion per pixel together with the motion vector difference (MVD) in the first reference frame is employed to shrink the RFS in order to early terminate the motion estimation process. In addition, to enhance the robustness of the proposed scheme, an expansion method for RFS is proposed to guarantee the video quality. Experimental results demonstrate that the proposed scheme can significantly save the encoding time. For Low Delay testing configuration, over 30% of time saving can be achieved on average with ignorable performance loss."
      }
    ]
  },
  {
    "tid":"111",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Adaptive control of the radical servo system of a compact disc player' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The radial servo system of a compact disc player has to cope with large gain variations which are due to disc dependent optical characteristics, tolerances in mechanical and electrical components and nonlinearity in the generation of the position index. In current players this problem has been solved by using an adaptive control scheme based on an injected signal to identify the process gain. This paper discusses the application of the general Self Tuning Regulator (STR) concept to the control problem of the radial servo system. The STR principle depends on parametric identification and control adaptation without additional excitation of the control loop. The concept has been implemented in an experimental arrangement; it gives good control performance and is insensitive to practical perturbations."
      }
    ]
  },
  {
    "tid":"112",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A New Method for Decomposition in the Jacobian of Small Genus Hyperelliptic Curves.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Decomposing a divisor over a suitable factor basis in the Jacobian of a hyperelliptic curve is a crucial step in an index calculus algorithm for the discrete log problem in the Jacobian. For small genus curves, in the year 2000, Gaudry had proposed a suitable factor basis and a decomposition method. In this work, we provide a new method for decomposition over the same factor basis. The advantage of the new method is that it admits a sieving technique which removes smoothness checking of polynomials required in Gaudry\u2019s method. Also, the total number of additions in the Jacobian required by the new method is less than that required by Gaudry\u2019s method. The new method itself is quite simple and we present some example decompositions and timing results of our implementation of the method using Magma."
      }
    ]
  },
  {
    "tid":"113",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Canonical Polyadic Decomposition: From 3-way to N-Way' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Canonical Polyadic (or CANDECOMP/PARAFAC, CP) decompositions are widely applied to analyze high order data, i.e. N-way tensors. Existing CP decomposition methods use alternating least square (ALS) iterations and hence need to compute the inverse of matrices and unfold tensors frequently, which are very time consuming for large-scale data and when N is large. Fortunately, once at least one factor has been correctly estimated, all the remaining factors can be computed efficiently and uniquely by using a series of rank-one approximations. Motivated by this fact, to perform a full N-way CP decomposition, we run 3-way CP decompositions on a sub-tensor to estimate two factors first. Then the remaining factors are estimated via an efficient Khatri-Rao product recovering procedure. In this way the whole ALS iterations with respect to each mode are avoided and the efficiency can be significantly improved. Simulations show that, compared with ALS based CP decomposition methods, the proposed method is more efficient and is easier to escape from local solutions for high order tensors."
      }
    ]
  },
  {
    "tid":"114",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Extending the Balance Theory by Measuring Bidirectional Opinions with Interactive Language.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The signs of social relationship in balance theory is objective and undirected which cannot characterize participants' bidirectional subjective opinions on social relationships. By redefining the signs with the bidirectional opinions according to homogeneity principle, we extend the balance theory by measuring the opinions with interactive language. Initial experimental results on Enron emails shows that the extended model supports the homogeneity principle of balance theory and make it possible to use balance theory to predict the directed opinions besides undirected signs."
      }
    ]
  },
  {
    "tid":"115",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'CARAT: a toolkit for design and performance analysis of component-based embedded systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Solid frameworks and toolkits for design and analysis of embedded systems are of high importance, since they enable early reasoning about critical properties of a system. This paper presents a software toolkit that supports the design and performance analysis of real-time component-based software architectures deployed on heterogeneous multi-processor platforms. The tooling environment contains a set of integrated tools for (a) component storage and retrieval, (b) graphics-based design of software and hardware architectures, (c) performance analysis of the designed architectures and, (d) automated code generation. The cornerstone of the toolkit is a performance analysis framework that automates composition of the individual component models into a system executable model, allows simulation of the system model and gives design-time predictions of key performance properties like response time, data throughput, and usage of hardware resources. We illustrate the efficiency of this toolkit on a Car Radio Navigation benchmark system."
      }
    ]
  },
  {
    "tid":"116",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Applying SCRUM in an OSS Development Process: An Empirical Evaluation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Open Source Software development often resembles Agile models. In this paper, we report about our experience in using SCRUM for the development of an Open Source Software Java tool. With this work, we aim at answering the following research questions: 1) is it possible to switch successfully to the SCRUM methodology in an ongoing Open Source Software development process? 2) is it possible to apply SCRUM when the developers are geographically distributed? 3) does SCRUM help improve the quality of the product and the productivity of the process? We answer to these questions by identifying a set of measures and by comparing the data we collected before and after the introduction of SCRUM. The results seem to show that SCRUM can be introduced and used in an ongoing geographically distributed Open Source Software process and that it helps control the development process better."
      }
    ]
  },
  {
    "tid":"117",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Cookieless Monster: Exploring the Ecosystem of Web-Based Device Fingerprinting' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The web has become an essential part of our society and is currently the main medium of information delivery. Billions of users browse the web on a daily basis, and there are single websites that have reached over one billion user accounts. In this environment, the ability to track users and their online habits can be very lucrative for advertising companies, yet very intrusive for the privacy of users. In this paper, we examine how web-based device fingerprinting currently works on the Internet. By analyzing the code of three popular browser-fingerprinting code providers, we reveal the techniques that allow websites to track users without the need of client-side identifiers. Among these techniques, we show how current commercial fingerprinting approaches use questionable practices, such as the circumvention of HTTP proxies to discover a user's real IP address and the installation of intrusive browser plugins. At the same time, we show how fragile the browser ecosystem is against fingerprinting through the use of novel browser-identifying techniques. With so many different vendors involved in browser development, we demonstrate how one can use diversions in the browsers' implementation to distinguish successfully not only the browser-family, but also specific major and minor versions. Browser extensions that help users spoof the user-agent of their browsers are also evaluated. We show that current commercial approaches can bypass the extensions, and, in addition, take advantage of their shortcomings by using them as additional fingerprinting features."
      }
    ]
  },
  {
    "tid":"118",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Estimation of the epipole using optical flow at antipodal points' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper develops an algorithm for estimating the epipole or direction of translation of a moving monocular observer. To this end, we use constraints arising from two points that are antipodal on the image sphere. The antipodal point condition is necessary for decoupling rotation from translation. One such pair of points constrains the epipole to lie on a plane, and using two pairs of points, we have two such planes. The intersection of these two planes gives an estimate of the epipole. This means we require image motion measurements at two pairs of antipodal points to obtain an estimate. Repeating this will yield a set of possible solutions and a variety of methods could be applied to obtain a robust and refined estimate from this set. One robust and simple method is chosen for illustrative purposes and results on real images are shown. With real sequences, results of below 2deg error in the estimate of the epipole can be obtained. Since antipodal points on an image sphere are required, this algorithm must use some kind of omnidirectional or large field-of-view (FOV) sensor."
      }
    ]
  },
  {
    "tid":"119",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fine-grained dynamic voltage scaling on OLED display' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Organic Light Emitting Diode (OLED) has emerged as the new generation display technique for mobile multimedia devices. Compared to existing technologies OLEDs are thinner, brighter, lighter, and cheaper. However, OLED panels are still the biggest contributor to the total power consumption of mobile devices. In this work, we proposed a fine-grained dynamic voltage scaling (FDVS) technique to reduce the OLED power. An OLED panel is partitioned into multiple display areas of which the supply voltage is adaptively adjusted based on the displayed content. A DVS-friendly OLED driver design is also proposed to enhance the color accuracy of the OLED pixels at the scaled supply voltage. Our experimental results show that compared to the existing global DVS technique, FDVS technique can achieve 25.9%~43.1% more OLED power saving while maintaining a high image quality measured by Structural Similarity Index (SSIM=0.98). The further analysis shows shat FDVS technology can also effectively reduce the color remapping cost when color compensation is required to improve the image quality of an OLED panel working at a scaled voltage."
      }
    ]
  },
  {
    "tid":"120",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A combined stochastic programming and optimal control approach to personal finance and pensions' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We combine a dynamic programming approach (stochastic optimal control) with a multi-stage stochastic programming approach (MSP) in order to solve various problems in personal finance and pensions. Both optimization methods are integrated into one MSP formulation, making it possible to achieve a solution within a short computational time. The solution takes into account the entire lifetime of an individual, while focusing on practical constraints, such as limits on portfolio composition, limits on the sum insured, inclusion of transaction costs, and taxes on capital gains, during the first years of a contract. Two applications are considered: (A) optimal investment, consumption and sum insured for an individual maximizing the expected utility of consumption and bequest, and (B) optimal investment for a pension saver who wishes to maximize the expected utility of retirement benefits. Numerical results show that among the considered practical constraints, the presence of taxes affects the optimal controls the most. Furthermore, the individual\u2019s preferences, such as impatience level and risk aversion, have even a higher impact on the controlled processes than the taxes on capital gains."
      }
    ]
  },
  {
    "tid":"121",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'TCP improvement in multi-radio multi-channel multi-hop networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we seek to enhance the poor performance of original TCP in wireless multi-hop environments due to the intra-flow contention between TCP-DATA and TCP-ACK packets. Assuming multi-radio multi-channel networks, where each station is equipped with multiple radios and the same number of orthogonal channels, we propose to use virtually different paths with different channel assignments for TCP-DATA and TCP-ACK transmissions. Simulations show that TCP performance can be improved significantly since TCP-DATA and TCP-ACK paths hardly interfere with each other."
      }
    ]
  },
  {
    "tid":"122",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Low complexity track initialization from a small set of non-invertible measurements' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Target tracking from non-invertible measurement sets, for example, incomplete spherical coordinates measured by asynchronous sensors in a sensor network, is a task of data fusion present in a lot of applications. Difficulties in tracking using extended Kalman filters lead to unstable behavior, mainly caused by poor initialization. Instead of using high complexity numerical batch-estimators, we offer an analytical approach to initialize the filter from a minimum number of observations. This directly pertains to multihypothesis tracking (MHT), where in the presence of clutter and/or multiple targets (i) low complexity algorithms are desirable and (ii) using a small set of measurements avoids the combinatorial explosion. Our approach uses no numerical optimization, simply evaluating several equations to find the state estimates. This is possible since we avoid an over-determined setup by initializing only from the minimum necessary subset of measurements. Loss in accuracy is minimized by choosing the best subset using an optimality criterion and incorporating the leftover measurements afterwards. Additionally, we provide the possibility to estimate only sub-sets of parameters, and to reliably model the resulting added uncertainties by the covariance matrix. We compare two different implementations, differing in the approximation of the posterior: linearizing the measurement equation as in the extended Kalman filter (EKF) or employing the unscented transform (UT). The approach will be studied in two practical examples: 3D track initialization using bearingsonly measurements or using slant-range and azimuth only."
      }
    ]
  },
  {
    "tid":"123",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A new outlier detection method based on anti-sparse representations.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Detecting outliers plays a significant role in many areas such as statistics, machine learning, data-mining, etc. Depending on the importance of accurate outlier detection, they have been investigated extensively in the mentioned fields with many different type of algorithms and approaches. One of the popular one is distance based outlier detection approaches, mostly based on nearest neighbor search. In this study, anti-sparse representation is used for approximate nearest neighbor search. In order to deal with some mathematical difficulties of obtaining anti-sparse vector, a new smooth objective function is presented for l(infinity) (or maximum)-norm minimization problem. The obtained anti-sparse vector is binarized and used for embedded approximate nearest neighbor search for outlier detection. Performance of the new anti-sparse representation based outlier detection method is investigated on a real network data set by detecting denial-of-service (DoS) attacks."
      }
    ]
  },
  {
    "tid":"124",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A novel reduced parameter s-model of estimator learning automata in the switching non-stationary environment' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Learning automata (LA), a powerful tool for reinforcement learning in the field of machine learning, could explore its optimal state by continuously interacting with an external environment. Generally, the traditional LA algorithms, especially estimator LA algorithms, can be ultimately abstracted out as P- or Q-models, which are simply located in the stationary environments. A more comprehensive consideration would be S-model operating in the non-stationary environment. For this specific LA, presently the most popular achievement belongs to stochastic estimator LA (SELA). However, synchronously handing four parameters involved in SELA is an intractable job, as these parameters may vary dramatically in values under different environments, making it essential to develop a strategy for parameter tuning. In this paper, we first propose a scheme to determine the parameter searching scope and subsequently present a series of parameter searching methods, including a four-dimensional method and a two-dimensional method, making SELA applicable for any environment with switching non-stationary characteristics. Furthermore, to decrease the tuning cost, a reduced parameter SELA supported by the new two-dimensional parameter searching method emerges. And to break the traditional limit that the environmental reward probability must be symmetrically distributed, the S-model is constructed from a new perspective, thus forming a novel reduced parameter S-model of SELA (rpS-SELA). A detailed mathematical proof theoretically reveals the absolute expediency of rpS-SELA. In addition, it is demonstrated by experimental simulations that rpS-SELA outperforms others with a reduced tuning cost, a minor time consumption, a higher accuracy rate, and above all, a stronger tracking ability to the environmental switches."
      }
    ]
  },
  {
    "tid":"125",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Data Stream Processing Infrastructure For Intelligent Transport Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Intelligence Transportation Systems are critical to improve the efficiency of modern transportation. A system that is flexible and powerful enough to handle diverse demands from a large user base, is still elusive. Studies have shown that developing and integrating the various components constitute a significant portion of the capital cost and complexity of such systems.In this paper, we present a stream processing infrastructure we call System S. System S enables the deployment of large scale applications. It supports a mechanism for sharing data sources, software components, and even intermediate results allowing a reduction in the cost of software integration, and ownership.We experiment the stream processing infrastructure with a Fleet Management Center, and demonstrate how the infrastructure can be used to address unique issues in traffic management."
      }
    ]
  },
  {
    "tid":"126",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Adaptive Selection of Partition Size for Supercomputer Requests' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we show how application scheduling can be used to reduce the turn-around time of supercomputer jobs. Our approach focuses on the use of SA, an AppLeS application scheduler, to adaptively craft the request to be submitted to the supercomputer based on the current state of the system. We demonstrate that SA significantly improves a job's turn-around time in a variety of scenarios. We also identify how the state of the system, the characteristics of the job, and the quality of the information made available to SA influence its performance."
      }
    ]
  },
  {
    "tid":"127",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimizing spectrum trading in cognitive mesh network using machine learning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In a cognitive wireless mesh network, licensed users (primary users, PUs) may rent surplus spectrum to unlicensed users (secondary users, SUs) for getting some revenue. For such spectrumsharing paradigm, maximizing the revenue is the key objective of the PUs while that of the SUs is to meet their requirements. These complex contradicting objectives are embedded in our reinforcement learning (RL) model that is developed and implemented as shown in this paper. The objective function is defined as the net revenue gained by PUs from renting some of their spectrum. RL is used to extract the optimal control policy that maximizes the PUs' profit continuously over time. The extracted policy is used by PUs to manage renting the spectrum to SUs and it helps PUs to adapt to the changing network conditions. Performance evaluation of the proposed spectrum trading approach shows that it is able to find the optimal size and price of spectrum for each primary user under different conditions. Moreover, the approach constitutes a framework for studying, synthesizing and optimizing other schemes. Another contribution is proposing a new distributed algorithm to manage spectrum sharing among PUs. In our scheme, PUs exchange channels dynamically based on the availability of neighbor's idle channels. In our cooperative scheme, the objective of spectrum sharing is to maximize the total revenue and utilize spectrum efficiently. Compared to the poverty-line heuristic that does not consider the availability of unused spectrum, our scheme has the advantage of utilizing spectrum efficiently."
      }
    ]
  },
  {
    "tid":"128",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Common-Sense Rule Inference' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In the paper we show how rule-based inference can be made more flexible by exploiting semantic information associated with the concepts involved in the rules. We introduce flexible forms of common sense reasoning in which whenever no rule applies to a given situation, the inference engine can fire rules that apply to more general or to similar situations. This can be obtained by defining new forms of match between rules and the facts in the working memory and new forms of conflict resolution. We claim that in this way we can overcome some of the brittleness problems that are common in rule-based systems."
      }
    ]
  },
  {
    "tid":"129",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mining Top-K Graph Patterns That Jointly Maximize Some Significance Measure' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Most of graph pattern mining algorithms focus on finding frequent subgraphs and its compact representations, such as closed frequent subgraphs and maximal frequent subgraphs. However, little attention has been paid to mining graph patterns with user-specified significance measure. In this paper, we study a new problem of mining top-k graph patterns that jointly maximize some significance measure from graph databases. Exploiting entropy and information gain, we give two problem formulations, EM and IGM. We first prove them to be NP-hard and then propose two efficient algorithms, PP-TopK and DM-TopK, to solve them. PP-TopK greedily selects top-k graph patterns among frequent subgraphs. DM-TopK integrates the pruning techniques into the mining framework, and directly mines top-k graph patterns from graph databases. Empirical results demonstrate the quality of our top-k graph patterns, and validate the efficiency and scalability of our algorithms."
      }
    ]
  },
  {
    "tid":"130",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On Creating Industry-Wide Reference Architectures' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The discontent over the limited capability of Web services to reflect the organization\u2019s business model, and efficiently adhere to business strategies and requirements, caused a shift in business thinking. As a result, the enterprise has been recognizing ..."
      }
    ]
  },
  {
    "tid":"131",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Information System for Real-Time Online Interactive Applications' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The edutain@grid European project [1] is developing a support platform for deployment, management and execution of Real-Time Online Interactive Applications (ROIA) on Grid. In this paper we present an information system designed by the edutain@grid project which provides support for ROIA deployment and monitoring, and offers a generic frontend for ROIA-specific optimisations. We conduct a variety of experiments that justify various decisions of our design, and investigate the performance and scalability of our system with respect to various types of queries."
      }
    ]
  },
  {
    "tid":"132",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Hierarchical extreme learning machine based image denoising network for visual Internet of Things.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In the visual Internet of Things (VIoT), imaging sensors must achieve a balance between limited bandwidth and useful information when images contain heavy noise. In this paper, we address the problem of removing heavy noise and propose a novel hierarchical extreme learning machine-based image denoising network, which comprises a sparse auto-encoder and a supervised regression. Due to the fast training of a hierarchical extreme learning machine, an effective image denoising system that is robust for various noise levels can be trained more efficiently than other denoising methods, using a deep neural network. Our proposed framework also contains a non-local aggregation procedure that aims to fine-tune noise reduction according to structural similarity. Compared to the compression ratio in noisy images, the compression ratio of denoised images can be dramatically improved. Therefore, the method can achieve a low communication cost for data interactions in the VIoT. Experimental studies on images, including both hand-written digits and natural scenes, have demonstrated that the proposed technique achieves excellent performance in suppressing heavy noise. Further, it greatly reduces the training time, and outperforms other state-of-the-art approaches in terms of denoising indexes for the peak signal-to-noise ratio (PSNR) or the structural similarity index (SSIM)."
      }
    ]
  },
  {
    "tid":"133",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Change Priority Determination in IT Service Management Based on Risk Exposure' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In the Change Management process within IT Service Management, some activities need to evaluate the risk exposure associated with changes to be made to the infrastructure and services. The paper presents a method to evaluate risk exposure associated with a change. Further, we show how to use the risk exposure metric to automatically assign priorities to changes. The formal model developed for this purpose captures the business perspective by using financial metrics in the evaluation of risk. Thus the method is an example of Business-Driven IT Management. A case study, performed in conjunction with a large IT service provider, is reported and provides good results when compared to decisions made by human managers."
      }
    ]
  },
  {
    "tid":"134",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Novel clock synchronization algorithm of parametric difference for parallel and distributed simulations' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Clock synchronization is crucial in distributed simulation applications (DSAs). Traditional synchronization methods are mainly based on computers' physical clocks and have network delays. Consequently, the clock jumps easily, and the stability of the algorithm is less than excellent. Furthermore, a new class of DSAs with flexible and dynamically assembled simulation components to optimize the performance of clock clusters is emerging. To remedy the problems, we propose a novel logic parametric difference clock (PDC), which is recursively constructed by an alterable parametric difference frequency (PDF) and its counter. We also develop a simple and efficient synchronization algorithm of parametric difference (SAPD), in which the indeterminate network delay is considered in the PDF, and the PDC counter is used as the synchronous symbol. Compared with existing typical algorithms, the SAPD has the predominant advantages of convergence, stability, and precision for DSAs in local area networks."
      }
    ]
  },
  {
    "tid":"135",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Crowd-Empowered Privacy-Preserving Data Aggregation for Mobile Crowdsensing.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We develop an auction framework for privacy-preserving data aggregation in mobile crowdsensing, where the platform plays the role as an auctioneer to recruit workers for a sensing task. In this framework, the workers are allowed to report privacy-preserving versions of their data to protect their data privacy; and the platform selects workers based on their sensing capabilities, which aims to address the drawbacks of game-theoretic models that cannot ensure the accuracy level of the aggregated result, due to the existence of multiple Nash Equilibria. Observe that in this auction based framework, there exists externalities among workers' data privacy, because the data privacy of each worker depends on both her injected noise and the total noise in the aggregated result that is intimately related to which workers are selected to fulfill the task. To achieve a desirable accuracy level of the data aggregation in a cost-effective manner, we explicitly characterize the externalities, i.e., the impact of the noise added by each worker on both the data privacy and the accuracy of the aggregated result. Further, we explore the problem structure, characterize the hidden monotonicity property of the problem, and determine the critical bid of workers, which makes it possible to design a truthful, individually rational and computationally efficient incentive mechanism. The proposed incentive mechanism can recruit a set of workers to approximately minimize the cost of purchasing private sensing data from workers subject to the accuracy requirement of the aggregated result. We validate the proposed scheme through theoretical analysis as well as extensive simulations.\n\n"
      }
    ]
  },
  {
    "tid":"136",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Artificial noise-aided biobjective transmitter optimization for service integration in multi-user MIMO broadcast channel.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper considers an artificial noise (AN)-aided transmit design for multi-user MIMO systems with integrated services. Specifically, two sorts of service messages are combined and served simultaneously: one multicast message intended for all receivers and one confidential message intended for only one receiver and required to be perfectly secure from other unauthorized receivers. Our interest lies in the joint design of input covariances of the multicast message, confidential message, and artificial noise (AN), such that the achievable secrecy rate and multicast rate are simultaneously maximized. This problem is identified as a secrecy rate region maximization (SRRM) problem in the context of physical-layer service integration. Since this biobjective optimization problem is inherently complex to solve, we put forward two different scalarization methods to convert it into a scalar optimization problem. First, we propose to prefix the multicast rate as a constant, and accordingly, the primal biobjective problem is converted into a secrecy rate maximization (SRM) problem with quality of multicast service (QoMS) constraint. By varying the constant, we can obtain different Pareto optimal points. The resulting SRM problem can be iteratively solved via a provably convergent difference-of-concave (DC) algorithm. In the second method, we aim to maximize the weighted sum of the secrecy rate and the multicast rate. Through varying the weighted vector, one can also obtain different Pareto optimal points. We show that this weighted sum rate maximization (WSRM) problem can be recast into a primal decomposable form, which is amenable to alternating optimization (AO). Then, we compare these two scalarization methods in terms of their overall performance and computational complexity via theoretical analysis as well as numerical simulation, based on which new insights can be drawn."
      }
    ]
  },
  {
    "tid":"137",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Energy efficiency optimization in hardware-constrained large-scale MIMO systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Large-scale multiple-input multiple-output (MIMO) communication systems can bring substantial improvement in spectral efficiency and/or energy efficiency, due to the excessive degrees-of-freedom and huge array gain. However, large-scale MIMO is expected to deploy lower-cost radio frequency (RF) components, which are particularly prone to hardware impairments. Unfortunately, compensation schemes are not able to remove the impact of hardware impairments completely, such that a certain amount of residual impairments always exists. In this paper, we investigate the impact of residual transmit RF impairments (RTRI) on the spectral and energy efficiency of training-based point-to-point large-scale MIMO systems, and seek to determine the optimal training length and number of antennas which maximize the energy efficiency. We derive deterministic equivalents of the signal-to-noise-and-interference ratio (SINR) with zero-forcing (ZF) receivers, as well as the corresponding spectral and energy efficiency, which are shown to be accurate even for small number of antennas. Through an iterative sequential optimization, we find that the optimal training length of systems with RTRI can be smaller compared to ideal hardware systems in the moderate SNR regime, while larger in the high SNR regime. Moreover, it is observed that RTRI can significantly decrease the optimal number of transmit and receive antennas."
      }
    ]
  },
  {
    "tid":"138",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Seizure Prediction In Intracranial Eeg: A Patient-Specific Rule-Based Approach' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this study, we report our development of a patient-specific rule-based seizure prediction system. Five univariate and one bivariate nonlinear measures were extracted from non-overlapping 10-second segments of intracranial EEG (iEEG) data recorded using both depth electrodes in the brain and subdural electrodes over the cortical surface. Nonlinear features representing the specific characteristic properties of EEG signal were then integrated spatio-temporally in a way to predict to predict seizure with high sensitivity. The present system was tested on 58 hours of iEEG data containing ten seizures recorded in two patients with medically intractable focal epilepsy. Within a prediction horizon of 30 and 60 minutes, our method showed an average sensitivity of 90% and 96.5% with an average false prediction rate of 0.06/h and 0.055/h, respectively. The present results suggest that such a rule-based system can become potentially a useful approach for predicting seizures prior to onset."
      }
    ]
  },
  {
    "tid":"139",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The MOLEN Processor Prototype' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present a prototype design of the MOLEN polymorphic processor, a CCM based on the co-processor architectural paradigm. The Xilinx Virtex II Pro technology is used as a prototyping platform. Experimental results prove the viability of the MOLEN concept. More precisely, the MPEG-2 application is accelerated very closely to its theoretical limits by implementing SAD, DCT and IDCT in reconfigurable hardware. The MPEG-2 encoder overall speedup is in the range between 2.80 and 2.96. The speedup of the MPEG-2 decoder varies between 1.56 and 1.63."
      }
    ]
  },
  {
    "tid":"140",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Learning artistic lighting template from portrait photographs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a method for learning artistic portrait lighting template from a dataset of artistic and daily portrait photographs. The learned template can be used for (1) classification of artistic and daily portrait photographs, and (2) numerical aesthetic quality assessment of these photographs in lighting usage. For learning the template, we adopt Haar-like local lighting contrast features, which are then extracted from pre-defined areas on frontal faces, and selected to form a log-linear model using a stepwise feature pursuit algorithm. Our learned template corresponds well to some typical studio styles of portrait photography. With the template, the classification and assessment tasks are achieved under probability ratio test formulations. On our dataset composed of 350 artistic and 500 daily photographs, we achieve a 89.5% classification accuracy in cross-validated tests, and the assessment model assigns reasonable numerical scores based on portraits' aesthetic quality in lighting."
      }
    ]
  },
  {
    "tid":"141",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Personalized music emotion recognition via model adaptation.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In the music information retrieval (MIR) research, developing a computational model that comprehends the affective content of music signal and utilizes such a model to organize music collections have been an essential topic. Emotion perception in music is in nature subjective. Consequently, building a general emotion recognition system that performs equally well for every user could be insufficient. In contrast, it would be more desirable for one's personal computer/device being able to understand his/her perception of music emotion. In our previous work, we have developed the acoustic emotion Gaussians (AEG) model, which can learn the broad emotion perception of music from general users. Such a general music emotion model, called the background AEG model in this paper, can recognize the perceived emotion of unseen music from a general point of view. In this paper, we go one step further to realize the personalized music emotion modeling by adapting the background AEG model with a limited number of emotion annotations provided by a target user in an online and dynamic fashion. A novel maximum a posteriori (MAP)-based algorithm is proposed to achieve this in a probabilistic framework. We carry out quantitative evaluations on a well-known emotion annotated corpus, MER60, to validate the effectiveness of the proposed method for personalized music emotion recognition."
      }
    ]
  },
  {
    "tid":"142",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Using profiling to reduce branch misprediction costs on a dynamically scheduled processor' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Modern dynamically scheduled processors use branch prediction hardware to speculatively fetch and execute most likely executed paths in a program. Complex branch predictors have been proposed which attempt to identify these paths accurately such that the hardware can benefit from out-of-order (OOO) execution. Recent studies have shown that inspite of such complex prediction schemes, there still exist many frequently executed branches which are difficult to predict. Predicated execution has been proposed as an alternative technique to eliminate some of these branches in various forms ranging from a restrictive support to a full-blown support. We call the restrictive form of predicated execution as guarded execution.In this paper, we propose a new algorithm which uses profiling and selectively performs if-conversion for architectures with guarded execution support. Branch profiling is used to gather the taken, non-taken and misprediction counts for every branch. This combined with block profiling is used to select paths which suffer from heavy mispredictions and are profitable to if-convert. Effects of three different selection criterias, namely size-based, predictability-based and profiled-based, on net cycle improvements, branch mispredictions and mis-speculated instructions are then studied. We also propose new mechanisms to convert unsafe instructions to safe form to enhance the applicability of the technique. Finally, we explain numerous adjustments that were made to the selection criterias to better reflect the OOO processor behavior."
      }
    ]
  },
  {
    "tid":"143",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Saliency modeling from image histograms' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We proposed a computational visual saliency modeling technique. The proposed technique makes use of a color co-occurrence histogram (CCH) that captures not only \"how many\" but also \"where and how\" image pixels are composed into a visually perceivable image. Hence the CCH encodes image saliency information that is usually perceived as the discontinuity between an image region or object and its surrounding. The proposed technique has a number of distinctive characteristics: It is fast, discriminative, tolerant to image scale variation, and involves minimal parameter tuning. Experiments over benchmarking datasets show that it predicts fixational eye tracking points accurately and a superior AUC of 71.25 is obtained."
      }
    ]
  },
  {
    "tid":"144",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Design Automation of Cyber-Physical Systems: Challenges, Advances, and Opportunities.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A cyber-physical system (CPS) is an integration of computation with physical processes whose behavior is defined by both computational and physical parts of the system. In this paper, we present a view of the challenges and opportunities for design automation of CPS. We identify a combination of characteristics that define the challenges unique to the design automation of CPS. We then present sele..."
      }
    ]
  },
  {
    "tid":"145",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Automatic Detection Of 2d And 3d Lung Nodules In Chest Spiral Ct Scans' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Automatic detection of lung nodules is an important problem in computer analysis of chest radiographs. In this paper, we propose a novel algorithm for isolating lung abnormalities (nodules) from spiral chest low-dose CT (LDCT) scans. The proposed algorithm consists of three main steps. The first step isolates the lung nodules, arteries, veins, bronchi, and bronchioles from the surrounding anatomical structures. The second step detects lung nodules using deformable 3D and 2D templates describing typical geometry and gray-level distribution within the nodules of the same type. The detection combines the normalized cross-correlation template matching and a genetic optimization algorithm. The final step eliminates the false positive nodules (FPNs) using three features that robustly define the true lung nodules. Experiments with 200 CT data sets show that the proposed approach provided comparable results with respect to the experts."
      }
    ]
  },
  {
    "tid":"146",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Outage Probability of Single-Carrier Cooperative Spectrum Sharing Systems with Decode-and-Forward Relaying and Selection Combining' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"For cyclic prefixed single-carrier (CP-SC) spectrum sharing relaying systems, a two-hop decode-and-forward (DF) relaying protocol with a direct link and selection combining are employed in the secondary user relay network. For this cooperative CP-SC spectrum sharing system, the end-to-end signal-to-noise ratio (e2e-SNR) is first derived, and then the outage probability performance of the secondary user relaying system is investigated. Having derived an asymptotic expression for the cumulative distribution function of the e2e-SNR, the asymptotic outage diversity is obtained under a limited maximum transmit power at the secondary source and relay while satisfying the maximum allowable interference at the primary user. Notably, when the maximum allowable interference is independent of a limited transmission power, an outage probability floor is observed. Moreover, under the assumption of an unlimited transmit power at the secondary nodes, the asymptotic outage diversity is derived as a function of the interference. It can be seen that the same outage diversity gain can be achieved as in the non-spectrum-sharing CP-SC relaying system. Analytically derived asymptotic outage diversity gains for limited and unlimited transmission power cases are verified by Monte Carlo simulations."
      }
    ]
  },
  {
    "tid":"147",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Data harvesting with mobile elements in wireless sensor networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In recent studies, using mobile elements (MEs) as mechanical carriers to relay data has been shown to be an effective way of prolonging sensor network life time and relaying information in partitioned networks. As the data generation rates of sensors may vary, some sensors need to be visited more frequently than others. In this paper, a partitioning-based algorithm is presented that schedules the movements of MEs in a sensor network such that there is no data loss due to buffer overflow. Simulation results show that the proposed Partitioning-Based Scheduling (PBS) algorithm performs well in terms of reducing the minimum required ME speed to prevent data loss, providing high predictability in inter-visit durations, and minimizing the data loss rate for the cases when the ME is constrained to move slower than the minimum required ME speed."
      }
    ]
  },
  {
    "tid":"148",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Forensic sensor pattern noise extraction from large image data set' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The sensor pattern noise (SPN) can be regarded as the unique identity of a digital camera which is highly useful in digital image forensics [1, 2]. Existing methods [1, 2] which works by denoising each individual natural image often took an investigator a long time and great efforts to collect sufficient photos of diversified enough natural scenes. These processes are hard to repeat or standardized for officially using by an authority. In this work, we create noise image data set by taking photos of random noises displayed on a high definition monitor and propose a homomorphic based SPN extraction method. It offers the forensic researcher a fast way to create a large image data set in a few minutes. And the extraction method only needs to denoise once, which is highly efficient to deal with large numbers of photos. We compared the source camera identification performance of the proposed SPN extraction method to a prior state-of-art with identical experimental settings. The experimental results confirm the effectiveness of the proposed method."
      }
    ]
  },
  {
    "tid":"149",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Goal-Based Modeling of Dynamically Adaptive System Requirements' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Self-adaptation is emerging as an increasingly important capability for many applications, particularly those deployed in dynamically changing environments, such as ecosystem monitoring and disaster management. One key challenge posed by Dynamically Adaptive Systems (DASs) is the need to handle changes to the requirements and corresponding behavior of a DAS in response to varying environmental conditions. Berry et al. previously identified four levels of RE that should be performed for a DAS. In this paper, we propose the Levels of RE for Modeling that reify the original levels to describe RE modeling work done by DAS developers. Specifically, we identify four types of developers: the system developer, the adaptation scenario developer, the adaptation infrastructure developer, and the DAS research community. Each level corresponds to the work of a different type of developer to construct goal model(s) specifying their requirements. We then leverage the Levels of RE for Modeling to propose two complementary processes for performing RE for a DAS. We describe our experiences with applying this approach to GridStix, an adaptive flood warning system, deployed to monitor the River Ribble in Yorkshire, England."
      }
    ]
  },
  {
    "tid":"150",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'WESD--Weighted Spectral Distance for Measuring Shape Dissimilarity' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a new distance for measuring shape dissimilarity between objects. Recent publications introduced the use of eigenvalues of the Laplace operator as compact shape descriptors. Here, we revisit the eigenvalues to define a proper distance, called Weighted Spectral Distance (WESD), for quantifying shape dissimilarity. The definition of WESD is derived through analyzing the heat trace. This analysis provides the proposed distance with an intuitive meaning and mathematically links it to the intrinsic geometry of objects. We analyze the resulting distance definition, present and prove its important theoretical properties. Some of these properties include: 1) WESD is defined over the entire sequence of eigenvalues yet it is guaranteed to converge, 2) it is a pseudometric, 3) it is accurately approximated with a finite number of eigenvalues, and 4) it can be mapped to the ([0,1)) interval. Last, experiments conducted on synthetic and real objects are presented. These experiments highlight the practical benefits of WESD for applications in vision and medical image analysis."
      }
    ]
  },
  {
    "tid":"151",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Generalized Spatial Modulation With Transmit Antenna Grouping For Correlated Channels' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, an effective generalized spatial modulation (GenSM) scheme with transmit antenna grouping is proposed to overcome the performance degradation caused by correlated channels. In the proposed scheme, the transmit antennas are divided into several equal-sized groups, and spatial modulation (SM) is carried out to select one active antenna in each group independently. It is quite different from the conventional GenSM which jointly selects active antenna set. Apart from the straightforward block grouping method, which collects the adjacent antennas to the same group, interleaved grouping is also introduced. It can maximize the average distance between the antennas in the same group, since the channel correlation depends on it. To evaluate the performance, a closed-form expression of the average bit error probability (ABEP) upper bound is derived for all proposed grouping methods and Monte-Carlo simulations are conducted to verify the analysis and reveal the performance gain of the proposed scheme in terms of bit error rate (BER) in comparison with conventional GenSM and SM."
      }
    ]
  },
  {
    "tid":"152",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Deriving reproducible biomarkers from multi-site resting-state data: An Autism-based example.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Resting-state functional Magnetic Resonance Imaging (R-fMRI) holds the promise to reveal functional biomarkers of neuropsychiatric disorders. However, extracting such biomarkers is challenging for complex multi-faceted neuropathologies, such as autism spectrum disorders. Large multi-site datasets increase sample sizes to compensate for this complexity, at the cost of uncontrolled heterogeneity. This heterogeneity raises new challenges, akin to those face in realistic diagnostic applications. Here, we demonstrate the feasibility of inter-site classification of neuropsychiatric status, with an application to the Autism Brain Imaging Data Exchange (ABIDE) database, a large (N=871) multi-site autism dataset. For this purpose, we investigate pipelines that extract the most predictive biomarkers from the data. These R-fMRI pipelines build participant-specific connectomes from functionally-defined brain areas. Connectomes are then compared across participants to learn patterns of connectivity that differentiate typical controls from individuals with autism. We predict this neuropsychiatric status for participants from the same acquisition sites or different, unseen, ones. Good choices of methods for the various steps of the pipeline lead to 67% prediction accuracy on the full ABIDE data, which is significantly better than previously reported results. We perform extensive validation on multiple subsets of the data defined by different inclusion criteria. These enables detailed analysis of the factors contributing to successful connectome-based prediction. First, prediction accuracy improves as we include more subjects, up to the maximum amount of subjects available. Second, the definition of functional brain areas is of paramount importance for biomarker discovery: brain areas extracted from large R-fMRI datasets outperform reference atlases in the classification tasks."
      }
    ]
  },
  {
    "tid":"153",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fuzzy connectedness image segmentation for newborn brain extraction in MR images.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Newborn's brain has a various shape, and easily changes with not only brain developing and cerebral diseases. Although the brain segmentation in MR images is an effective way to quantify the brain shape and size, there are few studies in neonatal brain MR image analysis. This paper introduces a novel method based on fuzzy connectedness (FC) with fuzzy object model (FOM). FOM is built from a training dataset, and gives fuzzy degree belonging to parenchyma with respect to location and intensity. FC is calculated from object affinity and homogeneous affinity, and the object affinity is given by the FOM. The method first segments the white matter, and then segments the surrounding cortex. The propose method has been applied to 10 newborn subjects whose revised age was between -1 month and +2 month. Leave-on-out cross-validation (LOOCV) was conducted, and the mean false-positive volume fraction was 1.33%, the mean false-negative volume fraction was 2.90%, and geometric-mean was 1.42%."
      }
    ]
  },
  {
    "tid":"154",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Towards a classification of energy aware MAC protocols for wireless sensor networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Power management is an important issue in wireless sensor networks (WSNs) because wireless sensor nodes are usually battery powered, and an efficient use of the available battery power becomes an important concern specially for those applications where the system is expected to operate for long durations. This necessity for energy efficient operation of a WSN has prompted the development of new protocols in all layers of the communication stack. Provided that, the radio transceiver is the most power consuming component of a typical sensor node, large gains can be achieved at the link layer where the medium access control (MAC) protocol controls the usage of the radio transceiver unit. MAC protocols for sensor networks differ greatly from typical wireless networks access protocols in many issues. MAC protocols for sensor networks must have built-in power conservation, mobility management, and failure recovery strategies. Furthermore, sensor MAC protocols should make performance trade-off between latency and throughput for a reduction in energy consumption to maximize the lifetime of the network. This is in general achieved through duty cycling the radio transceiver. Many MAC protocols with different objectives were proposed for wireless sensor networks in the literature. Most of these protocols take into account the energy efficiency as a main objective. There is much more innovative work should be done at the MAC layer to address the hard unsolved problems. In this paper, we first outline and discuss the specific requirements and design trade-offs of a typical wireless sensor MAC protocol by describing the properties of WSN that affect the design of MAC layer protocols. Then, a typical collection of wireless sensor MAC protocols presented in the literature are surveyed, classified, and described emphasizing their advantages and disadvantages whenever possible. Finally, we present research directions and identify open issues for future medium access research. Copyright \u00a9 2009 John Wiley & Sons, Ltd."
      }
    ]
  },
  {
    "tid":"155",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Joint Optimization of Scaling and Placement of Virtual Network Services.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The management of complex network services requires flexible and efficient service provisioning as well as optimized handling of continuous changes in the workload of the services. To adapt to changes in the demand, service components need to be replicated (scaling) and allocated to physical resources (placement) dynamically. In this paper, we propose a fully automated approach to the joint optimization problem of scaling and placement, enabling quick reaction to changes. We formalize the problem, analyze its complexity, and develop two algorithms to solve it. Empirical results show the applicability and effectiveness of the proposed approach."
      }
    ]
  },
  {
    "tid":"156",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Design and Implementation of a Distributed Crawler and Filtering Processor' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Web crawlers are the key component of services running on Internet and providing searching and indexing support for the entire Web, for corporate Intranets and large portal sites. More recently, crawlers have also been used as tools to conduct focused Web searches and to gather data about the characteristics of the WWW. In this paper, we study the employment of crawlers as a programmable, scalable, and distributed component in future Internet middleware infrastructures and proxy services. In particular, we present the architecture and implementation of, and experimentation withWebRACE, a high-performance, distributedWeb crawler, filtering server and object cache. We address the challenge of designing and implementing modular, open, distributed, and scalable crawlers, using Java. We describe our design and implementation decisions, and various optimizations. We discuss the advantages and disadvantages of using Java to implement the WebRACE-crawler, and present an evaluation of its performance. WebRACE is designed in the context of eRACE, an extensible Retrieval Annotation Caching Engine, which collects, annotates and disseminates information from heterogeneous Internet sources and protocols, according to XML-encoded user profiles that determine the urgency and relevance of collected information."
      }
    ]
  },
  {
    "tid":"157",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Neural mechanism underlying impaired visual judgement in the dysmetabolic brain: an fMRI study.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"An altered brain metabolism in the parietal and prefrontal regions of the cerebral cortex as well as cognitive alterations have been found in patients suffering from hepatic encephalopathy. The neural mechanisms underlying these metabolically induced cognitive alterations, however, are not known. Since patients with liver cirrhosis without clinically overt encephalopathy already show an impaired performance in a flicker light test, the aim of this study was to analyze the normal and pathologically impaired neural mechanisms of these patients using functional magnetic resonance imaging (fMRI). Nine subjects at the early stage of encephalopathy [nonmanifest hepatic encephalopathy (nmHE)] and ten controls underwent scanning, while they indicated the apparent transition from a steady light to the onset of a flicker light, that is, the critical flicker frequency (CFF). Judgement-related blood oxygenation level-dependent (BOLD) activation was decreased in nmHE compared to controls in the right inferior parietal cortex (IPL). Furthermore, the analysis of psychophysiologic interaction suggests impaired neural interaction in patients with nmHE, especially between the IPL and the parietooccipital cortex (Poc), the intraparietal sulcus, the anterior cingulate cortex (ACC), the right prefrontal cortex (PFC), the medial temporal lobe, and the extrastriate cortex V5. In contrast, nonmanifest patients revealed an enhanced coupling between IPL and the postcentral cortex. Our findings provide evidence of an early-impaired and compensatory neural mechanism during visual judgement already in the earliest stages of hepatic encephalopathy and suggest an aberrant coupling between cerebral regions in the dysmetabolic brain."
      }
    ]
  },
  {
    "tid":"158",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Physical-layer intrusion detection for wireless networks using compressed sensing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The broadcast nature of wireless networks has been widely exploited by adversaries in order to cause severe denial-of-service attacks. Several algorithms are proposed in the literature for the detection and mitigation of such attacks at the physical and medium access layers. In this work, we combine recent advances in compressed sensing theory, along with a cumulative-sum anomaly-based algorithm, for the detection of physical-layer attacks. The algorithm considers a metric based on the Signal-to-Interference-plus-Noise-Ratio (SINR). Compressed sensing makes feasible the use of far fewer SINR measurements for effective intrusion detection. The performance evaluation based on real experimental data shows that attacks are detected with high accuracy using a small number of measurements."
      }
    ]
  },
  {
    "tid":"159",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'CoReJava: Learning Functions Expressed as Object-Oriented Programs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Proposed and implemented is the language CoReJava (constraint optimization regression in Java), which extends the programming language Java with regression analysis, i.e., the capability to do parameter estimation for a function. CoReJava is unique in that functional forms for regression analysis are expressed as first-class citizens, i.e., as Java programs, in which some parameters are not a priori known, but need to be learned from training sets provided as input. Typical applications of CoReJava include calibration of parameters of computational processes, described as OO programs. To implement regression learning, the CoReJava compiler (1) analyses the structure of the parameterized Java program that represent a functional form, (2) automatically generates a constraint optimization problem, in which constraint variables are the unknown parameters, and the objective function to be minimized is the sum of squares of errors w.r.t. the training set, and (3) solves the optimization problem using an external non-linear optimization solver. CoReJava then executes as a regular Java program, in which the initially unknown parameters are replaced with the found optimal values. CoReJava syntax and semantics are formally defined and exemplified using a simple supply chain example."
      }
    ]
  },
  {
    "tid":"160",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Towards Context-Aware Workflow Management for Ubiquitous Computing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Ubiquitous computing is a user-centric distributed computing paradigm, allowing users to access to their preferred services even while moving around. To make such a vision a reality, context-aware workflow management is one of key issues because the context of ubiquitous applications is highly varying. In this paper, we propose a context model for intelligent campus navigation applications, then present a context-adaptive workflow management algorithm which can dynamically adjust workflow execution policies in terms of current context information. Moreover, we model the workflow management algorithm in Petri nets. Our context model and workflow management algorithm may be easily extended to other ubiquitous applications with a little change on context structure."
      }
    ]
  }
]