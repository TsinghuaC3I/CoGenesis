[
  {
    "tid":"0",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Reporting Intersecting Pairs of Polytopes in Two and Three Dimensions\nContent: Let P = {P1, ..., Pm} be a set of m convex polytopes in Rd, for d = 2, 3, with a total of n vertices. We present output-sensitive algorithms for reporting all k pairs of indices (i, j) such that Pi intersects Pj. For the planar case we describe a simple algorithm with running time O(n4/3 log n + k), and an improved randomized algorithm with expected running time O((n log m + k)\u03b1(n) log n) (which is faster for small values of k). For d = 3, we present an O(n8/5+\u0190 + k)-time algorithm, for any \u0190 0. Our algorithms can be modified to count the number of intersecting pairs in O(n4/3 logO(1) n) time for the planar case, and in O(n8/5+\u0190) time and R3.\n\nTitle: Approximate range searching using binary space partitions\nContent: We show how any BSP tree TP for the endpoints of a set of n disjoint segments in the plane can be used to obtain a BSP tree of size O(n\u00a2depth(TP)) for the segments themselves, such that the range-searching e-ciency remains almost the same. We apply this technique to obtain a BSP tree of size O(nlogn) such that \"-approximate range searching queries with any constant-complexity convex query range can be answered in O(min\">0f(1=\")+k\"glogn) time, where k\" is the number of segments intersecting the \"-extended range. The same result can be obtained for disjoint constant-complexity curves, if we allow the BSP to use splitting curves along the given curves. We also describe how to construct a linear-size BSP tree for low-density scenes consisting of n objects in Rd such that \"-approximate range searching with any constant-complexity convex query range can be done in O(logn + min\">0f(1=\"d\u00a11) + k\"g) time. Finally we show how to adapt our structures so that they become I/O-e-cient.\n\nTitle: Reporting intersecting pairs of convex polytopes in two and three dimensions\nContent: Let P = {P1....,Pm) be a set of m convex polytopes in Rd, for d = 2, 3, with a total of n vertices. We present output-sensitive algorithms for reporting all k pairs of indices (i, j) such that Pi intersects Pj. For the planar case we describe a simple algorithm with running time O(n4/3 log2+\u03b5 n + k), for any constant \u03b5 0, and an improved randomized algorithm with expected running time O((n logm + k)\u03b1(n)logn) (which is faster for small values of k). For d = 3, we present an O(n8/5+\u03b5 + k)-time algorithm, for any \u03b5 0. Our algorithms can be modified to count the number of intersecting pairs in O(n4/3 log2+\u03b5 n) time for the planar case, and in O(n8/5+\u03b5) time for the three-dimensional case.\n\nTitle: Efficient c-oriented range searching with DOP-trees\nContent: A c-dop is a c-oriented convex polytope, that is, a convex polytope whose facets have orientations that come from a fixed set of c (undirected) orientations. In this paper we study dop-trees-bounding-volume hierarchies that use c-dops as bounding volumes-in the plane. We prove that for any set S of n disjoint c-dops in the plane, one can construct a dop-tree such that all kdops in S that intersect any given query c-dop can be retrieved in O(n^1^/^2^+^@e+k) time in the worst case, when c and @e are constant. This is optimal up to the factor O(n^@e). The same query time can be achieved when the c-dops do not intersect too heavily, that is, when any point in the plane is contained in only a constant number of c-dops. When the c-dops in S may intersect arbitrarily, the worst-case query time becomes O(n^1^-^1^/^c+k), which is optimal.\n\nTitle: An intersection-sensitive algorithm for snap rounding\nContent: Snap rounding is a method for converting arbitrary-precision arrangements of segments into fixed-precision representation. We present an algorithm for snap rounding with running time O((n+I)logn), where I is the number of intersections between the input segments. In the worst case, our algorithm is an order of magnitude more efficient than the best previously known algorithms. We also propose a variant of the traditional snap-rounding scheme. The new method has all the desirable properties of traditional snap rounding and, in addition, guarantees that the rounded arrangement does not have degree-2 vertices in the interior of edges. This simplified rounded arrangement can also be computed in O((n+I)logn) time.\n\n## Task\nCompose an abstract for the title 'Finding Pairwise Intersections Inside a Query Range.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We study the following problem: preprocess a set \\(\\mathcal {O}\\) of objects into a data structure that allows us to efficiently report all pairs of objects from \\(\\mathcal {O}\\) that intersect inside an axis-aligned query range \\({Q}\\). We present data structures of size \\(O(n\\cdot {{\\mathrm{polylog\\,}}}n)\\) and with query time \\(O((k+1)\\cdot {{\\mathrm{polylog\\,}}}n)\\) time, where k is the number of reported pairs, for two classes of objects in \\({\\mathbb R}^2\\): axis-aligned rectangles and objects with small union complexity. For the 3-dimensional case where the objects and the query range are axis-aligned boxes in\u00a0\\({\\mathbb R}^3\\), we present a data structure of size \\(O(n\\sqrt{n}\\cdot {{\\mathrm{polylog\\,}}}n)\\) and query time \\(O((\\sqrt{n}+k)\\cdot {{\\mathrm{polylog\\,}}}n)\\). When the objects and query are fat, we obtain \\(O((k+1)\\cdot {{\\mathrm{polylog\\,}}}n)\\) query time using \\(O(n\\cdot {{\\mathrm{polylog\\,}}}n)\\) storage."
      }
    ]
  },
  {
    "tid":"1",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Distributed Multicell and Multiantenna Precoding: Characterization and Performance Evaluation\nContent: This paper considers downlink multiantenna communication with base stations that perform cooperative precoding in a distributed fashion. Most previous work in the area has assumed that transmitters have common knowledge of both data symbols of all users and full or partial channel state information (CSI). Herein, we assume that each base station only has local CSI, either instantaneous or statistical. For the case of instantaneous CSI, a parametrization of the beamforming vectors used to achieve the outer boundary of the achievable rate region is obtained for two multi-antenna transmitters and two single-antenna receivers. Distributed generalizations of classical beamforming approaches that satisfy this parametrization are provided, and it is shown how the distributed precoding design can be improved using the so-called virtual SINR framework [1]. Conceptually analog results for both the parametrization and the beamforming design are derived in the case of local statistical CSI. Heuristics on the distributed power allocation are provided in both cases, and the performance is illustrated numerically.\n\nTitle: Cooperative MIMO Precoding with Distributed CSI: A Hierarchical Approach\nContent: The problem of network multiple-input multiple-output precoding under distributed channel state information is a notoriously challenging question, for which optimal solutions with reasonable complexity remain elusive. In this context, we assess the value of hierarchical information exchange, whereby an order is established among the transmitters (TXs) in such a way that a given TX has access not only to its local channel estimate but also to the estimates available at the less informed TXs. Assuming regularized zero forcing (RZF) precoding at the TXs, we propose naive, locally robust, and globally robust suboptimal strategies for the joint precoding design. Numerical results show that hierarchical information exchange brings significant performance gains, with the locally and globally robust algorithms performing remarkably close to the optimal RZF strategy. Lastly, the cost of hierarchical information exchange relative to the cooperation gain is examined and the optimal tradeoff is numerically evaluated.\n\nTitle: Distributed Multicell-MISO Precoding Using the Layered Virtual SINR Framework\nContent: In this letter, we address the problem of distributed multi-antenna cooperative transmission in a cellular system. Most research in this area has so far assumed that base stations not only have the data dedicated to all the users but also share the full channel state information (CSI). In what follows, we assume that each base station (BS) only has local CSI knowledge. We propose a suboptimal, yet efficient, way in which the multicell MISO precoders may be designed at each BS in a distributed manner, as a superposition of so-called virtual SINR maximizations: a virtual SINR maximizing transmission scheme yields Pareto optimal rates for the MISO Interference Channel (IC); its extension to the multicell MISO channel is shown to provide a distributed precoding scheme achieving a certain fairness optimality for the two link case. We illustrate the performance of our algorithm through Monte Carlo simulations.\n\nTitle: Distributed Beamforming Coordination In Multicell Mimo Channels\nContent: Coordination in a multi-cell/link environment has been attracting a lot of attention in the research community recently. In this paper, we consider the problem of coordinated beamforming where base stations (BS) equipped with multiple antennas attempt to serve a separate user each despite the interference generated by the other bases. We propose a framework for a distributed optimization of the beamformers at each base, where distributed is defined as using \"local CSIT\" only. We present and compare two distributed approaches (one iterative and another direct approach) which have in common the optimization of the beamformers: as a combination of so-called egoistic and altruistic solutions for this problem. We provide the intuitions behind these approaches and some theoretical grounds for optimality in certain cases. Performance is finally illustrated through numerical simulations.\n\nTitle: Coordinated Shared Spectrum Precoding with Distributed CSIT\nContent: In this paper, the operation of a licensed shared access system is investigated, considering downlink communication. The system comprises a multiple-input-single-output (MISO) incumbent transmitter (TX)\u2013receiver (RX) pair, which offers a spectrum sharing opportunity to a MISO licensee TX-RX pair. Our main contribution is the design of a coordinated transmission scheme, inspired by the underlay cognitive radio (CR) approach, with the aim of maximizing the average rate of the licensee, subject to an average rate constraint for the incumbent. In contrast to most prior works on the underlay CR, the coordination of the two TXs takes place under a realistic channel state information (CSI) scenario, where each TX has solely access to the instantaneous direct channel of its served terminal. Such a CSI knowledge setting brings about a formulation based on the theory of Team Decisions, whereby the TXs aim at optimizing a common objective given the same constraint set, on the basis of individual channel information. Consequently, a novel set of applicable precoding schemes consisting in letting the two TXs cooperate on the basis of the statistical information is proposed. We verify by simulations that this novel, practically relevant, coordinated precoding scheme outperforms the standard underlay CR approach.\n\n## Task\nCompose an abstract for the title 'Cooperative Multicell Precoding: Rate Region Characterization and Distributed Strategies With Instantaneous and Statistical CSI' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Base station cooperation is an attractive way of increasing the spectral efficiency in multiantenna communication. By serving each terminal through several base stations in a given area, intercell interference can be coordinated and higher performance achieved, especially for terminals at cell edges. Most previous work in the area has assumed that base stations have common knowledge of both data dedicated to all terminals and full or partial channel state information (CSI) of all links. Herein, we analyze the case of distributed cooperation where each base station has only local CSI, either instantaneous or statistical. In the case of instantaneous CSI, the beamforming vectors that can attain the outer boundary of the achievable rate region are characterized for an arbitrary number of multi antenna transmitters and single-antenna receivers. This characterization only requires local CSI and justifies distributed precoding design based on a novel virtual signal-to-interference noise ratio (SINR) framework, which can handle an arbitrary SNR and achieves the optimal multiplexing gain. The local power allocation between terminals is solved heuristically. Conceptually, analogous results for the achievable rate region characterization and precoding design are derived in the case of local statistical CSI. The benefits of distributed cooperative transmission are illustrated numerically, and it is shown that most of the performance with centralized cooperation can be obtained using only local CSI."
      }
    ]
  },
  {
    "tid":"2",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Small-time scaling behavior of Internet backbone traffic\nContent: We perform an extensive wavelet analysis of Internet backbone traffic traces to observe and understand the causes of small-time scaling phenomena present in them. We observe that for a majority of the traces, the second-order scaling exponents at small time scales (1-100 ms) are fairly close to 0.5, indicating that traffic fluctuations at these time scales are nearly uncorrelated. Some traces, however, do exhibit moderately large scaling exponents (\u22480.7) at small time scales. In addition, the traces manifest mostly monofractal behaviors at small time scales. To identify the network causes of the observed scaling behavior, we analyze the flow composition of the traffic along two dimensions--flow byte contribution and flow density. Our study points to the dense flows (i.e., flows with densely clustered packets) as the correlation-causing factor in small time scales, and reveals that the traffic composition in terms of proportions of dense vs. sparse flows plays a major role in influencing the small-time scalings of aggregate traffic. Since queuing inside routers is influenced by traffic fluctuations at small time-scales, our observations and results have important implications for networking modeling, service provisioning and traffic engineering.\n\nTitle: Profiling internet backbone traffic: behavior models and applications\nContent: Recent spates of cyber-attacks and frequent emergence of applications affecting Internet traffic dynamics have made it imperative to develop effective techniques that can extract, and make sense of, significant communication patterns from Internet traffic data for use in network operations and security management. In this paper, we present a general methodology for building comprehensive behavior profiles of Internet backbone traffic in terms of communication patterns of end-hosts and services. Relying on data mining and information-theoretic techniques, the methodology consists of significant cluster extraction, automatic behavior classification and structural modeling for in-depth interpretive analyses. We validate the methodology using data sets from the core of the Internet. The results demonstrate that it indeed can identify common traffic profiles as well as anomalous behavior patterns that are of interest to network operators and security analysts.\n\nTitle: Quantile sampling for practical delay monitoring in Internet backbone networks\nContent: Point-to-point delay is an important network performance measure as it captures service degradations caused by various events. We study how to measure and report delay in a concise and meaningful way for an ISP, and how to monitor it efficiently. We analyze various measurement intervals and potential metric definitions. We find that reporting high quantiles (between 0.95 and 0.99) every 10-30min as the most effective way to summarize the delay in an ISP. We then propose an active probing scheme to estimate a high quantile with bounded error. We show that only a small number of probes are sufficient to provide an accurate estimate. We validate the proposed delay monitoring technique on real data collected on the Sprint IP backbone network. To make our work complete, we lastly compare the overhead of our active probing technique with a passive sampling scheme and show that for delay measurement, active probing is more practical.\n\nTitle: A first look at inter-data center traffic characteristics via Yahoo! datasets\nContent: Effectively managing multiple data centers and their traffic dynamics pose many challenges to their operators, as little is known about the characteristics of inter-data center (D2D) traffic. In this paper we present a first study of D2D traffic characteristics using the anonymized NetFlow datasets collected at the border routers of five major Yahoo! data centers. Our contributions are mainly two-fold: i) we develop novel heuristics to infer the Yahoo! IP addresses and localize their locations from the anonymized NetFlow datasets, and ii) we study and analyze both D2D and client traffic characteristics and the correlations between these two types of traffic. Our study reveals that Yahoo! uses a hierarchical way of deploying data centers, with several satellite data centers distributed in other countries and backbone data centers distributed in US locations. For Yahoo! US data centers, we separate the client-triggered D2D traffic and background D2D traffic from the aggregate D2D traffic using port based correlation, and study their respective characteristics. Our findings shed light on the interplay of multiple data centers and their traffic dynamics within a large content provider, and provide insights to data center designers and operators as well as researchers.\n\nTitle: On Properties of Internet Exchange Points and Their Impact on AS Topology and Relationship\nContent: Internet eXchange Points (IXPs) are one of two primary methods for Autonomous Systems (ASes) to interconnect with each other for exchanging traffic and for global Internet reachability. This paper explores the properties of IXPs and their impact on the AS topology and AS business relations using Scriptroute and Skitter traceroute probes, BGP routing archives and other data. With these datasets we develop an algorithm to discover IXPs and infer ASes that participate at these IXPs. Using the discovered IXPs and their inferred AS participants, we analyze and characterize the properties of IXPs and their participants such as size, geographical locations. We also investigate the impact of IXPs on the global AS topology and business relations between ASes. Our study sheds light on the Internet interconnection practices and the evolution of the Internet, in particular, the potential role IXPs play in such evolution.\n\n## Task\nCompose an abstract for the title 'Small-time scaling behaviors of Internet backbone traffic: an empirical study' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The small-time (sub-seconds) scaling behaviors of Internet backbone traffic, based on traces collected from OC3/12/48 links in a tier-1 ISP is studied. We observe that for a majority of these traces, the (second-order) scaling exponents at small time scales (1 ms - 100 ms) are fairly close to 0.5, indicating that traffic fluctuations at these time scales are (nearly) uncorrelated. In addition, the traces manifest mostly monofractal behaviors at small time scales. The objective of the paper is to understand the potential causes or factors that influence the small-time scalings of Internet backbone traffic via empirical data analysis. We analyze the traffic composition of the traces along two dimensions - flow size and flow density. Our study uncovers dense flows (i.e., flows with bursts of densely clustered packets) as the correlation-causing factor in small time scales, and reveals that the traffic composition in terms of proportions of dense vs. sparse flows plays a major role in influencing the small-time scalings of aggregate traffic."
      }
    ]
  },
  {
    "tid":"3",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Optimal Routing in Gossip Networks\nContent: In this paper, we introduce the Gossip Network model where travelers can obtain information about the state of dynamic networks by gossiping with peer travelers using ad hoc communication. Travelers then use the gossip information to recourse their path and find the shortest path to their destination. We study optimal routing in stochastic, time-independent gossip networks, and demonstrate that an optimal routing policy may direct travelers to make detours to gather information. A dynamic programming equation that produces the optimal policy for routing in gossip networks is presented. In general, the dynamic programming algorithm is intractable; however, for two special cases a polynomial optimal solution is presented. We show that ordinarily gossiping helps travelers decrease their expected path cost. However, in some scenarios, depending on the network parameters, gossiping could increase the expected path cost. The parameters that determine the effect of gossiping on the path costs are identified and their influence is analyzed. This dependency is fairly complex and was confirmed numerically on grid networks.\n\nTitle: A Model Of Internet Topology Using K-Shell Decomposition\nContent: We study a map of the Internet (at the autonomous systems level), by introducing and using the method of k-shell decomposition and the methods of percolation theory and f ractal geometry, to find a model for the structure of the Internet. In particular, our analysis uses information on the connectivity of the network shells to separate, in a unique (no parameters) way, the Internet into three subcomponents: (i) a nucleus that is a small (approximate to 100 nodes), very well connected globally distributed subgraph; (ii) a fractal subcomponent that is able to connect the bulk of the Internet without congesting the nucleus, with self-similar properties and critical exponents predicted from percolation theory; and (iii) dendrite-like structures, usually isolated nodes that are connected to the rest of the network through the nucleus only. We show that our method of decomposition is robust and provides insight into the underlying structure of the Internet and its functional consequences. Our approach of decomposing the network is general and also useful when studying other complex networks.\n\nTitle: An Active Network Approach to Efficient Network Management\nContent: Active networks is a framework where network elements, primarily routers and switches, are programmable. Programs that are injected into the network are executed by the network elements to achieve higher flexibility and to present new capabilities. This work describes a novel active network architecture which primarily addresses the management challenges of modern complex networks. Its primary component is an active engine that is attached to any IP router to form an active node. The active engine we designed and implemented executes programs that arrive from the network and monitors and controls the router actions. The design is based on standards (Java, SNMP, ANEP over UDP), and can be easily deployed in todays IP networks. The contribution of this paper is the introduction of novel architectural features such as: isolation of the active mechanism, the session concept, the ability of active session to control non-active packets, and blind addressing. Implementing these ideas, we built a system that enables the safe execution and rapid deployment of new distributed management applications in the network layer. This system can be gradually integrated in todays IP networks, and allows smooth migration from IP to active networking.\n\nTitle: Multipath routing slice experiments in federated testbeds\nContent: The Internet today consist of many heterogeneous infrastructures, owned and maintained by separate and potentially competing administrative authorities. On top of this a wide variety of applications has different requirements with regard to quality, reliability and security from the underlying networks. The number of stakeholders who participate in provisioning of network and services is growing. More demanding applications (like eGovernment, eHealth, critical and emergency infrastructures) are on the rise. Therefore we assume that these two basic characteristics, a) multiple authorities and b) applications with very diverse demands, are likely to stay or even increase in the Internet of the future. In such an environment federation and virtualization of resources are key features that should be supported in a future Internet. The ability to form slices across domains that meet application specific requirements enables many of the desired features in future networks. In this paper, we present a Multipath Routing Slice experiment that we performed over multiple federated testbeds. We combined capabilities from different experimental facilities, since one single testbed did not offer all the required capabilities. This paper summarizes the conducted experiment, our experience with the usability of federated testbeds and our experience with the use of advanced measurement technologies within experimental facilities. We believe that this experiment provides a good example use case for the future Internet itself because we assume that the Internet will consist of multiple different infrastructures that have to be combined in application specific overlays or routing slices, very much like the experimental facilities we used in this experiment. We also assume that the growing demands will push towards a much better measurement instrumentation of the future Internet. The tools used in our experiment can provide a starting point for this.\n\nTitle: Centralized and distributed algorithms for routing and weighted max-min fair bandwidth allocation\nContent: Given a set of demands between pairs of nodes, we examine the traffic engineering problem of flow routing and fair bandwidth allocation where flows can be split to multiple paths (e.g., MPLS tunnels). This paper presents an algorithm for finding an optimal and global per-commodity max-min fair rate vector in a polynomial number of steps. In addition, we present a fast and novel distributed algorithm where each source router can find the routing and the fair rate allocation for its commodities while keeping the locally optimal max-min fair allocation criteria. The distributed algorithm is a fully polynomial epsilon-approximation (FPTAS) algorithm and is based on a primal-dual alternation technique. We implemented these algorithms to demonstrate its correctness, efficiency, and accuracy.\n\n## Task\nCompose an abstract for the title 'Routing through networks with hierarchical topology aggregation' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In the future, global networks will consist of a hierarchy ofsubnetworks called domains. For reasons of both scalability andsecurity, domains will not reveal details of their internalstructure to outside nodes. Instead, these domains will advertiseonly a summary, or aggregated view, of their internal structure,e.g., as proposed by the ATM PNNI standard.This work compares, by simulation, the performance of severaldifferent aggregation schemes in terms of network throughput (thefraction of attempted connections that are realized), and networkcontrol load (the average number of crankbacks per realizedconnection). The simulation emulate a connection oriented networkwith a PNNI-like hierarchical source routing algorithm.Our main results are: (1) minimum spanning tree is a goodaggregation scheme; (2) exponential link cost functions performbetter than min-hop routing; (3) our suggested logarithmic updatescheme that determine when re-aggregation should be computed cansignificantly reduce the computational overhead due tore-aggregation with a negligible decrease in performance."
      }
    ]
  },
  {
    "tid":"4",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Approximate Frequent Itemset Mining for streaming data on FPGA\nContent: Frequent Itemset Mining (FIM) is designed to find frequently occurring itemsets among a series of transactions. It is extremely memory and time expensive. Frequent Itemset Mining from a Data Stream (FIM-DS) is even more challenging since storing the infinite data to memory is infeasible. In recent years, researchers have proposed various approximation algorithms for FIM-DS. However, the computation complexity is still high, and these methods are difficult to be accelerated using hardware accelerators. In this paper, we propose a Space-Saving based approximate algorithm for FIM-DS. It avoids exponential candidates generation and comparisons. We realize a hardware accelerator design and implement it on an FPGA platform. Experimental results show that our algorithm in software implementation achieves up to 8.4\u00d7 speedup for transactions with small item database, and our hardware accelerator achieves up to 50,000\u00d7 speedup for transactions with small number of items, and 5.3\u00d7 speedup for transactions with extremely large number of items.\n\nTitle: Exploiting Stable Data Dependency in Stream Processing Acceleration on FPGAs\nContent: With the unique feature of fine-grained parallelism, field-programmable gate arrays (FPGAs) show great potential for streaming algorithm acceleration. However, the lack of a design framework, restrictions on FPGAs, and ineffective tools impede the utilization of FPGAs in practice. In this study, we provide a design paradigm to support streaming algorithm acceleration on FPGAs. We first propose an abstract model to describe streaming algorithms with homogeneous sub-functions (HSF) and stable data dependency (SDD), which we call the HSF-SDD model. Using this model, we then develop an FPGA framework, PE-Ring, that has the advantages of (1) fully exploiting algorithm parallelism to achieve high performance, (2) leveraging block RAM to serve large scale parameters, and (3) enabling flexible parameter adjustments. Based on the proposed model and framework, we finally implement a specific converter to generate the register-transfer level representation of the PE-Ring. Experimental results show that our method outperforms ordinary FPGA design tools by one to two orders of magnitude. Experiments also demonstrate the scalability of the PE-Ring.\n\nTitle: FPMR: MapReduce framework on FPGA\nContent: Machine learning and data mining are gaining increasing attentions of the computing society. FPGA provides a highly parallel, low power, and flexible hardware platform for this domain, while the difficulty of programming FPGA greatly limits its prevalence. MapReduce is a parallel programming framework that could easily utilize inherent parallelism in algorithms. In this paper, we describe FPMR, a MapReduce framework on FPGA, which provides programming abstraction, hardware architecture, and basic building blocks to developers. An on-chip processor scheduler is implemented to maximize the utilization of computation resources and achieve better load balancing. An efficient data access scheme is carefully designed to maximize data reuse and throughput. Meanwhile, the FPMR framework hides the task control, synchronization, and communication away from designers so that more attention can be paid to the application itself. A case study of RankBoost acceleration based on FPMR demonstrates that FPMR efficiently helps with the development productivity; and the speedup is 31.8x versus CPU-based implementation. This performance is comparable to a fully manually designed version, which achieves 33.5x speedup. Two other applications: SVM, PageRank are also discussed to show the generalization of the framework.\n\nTitle: ForeGraph: Exploring Large-scale Graph Processing on Multi-FPGA Architecture.\nContent: The performance of large-scale graph processing suffers from challenges including poor locality, lack of scalability, random access pattern, and heavy data conflicts. Some characteristics of FPGA make it a promising solution to accelerate various applications. For example, on-chip block RAMs can provide high throughput for random data access. However, large-scale processing on a single FPGA chip is constrained by limited on-chip memory resources and off-chip bandwidth. Using a multi-FPGA architecture may alleviate these problems to some extent, while the data partitioning and communication schemes should be considered to ensure the locality and reduce data conflicts. In this paper, we propose ForeGraph, a large-scale graph processing framework based on the multi-FPGA architecture. In ForeGraph, each FPGA board only stores a partition of the entire graph in off-chip memory. Communication over partitions is reduced. Vertices and edges are sequentially loaded onto the FPGA chip and processed. Under our scheduling scheme, each FPGA chip performs graph processing in parallel without conflicts. We also analyze the impact of system parameters on the performance of ForeGraph. Our experimental results on Xilinx Virtex UltraScale XCVU190 chip show ForeGraph outperforms state-of-the-art FPGA-based large-scale graph processing systems by 4.54x when executing PageRank on the Twitter graph (1.4 billion edges). The average throughput is over 900 MTEPS in our design and 2.03x larger than previous work.\n\nTitle: FPGA-based acceleration of neural network for ranking in web search engine with a streaming architecture\nContent: Web search engine companies are intensively running learning to rank algorithms to improve the search relevance. Neural network (NN)-based approaches, such as LambdaRank, can significantly increase the ranking quality. While, their training is very slow on a single computer and inherent coarse-grained parallelism could be hardly utilized by computer clusters. Thus an efficient implementation is necessary to timely generate acceptable NN models on frequently updated training datasets. This paper presents our work in accelerator. A SIMD streaming architecture is proposed to i) efficiently map the query-level NN computation and data structure to FPGA, ii) fully exploit the inherent fine-grained parallelism, and iii) provide scalability to large scale datasets. The accelerator shows up to 17.9X speedup over the software implementation on datasets from a commercial search engine.\n\n## Task\nCompose an abstract for the title 'A self-aware data compression system on FPGA in Hadoop' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"With the exponential growth of data size, data storage and analysis have been exposed to more challenges due to the lack of disk capacity and the limited network bandwidth. Data compression technique provides a good solution to mitigate these effects. In this paper, we propose a self-aware data compression system on FPGA for typical data warehousing, such as Hive, with column stored data and multi-threading requirements. The hardware accelerators can change the degree and hierarchy of parallelism depending on the data to be compressed (during the runtime). We test the system performance on a Xilinx VC707 FPGA board and the experimental results show that, up to 16 3-parallelism accelerators can be implemented and the throughput could be improved up to 432 MB/s. It is 6.25X speedup compared with the software solution under the same number of threads."
      }
    ]
  },
  {
    "tid":"5",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Refactoring of Ontologies: Improving the Design of Ontological Models with Concept Analysis\nContent: It is now widely accepted that in order to optimize both their usage and their design and maintenance ontologies should comply to design quality criteria, e.g., absence of redundancies and appropriate level of abstraction. Yet given the variety and scope of activities comprised in the life-cycle of an ontological model (OM), such as adapting, splitting, populating, this quality is easily compromised, especially with ontologies of larger size and/or resulting from the merge of smaller ones. Conversely, restoring it through refactoring, i.e., restructuring of the ontology to improve defects, is knowingly a challenging task as relocating an ontology element can adversely affect its neighbors. We investigate here a holistic refactoring approach that, given an ontology, amounts to presenting its designer with a list of the most plausible abstract entities missing in it. The core of the approach is a recently devised concept analysis method, called 'relational', that allows deeper refactoring by feeding into the process various ontological relations, e.g., concept-to-property incidences. The focus here is put on the NLP-aspects of the refactoring, while we also provide some preliminary results from a series of validating experiments.\n\nTitle: Using FCA to suggest refactorings to correct design defects\nContent: Design defects are poor design choices resulting in a hard-to-maintain software, hence their detection and correction are key steps of a disciplined software process aimed at yielding high-quality software artifacts. While modern structure- and metric-based techniques enable precise detection of design defects, the correction of the discovered defects, e.g., by means of refactorings, remains a manual, hence error-prone, activity. As many of the refactorings amount to re-distributing class members over a (possibly extended) set of classes, formal concept analysis (FCA) has been successfully applied in the past as a formal framework for refactoring exploration. Here we propose a novel approach for defect removal in object-oriented programs that combines the effectiveness of metrics with the theoretical strength of FCA. A case study of a specific defect, the Blob, drawn from the Azureus project illustrates our approach.\n\nTitle: Relational concept analysis: mining concept lattices from multi-relational data\nContent: The processing of complex data is admittedly among the major concerns of knowledge discovery from data (kdd). Indeed, a major part of the data worth analyzing is stored in relational databases and, since recently, on the Web of Data. This clearly underscores the need for Entity-Relationship and rdf compliant data mining (dm) tools. We are studying an approach to the underlying multi-relational data mining (mrdm) problem, which relies on formal concept analysis (fca) as a framework for clustering and classification. Our relational concept analysis (rca) extends fca to the processing of multi-relational datasets, i.e., with multiple sorts of individuals, each provided with its own set of attributes, and relationships among those. Given such a dataset, rca constructs a set of concept lattices, one per object sort, through an iterative analysis process that is bound towards a fixed-point. In doing that, it abstracts the links between objects into attributes akin to role restrictions from description logics (dls). We address here key aspects of the iterative calculation such as evolution in data description along the iterations and process termination. We describe implementations of rca and list applications to problems from software and knowledge engineering.\n\nTitle: On the Assessment of Concept Relevance in FCA-Based Ontology Restructuring\nContent: Along their lif-cycle, Ontologies typically undergo a number of structural operations that might deplete their structural quality. Ontology restructuring is a process of improving that quality by reconsidering the way the knowledge is spread across the class and property hierarchies. This often leads to the discovery of new abstractions whose relevance to the ontology must be assessed. We address the relevance assessment within a context where it is crucial: Our restructuring method performs a enhanced concept analysis on the initial ontology that outputs all abstractions of a predefined language. Thus, the key step is the selection of the abstractions to include into the restructured ontology. We propose a set of relevance metrics and a rule-based algorithm for combining them into a single filtering criterion. Their effectiveness is evaluated using a collection of ontologies that have been analyzed beforehand to provide ground truth.\n\nTitle: Modeling and reasoning in context-aware systems based on relational concept analysis and description logic\nContent: As we are moving towards pervasive, ubiquitous and computing paradigm, the interest and research for context-aware systems have substantially taken interest over the past decade and has become the new era of anytime, anywhere and anything computing. Delivering acceptable services for the users requires services to be aware of their contexts and able to adapt automatically to their changing contexts. Context modeling is an important part of a context life cycle to deal and represent context whereas awareness implies reasoning about the context. Each context modeling approach brings along a reasoning method. In this paper, we propose to model the context based on relational concept analysis, an extension of formal concept analysis, and employ the existing mapping rules between the source entity of relational concept analysis and the targeted element of description logic to obtain a FL-E knowledge base and be able to reason about context.\n\n## Task\nCompose an abstract for the title 'Refactorings of design defects using relational concept analysis' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Software engineers often need to identify and correct design defects, i.e., recurring design problems that hinder development and maintenance by making programs harder to comprehend and/or evolve. While detection of design defects is an actively researched area, their correction- mainly a manual and time-consuming activity- is yet to be extensively investigated for automation. In this paper, we propose an automated approach for suggesting defect-correcting refactorings using relational concept analysis (RCA). The added value of rca consists in exploiting the links between formal objects which abound in a software re-engineering context. We validated our approach on instances of the Blob design defect taken from four different open-source programs."
      }
    ]
  },
  {
    "tid":"6",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Adaptive Nonseparable Interpolation for Image Compression With Directional Wavelet Transform\nContent: The adaptive directional lifting-based wavelet transform (ADL) locally adapts the filtering directions to the local properties of the image. In this letter, instead of using the conventional interpolation filter for the directional prediction with fractional-pel accuracy, a new two-dimensional nonseparable adaptive interpolation filter is proposed. The adaptive filter is calculated for every fractional-pel direction so as to minimize the energy of the prediction error. The tradeoff between reducing the prediction error and the overhead to code the interpolation filter is discussed. This enables coding gains of up to 0.98 dB, compared to ADL coder, and up to 2.4 dB, compared to the JPEG 2000 for typical test images.\n\nTitle: Subsampling-Adaptive Directional Wavelet Transform for Image Coding\nContent: In lifting-based directional wavelet transforms, different subsampling patterns may show significant difference for directional signals in image coding. This paper investigates the influence of subsampling in directional wavelet transform. We show that the best subsampling depends on the direction and the directionality strength of the signal. To improve the coding performance, we further propose a subsampling-adaptive directional wavelet transform, which can use different subsampling patterns adaptively and according to the local characteristics of the image. To handle the boundary transition when subsampling changes, a phase completion process is applied to ensure that wavelet transform with various subsampling can be performed without introducing boundary effects and performance loss. Experimental results show that the proposed transform can achieve significant coding gain in image coding compared to other existing directional wavelet transforms.\n\nTitle: Directional lapped transforms for image coding\nContent: In this paper, we present the design of directional lapped transforms for image coding. A lapped transform, which can be implemented by a prefilter followed by a discrete cosine transform (DCT), can be factorized into elementary operators. The corresponding directional lapped transform is generated by applying each elementary operator along a given direction. The proposed directional lapped transforms are not only nonredundant and perfectly reconstructed, but they can also provide a basis along an arbitrary direction. These properties, along with the advantages of lapped transforms, make the proposed transforms appealing for image coding. A block-based directional transform scheme is also presented and integrated into HD Phtoto, one of the state-of-the-art image coding systems, to verify the effectiveness of the proposed transforms.\n\nTitle: Three-Dimensional Shape-Adaptive Discrete Wavelet Transforms For Efficient Object-Based Video Coding\nContent: In this paper, we present an object-based coding scheme using three-dimensional shape-adaptive discrete wavelet transforms (SA-DWT). Rather than straightforward extension of 2D SA-DWT, a novel way to handle the temporal wavelet transform using a motion model is proposed to achieve higher coding efficiency. Corresponding to this transform scheme, we use a 3D entropy coding algorithm called Motion-based Embedded Subband Coding with Optimized Truncation (ESCOT) to code the wavelet coefficients. Results show that ESCOT can achieve comparable coding performance with the state-of-the-art MPEG-4 verification model (VM) 13.0 while having the scalability and flexibility of the bitstream in low bit-rate object-based video coding. And in relative higher bit-rate, our coding approach outperforms MPEG-4 VM 13.0 by about 2.5dB.\n\nTitle: 2-D Dictionary Based Video Coding for Screen Contents\nContent: Screen contents with complex structure contain random combination of texts, graphics and camera-captured images, which makes them difficult to be compressed efficiently by traditional video codecs. In this paper, we propose a 2-D dictionary based scheme to exploit the repeated patterns on screen content. In the proposed scheme, the current block is predicted from the reconstructed region using a hash-based block searching scheme. A hierarchical two-level hash based searching scheme is designed to find the best matching block for each block. The first-level hash function is used to search the blocks similar to the current block in the constructed 2-D dictionary. The second-level hash function is used to update the 2-D dictionary, which filters out the identical blocks from the blocks found using the first-level hash function. The proposed scheme is incorporated into HEVC framework as an additional mode. Experimental results show that the proposed scheme achieves significantly coding performance improvements on screen contents compared with HEVC.\n\n## Task\nCompose an abstract for the title 'Image Coding using 2-D Anisotropic Dual-Tree Discrete Wavelet Transform' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We propose an image coding scheme using 2-D anisotropic dual-tree discrete wavelet transform (DDWT). First, we extend 2-D DDWT to anisotropic decomposition, and obtain more directional subbands. Second, an iterative projection-based noise shaping algorithm is employed to further sparsify anisotropic DDWT coefficients. At last, the resulting coefficients are rearranged to preserve zero-tree relationship so that they can be efficiently coded with SPIHT. Experimental results show that our proposed scheme outperforms JPEG2000 and SPIHT at low bit rates despite the redundancy of DDWT."
      }
    ]
  },
  {
    "tid":"7",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Optical flow-based real-time object tracking using non-prior training active feature model\nContent: This paper presents a feature-based object tracking algorithm using optical flow under the non-prior training (NPT) active feature model (AFM) framework. The proposed tracking procedure can be divided into three steps: (i) localization of an object-of-interest, (ii) prediction and correction of the object's position by utilizing spatio-temporal information, and (iii) restoration of occlusion using NPT-AFM. The proposed algorithm can track both rigid and deformable objects, and is robust against the object's sudden motion because both a feature point and the corresponding motion direction are tracked at the same time. Tracking performance is not degraded even with complicated background because feature points inside an object are completely separated from background. Finally, the AFM enables stable tracking of occluded objects with maximum 60% occlusion. NPT-AFM, which is one of the major contributions of this paper, removes the off-line, preprocessing step for generating a priori training set. The training set used for model fitting can be updated at each frame to make more robust object's features under occluded situation. The proposed AFM can track deformable, partially occluded objects by using the greatly reduced number of feature points rather than taking entire shapes in the existing shape-based methods. The on-line updating of the training set and reducing the number of feature points can realize a real-time, robust tracking system. Experiments have been performed using several in-house video clips of a static camera including objects such as a robot moving on a floor and people walking both indoor and outdoor. In order to show the performance of the proposed tracking algorithm, some experiments have been performed under noisy and low-contrast environment. For more objective comparison, PETS 2001 and PETS 2002 datasets were also used.\n\nTitle: Camera handoff with adaptive resource management for multi-camera multi-object tracking\nContent: Camera handoff is a crucial step to obtain a continuously tracked and consistently labeled trajectory of the object of interest in multi-camera surveillance systems. Most existing camera handoff algorithms concentrate on data association, namely consistent labeling, where images of the same object are identified across different cameras. However, there exist many unsolved questions in developing an efficient camera handoff algorithm. In this paper, we first design a trackability measure to quantitatively evaluate the effectiveness of object tracking so that camera handoff can be triggered timely and the camera to which the object of interest is transferred can be selected optimally. Three components are considered: resolution, distance to the edge of the camera's field of view (FOV), and occlusion. In addition, most existing real-time object tracking systems see a decrease in the frame rate as the number of tracked objects increases. To address this issue, our handoff algorithm employs an adaptive resource management mechanism to dynamically allocate cameras' resources to multiple objects with different priorities so that the required minimum frame rate is maintained. Experimental results illustrate that the proposed camera handoff algorithm can achieve a substantially improved overall tracking rate by 20% in comparison with the algorithm presented by Khan and Shah.\n\nTitle: A Reliability Assessment Paradigm for Automated Video Tracking Systems\nContent: Most existing performance evaluation methods concentrate on defining separate metrics over a wide range of conditions and generating standard benchmarking video sequences for examining the effectiveness of video tracking systems. In other words, these methods attempt to design a robustness margin or factor for the system. These methods are deterministic in which a robustness factor, for example, 2 or 3 times the expected number of subjects to track or the strength of illumination would be required in the design. This often results in over design, thus increasing costs, or under design causing failure by unanticipated factors. In order to overcome these limitations, we propose in this paper an alternative framework to analyze the physics of the failure process and, through the concept of reliability, determine the time to failure in automated video tracking systems. The benefit of our proposed framework is that we can provide a unified and statistical index to evaluate the performance of automated video tracking system for a task to be performed. At the same time, the uncertainty problem about a failure process, which may be caused by the system\u2019s complexity, imprecise measurements of the relevant physical constants and variables, or the indeterminate nature of future events, can be addressed accordingly based on our proposed framework.\n\nTitle: 3D Target Scale Estimation and Motion Segmentation for Size Preserving Tracking in PTZ Video\nContent: In size preserving video tracking, the camera\u0092s focal length (zoom) is adjusted automatically to compensate for the changes in the target\u0092s image size caused by the relative motion between the camera and the target. The accurate estimation of these changes is paramount to the system performance. The existing method of choice for real-time target scale estimation applies structure from motion (SFM) based on the weak perspective projection model [1]. We design a target scale estimation algorithm with linear solution based on the more advanced paraperspective projection model. Another key problem in SFM based algorithms is the separation between foreground and background features (image corners), especially when composite camera and target motions are involved. This paper also addresses a fast foreground/background separation algorithm, the affine shape method. The resulting segmentation automatically adapts to the target\u0092s 3D geometry and motion. Experimental results illustrate the effectiveness of the proposed scale estimation and segmentation algorithms in tracking translating and rotating objects with a PTZ camera while preserving their sizes.\n\nTitle: Fusion of Omnidirectional and PTZ Cameras for Accurate Cooperative Tracking\nContent: Dual camera systems (omnidirectional and slaved PTZ cameras) are widely used in public area monitoring and target tracking. However, due to their low and non-uniform resolution, omnidirectional cameras are only able to provide moderate accuracy in both motion/target detection and tracking. To overcome this disadvantage, two methods are proposed: geometry mapping based on a polynomial imaging model and closed loop cooperative tracking. Using a unified polynomial approximation [1], the geometry relationship between the omnidirectional and PTZ cameras can be formulated in a general solution from camera calibration with a fully automated model selection. Distributed Kalman filters are developed to exchange estimated trajectories among cameras forming closed loop sub-systems for individual cameras. The effectiveness of the proposed algorithms is illustrated via experiments and comparisons with existing systems.\n\n## Task\nCompose an abstract for the title 'A Moving Object Tracked by A Mobile Robot with Real-Time Obstacles Avoidance Capacity' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper describes a robotic application that tracks a moving object by utilizing a mobile robot with multiple sensors. The robotic platform uses a visual camera to sense the movement of the desired object and a range sensor to help the robot detect and then avoid obstacles in real time while continuing to track and follow the desired object. In terms of real-time obstacle avoidance capacity, this paper also presents a modified potential field algorithm called Dynamic Goal Potential Field algorithm (DGPF) for this robotic application specifically. Experimental results show that the robotic and intelligent system can fulfill the requirements of tracking an object and avoiding obstacles simultaneously when the object is moving."
      }
    ]
  },
  {
    "tid":"8",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Reducing VM Startup Time and Storage Costs by VM Image Content Consolidation.\nContent: Elastic cloud applications rely on fast virtual machine (VM) startup, e.g. when scaling out for handling increased workload. While there have been recent studies into the VM startup time in clouds, the effects of the VM image (VMI) disk size and its contents are little understood. To fill this gap, we present a detailed study of these factors on Amazon EC2. Based on our findings, we developed a novel approach for consolidating size and contents of VMIs. We then evaluated our approach with the ConPaaS VMI, an open-source Platform-as-a-Service runtime. Compared to an unmodified ConPaaS VMI, our approach results in up to four times reduction of the disk size, three times speedup for the VM startup time, and three times reduction of storage cost.\n\nTitle: Fast (re-)configuration of mixed on-demand and spot instance pools for high-throughput computing\nContent: Commercial cloud offerings let users allocate compute resources on demand, charging based on reserved time intervals. Users, however, lack guidance for assembling instance pools from different cloud instance types, in order to control completion time and monetary budget. BaTS, our budget-constrained scheduler uses tiny statistical samples of task executions in order to predict completion times (and associated costs) for given bags of tasks, allowing the user to favor either fast execution or low computation budget. BaTS' estimator, however, can not handle variably-priced spot instances appropriately. In this work, we present a new prediction module for BaTS that quickly computes accurate approximations to the Pareto set of mixed on-demand and spot instance pools, based on a genetic algorithm (GA). This new approach allows BaTS to react to changing spot instance prices at runtime by re-configuring the instance pool according to the user's runtime and budget constraints. Simulator-based results show that the GA can approximate the Pareto sets for machine configurations in about 30 seconds time, without noticeable loss of quality, compared to an exact solution, computed offline within 40 to 60 minutes time.\n\nTitle: Towards Resource Disaggregation \u2014 Memory Scavenging for Scientific Workloads\nContent: Compute clusters, consisting of many, uniformly built nodes, are used to run a large spectrum of different workloads, like tightly coupled (MPI) jobs, MapReduce, or graph-processing data-analytics applications, each of which with their own resource requirements. Many studies consistently highlight two types of under-utilized cluster resources: memory (up to 50%) and network. In this work, we take a step towards (software) resource disaggregation, and therefore increased resource utilization, by designing a memory scavenging technique that makes unused memory available to applications on other cluster nodes. We implement this technique in MemFSS, an in-memory distributed file system. The scavenging MemFSS extends its storage space by taking advantage of the unused memory and bandwidth of cluster nodes already running other tenants' applications. Our experiments show that our memory scavenging approach incurs negligible overhead (below 10%) for most tenant applications, while the compute resource comsumption of MemFSS applications is largely reduced (by 17%-74%).\n\nTitle: Dynamic Load-Balanced Multicast for Data-Intensive Applications on Clouds\nContent: Data-intensive parallel applications on clouds need to deploy large data sets from the cloud's storage facility to all compute nodes as fast as possible. Many multicast algorithms have been proposed for clusters and grid environments. The most common approach is to construct one or more spanning trees based on the network topology and network monitoring data in order to maximize available bandwidth and avoid bottleneck links. However, delivering optimal performance becomes difficult once the available bandwidth changes dynamically. In this paper, we focus on Amazon EC2/S3 (the most commonly used cloud platform today) and propose two high performance multicast algorithms. These algorithms make it possible to efficiently transfer large amounts of data stored in Amazon S3 to multiple Amazon EC2 nodes. The three salient features of our algorithms are (1) to construct an overlay network on clouds without network topology information, (2) to optimize the total throughput dynamically, and (3) to increase the download throughput by letting nodes cooperate with each other. The two algorithms differ in the way nodes cooperate: the first `non-steal' algorithm lets each node download an equal share of all data, while the second `steal' algorithm uses work stealing to counter the effect of heterogeneous download bandwidth. As a result, all nodes can download files from S3 quickly, even when the network performance changes while the algorithm is running. We evaluate our algorithms on EC2/S3, and show that they are scalable and consistently achieve high throughput. Both algorithms perform much better than having each node downloading all data directly from S3.\n\nTitle: Stochastic Tail-Phase Optimization for Bag-of-Tasks Execution in Clouds\nContent: Elastic applications like bags of tasks benefit greatly from Infrastructure as a Service (IaaS) clouds that let users allocate compute resources on demand, charging based on reserved time intervals. Users, however, still need guidance for mapping their applications onto multiple IaaS offerings, both minimizing execution time and respecting budget limitations. For budget-controlled execution of bags of tasks, we built Bats, a scheduler that estimates possible budget and make spancombinations using a tiny task sample, and then executes a bag within the user's budget constraints. Previous work has shown the efficacy of this approach. There remains, however, the risk of outlier tasks causing the execution to exceed the predicted make span. In this work, we present a stochastic optimization of the tail phase for Bats' execution. The main idea is to use the otherwise idling machines up until the end of their (already paid-for) allocation time. Using the task completion time information acquired during the execution, BaTS decides which tasks to replicate onto idle machines in the tail phase, reducing the make span and improving the tolerance to outlier tasks. Our evaluation results show that this effect is robust w.r.t. the quality of runtime predictions and is the strongest with more expensive schedules in which many fast machines are available.\n\n## Task\nCompose an abstract for the title 'Scalable virtual machine deployment using VM image caches' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In IaaS clouds, VM startup times are frequently perceived as slow, negatively impacting both dynamic scaling of web applications and the startup of high-performance computing applications consisting of many VM nodes. A significant part of the startup time is due to the large transfers of VM image content from a storage node to the actual compute nodes, even when copy-on-write schemes are used. We have observed that only a tiny part of the VM image is needed for the VM to be able to start up. Based on this observation, we propose using small caches for VM images to overcome the VM startup bottlenecks. We have implemented such caches as an extension to KVM/QEMU. Our evaluation with up to 64 VMs shows that using our caches reduces the time needed for simultaneous VM startups to the one of a single VM."
      }
    ]
  },
  {
    "tid":"9",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Package routability- and IR-drop-aware finger/pad assignment in chip-package co-design\nContent: Due to increasing complexity of design interactions between the chip, package and PCB, it is essential to consider them at the same time. Specifically the finger/pad locations affect the performance of the chip and the package significantly. In this paper, we have developed techniques in chip-package codesign to decide the locations of fingers/pads for package routability and signal integrity concerns in chip core design. Our finger/pad assignment is a two-step method: first we optimize the wire congestion problem in package routing, and then we try to minimize the IR-drop violation with finger/pad solution refinement. The experimental results are encouraging. Compared with the randomly optimized methods, our approaches reduce in average 42% and 68% of the maximum density in package and 10.61% of IR-drop for test circuits.\n\nTitle: Package routability- and IR-drop-aware finger/pad planning for single chip and stacking IC designs\nContent: Due to the increasing complexity of the design interactions between the chip and package, it is necessary to consider them at the same time. In order to simultaneously handle chip and package performances, co-design of chip and package is a widely adopted solution, particularly because the finger/pad locations significantly affect IR-drop of the core and the package routing. In this paper, we develop chip-package co-design techniques to determine the locations of the fingers/pads for package routability and signal integrity concerns in IC designs, this method can be used in the 2-D and stacking IC design. Our finger/pad assignment is a two-step method: we first solve the wire congestion problem in package routing, and then try to minimize the IR-drop violation and the length of the bonding wires under a compact IR-drop model. The experimental results are encouraging. Compared with the randomly optimized method, on average, our approaches reduce the maximum package density by 42% and 68% for both technologies, IR-drop by 10.61% and 4.58%; and the bonding wires is reduced by 15.66% if we use stacking chips.\n\nTitle: An Implementation of Performance-Driven Block and I/O Placement for Chip-Package Codesign\nContent: As silicon technology scales, we can integrate more and more circuits on a single chip, which means more I/Os are needed in modern designs. The flip-chip technology which was developed by IBM is better suited for I/O increase than the typical peripheral wire-bond design.One of the most important characteristics of flip-chip designs is that the I/O buffers could be placed anywhere inside a chip, just like core cells. Motivated by [14] in proposing various I/O planning constraints, we develop a block and I/O buffer placement method in wirelength and signal skew optimization (especially for differential pair signals), and power integrity awareness for chip-package codesign. The results have shown that our approach takes care of power integrity and outperforms [12] in weightedperformance metrics optimization.\n\nTitle: Efficient Package Pin-Out Planning With System Interconnects Optimization for Package-Board Codesign\nContent: In conventional package design, engineers designate the ball grid array (BGA) pin-out manually, this always postpones the time-to-market (TTM) of products due to the turn-around between package and design houses. Recent papers propose a method of automatically generating the pin-out and taking signal integrity (SI), power delivery integrity (PI), and routability (RA) into account simultaneously by pin-block design and floorplanning, thus dramatically speeding up the developing time. However, this approach ignores the considerations of shorter path length and equilength/length matching in routing printed circuit board (PCB) trace and pin-out assignment for high-speed interface IP designs, such as USB and PCI Express. Since these features are the most important performance metrics during chip-package-board codesign, in this paper we propose the ideas to optimize the system interconnects during package pin-out design. These ideas keep the same minimized package size as aforementioned recent work and ensure that SI, PI, and RA can still be considered with significant reduction in design cost. It is achieved by relaxing the restriction of pin-block side and order on the package, usually specified by package designers. The experimental results on industrial chipset design cases show that the average improvement of our pin-block planner is over 40% when comparing the design cost with the previous work, among which we have one case accommodated over a thousand pins. Our ideas also work for any kind of pin-block or pin-group configurations.\n\nTitle: A study of row-based area-array I/O design planning in concurrent chip-package design flow\nContent: IC-centric design flow has been a common paradigm when designing and optimizing a system. Package and board/system designs are usually followed by almost-ready chip designs, which causes long turn-around time communicating with package and system houses. In this article, the realizations of area-array I/O design methodologies are studied. Different from IC-centric flow, we propose a chip-package concurrent design flow to speed up the design time. Along with the flow, we design the I/O-bump (and P/G-bump) tile that combines I/O (and P/G) and bump into a hard macro with the considerations of I/O power connection and electrostatic discharge (ESD) protection. We then employ an I/O-row based scheme to place I/O-bump tiles with existed metal layers. By such a scheme, it reduces efforts in I/O placement legalization and the redistribution layer (RDL) routing. With the emphasis on package design awareness, the proposed methods map package balls onto chip I/Os, thus providing an opportunity to design chip and package in parallel. Due to this early study of I/O and bump planning, faster convergence can be expected with concurrent design flow. The results are encouraging and the merits of this flow are reassuring.\n\n## Task\nCompose an abstract for the title 'Routability-driven bump assignment for chip-package co-design' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In current chip and package designs, it is a bottleneck to simultaneously optimize both pin assignment and pin routing for different design domains (chip, package, and board). Usually the whole process costs a huge manual effort and multiple iterations thus reducing profit margin. Therefore, we propose a fast heuristic chip-package co-design algorithm in order to automatically obtain a bump assignment which introduces high routability both in RDL routing and substrate routing (100% in our real case). Experimental results show that the proposed method (inspired by board escape routing algorithms) automatically finishes bump assignment, RDL routing and substrate routing in a short time, while the traditional co-design flow requires weeks even months."
      }
    ]
  },
  {
    "tid":"10",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: W-privacy: understanding , and  inference channels in multi-camera surveillance video\nContent: Huge amounts of video are being recorded every day by surveillance systems. Since video is capable of recording and preserving an enormous amount of information which can be used in many applications, it is worth examining the degree of privacy loss that might occur due to public access to the recorded video. A fundamental requirement of privacy solutions is an understanding and analysis of the inference channels than can lead to a breach of privacy. Though inference channels and privacy risks are well studied in traditional data sharing applications (e.g., hospitals sharing patient records for data analysis), privacy assessments of video data have been limited to the direct identifiers such as people's faces in the video. Other important inference channels such as location (Where), time (When), and activities (What) are generally overlooked. In this paper we propose a privacy loss model that highlights and incorporates identity leakage through multiple inference channels that exist in a video due to what, when, and where information. We model the identity leakage and the sensitive information separately and combine them to calculate the privacy loss. The proposed identity leakage model is able to consolidate the identity leakage through multiple events and multiple cameras. The experimental results are provided to demonstrate the proposed privacy analysis framework.\n\nTitle: Adaptive transformation for robust privacy protection in video surveillance\nContent: Privacy is a big concern in current video surveillance systems. Due to privacy issues, many strategic places remain unmonitored leading to security threats. The main problem with existing privacy protection methods is that they assume availability of accurate region of interest (RoI) detectors that can detect and hide the privacy sensitive regions such as faces. However, the current detectors are not fully reliable, leading to breaches in privacy protection. In this paper, we propose a privacy protection method that adopts adaptive data transformation involving the use of selective obfuscation and global operations to provide robust privacy even with unreliable detectors. Further, there are many implicit privacy leakage channels that have not been considered by researchers for privacy protection. We block both implicit and explicit channels of privacy leakage. Experimental results show that the proposed method incurs 38% less distortion of the information needed for surveillance in comparison to earlier methods of global transformation; while still providing near-zero privacy loss.\n\nTitle: Collective control over sensitive video data using secret sharing\nContent: Today digital video is used extensively in many applications. Sometimes a video could be treated as a top secret for an organization, for example military secrets, surveillance footage and corporate product designs, and may need to be shared among a group of people in a secure manner. Traditional data security methods such as encryption techniques are prone to single-point attack, i.e. the secret can be revealed by obtaining the decryption key from any single person. Alternatively, the secret sharing scheme provides collective control over the secrecy of information and is considered information theoretically secure. In this paper, we propose to adopt a secret sharing based approach to provide collective control over a given sensitive video. We present three methods that utilize the spatial and temporal redundancy in videos in different ways. We analyze the security of these methods and compare them for efficiency in terms of computation time and space using extensive experimentation.\n\nTitle: A Hierarchical Signature Scheme for Robust Video Authentication using Secret Sharing\nContent: Ensuring the integrity of a digital video is an important and challenging research problem arising out of many video applications. In this paper, we present a hierarchical framework for video authentication based on cryptographic secret sharing that protects a video from spatial cropping and temporal jittering, yet is robust against frame dropping in the streaming video scenario. Our algorithm provides a tradeoff between security and robustness by having configurable inputs. The authentication signature is compact and very sensitive against spatial attacks such as region tampering, and interframe attacks like frame replacement, major frame dropping, and frame reordering. Given a video, we identify the key frames based on different energy between the frames. Considering video frames as shares, we compute the secret at three hierarchical levels. The master secret is used as digital signature to authenticate the video. We present extensive experimental results which show the utility of our technique.\n\nTitle: Modeling Quality of Information in Multi-sensor Surveillance Systems\nContent: Current surveillance systems use multiple sensors and media processing techniques in order to record/detect information of interest in terms of events. Assessing the quality of information (QoI) of a surveillance system is an important task as any misleading information may lead to suspicion, undesired consequences, and unwanted invasion of privacy. In this paper, we propose a model to characterize QoI in multi-sensor surveillance systems in terms of four quality parameters, which are: accuracy, certainty, time-liness and integrity. The proposed model is extendable to include other quality parameters if deemed necessary for different task-specific scenarios, such as the ambient intelligence environment, which aims to provide context-aware personalized services to the poeple living in that environment. To demonstrate the utility of the proposed method, we provide experimental results in a surveillance system designed for identifying authorized entry in the observation area.\n\n## Task\nCompose an abstract for the title 'Privacy modeling for video data publication' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Video cameras are being extensively used in many applications. Huge amounts of video are being recorded and stored everyday by surveillance systems. Any proposed application of this data raises severe privacy concerns. An assessment of privacy loss is necessary before any potential application of the data. In traditional methods of privacy modeling, researchers have focused on explicit means of identity leakage like facial information, etc. However, other implicit inference channels through which individual's an identity can be learned have not been considered. For example, an adversary can observe the behavior, look at the places visited and combine that with the temporal information to infer the identity of the person in the video. In this work, we thoroughly investigate privacy issues involved with the video data considering both implicit and explicit channels. We first establish an analogy with the statistical databases and then propose a model to calculate the privacy loss that might occur due to publication of the video data. The experimental results demonstrate the utility of the proposed model."
      }
    ]
  },
  {
    "tid":"11",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Calibration of a 3D endoscopic system based on active stereo method for shape measurement of biological tissues and specimen.\nContent: For endoscopic medical treatment, measuring the size and shape of the lesion, such as a tumor, is important for the improvement of diagnostic accuracy. We are developing a system to measure the shapes and sizes of living tissue by active stereo method using a normal endoscope on which a micro pattern projector is attached. In order to perform 3D reconstruction, estimating the intrinsic and extrinsic parameters of the endoscopic camera and the pattern projector is required. Particularly, calibration of the pattern projector is difficult. In this paper, we propose a simultaneous estimation method of both intrinsic and extrinsic parameters of the pattern projector. This simplifies the calibration procedure required in practical scenes. Furthermore, we have developed an efficient user interface to intuitively operate the calibration and reconstruction procedures. Using the developed system, we measured the shape of an internal tissue of the soft palate of a human and a biological specimen.\n\nTitle: Analyzing Muscle Activity and Force with Skin Shape Captured by Non-contact Visual Sensor.\nContent: Estimating physical information by vision as humans do is useful for the applications with physical interaction in the real world. For example, observing muscle bulging infers how much force a person puts on the muscle to interact with an object or environment. Since the human skin deforms due to muscle activity, it is expected that skin deformation gives information to analyze human motion. This paper demonstrates that biomechanical information can be derived from skin shape by analyzing the relationship between skin deformation, force produced by muscles, and muscle activity. We first obtained the dataset simultaneously acquired by a range sensor, a force sensor, and electromyograph EMG sensors. Since recent range sensors based on non-contact visual measurement acquires accurate and dense shape of an object at high frame rate, the deforming skin can be observed. The deformation is calculated by finding the correspondence between a template shape and each range scan. The relationship between skin deformation and other data is learned. In this paper, the following problems are considered: 1 estimating force from skin shape, 2 estimating muscle activity from skin shape, 3 synthesizing skin shape from muscle activity. In the experiments, the database learned from the sensor data can be used for the above problems, and the skin shape gives useful information to explain the muscle activity.\n\nTitle: Grid-Based Active Stereo with Single-Colored Wave Pattern for Dense One-shot 3D Scan\nContent: In this paper, we propose a method to reconstruct the shapes of moving objects. The proposed method is a projector-camera system that reconstructs a shape from a single image where a static pattern is cast by a projector, such a method is ideal for acquisition of moving objects at a high frame rate. The issues tackled in this paper are as follows: 1) realize one-shot 3D reconstruction with a single-colored pattern, and 2) obtain accurate shapes by finding correspondences in sub-pixel accuracy. To achieve these goals, we propose the following methods: 1) implicit encoding of projector information by a grid of wave lines, 2) grid-based stereo between projector pattern and camera images to determine unique correspondences, 3) (quasi-)pixel-wise interpolations and optimizations to reconstruct dense shapes, and 4) a single-colored pattern contributes to simplify pattern projecting devices compared to color-coded methods. In the experiment, we show the proposed method is efficient to solve the issues above.\n\nTitle: Single-wavelength and multi-parallel dotted- and solid-lines for dense and robust active 3D reconstruction\nContent: A dense one-shot scanning technique that is robust to subsurface scattering is proposed. In this technique, a novel pattern, consisting of multiple parallel dotted lines and solid lines, that are aligned alternately, is proposed. To project such a pattern efficiently, a single-wavelength laser-based pattern projector is developed. To detect patterns robustly from captured images, a black and white camera attached with a narrow-band-path filter is used in conjunction with our novel deep learning based algorithm, which is based on a convolutional neural network (CNN). Because the detected lines must be identified for shape reconstruction, we apply a gap-coding technique, which is originally based on a grid-line pattern, to the dot pattern. To this end, we introduce a virtual grid-line structure, which is generated from the dot pattern. Additionally, we propose a calibration algorithm specialized for our system, where the pattern is static and shared with the shape reconstruction algorithm, i. e., correspondence problem remains. For a solution, gap-coding is further applied to find correspondences under epipolar constraints. The experimental results of scanning real objects are presented to demonstrate the effectiveness of our calibration and reconstruction techniques.\n\nTitle: Real-Time Estimation Of Fast Egomotion With Feature Classification Using Compound Omnidirectional Vision Sensor\nContent: For fast egomotion of a camera, computing feature correspondence and motion parameters by global search becomes highly time-consuming. Therefore, the complexity of the estimation needs to be reduced for real-time applications. In this paper, we propose a compound omnidirectional vision sensor and an algorithm for estimating its fast egomotion. The proposed sensor has both multi-baselines and a large field of view (FOV). Our method uses the multi-baseline stereo vision capability to classify feature points as near or far features. After the classification, we can estimate the camera rotation and translation separately by using random sample consensus (RANSAC) to reduce the computational complexity. The large FOV also improves the robustness since the translation and rotation are clearly distinguished. To date, there has been no work on combining multi-baseline stereo with large FOV characteristics for estimation. even though these characteristics are individually are important in improving egomotion estimation. Experiments showed that the proposed method is robust and produces reasonable accuracy in real time for fast motion of the sensor.\n\n## Task\nCompose an abstract for the title 'Noncontact measurement of cardiac beat by using active stereo with waved-grid pattern projection.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We propose a method to observe cardiac beat from 3D shape information of body surface by using the active stereo with waved-grid pattern projection, and report preliminary experiments to evaluate validities of the proposed method. By comparing results of our method with those of electrocardiogram (ECG), we confirmed sufficient correspondences between peak intervals of depth changes between contiguous frames measured by the active stereo and R-R intervals measured by ECG. We proposed the visualization of the spatial distribution of depth change plotted on the 3D shape of chest surface. We confirm that the spatial phase difference, which is caused by heart pump ability, appears in the 3-D shape change of chest surface."
      }
    ]
  },
  {
    "tid":"12",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Efficient variants of the Naor-Yung and Dolev-Dwork-Naor transforms for CCA secure key encapsulation mechanism\nContent: In this paper, we present novel constructions of chosen-ciphertext secure (CCA secure) key encapsulation mechanism (KEM) from chosen-plaintext secure (CPA secure) KEM in the standard model. It is already known that CCA secure public key encryption (PKE) can be generically constructed from CPA secure PKE and ((simulation-sound) non-interactive zero-knowledge proof) via the Naor-Yung or Dolev-Dwork-Naor transforms. Thus, one can also immediately construct CCA secure PKE from CPA secure KEM by converting CPA secure KEM into CPA secure PKE and transforming it to be CCA secure PKE. However, such a construction seems redundant since in general PKE is less efficient than KEM and it would be more efficient if we can directly construct CCA secure KEM from CPA secure KEM without intermediating CPA secure PKE. In this work, we propose new variants of the Naor-Yung and Dolev-Dwork-Naor transforms that directly convert CPA secure KEM into CCA secure KEM, and show that our proposed schemes are more efficient than the above straightforward constructions. For example, when instantiating from the decision linear assumption, ciphertext size of our Naor-Yung variant consists of 34 group elements while that of the straightforward construction consists of 47 group elements. Furthermore, we also propose another variant of the Dolev-Dwork-Naor transform from multiple KEM and show that a KEM which is obtained from Wee's extractable hash proof system can also be considered as an efficient construction of multiple KEM.\n\nTitle: Simple CCA-Secure Public Key Encryption from Any Non-Malleable Identity-Based Encryption\nContent: In this paper, we present a simple and generic method for constructing public key encryption (PKE) secure against chosen ciphertext attacks (CCA) from identity-based encryption (IBE). Specifically, we show that a CCA-secure PKE scheme can be generically obtained by encrypting (m ||r ) under identity \"f (r )\" with the encryption algorithm of the given IBE scheme, assuming that the IBE scheme is non-malleable and f is one-way. In contrast to the previous generic methods (such as Canetti-Halevi-Katz), our method requires stronger security for the underlying IBE schemes, non-malleability, and thus cannot be seen as a direct improvement of the previous methods. However, once we have an IBE scheme which is proved (or can be assumed) to be non-malleable, we will have a PKE scheme via our simple method, and we believe that the simpleness of our proposed transformation itself is theoretically interesting. Our proof technique for security of the proposed scheme is also novel. In the security proof, we show how to deal with certain types of decryption queries which cannot be handled by straightforwardly using conventional techniques.\n\nTitle: Efficient Bit-Decomposition And Modulus-Conversion Protocols With An Honest Majority\nContent: In this paper, we propose secret-sharing-based bit-decomposition and modulus-conversion protocols for a prime order ring Z(p) with an honest majority: an adversary can corrupt k - 1 parties of n parties and 2k - 1 <= n. Our protocols are secure against passive and active adversaries depending on the components of our protocols. We assume a secret is an l-bit element and 2(l+inverted right perpendicular log m )(inverted left perpendicular) < p, where m = k in the passive security and m ((n)(k-1)) in the active security. The outputs of our bit-decomposition and modulus-conversion protocols are tuple of shares in Z(2) and a share Z(p'), in respectively, where p' is the modulus after the conversion. If k and n are small, the communication complexity of our passively secure bit-decomposition and modulus-conversion protocols are O(l) bits and O (inverted right perpendicular log p' inverted left perpendicular) bits, respectively. Our key observation is that a quotient of additive shares can be computed from the least significant inverted right perpendicular log m( )inverted left perpendicular bits. If a secret a is \"shifted\" and additively shared as x(i)s so that 2(inverted right perpendicular log m) (inverted left perpendicular) a = Sigma(m-1)(i=0) x(i) = 2(inverted right perpendicular log m) (inverted left perpendicular) a + qp, the least significant Flog ml bits of Sigma(m-1)(i=0) determine q since p is an odd prime and the least significant inverted (right perpendicular log m) (inverted left perpendicular) bits of 2(inverted right perpendicular log m) (inverted left perpendicular) a are 0s.\n\nTitle: Parallel decryption queries in bounded chosen ciphertext attacks\nContent: Whether it is possible to construct a chosen ciphertext secure (CCA secure) public key encryption (PKE) scheme only from a chosen plaintext secure (CPA secure) one is a fundamental open problem, and the best known positive results regarding this problem are the constructions of so-called bounded CCA secure schemes. Since we can achieve the best possible security in the bounded CCA security notions, in order to further tackle the problem, we would need other new security notions that capture intermediate security notions that lie between CPA and CCA security. Motivated by this situation, we focus on \"parallel\" decryption queries (originally introduced by Bellare and Sahai) for the extension of bounded CCA security, and introduce a new security notion which we call mixed CCA security. It captures security against adversaries that make single and parallel decryption queries in a predetermined order, where each parallel query can contain unboundedly many ciphertexts. Moreover, how the decryption oracle is available before and after the challenge is also taken into account in this new security definition, which enables us to capture existing major security notions that lie between CPA and CCA security in a unified security notion. We investigate the relations among mixed CCA security notions, and show a necessary and sufficient condition of implications/separations between any two notions in mixed CCA security. We also show two black-box constructions of PKE schemes with improved security only using CPA secure schemes as building blocks.\n\nTitle: A Generic Construction of Timed-Release Encryption with Pre-open Capability\nContent: In 2005, Hwang et al. proposed a concept of timed-release encryption with pre-open capability (TRE-PC), where a receiver can decrypt a ciphertext not only by using a time-release key which is provided after its release-time, but also using a secret information called a pre-open key provided from a sender even before the release-time. Though there are several concrete constructions of TRE-PC proposed so far, no generic construction has been known. In this paper, we show a generic construciton of TRE-PC. Specifically, we construct a TRE-PC scheme from a chosen-ciphertext secure public key encryption scheme (PKE), a chosen plaintext secure identity-based encryption (IBE) scheme with specific property that we call target collision resistance for randomness , and a one-time signature scheme. Interestingly, our proposed construction of TRE-PC is essentially the same as the generic construciton of (normal) TRE based on multiple encryption of IBE and PKE. As one of the consequences of our result, we can build a TRE-PC scheme secure in the standard model based on weaker assumptions than the ones used by the existing standard model TRE-PC scheme.\n\n## Task\nCompose an abstract for the title 'Simulation-Based Receiver Selective Opening CCA Secure PKE from Standard Computational Assumptions.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In the situation where there are one sender and multiple receivers and the sender transmits ciphertexts of correlated plaintexts, a receiver selective opening (RSO) attack for a public key encryption (PKE) scheme considers adversaries that can corrupt some of the receivers and get their secret keys and plaintexts. Security against RSO attacks for a PKE scheme ensures confidentiality of ciphertexts of uncorrupted receivers. Simulation-based RSO security against chosen ciphertext attacks (SIM-RSO-CCA) is the strongest security notion in all RSO attack scenarios. Jia, Lu, and Li (2016) [19] proposed the first SIM-RSO-CCA secure PKE scheme. However, their scheme used indistinguishability obfuscation, which is not known to be constructed from any standard computational assumption. In this paper, we give two contributions for constructing SIM-RSO-CCA secure PKE from standard computational assumptions. Firstly, we propose a generic construction of SIM-RSO-CCA secure PKE using an IND-CPA secure PKE scheme and a non-interactive zero-knowledge proof system satisfying one-time simulation soundness. Secondly, we propose an efficient and concrete construction of SIM-RSO-CCA secure PKE based on the decisional Diffie-Hellman (DDH) assumption. Moreover, we give a method for efficiently expanding the plaintext space of the DDH-based construction. By applying this method to the construction, we obtain the first DDH-based SIM-RSO-CCA secure PKE scheme supporting a super-polynomially large plaintext space with compact ciphertexts."
      }
    ]
  },
  {
    "tid":"13",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Analyzing and Modeling Router---Level Internet Topology\nContent: Measurement studies on the Internet topology show that connectivities of nodes exhibit power---law attribute, but it is apparent that only the degree distribution does not determine the network structure, and especially true when we study the network---related control like routing control. In this paper, we first reveal structures of the router---level topologies using the working ISP networks, which clearly indicates ISP topologies are highly clustered; a node connects two or more nodes that also connected each other, while not in the existing modeling approaches. Based on this observation, we develop a new realistic modeling method for generating router---level topologies. In our method, when a new node joins the network, the node likely connects to the nearest nodes. In addition, we add the new links based on the node utilization in the topology, which corresponds to an enhancement of network equipments in ISP networks. With appropriate parameters, important metrics, such as the a cluster coefficient and the number of node-pairs that pass through nodes, exhibit the similar value of the actual ISP topology while keeping the degree distribution of resulting topology to follow power---law.\n\nTitle: Performance analysis of fast reservation protocol in ATM networks with arbitrary topologies\nContent: One of the main features of ATM (Asynchronous Transfer Mode) is that the transmission rate can be adjusted according to the degree of network congestion. FRP (Fast Reservation Protocol) is a recently proposed protocol which can utilize this unique feature of ATM technology in such a way that the amount of bandwidth a source requests is reduced if an attempt to reserve bandwidth is rejected by the network. A rationale behind this is that the rejection of the bandwidth request indicates network congestion. Therefore, requests for smaller amounts of bandwidth after reservation failures enable sources to share network resources efficiently. To evaluate burst (a protocol data unit of FRP) level performance of FRP, this paper provides an approximate analysis of FRP-based ATM networks with arbitrary topologies. Its mathematical model is considered as a loss network with retrials, which allows that the amount of bandwidth requested by retrial bursts is reduced and that bursts may have alternative routes. Through numerical results, the appropriate bandwidth reservation method and routing strategy are studied in depth.\n\nTitle: Location-Aware Store-Carry-Forward Routing Based On Node Density Estimation\nContent: We consider a location-aware store-carry-forward routing scheme based on node density estimation (LA Routing in short), which adopts different message forwarding strategies depending on node density at contact locations where two nodes encounter. To do so, each node estimates a node density distribution based on information about contact locations. In this paper, we clarify how the estimation accuracy affects the performance of LA Routing. We also examine the performance of LA Routing when it applies to networks with homogeneous node density. Through simulation experiments, we show that LA Routing is fairly robust against the accuracy of node density estimation and its performance is comparable with Probabilistic Routing even in the case that that node density is homogeneous.\n\nTitle: Survey Of Network Coding And Its Applications\nContent: This survey summarizes the state-of-the-art research on network coding, mainly focusing on its applications to computer networking. Network coding generalizes traditional store-and-forward routing techniques by allowing intermediate nodes in networks to encode several received packets into a single coded packet before forwarding. Network coding was proposed in 2000, and since then, it has been studied extensively in the field of computer networking. In this survey, we first summarize linear network coding and provide a taxonomy of network coding research, i.e., the network coding design problem and network coding applications. Moreover, the latter is subdivided into throughput/capacity enhancement, robustness enhancement, network tomography, and security. We then discuss the fundamental characteristics of network coding and diverse applications of network coding in details, following the above taxonomy.\n\nTitle: Design And Analysis Of Self-Organized Data Aggregation Using Evolutionary Game Theory In Delay Tolerant Networks\nContent: In delay tolerant networks (DTNs), custody transfer mechanism provides reliable end-to-end data transfer in which special nodes (custodians) transfer data with custody in a hop-by-hop manner As a result, storage congestion occurs when data with custody increases and/or the network, is partitioned into multiple sub-networks for a long time. The storage congestion can be alleviated with the help of message ferries. In such a scenario, data should be aggregated to some custodians so that message ferries can effectively collect them. In this paper, we propose a scheme to aggregate data into selected custodians, called aggregators, in a fully distributed and autonomous manner by, using evolutionary game theoretical approach where we can also control the number of aggregators to a desired value.\n\n## Task\nCompose an abstract for the title 'Analyzing and modeling router-level internet topology and application to routing control' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Measurement studies on the Internet topology show that connectivities of nodes exhibit power-law attribute, but it is apparent that only the degree distribution does not determine the network structure, and especially true when we study the network-related control like routing control. In this paper, we first reveal structures of the router-level topologies using the working ISP networks, which clearly indicates ISP topologies are highly clustered; a node connects two or more nodes that also connected each other, while not in the existing modeling approaches. Based on this observation, we develop a new realistic modeling method for generating router-level topologies. In our method, when a new node joins the network, the node likely connects to the nearest nodes. In addition, we add the new links based on the node utilization in the topology, which corresponds to an enhancement of network equipments in ISP networks. With appropriate parameters, important metrics, such as the a clustering coefficient and the amount of traffic that pass through nodes, exhibit the similar value of the actual ISP topology while keeping the degree distribution of resulting topology to follow power-law. We then apply the routing control method to the ISP topologies and show that the optimal routing method gives much smaller maximum link utilization (about 1/3) compared with the minimum hop routing which is often used in the operating networks. Accordingly, we examine a heuristic routing method suitable to the ISP topologies with consideration of technology constraints of IP routers. The evaluation results show that our modeling method can be actually used for evaluations on routing control."
      }
    ]
  },
  {
    "tid":"14",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Virtual routers on the move: live router migration as a network-management primitive\nContent: The complexity of network management is widely recognized as one of the biggest challenges facing the Internet today. Point solutions for individual problems further increase system complexity while not addressing the underlying causes. In this paper, we argue that many network-management problems stem from the same root cause---the need to maintain consistency between the physical and logical configuration of the routers. Hence, we propose VROOM (Virtual ROuters On the Move), a new network-management primitive that avoids unnecessary changes to the logical topology by allowing (virtual) routers to freely move from one physical node to another. In addition to simplifying existing network-management tasks like planned maintenance and service deployment, VROOM can also help tackle emerging challenges such as reducing energy consumption. We present the design, implementation, and evaluation of novel migration techniques for virtual routers with either hardware or software data planes. Our evaluation shows that VROOM is transparent to routing protocols and results in no performance impact on the data traffic when a hardware-based data plane is used.\n\nTitle: Transparent, Live Migration of a Software-Defined Network\nContent: Increasingly, datacenters are virtualized and software-defined. Live virtual machine (VM) migration is becoming an indispensable management tool in such environments. However, VMs often have a tight coupling with the underlying network. Hence, cloud providers are beginning to offer tenants more control over their virtual networks. Seamless migration of all (or part) of a virtual network greatly simplifies management tasks like planned maintenance, optimizing resource usage, and cloud bursting. Our LIME architecture efficiently migrates an ensemble, a collection of virtual machines and virtual switches, for any arbitrary controller and end-host applications. To minimize performance disruptions, during the migration, LIME temporarily runs all or part of a virtual switch on multiple physical switches. Running a virtual switch on multiple physical switches must be done carefully to avoid compromising application correctness. To that end, LIME merges events, combines traffic statistics, and preserves consistency among multiple physical switches even across changes to the packet-handling rules. Using a formal model, we prove that migration under LIME is transparent to applications, i.e., any execution of the controller and end-host applications during migration is a completely valid execution that could have taken place in a migration-free setting. Experiments with our prototype, built on the Floodlight controller, show that ensemble migration can be an efficient tool for network management.\n\nTitle: Towards Robust Multi-Layer Traffic Engineering: Optimization of Congestion Control and Routing\nContent: In the Internet today, traffic engineering is performed assuming that the offered traffic is inelastic. In reality, end hosts adapt their sending rates to network congestion, and network operators adapt the routing to the measured traffic. This raises the question of whether the joint system of congestion control (transport layer) and routing (network layer) is stable and optimal. Using the established optimization models for TCP and traffic engineering as a basis, we find the joint system can be stabilized and often maximizes aggregate user utility. We prove that both stability and optimality of the joint system can be guaranteed for sufficiently elastic traffic simply by tuning the cost function used for traffic engineering. Then, we present a new algorithm that adapts on a smaller timescale to changes in traffic distribution and is more robust to large traffic bursts. Uniting the network and transport layers in a multi-layer approach, this algorithm, distributed adaptive traffic engineering (DATE), jointly optimizes the goals of end users and network operators and reacts quickly to avoid bottlenecks. Simulations demonstrate that DATE converges quickly\n\nTitle: A formally-verified migration protocol for mobile, multi-homed hosts\nContent: Modern consumer devices, like smartphones and tablets, have multiple interfaces (e.g., WiFi and 4G) that attach to new access points as users move. These mobile, multi-homed computers are a poor match with an Internet architecture that binds connections to fixed endpoints with topology-dependent addresses. As a result, hosts typically cannot spread a connection over multiple interfaces or paths, or change locations without breaking existing connections. In this paper, we create an end-to-end connection control protocol (ECCP) that allows hosts to communicate over multiple interfaces with dynamically-changing IP addresses and works with multiple data-delivery protocols (i.e., reliable or unreliable transport). Each ECCP connection consists of one or more flows, each associated with an interface or path. Through end-to-end signaling, a host can move an existing flow from one interface to another, or change its IP address, without any support from the underlying network. We develop formal models to verify that ECCP works correctly in the presence of packet loss, out-of-order delivery, and frequent mobility, and to identify bugs and design limitations in earlier mobility protocols.\n\nTitle: Optimizing the \"one big switch\" abstraction in software-defined networks\nContent: Software Defined Networks (SDNs) support diverse network policies by offering direct, network-wide control over how switches handle traffic. Unfortunately, many controller platforms force applications to grapple simultaneously with end-to-end connectivity constraints, routing policy, switch memory limits, and the hop-by-hop interactions between forwarding rules. We believe solutions to this complex problem should be factored in to three distinct parts: (1) high-level SDN applications should define their end-point connectivity policy on top of a \"one big switch\" abstraction; (2) a mid-level SDN infrastructure layer should decide on the hop-by-hop routing policy; and (3) a compiler should synthesize an effective set of forwarding rules that obey the user-defined policies and adhere to the resource constraints of the underlying hardware. In this paper, we define and implement our proposed architecture, present efficient rule-placement algorithms that distribute forwarding policies across general SDN networks while managing rule-space constraints, and show how to support dynamic, incremental update of policies. We evaluate the effectiveness of our algorithms analytically by providing complexity bounds on their running time and rule space, as well as empirically, using both synthetic benchmarks, and real-world firewall and routing policies.\n\n## Task\nCompose an abstract for the title 'Rethinking virtual network embedding: substrate support for path splitting and migration' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Network virtualization is a powerful way to run multiple architectures or experiments simultaneously on a shared infrastructure. However, making efficient use of the underlying resources requires effective techniques for virtual network embedding--mapping each virtual network to specific nodes and links in the substrate network. Since the general embedding problem is computationally intractable, past research restricted the problem space to allow efficient solutions, or focused on designing heuristic algorithms. In this paper, we advocate a different approach: rethinking the design of the substrate network to enable simpler embedding algorithms and more efficient use of resources, without restricting the problem space. In particular, we simplify virtual link embedding by: i) allowing the substrate network to split a virtual link over multiple substrate paths and ii) employing path migration to periodically re-optimize the utilization of the substrate network. We also explore node-mapping algorithms that are customized to common classes of virtual-network topologies. Our simulation experiments show that path splitting, path migration,and customized embedding algorithms enable a substrate network to satisfy a much larger mix of virtual networks"
      }
    ]
  },
  {
    "tid":"15",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Coordination guided reinforcement learning\nContent: In this paper, we propose to guide reinforcement learning (RL) with expert coordination knowledge for multi-agent problems managed by a central controller. The aim is to learn to use expert coordination knowledge to restrict the joint action space and to direct exploration towards more promising states, thereby improving the overall learning rate. We model such coordination knowledge as constraints and propose a two-level RL system that utilizes these constraints for online applications. Our declarative approach towards specifying coordination in multi-agent learning allows knowledge sharing between constraints and features (basis functions) for function approximation. Results on a soccer game and a tactical real-time strategy game show that coordination constraints improve the learning rate compared to using only unary constraints. The two-level RL system also outperforms existing single-level approach that utilizes joint action selection via coordination graphs.\n\nTitle: Distributed relational temporal difference learning\nContent: Relational representations have great potential for rapidly generalizing learned knowledge in large Markov decision processes such as multi-agent problems. In this work, we introduce relational temporal difference learning for the distributed case where the communication links among agents are dynamic. Thus no critical components of the system should reside in any one agent. Relational generalization among agents' learning is achieved through the use of partially bound relational features and a message passing scheme. We further describe how the proposed concepts can be applied to distributed reinforcement learning methods that use value functions. Experiments were conducted on soccer and real-time strategy game domains with dynamic communication. Results show that our methods improve goal achievement in online learning with a greatly decreased number of parameters to learn when compared with existing distributed learning methods.\n\nTitle: Mode Committee: A Novel Ensemble Method by Clustering and Local Learning\nContent: Ensemble methods have proved effective to achieve higher accuracy. Some simple ensemble methods, such as Bagging, work well with unstable base algorithms, but fail with stable ones. The reason is that such methods achieve higher accuracy by reducing only the variance of the base algorithms. It does not touch the bias. Here, we propose a novel ensemble method, Mode Committee, intended to work for both stable and unstable base algorithms. It first derive a new algorithm, called mode competitor, from given base algorithm, with the help of k-modes clustering method and the local learning strategy. Randomness is injected into each mode competitor by the process of random seeding. The aim of deriving mode competitor is to reduce the bias with the possible increasing variance. Then, multiple mode competitors form a committee and vote on the decision of new example, with the aim to reduce the variance of mode competitors. Such an arithmetic framework has been materialized by two base algorithms, the unstable C4.5 and the stable na\u00efve Bayes. Extensive empirical results demonstrate this method\u00fds superiority, and further analysis by bias-variance decomposition reveals that it is due to the low-bias of mode competitors.\n\nTitle: Towards self-tuning data placement in parallel database systems\nContent: Parallel database systems are increasingly being deployed to support the performance demands of end-users. While declustering data across multiple nodes facilitates parallelism, initial data placement may not be optimal due to skewed workloads and changing access patterns. To prevent performance degradation, the placement of data must be reorganized, and this must be done on-line to minimize disruption to the system.In this paper, we consider a dynamic self-tuning approach to reorganization in a shared nothing system. We introduce a new index-based method that faciliates fast and efficient migration of data. Our solution incorporates a globally height-balanced structure and load tracking at different levels of granularity. We conducted an extensive performance study, and implemented the methods on the Fujitsu AP3000 machine. Both the simulation and empirical results demonstratic that our proposed method is indeed scalable and effective in correcting any deterioration in system throughput.\n\nTitle: Finding Orientation-Sensitive Patterns in Snapshot Databases\nContent: Snapshot data have become ubiquitous, e.g., maps, images and videos. By extracting interesting features from snapshot data and analyzing their relative orientations and proximities, we can discover important structure configuration information among groups of features in a snapshot database. In this paper, we introduce a class of pattern called orientation-sensitive patterns, which occur in many applications ranging from weather study, sport game analysis to medical image processing. We examine three approaches to discover orientation-sensitive patterns. We show that the first apriori-based approach is expensive while the second enumeration-based approach is memory intensive. The third approach decomposes an orientation- sensitive pattern into an H-list and a V-list, which greatly simplifies the mining process. Extensive experiment studies show that the third method is more efficient and scalable than the apriori and enumeration algorithms. We also present case studies on soccer game snapshots to demonstrate the interesting patterns discovered.\n\n## Task\nCompose an abstract for the title 'Distributed Coordination Guidance in Multi-agent Reinforcement Learning' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper we present a distributed reinforcement learning system that leverages on expert coordination knowledge to improve learning in multi-agent problems. We focus on the scenario where agents can communicate with their neighbors but this communication structure and the number of agents may change over time. We express coordination knowledge as constraints to reduce the joint action space for exploration. We introduce an extra learning level to learn when to make use of these constraints. This extra level is decentralized among the agents, making it suitable for our communication restrictions. Experiment results on tactical real-time strategy and soccer games show that our system is effective in online learning as opposed to existing methods that use individual constraints on agents and coordinated action selection."
      }
    ]
  },
  {
    "tid":"16",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Cooperative coevolutionary algorithms for dynamic optimization: an experimental study.\nContent: Abstract In this paper, we study the cooperative coevolutionary algorithms (CCEAs) for dynamic optimization. We introduce the CCEAs with two popular types of individuals: (1) random immigrants (RIs) that increase the diversity for changing environments, and (2) elitist individuals that increase the local convergence to the optima. The CCEAs are evaluated on a standard suite of benchmark problems and are compared with evolution strategies (ES). Our experimental results show that the CCEAs are efficient in locating and tracking optima in dynamic environments. They are superior to the ES when the RI individuals and the elitist individuals are used. In addition, we empirically investigate how the CCEAs perform with different parameter settings. These settings include collaboration methods, the use of plus\u2013comma selections, and the number of RI individuals and elitist individuals. We also investigate the CCEAs that use a mutative \u03c3-self adaptation. The CCEAs perform the best when they use the best collaboration method and the plus selection. The use of the mutative \u03c3-self adaptation is insignificant. Our results also show that the CCEAs are more scalable than the ES in dynamic environments.\n\nTitle: Investigating collaboration methods of random immigrant scheme in cooperative coevolution\nContent: Previous study shows that using a random immigrant scheme in a cooperative coevolutionary algorithm (RI-CCEA) can significantly track the moving peaks in dynamic optimization. In this paper, we further investigate its behavior in the multi-modal environments where peak locations, peak coverage and peak heights of the moving peaks are changing during the course of optimization. Of the particular interest to us is the different combinations of the collaboration methods used by the original individuals and the RI individuals of the CCEA populations. Empirical comparisons show that in the moderate-changing or slow-changing environments, using the best collaborations in original individuals in the RICCEA outperforms other variants in our experiments, while the choice of the collaboration methods in RI individuals is insignificant. In a fast-changing environment, using the random collaborations in original individuals is crucial to achieve a better performance and the choice of the collaboration methods in RI individuals is also significant.\n\nTitle: Biasing mutations in cooperative coevolution\nContent: In coevolution, species are coevolving in a way that the genetic changes of one species in response to another species are reciprocal. One class of coevolution is cooperative coevolution in which species collaborate to solve the problems. The fitness of an individual in a species is assigned based on how well its collaboration with other individuals of another species can perform. As an extension of evolutionary algorithms (EAs), cooperative revolutionary algorithms (CCEAs) operate similar to EAs, except during fitness evaluations. In this paper, we focus on genetic variation operations of a CCEA: mutations. We present how to bias mutations in cooperative coevolution and compare the performance of a CCEA adopting biasing mutations (CCEA-BM) and a conventional CCEA in which all individuals are encoded in binary representations. Our experimental study shows that biasing mutations can improve the performance of a CCEA on function optimization, in particular when high orders of binary representations are used.\n\nTitle: Improving CMA-ES by random evaluation on the minor eigenspace\nContent: This paper proposes a modification to the covariance matrix adaptation evolution strategies (CMA-ES). The goal of our modification is to reduce the number of function evaluations to adapt the covariance matrix to the optimal one when the standard CMA-ES is used to optimize convex-quadratic objective functions which have repeated or clustered eigenvalues in their Hessian matrices. By randomly evaluating the minor eigenspace, the modified CMA-ES is evaluated on a standard suite of benchmark problems and its performance is compared with that of the standard CMA-ES. The experimental results show that our proposed modification can improve the performance of the CMA-ES when dominant eigenspaces and minor eigenspaces exist in the Hessian matrices of the underlying objective functions.\n\nTitle: An empirical comparison of CMA-ES in dynamic environments\nContent: This paper empirically investigates the behavior of three variants of covariance matrix adaptation evolution strategies (CMA-ES) for dynamic optimization. These three strategies include the elitist (1+1)-CMA-ES, the non-elitist (\u03bc,\u03bb)-CMA-ES and sep-CMA-ES. To better understand the influence of covariance matrix adaptation methods and of the selection methods to the strategies in dynamic environments, we use the state-of-art dynamic optimization benchmark problems to evaluate the performance. We compare these CMA-ES variants with the traditional (1+1)-ES with the one-fifth success rule. Our experimental results show that the simple elitist strategies including the (1+1)-ES and the (1+1)-CMA-ES generally outperform those non-elitist CMA-ES variants on one out of the six dynamic functions. We also investigate the performance when the dynamic environments change with different severity and when the problems are in higher dimensions. The elitist strategies are robust to different severity of dynamic changes, but the performance is worse when the problem dimensions are increased. In high dimensions, the performance of the elitist and the non-elitist versions of CMA-ES are marginally the same.\n\n## Task\nCompose an abstract for the title 'Guided mutations in cooperative coevolutionary algorithms for function optimization' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this study, a mutation method called \"guided mutation\" is proposed. In guided mutation, each individual maintains two mutation control parameters: mutation direction and mutation step size. Guidance in mutations is provided by continuous updates on these parameters during the evolutionary process. We apply guided mutations to cooperative coevolutionary algorithm (CCEA), and compare its performance of optimizing nine common problem domains with those of other CCEAs. Our results show that guided mutations can improve the performance of a CCEA in some problem domains. We discuss the implications of our results and suggest some directions for future research."
      }
    ]
  },
  {
    "tid":"17",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Enabling Work-Efficiency for High Performance Vertex-Centric Graph Analytics on GPUs\nContent: Massive parallel processing power of GPUs has attracted researchers to develop iterative vertex-centric graph processing frameworks for GPUs. Enabling work-efficiency in these solutions, however, is not straightforward and comes at the cost of SIMD-inefficiency and load imbalance. This paper offers techniques that overcome these challenges when processing the graph on a GPU. For a SIMD-efficient kernel operation involving gathering of neighbors and performing reduction on them, we employ an effective task expansion strategy that avoids intra-warp thread underutilization. As recording vertex activeness requires additional data structures, to attenuate the graph storage overhead on limited GPU DRAM, we introduce vertex grouping as a technique that enables trade-off between memory consumption and the work efficiency in our solution. Our experiments show that these techniques provide up to 5.46x over the recently proposed WS-VR [4] framework over multiple algorithms and inputs.\n\n\n\nTitle: Scalable SIMD-Efficient Graph Processing on GPUs.\nContent: The vast computing power of GPUs makes them an attractive platform for accelerating large scale data parallel computations such as popular graph processing applications. However, the inherent irregularity and large sizes of real-world power law graphs makes effective use of GPUs a major challenge. In this paper we develop techniques that greatly enhance the performance and scalability of vertex-centric graph processing on GPUs. First, we present Warp Segmentation, a novel method that greatly enhances GPU device utilization by dynamically assigning appropriate number of SIMD threads to process a vertex with irregular-sized neighbors while employing compact CSR representation to maximize the graph size that can be kept inside the GPU global memory. Prior works can either maximize graph sizes (VWC uses the CSR representation) or device utilization (e.g., CuSha uses the CW representation, however, CW is roughly 2.5x the size of CSR). Second, we further scale graph processing to make use of multiple GPUs while proposing Vertex Refinement to address the challenge of judiciously using the limited bandwidth available for transferring data between GPUs via the PCIe bus. Vertex refinement employs parallel binary prefix sum to dynamically collect only the updated boundary vertices inside GPUs' outbox buffers for dramatically reducing inter-GPU data transfer volume. Whereas existing multi-GPU techniques (Medusa, TOTEM) perform high degree of wasteful vertex transfers. On a single GPU, our framework delivers average speedups of 1.29x to 2.80x over VWC. When scaled to multiple GPUs, our framework achieves up to 2.71x performance improvement compared to inter-GPU vertex communication schemes used by other multi-GPU techniques (i.e., Medusa, TOTEM).\n\nTitle: CuSha: vertex-centric graph processing on GPUs\nContent: Vertex-centric graph processing is employed by many popular algorithms (e.g., PageRank) due to its simplicity and efficient use of asynchronous parallelism. The high compute power provided by SIMT architecture presents an opportunity for accelerating these algorithms using GPUs. Prior works of graph processing on a GPU employ Compressed Sparse Row (CSR) form for its space-efficiency; however, CSR suffers from irregular memory accesses and GPU underutilization that limit its performance. In this paper, we present CuSha, a CUDA-based graph processing framework that overcomes the above obstacle via use of two novel graph representations: G-Shards and Concatenated Windows (CW). G-Shards uses a concept recently introduced for non-GPU systems that organizes a graph into autonomous sets of ordered edges called shards. CuSha's mapping of GPU hardware resources on to shards allows fully coalesced memory accesses. CW is a novel representation that enhances the use of shards to achieve higher GPU utilization for processing sparse graphs. Finally, CuSha fully utilizes the GPU power by processing multiple shards in parallel on GPU's streaming multiprocessors. For ease of programming, CuSha allows the user to define the vertex-centric computation and plug it into its framework for parallel processing of large graphs. Our experiments show that CuSha provides significant speedups over the state-of-the-art CSR-based virtual warp-centric method for processing graphs on GPUs.\n\nTitle: A hybrid shared memory heterogeneous execution platform for PCIe-based GPGPUs\nContent: The disparity between the CPU and GPU domains has forced the programmers to adhere to the traditional driver-based GPU programming approach. The negative implications of this approach are inter-domain data transfer overhead, host memory pressure and CPU underutilization. In this paper, we propose a novel hybrid shared memory-based execution approach to enhance the throughput of the General Purpose GPU(GPGPU) applications. To achive optimal GPU execution, we adopted a midway approach between the shared memory and traditional disjoint memory GPU programming approach. Our design involves OS enhancements and extensions to an OS-integrated open-source GPU driver(GDev) which together provide the GPU application a shared memory execution platform. Our design not only eliminates several drawbacks associated with the traditional GPU programming approach, but allows data-parallel execution across CPUs and GPU.\n\nTitle: An efficient dynamic multiple-candidate motion vector approach for GPU-based hierarchical motion estimation\nContent: Hierarchical or pyramid search is a widely used approach in motion estimation, a most expensive function in video encoding, for its low computational complexity and high efficiency. In this approach, multiple down-sampled resolutions from video frames are created. An initial motion estimation is quickly made at a lowest resolution. The final motion estimation result is achieved by propagating the initial estimation towards the original resolution. GPU or General purpose GPU embedded hundreds of number of SIMD-based cores is best suitable for motion estimation, especially with full-search-based approaches as the process can be efficiently parallelized. However, a common fundamental drawback of the hierarchical search is the erroneous estimation from the reduced resolutions may cause the final motion estimation inaccurate. Multiple-candidate motion vector approaches are proposed, however, they lack a mechanism to select the best multiple-candidate schemes considering diverse video encoding characteristics. In this paper we analyse and verify the computational complexity of the hierarchical search using NVIDIA's GPU with realistic workloads. Based on this analysis, we propose an efficient dynamic multiple-candidate motion vector approach to dynamically select best multiple-candidate motion vector schemes at runtime. This approach can achieve highest possible speedups and satisfy a desire motion estimation efficiency. Experiments on realistic workloads show the dynamic scheme selection outperforms the fixed scheme selection based on profiling.\n\n## Task\nCompose an abstract for the title 'GreenLA: green linear algebra software for GPU-accelerated heterogeneous computing.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"While many linear algebra libraries have been developed to optimize their performance, no linear algebra library considers their energy efficiency at the library design time. In this paper, we present GreenLA - an energy efficient linear algebra software package that leverages linear algebra algorithmic characteristics to maximize energy savings with negligible overhead. GreenLA is (1) energy efficient: it saves up to several times more energy than the best existing energy saving approaches that do not modify library source codes; (2) high performance: its performance is comparable to the highly optimized linear algebra library MAGMA; and (3) transparent to applications: with the same programming interface, existing MAGMA users do not need to modify their source codes to benefit from GreenLA. Experimental results demonstrate that GreenLA is able to save up to three times more energy than the best existing energy saving approaches while delivering similar performance compared to the state-of-the-art linear algebra library MAGMA."
      }
    ]
  },
  {
    "tid":"18",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Locality Preserving Composite Kernel Feature Extraction for Multi-Source Geospatial Image Analysis\nContent: Multi-source data, either from different sensors or disparate features extracted from the same sensor, are valuable for geospatial image analysis due to their potential for providing complementary features. In this paper, a composite-kernel-based feature extraction method is proposed for multi-source remote sensing data classification. Features from different sources are first fused via a weighted composite kernel mapping, and then projected to a lower-dimensional subspace in which kernel local Fisher discriminant analysis (KLFDA) is used to extract the most discriminative information. We hypothesize that after such a projection, multi-source data would have better class separability between classes, and an efficient linear classification model-multinomial logistic regression (MLR) would be suitable for classification. The efficacy of the proposed method is demonstrated via experiments using two different sets of multi-source geospatial data. For feature fusion, the raw spectral data and extended multi-attribute profiles (EMAPs) derived from the hyperspectral image are used as a testbed for multi-source image analysis. The second multi-source testbed used for validation involves sensor fusion, in which the hyperspectral and light detection and ranging (LiDAR) data are utilized. Experimental results show that composite kernel local Fisher's discriminant analysis when combined with MLR based classifier (CKLFDA-MLR) is very effective at feature extraction and classification of multi-source geospatial images.\n\nTitle: Sparse Representations For Classification Of High Dimensional Multi-Sensor Geospatial Data\nContent: Modern active and passive geospatial sensing modalities often result in high dimensional feature spaces. Hyper-spectral imagery results in per-pixel spectral \"signatures\" that represent the reflectance/radiance over hundreds of contiguous spectral bands. Likewise, full-waveform LiDAR systems record the entire waveform of the return Laser pulse. Although the resulting feature spaces are high dimensional, the underlying dimensionality of the information content is often small (i.e., information resides in a lower dimensional subspace), and such data is often sparsely represented in an appropriate dictionary. In recent work, sparse representation based classification has been utilized for effective face recognition tasks. The traditional solution is posed as an optimization problem to learn the representation of test samples using the entire training dictionary under the constraint of sparsity - i.e., Sparse Representation based Classification (SRC). Although this has resulted in promising performance for face recognition tasks (where the number of classes is very large), and to some extent for geospatial image analysis tasks, we contend that the SRC formulation should be modified to yield robust performance for traditional image analysis tasks where the number of classes is not large. In recent work, we developed an alternate approach to use sparse representations for classification of hyperspectral imagery. In this paper, we develop a subspace learning preprocessing that effectively reduces the dimensionality of high dimensional geospatial data (both hyperspectral imagery, and full-waveform LiDAR data), and demonstrate its efficacy when combined with this recently developed sparse representation approach.\n\nTitle: Multiple kernel active learning for robust geo-spatial image analysis\nContent: Exploiting disparate features from potentially different data sources with multiple-kernel based machine learning is a promising approach for analyzing geo-spatial data. A mixture-of-kernel approach can facilitate construction of a more effective training data pool with Active Learning (AL). In addition, this could alleviate the computational burden in AL implementations. Kernel based learning requires hyperparameter tuning for model selection. Further, an optimal function is required to integrate different features or data sources appropriately in the kernel induced space. Both kernel parameters and kernel combination functions may need to be tuned at each AL learning step, which is potentially very time-consuming. In this paper, a novel multiple kernel active learning algorithm is proposed that promises enhanced classification, improved AL performance, and a mechanism for automatic selection of kernel weights in the mixture-of-kernels. We demonstrate the usefulness of the proposed framework with results for both feature fusion and sensor fusion tasks.\n\nTitle: Ensemble Multiple Kernel Active Learning For Classification of Multisource Remote Sensing Data\nContent: Incorporating disparate features from multiple sources can provide valuable diverse information for remote sensing data analysis. However, multisource remote sensing data require large quantities of labeled data to train robust supervised classifiers, which are often difficult and expensive to acquire. A mixture-of-kernel approach can facilitate the construction of an effective formulation for acquiring useful samples via active learning (AL). In this paper, we propose an ensemble multiple kernel active learning (EnsembleMKL-AL) framework that incorporates different types of features extracted from multisensor remote sensing data (hyperspectral imagery and LiDAR data) for robust classification. An ensemble of probabilistic multiple kernel classifiers is embedded into a maximum disagreement-based AL system, which adaptively optimizes the kernel for each source during the AL process. At the end of each learning step, a decision fusion strategy is implemented to make a final decision based on the probabilistic outputs. The proposed framework is tested in a multisource environment, including different types of features extracted from hyperspectral and LiDAR data. The experimental results validate the efficacy of the proposed approach. In addition, we demonstrate that using ensemble classifiers and a large number of disparate but relevant features can further improve the performance of an AL-based classification approach.\n\nTitle: Locality Preserving Genetic Algorithms for Spatial-Spectral Hyperspectral Image Classification\nContent: Recent developments in remote sensing technologies have made hyperspectral imagery (HSI) readily available to detect and classify objects on the earth using pattern recognition techniques. Hyperspectral signatures are composed of densely sampled reflectance values over a wide range of the spectrum. Although most of the traditional approaches for HSI analysis entail per-pixel spectral classification, spatial-spectral exploitation of HSI has the potential to further improve the classification performance-particularly when there is unique class-specific textural information in the scene. Since the dimensionality of such remotely sensed imagery is often very large, especially in spatial-spectral feature domain, a large amount of training data is required to accurately model the classifier. In this paper, we propose a robust dimensionality reduction approach that effectively addresses this problem for hyperspectral imagery (HSI) analysis using spectral and spatial features. In particular, we propose a new dimensionality reduction algorithm, GA-LFDA where a Genetic Algorithm (GA) based feature selection and Local-Fisher's Discriminant Analysis (LFDA) based feature projection are performed in a raw spectral-spatial feature space for effective dimensionality reduction. This is followed by a parametric Gaussian mixture model classifier. Classification results with experimental data show that our proposed method outperforms traditional dimensionality reduction and classification algorithms in challenging small training sample size and mixed pixel conditions.\n\n## Task\nCompose an abstract for the title 'Multisource Geospatial Data Fusion via Local Joint Sparse Representation.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose an adaptive locality weighted multisource joint sparse representation classification (ALWMJ-SRC) model for the classification of multisource remote sensing data. Although the notion of multitask joint sparsity has been recently developed for data fusion and has shown to be effective for various applications, in this paper, we suggest that there are important limitations s..."
      }
    ]
  },
  {
    "tid":"19",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Greedy rule generation from discrete data and its use in neural network rule extraction.\nContent: This paper proposes a GRG (Greedy Rule Generation) algorithm, a new method for generating classification rules from a data set with discrete attributes. The algorithm is \"greedy\" in the sense that at every iteration, it searches for the best rule to generate. The criteria for the best rule include the number of samples and the size of subspaces that it covers, as well as the number of attributes in the rule. This method is employed for extracting rules from neural networks that have been trained and pruned for solving classification problems. The classification rules are extracted from the neural networks using the standard decompositional approach. Neural networks with one hidden layer are trained and the proposed GRG algorithm is applied to their discretized hidden unit activation values. Our experimental results show that neural network rule extraction with the GRG method produces rule sets that are accurate and concise. Application of GRG directly on three medical data sets with discrete attributes also demonstrates its effectiveness for rule generation.\n\nTitle: Extracting rules from neural networks by pruning and hidden-unit splitting.\nContent: An algorithm for extracting rules from a standard three-layer feedforward neural network is proposed. The trained network is first pruned not only to remove redundant connections in the network but, more important, to detect the relevant inputs. The algorithm generates rules from the pruned network by considering only a small number of activation values at the hidden units. If the number of inputs connected to a hidden unit is sufficiently small, then rules that describe how each of its activation values is obtained can be readily generated. Otherwise the hidden unit will be split and treated as output units, with each output unit corresponding to an activation value. A hidden layer is inserted and a new subnetwork is formed, trained, and pruned. This process is repeated until every hidden unit in the network has a relatively small number of input units connected to it. Examples on how the proposed algorithm works are shown using real-world data arising from molecular biology and signal processing. Our results show that for these complex problems, the algorithm can extract reasonably compact rule sets that have high predictive accuracy rates.\n\nTitle: Understanding neural networks via rule extraction\nContent: Although backpropagation neural networks generally predict better than decision trees do for pattern classification problems, they are often regarded as black boxes, i.e., their predictions are not as interpretable as those of decision trees. This paper argues that this is because there has been no proper technique that enables us to do so. With an algorithm that can extract rules, by drawing parallels with those of decision trees, we show that the predictions of a network can be explained via rules extracted from it, thereby, the network can be understood. Experiments demonstrate that rules extracted from neural networks are comparable with those of decision trees in terms of predictive accuracy, number of rules and average number of conditions for a rule; they preserve high predictive accuracy of original networks.\n\nTitle: Extracting M-of-N rules from trained neural networks\nContent: An effective algorithm for extracting M-of-N rules from trained feedforward neural networks is proposed. Two components of the algorithm distinguish our method from previously proposed algorithms which extract symbolic rules from neural networks. First, we train a network where each input of the data can only have one of the two possible values, -1 or one. Second, we apply the hyperbolic tangent function to each connection from the input layer to the hidden layer of the network. By applying this squashing function, the activation values at the hidden units are effectively computed as the hyperbolic tangent (or the sigmoid) of the weighted inputs, where the weights have magnitudes that are equal one. By restricting the inputs and the weights to binary values either -1 or one, the extraction of M-of-N rules from the networks becomes trivial. We demonstrate the effectiveness of the proposed algorithm on several widely tested datasets. For datasets consisting of thousands of patterns with many attributes, the rules extracted by the algorithm are surprisingly simple and accurate.\n\nTitle: An Effective Method for Generating Multiple Linear Regression Rules from Artificial Neural Networks\nContent: We describe a method for multivariate function approximation which combines neural network learning, clustering and multiple regression. Neural networks with a single hidden layer are universal function approximators. However, due to the complexity of the network topology and the nonlinear transfer function used in computing the activation of the hidden units, the predictions of a trained network are difficult to comprehend. On the other hand, predictions from a multiple linear regression equation are easy to understand but not accurate when the underlying relationship between the input variables and the output variable is nonlinear. The method presented in this paper generates a set of multiple linear regression equations using neural networks. The number of regression equations is determined by clustering the weighted input variables. The predictions for samples in the same cluster are computed by the same regression equation. Experimental results on real-world data demonstrate that the new method generates relatively few regression equations from the training data samples. The errors in prediction using these equations are comparable to or lower than those achieved by existing function approximation methods.\n\n## Task\nCompose an abstract for the title 'Neural network training and rule extraction with augmented discretized input.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The classification and prediction accuracy of neural networks can be improved when they are trained with discretized continuous attributes as additional inputs. Such input augmentation makes it easier for the network weights to form more accurate decision boundaries when the data samples of different classes in the data set are contained in distinct hyper-rectangular subregions in the original input space. In this paper, we present first how a neural network can be trained with augmented discretized inputs. The additional inputs are obtained by dividing the original interval of each continuous attribute into subintervals of equal length. The network is then pruned to remove most of the discretized inputs as well as the original continuous attributes as long as the network still achieves a minimum preset accuracy requirement. We then discuss how comprehensible classification rules can be extracted from the pruned network by analyzing the activations of the network hidden units and the weights of the network connections that remain in the pruned network. Our experiments on artificial data sets show that the rules extracted from the neural networks can perfectly replicate the class membership rules used to create the data perfectly. On real-life benchmark data sets, neural networks trained with augmented discretized inputs are shown to achieve better accuracy than neural networks trained with the original data."
      }
    ]
  },
  {
    "tid":"20",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Meta-level patterns for interactive knowledge capture\nContent: Current knowledge acquisition tools have limited understanding of how users enter knowledge and how acquired knowledge is used, and provide limited assistance in organizing various knowledge authoring tasks. Users have to make up for these shortcomings by keeping track of past mistakes, current status, potential new problems, and possible courses of actions by themselves. In this paper, we present a novel extension to existing knowledge acquisition tools where the system organizes the episodes of past interactions through a set of declarative meta-level patterns and improves its suggestions based on relevant episodes. In particular, we focus on 1) assessing the level of confidence in suggesting an action, 2) suggesting how a knowledge authoring action can be done based on successful past actions, and 3) monitoring dynamic changes in the environment to suggest relevant modifications in the knowledge base. A preliminary study with varying synthetic user interactions shows that this meta-level assessment may reduce the number of incorrect suggestions, prevent some of the user mistakes and improve the overall problem solving results.\n\nTitle: Memory based meta-level reasoning for interactive knowledge capture\nContent: Current knowledge acquisition tools are oblivious to the process or strategy that the user may be following in entering new knowledge and unaware of their progress during a session. Users have to make up for these shortcomings by keeping track of the status, progress, potential problems and possible courses of actions by themselves. We present a novel extension to existing systems that 1) keeps track of past problem solving episodes and relates them to user entered knowledge, 2) assesses the current status of the knowledge and the problem solving using such relations, and 3) provides assistance to the user based on the assessment. We applied the approach in developing an intelligent assistant for decision making tasks. The resulting interaction shows that the system helps the user understand the progress and guides the knowledge authoring process in terms of making the knowledge more useful, adapting the knowledge to dynamic changes over time, and making the overall problem solving more successful.\n\nTitle: Proactive dialogue for interactive knowledge capture\nContent: Current tools for interactive knowledge capture have little or no learning aptitude. They are mostly oblivious to the process or strategy that the user may be following in entering new knowledge, unaware of their progress during a session, and ignorant of typical skills expected from a good student. We present an approach to make acquisition interfaces more proactive by extending them with: 1) goals that represent what remains to be learned, 2) strategies to achieve these goals and acquire further knowledge, and 3) awareness of the current status of the body of knowledge learned. The resulting interaction shows that the system is aware of its progress towards acquiring the new knowledge, and moves forward by understanding what acquisition goals and strategies to pursue.\n\nTitle: User studies of an interdependency-based interface for acquiring problem-solving knowledge\nContent: This paper describes a series of experiments with a range of users to evaluate an intelligent interface for acquiring problem-solving knowledge to describe how to accomplish a task. The tool derives the interdependencies between different pieces of knowledge in the system and uses them to guide the user in completing the acquisition task. The paper describes results obtained when the tool was tested with a wide range of users, including end users. The studies show that our acquisition interface saves users an average of 32% of the time it takes to add new knowledge, and highlight some interesting differences across user groups. The paper also describes what are the areas that need to be addressed in future research in order to make these tools usable by end users.\n\nTitle: Principles for interactive acquisition and validation of workflows\nContent: Workflows, also known as process models, are essential in many science and engineering fields. Workflows express compositions of individual steps or tasks that assembled together account for various aspects of an overall process. When workflows include dozens of components and many links among them, the creation of valid workflows becomes challenging since users have to track many interdependencies and constraints. This article describes principles for assisting users to create valid workflows that are based on two knowledge acquisition systems that we have developed. A shared goal in these projects was to enable end users who do not have computer science backgrounds, such as biologists, military officers or engineers, to create valid end-to-end process models or workflows. Our approach exploits knowledge-rich descriptions of the individual components and their constraints in order to validate the composition, and uses artificial intelligence planning techniques in order to systematically verify formal properties of valid workflows. Both systems analyse partial workflows created by the user, determine whether they are consistent with the background knowledge that the system has, notifies the user of issues to be resolved in the current workflow, and suggests to the user what actions could be taken to correct those issues.\n\n## Task\nCompose an abstract for the title 'Reflection patterns for interactive knowledge capture' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Current knowledge acquisition tools have limited understanding of how users enter knowledge and how acquired knowledge is used, and provide limited assistance in organizing various knowledge authoring tasks. In this paper, we present a novel extension to existing knowledge acquisition tools where the system 1) captures the episodes of knowledge acquisition and knowledge use through a set of declarative reflection patterns 2) performs assessment on how to improve the future knowledge acquisition and knowledge use based on captured episodes, and 3) provides assistance to the users by combining the assessment results."
      }
    ]
  },
  {
    "tid":"21",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Strategic Choices of Inter-Organizational Information Systems: A Customer-Supplier Network Perspective\nContent: As more and more cooperate in a networked manner via various inter-organizational information systems (IOISs), it is important to choose appropriate IOISs for different types of organizations in the network environment. In this study, we analyzed customer-supplier relationships among organizations in five industries using social network analysis (SNA) methods and empirical data, aiming to help organizations to choose appropriate IOISs strategically. Three types of customer-supplier networks were identified in our analysis: customer-centric, supplier-centric and balanced networks. We then propose how to choose appropriate IOISs for the firms in each of these networks/industries and discussed the pros and cons of the choices. To the best of our knowledge, this is the first empirical research that applied SNA methods to study customer-supplier networks in the context of inter-organizational information systems.\n\nTitle: Web Services and Service-Oriented Application Provisioning: An Analytical Study of Application Service Strategies\nContent: As a new network computing technology, Web services help reduce application integration cost and have become the foundation for the next generation of service-oriented architecture. While there is ample research on the technological issues of Web services, little has been done thus far on the business implications of this groundbreaking technology. In this paper, we attempt to close this research gap by comparing three application service strategies for providing Web services of complementary functionalities, i.e., independent service vendors (ISVs), joint venture (JV), and strategic alliance (SA). Although Web service integration is analogous to product bundling, the unique features of software integration, such as low integration cost and the possibility of creating an integrated Web service with new functionalities, have led to interesting analytical results not seen in the product bundling literature. We find that the optimal application service strategy is determined by several factors, including the integration cost, the valuations, and market potentials of the individual and integrated Web services. Our study shows that the application service strategy of ISVs is always dominated by the SA, implying that Web service providers benefit from the increased interoperability of Web services. In cases where the integrated Web service is highly valuated and the integration cost is small, JV becomes the optimal application service strategy. In other cases, the Web service providers are better off to form SAs\n\nTitle: Designing Workflow Management Systems for Virtual Organizations: An Empirically Grounded Approach\nContent: The extant literature on business process re-engineering has reported extensively on process automation and re-engineering for regular organizations. In contrast, relatively little attention has been paid to transient organizational structures such as virtual teams. This study examines how a virtual team can benefit from workflow management systems. Our point of departure is an empirical analysis of the communication patterns and problems in an existing virtual team. The analysis indicates that the communication activities in the team are heavily skewed in time, on subjects, and towards certain team members. This causes overload, bottlenecks and other types of dysfunctionality. We suggest how the communication patterns can be re-engineered through a set of Web-based information technology solutions.\n\nTitle: A case-based reasoning framework for workflow model management\nContent: In order to support efficient workflow design, recent commercial workflow systems are providing templates of common business processes. These templates, called cases, can be modified individually or collectively into a new workflow to meet the business specification. However, little research has been done on how to manage workflow models, including issues such as model storage, model retrieval, model reuse and assembly. In this paper, we propose a novel framework to support workflow modeling and design by adapting workflow cases from a repository of process models. Our approach to workflow model management is based on a structured workflow lifecycle and leverages recent advances in model management and case-based reasoning techniques. Our contributions include a conceptual model of workflow cases, a similarity flooding algorithm for workflow case retrieval, and a domain-independent AI planning approach to workflow case composition. We illustrate the workflow model management framework with a prototype system called Case-Oriented Design Assistant for Workflow Modeling (CODAW).\n\nTitle: A Framework for Dynamic Routing and Operational Integrity Controls in a Workflow Management System\nContent: This paper describes a general framework for implementing dynamic routing and operational integrity control mechanisms in a Workflow Management System (WMS). The framework consists of three techniques: workflow control matrices, sequence constraints, and event-based constraints. These techniques are described and illustrated by means of example applications. We show how these techniques can exploit parallelism, and lead to more efficient processes and better workflow systems. Another unique aspect of this framework is that it can handle various kinds of exception situations that arise in workflow applications. The application of the three techniques are discussed in the context of the case of a consumer loan management application. Finally, the unique features of our framework are compared with other approaches based on static routing.\n\n## Task\nCompose an abstract for the title 'Outsourcing Resource Selection: A Rough Set Approach' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Today's economic reality is forcing firms to become increasingly more efficient in managing their resource functions. Outsourcing has moved to the mainstream of business development and promised to be one of the many enterprise strategies for cost-effective service delivery. Proper screening and automatic selection of outsource partners are critical to the business. Resource selection is one of the most important steps in outsourcing decision-making processes. This paper considers a data intensive selection problem in outsourcing software development projects. We analyze the properties of the resource selection problem and propose some criteria for an automatic resource selection model. A naive model, a traditional rough set model, and a generalized rough set (GRS) model are introduced and the advantages and disadvantages of each model are compared. Experimental results indicate that the GRS model is superior to other models."
      }
    ]
  },
  {
    "tid":"22",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A signal-flow-graph approach to on-line gradient calculation.\nContent: A large class of nonlinear dynamic adaptive systems such as dynamic recurrent neural networks can be effectively represented by signal flow graphs (SFGs). By this method, complex systems are described as a general connection of many simple components, each of them implementing a simple one-input, one-output transformation, as in an electrical circuit. Even if graph representations are popular in the neural network community, they are often used for qualitative description rather than for rigorous representation and computational purposes. In this article, a method for both on-line and batch-backward gradient computation of a system output or cost function with respect to system parameters is derived by the SFG representation theory and its known properties. The system can be any causal, in general nonlinear and time-variant, dynamic system represented by an SFG, in particular any feedforward, time-delay, or recurrent neural network. In this work, we use discrete-time notation, but the same theory holds for the continuous-time case. The gradient is obtained in a straightforward way by the analysis of two SFGs, the original one and its adjoint (obtained from the first by simple transformations), without the complex chain rule expansions of derivatives usually employed. This method can be used for sensitivity analysis and for learning both off-line and on-line. On-line learning is particularly important since it is required by many real applications, such as digital signal processing, system identification and control, channel equalization, and predistortion.\n\nTitle: A novel signal flow graph based solution for calculation of first and second derivatives of dynamical nonlinear systems\nContent: In this paper, the problem of calculation of first and second derivatives in general non-linear dynamical systems is addressed and an attempt of solution by means of signal flow graph (SFG) techniques is proposed. First and full second derivatives of an output of the initial system respect with the node variables of the starting SFG are delivered through an adjoint graph derived without using Lee's theorem. Mixed second derivatives are deduced by quantities attained in adjoint graphs of the original graph or graphs related to it. A detailed theoretical demonstration of these formulations is given. Even though no adjoint graph has been derived in case of mixed derivatives, the ability of the proposed method to determine all Hessian matrix entries in a complete automatic way is highlighted.\n\nTitle: Calculation of non-mixed second derivatives in multirate systems through signal flow graph techniques\nContent: This paper proposes a new approach for calculation of derivatives in general multirate systems through a signal flow graph (SFG) technique. The first original aspect consists of the derivation of an adjoint graph without using Lee's theorem. Secondly, such a graph is able to deliver not only the first derivatives but also the full second derivatives of an output of the initial system with respect to the node variables of the starting SFG. Some examples are reported to show the right way of working of the proposed method on derivative calculation in general situations. Hence, the overall algorithm represents a useful tool for determination of Jacobean and Hessian based information in learning systems, as was already done in other related but less general contributions in the literature.\n\nTitle: A General Approach to Gradient Based Learning in Multirate Systems and Neural Networks\nContent: A large class of non-linear dynamic adaptive systems such as dynamic recurrent neural networks can be very effectively represented by Signal-Flow-Graphs (SFGs). By this method, complex systems are described as a general connection of many simple components, each of them implementing a simple one-input one-output transformation, as in an electrical circuit. Even if graph representations are popular in the neural network community, they are often use d for qualitative description rather than for rigorous representation and computational purposes. Following an approach originally developed by A.Y. Lee for continuous-time systems based on the concept of adjoint graph, a new algorithm to estimate the derivative of the output with respect to an internal parameter was recently proposed by some of the authors for discrete-time systems. This paper extends further this approach to multirate digital systems, which are nowadays widely used. The new method can be employed for gradient-based learning of general multirate circuits, such as new \u9a74multirate\u9a74 neural networks.\n\nTitle: On-Line Learning Algorithms For Neural Networks With Iir Synapses\nContent: This paper is focused on the learning algorithms for dynamic multilayer perceptron neural networks where each neuron synapsis is modelled by an infinite impulse response (IIR) filter (IIR MLP). In particular, the Backpropagation Through Time (BPTT) algorithm and its less demanding approximated on-line versions are considered. In fact it is known that the BPTT algorithm is not causal and therefore can be implemented only in batch mode, while many real problems require on-line adaptation. In this paper we give the complete BPTT formulation for the IIR MLP, derive an already known on-line learning algorithm as a particular approximation of the BPTT, and propose a new approximated algorithm. Several computer simulations of identification of dynamical systems will also be presented to assess the performance of the approximated algorithms and to compare the IIR MLP with more traditional dynamic networks.\n\n## Task\nCompose an abstract for the title 'Signal-Flow-Graph Derivation Of On-Line Gradient Learning Algorithms' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, making use of the Signal-Flow-Graph (SFG) representation and its known properties, we derive a new general method for backward gradient computation of a system output or cost function with respect to past (or present) system parameters. The system can be any causal, in general non-linear and time-variant, dynamic system represented by a SFG, in particular any feed forward or recurrent neural network. In this work we use discrete time notation, but the same theory holds for the continuos time case. The gradient is obtained by the analysis of two SFGs, the original one and its adjoint. This method can be used both for on-line and off-line learning. In the latter case using the Mean Square Error cost function, our approach particularises to E. Wan's method that is not suited for online training of recurrent networks. Computer simulations of non-linear dynamic systems identification will also be presented to assess the performance of the algorithm resulting from the application of the proposed method in the case of locally recurrent neural networks."
      }
    ]
  },
  {
    "tid":"23",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Improving manual reviews in function-centered engineering of embedded systems using a dedicated review model\nContent: In model-based engineering of embedded systems, manual validation activities such as reviews and inspections are needed to ensure that the system under development satisfies the stakeholder intentions. During the engineering process, changes in the stakeholder intentions typically trigger revisions of already developed and documented engineering artifacts including requirements and design specifications. In practice, changes in stakeholder intentions are often not immediately perceived and not properly documented. Moreover, they are quite often not consistently incorporated into all relevant engineering artifacts. In industry, typically manual reviews are executed to ensure that the relevant stakeholder intentions are adequately considered in the engineering artifacts. In this article, we introduce a dedicated review model to aid the reviewer in conducting manual reviews of behavioral requirements and functional design specification\u2014two core artifacts in function-centered engineering of embedded software. To investigate whether the proposed solution is beneficial we conducted controlled experiments showing that the use of the dedicated review model can significantly increase the effectiveness and efficiency of manual reviews. Additionally, the use of the dedicated review model leads to significantly more confident decisions of the reviewers and is perceived by the reviewers as significantly more supportive compared with reviews without the dedicated review model.\n\nTitle: Requirements engineering for embedded systems: an investigation of industry needs\nContent: [Context and Motivation] Requirements engineering (RE) research is expected to provide methods that address the specific challenges of industrial systems engineering. [Question/problem] For this purpose, researchers need a detailed understanding of the needs and expectations that the industry has regarding RE methods. [Principal ideas/results] To identify the key industry needs, we have conducted an in-depth study with representatives from large, internationally operating companies in the domain of embedded systems in Germany. [Contribution] This paper reports on the identified industry needs related to the topics natural language vs. requirements models, support for high system complexity, quality assurance of requirements, and intertwining of RE and design.\n\nTitle: Industry needs and research directions in requirements engineering for embedded systems\nContent: The industry has a strong demand for sophisticated requirements engineering (RE) methods in order to manage the high complexity of requirements specifications for software-intensive embedded systems and ensure a high requirements quality. RE methods and techniques proposed by research are only slowly adopted by the industry. An important step to improve the adoption of novel RE approaches is to gain a detailed understanding of the needs, expectations, and constraints that RE approaches must satisfy. We have conducted an industrial study to gain an in-depth understanding of practitioners\u2019 needs concerning RE research and method development. The study involved qualitative interviews as well as quantitative data collection by means of questionnaires. We report on the main results of our study related to five aspects of RE approaches: the use of requirements models, the support for high system complexity, quality assurance for requirements, the transition between RE and architecture design, and the interrelation of RE and safety engineering. Based on the results of the study, we draw conclusions for future RE research.\n\nTitle: Guiding requirements engineering for software-intensive embedded systems in the automotive industry\nContent: Over the past decade, a dramatic increase of functionality, quantity, size, and complexity of software-intensive embedded systems in the automotive industry can be observed. In particular, the growing complexity drives current requirements engineering practices to the limits. In close cooperation between partners from industry and academia, the recently completed REMsES (Requirements Engineering and Management for software-intensive Embedded Systems) project has developed a guideline to support requirements engineering processes in the automotive industry. The guideline enables the requirements engineers to cope with the challenges that arise due to quantity, size and complexity of software-intensive systems. This article presents the major results of the project, namely, the fundamental principles of the approach, the guideline itself, the tool support, and the major findings obtained during the evaluation of the approach.\n\nTitle: Structuring variability in the context of embedded systems during software engineering\nContent: During the development of embedded software, the system context (mechanical, electronical, business, etc.) has to be considered. Typically, this context is diverse and highly complex. Moreover, the context in which the system is embedded can vary. For example, the system can be used in different technical environments or in different countries. This variability in the context influences the software to be developed and typically leads to system variability. This paper systematically analyses the impact of context variability on the system development, more precisely, on the variability of the system. Related work is discussed and an example from the automotive domain is presented to identify open issues that need to be addressed.\n\n## Task\nCompose an abstract for the title 'Validating the Functional Design of Embedded Systems against Stakeholder Intentions.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In the embedded systems industry, function-centered engineering is commonly applied to address the increasing number and complexity of system functions. During function-centered engineering, the functional design that is created based on the defined requirements for the system is the main artifact that serves as a basis for subsequent development activities. If stakeholder intentions change and modifications become necessary, they are frequently incorporated directly into the functional design without updating the behavioral requirements accordingly. As a consequence, the correctness of the interplay of system functions as defined in the functional design cannot be assessed by checking it against the defined requirements (since they are outdated) but needs to be checked against the current stakeholder intentions. More precisely, the requirements engineer has to validate the functional design against the stakeholder intentions because he is the expert concerning the stakeholder intentions and can communicate with the stakeholders regarding them, if necessary. However, the requirements engineer is typically not familiar with the functional design and its notation on the one hand, and, on the other hand, the overall behavior of the system is spread across various diagrams in the functional design. Therefore, the requirements engineer needs a more abstract and consolidated view of the functional design in order to be able to validate its correctness with regard to the current stakeholder intentions. In this paper, we present an approach which is based on a specific kind of review model that is automatically generated from the functional design and supports the requirements engineer in her task. The approach that is presented in this paper is subject of ongoing research."
      }
    ]
  },
  {
    "tid":"24",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Combining image-level and segment-level models for automatic annotation\nContent: For the task of assigning labels to an image to summarize its contents, many early attempts use segment-level information and try to determine which parts of the images correspond to which labels. Best performing methods use global image similarity and nearest neighbor techniques to transfer labels from training images to test images. However, global methods cannot localize the labels in the images, unlike segment-level methods. Also, they cannot take advantage of training images that are only locally similar to a test image. We propose several ways to combine recent image-level and segment-level techniques to predict both image and segment labels jointly. We cast our experimental study in an unified framework for both image-level and segment-level annotation tasks. On three challenging datasets, our joint prediction of image and segment labels outperforms either prediction alone on both tasks. This confirms that the two levels offer complementary information.\n\nTitle: Efficient Object Annotation via Speaking and Pointing.\nContent: Deep neural networks deliver state-of-the-art visual recognition, but they rely on large datasets, which are time-consuming to annotate. These datasets are typically annotated in two stages: (1) determining the presence of object classes at the image level and (2) marking the spatial extent for all objects of these classes. In this work we use speech, together with mouse inputs, to speed up this process. We first improve stage one, by letting annotators indicate object class presence via speech. We then combine the two stages: annotators draw an object bounding box via the mouse and simultaneously provide its class label via speech. Using speech has distinct advantages over relying on mouse inputs alone. First, it is fast and allows for direct access to the class name, by simply saying it. Second, annotators can simultaneously speak and mark an object location. Finally, speech-based interfaces can be kept extremely simple, hence using them requires less mouse movement compared to existing approaches. Through extensive experiments on the COCO and ILSVRC datasets we show that our approach yields high-quality annotations at significant speed gains. Stage one takes $$2.3{\\times }-14.9{\\times }$$ less annotation time than existing methods based on a hierarchical organization of the classes to be annotated. Moreover, when combining the two stages, we find that object class labels come for free: annotating them at the same time as bounding boxes has zero additional cost. On COCO, this makes the overall process $$1.9\\times $$ faster than the two-stage approach.\n\nTitle: ImageNet Auto-Annotation with Segmentation Propagation\nContent: ImageNet is a large-scale hierarchical database of object classes with millions of images.We propose to automatically populate it with pixelwise object-background segmentations, by leveraging existing manual annotations in the form of class labels and bounding-boxes. The key idea is to recursively exploit images segmented so far to guide the segmentation of new images. At each stage this propagation process expands into the images which are easiest to segment at that point in time, e.g. by moving to the semantically most related classes to those segmented so far. The propagation of segmentation occurs both (a) at the image level, by transferring existing segmentations to estimate the probability of a pixel to be foreground, and (b) at the class level, by jointly segmenting images of the same class and by importing the appearance models of classes that are already segmented. Through experiments on 577 classes and 500k images we show that our technique (i) annotates a wide range of classes with accurate segmentations; (ii) effectively exploits the hierarchical structure of ImageNet; (iii) scales efficiently, especially when implemented on superpixels; (iv) outperforms a baseline GrabCut (Rother et al. 2004 ) initialized on the image center, as well as segmentation transfer from a fixed source pool and run independently on each target image (Kuettel and Ferrari 2012 ). Moreover, our method also delivers state-of-the-art results on the recent iCoseg dataset for co-segmentation.\n\nTitle: Using Multi-view Recognition and Meta-data Annotation to Guide a Robot's Attention\nContent: In the transition from industrial to service robotics, robots will have to deal with increasingly unpredictable and variable environments. We present a system that is able to recognize objects of a certain class in an image and to identify their parts for potential interactions. The method can recognize objects from arbitrary viewpoints and generalizes to instances that have never been observed during training, even if they are partially occluded and appear against cluttered backgrounds. Our approach builds on the implicit shape model of Leibe et al. We extend it to couple recognition to the provision of meta-data useful for a task and to the case of multiple viewpoints by integrating it with the dense multi-view correspondence finder of Ferrari et al. Meta-data can be part labels but also depth estimates, information on material types, or any other pixelwise annotation. We present experimental results on wheelchairs, cars, and motorbikes.\n\nTitle: Who's Doing What: Joint Modeling of Names and Verbs for Simultaneous Face and Pose Annotation.\nContent: Given a corpus of news items consisting of images accompanied by text captions, we want to find out \"who's doing what\", i.e. associate names an d action verbs in the captions to the face and body pose of the persons in the images. We present a joint model for simultaneously solving the image-caption correspondences and learning visual appearance models for the face and pose classes occurring in the corpus. These models can then be used to recognize people and actions in novel images without captions. We demonstrate experimentally that our joint 'face and pose' model solves the correspondence problem better than e arlier models cover- ing only the face, and that it can perform recognition of new uncaptioned images.\n\n## Task\nCompose an abstract for the title 'Fluid Annotation: a human-machine collaboration interface for full image annotation.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We introduce Fluid Annotation, an intuitive human-machine collaboration interface for annotating the class label and outline of every object and background region in an image. Fluid annotation is based on three principles:(I) Strong Machine-Learning aid. We start from the output of a strong neural network model, which the annotator can edit by correcting the labels of existing regions, adding new regions to cover missing objects, and removing incorrect regions.The edit operations are also assisted by the model.(II) Full image annotation in a single pass. As opposed to performing a series of small annotation tasks in isolation [51,68], we propose a unified interface for full image annotation in a single pass.(III) Empower the annotator.We empower the annotator to choose what to annotate and in which order. This enables concentrating on what the ma-chine does not already know, i.e. putting human effort only on the errors it made. This helps using the annotation budget effectively.\n\nThrough extensive experiments on the COCO+Stuff dataset [11,51], we demonstrate that Fluid Annotation leads to accurate an-notations very efficiently, taking 3x less annotation time than the popular LabelMe interface [70].\n\n"
      }
    ]
  },
  {
    "tid":"25",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Towards situated speech understanding: visual context priming of language models\nContent: Fuse is a situated spoken language understanding system that uses visual context to steer the interpretation of speech. Given a visual scene and a spoken description, the system finds the object in the scene that best fits the meaning of the description. To solve this task, Fuse performs speech recognition and visually-grounded language understanding. Rather than treat these two problems separately, knowledge of the visual semantics of language and the specific contents of the visual scene are fused during speech processing. As a result, the system anticipates various ways a person might describe any object in the scene, and uses these predictions to bias the speech recognizer towards likely sequences of words. A dynamic visual attention mechanism is used to focus processing on likely objects within the scene as spoken utterances are processed. Visual attention and language prediction reinforce one another and converge on interpretations of incoming speech signals which are most consistent with visual context. In evaluations, the introduction of visual context into the speech recognition process results in significantly improved speech recognition and understanding accuracy. The underlying principles of this model may be applied to a wide range of speech understanding problems including mobile and assistive technologies in which contextual information can be sensed and semantically interpreted to bias processing.\n\nTitle: A visually grounded natural language interface for reference to spatial scenes\nContent: Many user interfaces, from graphic design programs to navigation aids in cars, share a virtual space with the user. Such applications are often ideal candidates for speech interfaces that allow the user to refer to objects in the shared space. We present an analysis of how people describe objects in spatial scenes using natural language. Based on this study, we describe a system that uses synthetic vision to \"see\" such scenes from the person's point of view, and that understands complex natural language descriptions referring to objects in the scenes. This system is based on a rich notion of semantic compositionality embedded in a grounded language understanding framework. We describe its semantic elements, their compositional behaviour, and their grounding through the synthetic vision system. To conclude, we evaluate the performance of the system on unconstrained input.\n\nTitle: Grounded semantic composition for visual scenes\nContent: We present a visually-grounded language understanding model based on a study of how people verbally describe objects in scenes. The emphasis of the model is on the combination of individual word meanings to produce meanings for complex referring expressions. The model has been implemented, and it is able to understand a broad range of spatial referring expressions. We describe our implementation of word level visually-grounded semantics and their embedding in a compositional parsing framework. The implemented system selects the correct referents in response to natural language expressions for a large percentage of test cases. In an analysis of the system's successes and failures we reveal how visual context influences the semantics of utterances and propose future extensions to the model that take such context into account.\n\nTitle: Grounded Situation Models For Robots: Where Words And Percepts Meet\nContent: Our long-term objective is to develop robots that engage in natural language-mediated cooperative tasks with humans. To support this goal, we are developing an amodal representation and associated processes which is called a grounded situation model (GSM). We are also developing a modular architecture in which the GSM resides in a centrally located module, around which there are language, perception, and action-related modules. The GSM acts as a sensor-updated \"structured blackboard\", that serves as a workspace with contents similar to a \"theatrical stage\" in the robot's \"mind\", which might be filled in with present, past or imagined situations. Two main desiderata drive the design of the GSM: first, \"parsing\" situations into ontological types and relations that reflect human language semantics, and second, allowing bidirectional translation between sensory-derived data/expectations and linguistic descriptions. We present an implemented system that allows of a range of conversational and assistive behavior by a manipulator robot. The robot updates beliefs (held in the GSM) about its physical environment, the human user, and itself, based on a mixture of linguistic, visual and proprioceptive evidence. It can answer basic questions about the present or past and also perform actions through verbal interaction. Most importantly, a novel contribution of our approach is the robot's ability for seamless integration of both language- and sensor-derived information about the situation: For example, the system can acquire parts of situations either by seeing them or by \"imagining\" them through descriptions given by the user: \"There is a red ball at the left\". These situations can later be used to create mental imagery and sensory expectations, thus enabling the aforementioned bidirectionality.\n\nTitle: A visual context-aware multimodal system for spoken language processing\nContent: Recent psycholinguistic experiments show that acoustic and syntactic aspects of online speech processing are influenced by visual context through cross-modal influences. During inter- pretation of speech, visual context seems to steer speech pro- cessing and vice versa. We present a real-time multimodal sys- tem motivated by these findings that performs early integration of visual contextual information to recognize the most likely word sequences in spoken language utterances. The system first acquires a grammar and a visually grounded lexicon from a \"show-and-tell\" procedure where the training input consists of camera images consisting of sets of objects paired with ver- bal object descriptions. Given a new scene, the system gener- ates a dynamic visually-grounded language model and drives a dynamic model of visual attention to steer speech recognition search paths towards more likely word sequences.\n\n## Task\nCompose an abstract for the title 'Learning visually grounded words and syntax for a scene description task' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"A spoken language generation system has been developed that learns to describe objects in computer-generated visual scenes. The system is trained by a \u2018show-and-tell\u2019 procedure in which visual scenes are paired with natural language descriptions. Learning algorithms acquire probabilistic structures which encode the visual semantics of phrase structure, word classes, and individual words. Using these structures, a planning algorithm integrates syntactic, semantic, and contextual constraints to generate natural and unambiguous descriptions of objects in novel scenes. The system generates syntactically well-formed compound adjective noun phrases, as well as relative spatial clauses. The acquired linguistic structures generalize from training data, enabling the production of novel word sequences which were never observed during training. The output of the generation system is synthesized using word-based concatenative synthesis drawing from the original training speech corpus. In evaluations of semantic comprehension by human judges, the performance of automatically generated spoken descriptions was comparable to human-generated descriptions. This work is motivated by our long-term goal of developing spoken language processing systems which grounds semantics in machine perception and action."
      }
    ]
  },
  {
    "tid":"26",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A UML 2.0/OCL Extension for Designing Secure Data Warehouses\nContent: At present, it is very difficult to develop a methodology that fulfills all criteria and comprises all security constraints in the successful design of data warehouses. If that methodology were developed, its complexity would hinder its success. The solution, therefore, would be an approach in which techniques and models defined by the most accepted model standards were extended by integrating the necessary security aspects that at this moment in time are not covered by the existing methodologies. In this paper, we will focus on solving confidentiality problems in the conceptual modelling of data warehouses by defining a profile using the UML 2.0 extensibility mechanisms. In addition, we define an OCL extension that allows us to specify the security constraints of the elements in conceptual modelling of data warehouses and we apply this profile to an example.\n\nTitle: Using UML packages for designing secure data warehouses\nContent: Due to the sensitive data contained in Data Warehouses (DWs), it is essential to specify security measures from the early stages of the DWs design and enforce them. In this paper, we will present a UML profile to represent multidimensional and security aspects of our conceptual modeling. Our approach proposes the use of UML packages in order to group classes together into higher level units creating different levels of abstraction, and therefore, simplifying the final model. Furthermore, we present an extension of the relational model to consider security and audit measures represented in the conceptual modeling. To accomplish this, we based on the Relational Package of the Common Warehouse Metamodel (CWM) and extend it to properly represent all security and audit rules defined in the conceptual modeling of DWs. Finally, we will show an example to illustrate the applicability of our proposal.\n\nTitle: A UML 2.0 profile to define security requirements for Data Warehouses\nContent: Many Data Warehouses (DWs) fail to provide the appropriate information because the users' requirements are not correctly modeled. In addition, the security requirements are considered in the final implementation, and do not take the users' necessities into consideration. However, as DWs store confidential and sensitive information, it is crucial to take security measures into account from early DW design phases, and to enforce them. This paper proposes a profile which uses the Unified Modeling Language (UML) extensibility mechanisms. This profile allows us to define security requirements for DWs at the business level, taking into account the information requirements modeled with a previous profile. Our proposal is aligned with Model Driven Architecture (MDA), thus permitting the transformation of security requirements throughout the entire DW life cycle. Finally, in order to show the benefits of our profile, we develop a case study related to the management of a pharmacy consortium business.\n\nTitle: Designing Secure Data Warehouses by Using MDA and QVT\nContent: The Data Warehouse (DW) design is based on multidimensional ( MD) modeling which structures information into facts and dimensions. Due to the confidentiality of the data that it stores, it is crucial to specify security and audit measures from the early stages of design and to enforce them throughout the lifecycle. Moreover, the standard framework for software development, Model Driven Architecture (MDA), allows us to de. ne transformations between models by proposing Query/View/Transformations (QVT). This proposal permits the definition of formal, elegant and unequivocal transformations between Platform Independent Models (PIM) and Platform Specific Models (PSM). This paper introduces a new framework for the design of secure DWs based on MDA and QVT, which covers all the design phases (conceptual, logical and physical) and specifies security measures in all of them. We first define two metamodels with which to represent security and audit measures at the conceptual and logical levels. We then go on to de. ne a transformation between these models through which to obtain the traceability of the security rules from the early stages of development to the final implementation. Finally, in order to show the benefits of our proposal, it is applied to a case study.\n\nTitle: Model driven development of secure XML data warehouses: a case study\nContent: Data Warehouses (DWs) are currently considered to be the cornerstone of Business Intelligence (BI) systems. Security is a key issue in DWs since the business information that they manage is crucial and highly sensitive, and should be carefully protected. However, the increasing amount of data available on the Web signifies that more and more DW systems are considering the Web as the primary data source through which to populate their DWs. XML is therefore widely accepted as being the principal means through which to provide easier data and metadata interchange among heterogeneous data sources from the Web and the DW systems. Although security issues have been considered during the whole development process of traditional DWs, current research lacks approaches with which to consider security when the target platform is based on the Web and XML technologies. The idiosyncrasy of the unstructured and semi-structured data available on the Web definitely requires particular security rules that are specifically tailored to these systems in order to permit their particularities to be captured correctly. In order to tackle this situation, in this paper, we propose a methodological approach based on the Model Driven Architecture (MDA) for the development of Secure XML DWs. We therefore specify a set of transformation rules that are able to automatically generate not only the corresponding XML structure of the DW from secure conceptual DW models, but also the security rules specified within the DW XML structure, thus allowing us to implement both aspects simultaneously. A case study is provided at the end of the paper to show the benefits of our approach.\n\n## Task\nCompose an abstract for the title 'Developing secure data warehouses with a UML extension' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Data Warehouses (DWs), Multidimensional (MD) Databases, and On-Line Analytical Processing Applications are used as a very powerful mechanism for discovering crucial business information. Considering the extreme importance of the information managed by these kinds of applications, it is essential to specify security measures from the early stages of the DW design in the MD modeling process, and enforce them. In the past years, some proposals for representing main MD modeling properties at the conceptual level have been stated. Nevertheless, none of these proposals considers security issues as an important element in its model, so they do not allow us to specify confidentiality constraints to be enforced by the applications that will use these MD models. In this paper, we will discuss the specific confidentiality problems regarding DWs as well as present an extension of the Unified Modeling Language for specifying security constraints in the conceptual MD modeling, thereby allowing us to design secure DWs. One key advantage of our approach is that we accomplish the conceptual modeling of secure DWs independently of the target platform where the DW has to be implemented, allowing the implementation of the corresponding DWs on any secure commercial database management system. Finally, we will present a case study to show how a conceptual model designed with our approach can be directly implemented on top of Oracle 10g."
      }
    ]
  },
  {
    "tid":"27",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Improving contextual advertising matching by using Wikipedia thesaurus knowledge\nContent: a prevalent type of Web advertising, contextual advertising refers to the placement of the most relevant commercial ads within the content of a Web page, to provide a better user experience and as a result increase the user's ad-click rate. However, due to the intrinsic problems of homonymy and polysemy, the low intersection of keywords, and a lack of sufficient semantics, traditional keyword matching techniques are not able to effectively handle contextual matching and retrieve relevant ads for the user, resulting in an unsatisfactory performance in ad selection. In this paper, we introduce a new contextual advertising approach to overcome these problems, which uses Wikipedia thesaurus knowledge to enrich the semantic expression of a target page (or an ad). First, we map each page into a keyword vector, upon which two additional feature vectors, the Wikipedia concept and category vector derived from the Wikipedia thesaurus structure, are then constructed. Second, to determine the relevant ads for a given page, we propose a linear similarity fusion mechanism, which combines the above three feature vectors in a unified manner. Last, we validate our approach using a set of real ads, real pages along with the external Wikipedia thesaurus. The experimental results show that our approach outperforms the conventional contextual advertising matching approaches and can substantially improve the performance of ad selection.\n\nTitle: An efficient Wikipedia semantic matching approach to text document classification.\nContent: A traditional classification approach based on keyword matching represents each text document as a set of keywords, without considering the semantic information, thereby, reducing the accuracy of classification. To solve this problem, a new classification approach based on Wikipedia matching was proposed, which represents each document as a concept vector in the Wikipedia semantic space so as to understand the text semantics, and has been demonstrated to improve the accuracy of classification. However, the immense Wikipedia semantic space greatly reduces the generation efficiency of a concept vector, resulting in a negative impact on the availability of the approach in an online environment. In this paper, we propose an efficient Wikipedia semantic matching approach to document classification. First, we define several heuristic selection rules to quickly pick out related concepts for a document from the Wikipedia semantic space, making it no longer necessary to match all the concepts in the semantic space, thus greatly improving the generation efficiency of the concept vector. Second, based on the semantic representation of each text document, we compute the similarity between documents so as to accurately classify the documents. Finally, evaluation experiments demonstrate the effectiveness of our approach, i.e., which can improve the classification efficiency of the Wikipedia matching under the precondition of not compromising the classification accuracy.\n\nTitle: An efficient approach to suggesting topically related web queries using hidden topic model\nContent: Keyword-based Web search is a widely used approach for locating information on the Web. However, Web users usually suffer from the difficulties of organizing and formulating appropriate input queries due to the lack of sufficient domain knowledge, which greatly affects the search performance. An effective tool to meet the information needs of a search engine user is to suggest Web queries that are topically related to their initial inquiry. Accurately computing query-to-query similarity scores is a key to improve the quality of these suggestions. Because of the short lengths of queries, traditional pseudo-relevance or implicit-relevance based approaches expand the expression of the queries for the similarity computation. They explicitly use a search engine as a complementary source and directly extract additional features (such as terms or URLs) from the top-listed or clicked search results. In this paper, we propose a novel approach by utilizing the hidden topic as an expandable feature. This has two steps. In the offline model-learning step, a hidden topic model is trained, and for each candidate query, its posterior distribution over the hidden topic space is determined to re-express the query instead of the lexical expression. In the online query suggestion step, after inferring the topic distribution for an input query in a similar way, we then calculate the similarity between candidate queries and the input query in terms of their corresponding topic distributions; and produce a suggestion list of candidate queries based on the similarity scores. Our experimental results on two real data sets show that the hidden topic based suggestion is much more efficient than the traditional term or URL based approach, and is effective in finding topically related queries for suggestion.\n\nTitle: Enhancing Web Search by Aggregating Results of Related Web Queries\nContent: Currently, commercial search engines have implemented methods to suggest alternative Web queries to users, which helps them specify alternative related queries in pursuit of finding needed Web pages. In this paper, we address the Web search problem on related queries to improve retrieval quality by devising a novel search rank aggregation mechanism. Given an initial query and the suggested related queries, our search system concurrently processes their search result lists from an existing search engine and then forms a single list aggregated by all the retrieved lists. In particular we propose a generic rank aggregation framework which considers not only the number of wins that an item won in a competition, but also the quality of its competitor items in calculating the ranking of Web items. The framework combines the traditional and random walk based rank aggregation methods to produce a more reasonable list to users. Experimental results show that the proposed approach can clearly improve the retrieval quality in a parallel manner over the traditional search strategy that serially returns result lists. Moreover, we also empirically investigate how different rank aggregation methods affect the retrieval performance.\n\nTitle: A web recommendation technique based on probabilistic latent semantic analysis\nContent: Web transaction data between Web visitors and Web functionalities usually convey user task-oriented behavior pattern. Mining such type of clickstream data will lead to capture usage pattern information. Nowadays Web usage mining technique has become one of most widely used methods for Web recommendation, which customizes Web content to user-preferred style. Traditional techniques of Web usage mining, such as Web user session or Web page clustering, association rule and frequent navigational path mining can only discover usage pattern explicitly. They, however, cannot reveal the underlying navigational activities and identify the latent relationships that are associated with the patterns among Web users as well as Web pages. In this work, we propose a Web recommendation framework incorporating Web usage mining technique based on Probabilistic Latent Semantic Analysis (PLSA) model. The main advantages of this method are, not only to discover usage-based access pattern, but also to reveal the underlying latent factor as well. With the discovered user access pattern, we then present user more interested content via collaborative recommendation. To validate the effectiveness of proposed approach, we conduct experiments on real world datasets and make comparisons with some existing traditional techniques. The preliminary experimental results demonstrate the usability of the proposed approach.\n\n## Task\nCompose an abstract for the title 'An Improved Contextual Advertising Matching Approach based on Wikipedia Knowledge' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The current boom of the Web is associated with the revenues originated from Web advertising. As one prevalent type of Web advertising, contextual advertising refers to the placement of the most relevant commercial textual ads within the content of a Web page, so as to provide a better user experience and thereby increase the revenues of Web site owners and an advertising platform. Therefore, in contextual advertising, the relevance of selected ads with a Web page is essential. However, some problems, such as homonymy and polysemy, low intersection of keywords and context mismatch, can lead to the selection of irrelevant textual ads for a Web page, making that a simple keyword matching technique generally gives poor accuracy. To overcome these problems and thus to improve the relevance of contextual ads, in this paper we propose a novel Wikipedia-based matching technique which, using selective matching strategies, selects a certain amount of relevant articles from Wikipedia as an intermediate semantic reference model for matching Web pages and textual ads. We call this technique SIWI: Selective Wikipedia Matching, which, instead of using the whole Wikipedia articles, only matches the most relevant articles for a page (or a textual ad), resulting in the effective improvement of the overall matching performance. An experimental evaluation is conducted, which runs over a set of real textual ads, a set of Web pages from the Internet and a dataset of more than 260\u8117\u807d000 articles from Wikipedia. The experimental results show that our method performs better than existing matching strategies, which can deal with the matching over the large dataset of Wikipedia articles efficiently, and achieve a satisfactory contextual advertising effect."
      }
    ]
  },
  {
    "tid":"28",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: WHOIS Lost in Translation: (Mis)Understanding Domain Name Expiration and Re-Registration.\nContent: Internet domain names expire when not renewed and may be claimed by a new owner. To date, despite existing work on abuses of residual trust after domain ownership changes, it is not well understood how often and how fast re-registrations occur, and the underlying processes are often over-simplified in scientific literature, leading to a potential bias in those studies. While in principle registration data is available in Whois databases, scalability issues and data ambiguities make re-registrations a challenging subject of study in practice. By focusing on domains about to be deleted, we were able to track 7.4 M com, net, org, biz and name domains over up to ten months to gather data for a survival analysis of re-registrations. Our results show that expiration processes may vary, and that many re-registrations happen soon after deletion, especially for older domains. We also discuss intricacies of Whois data to aid in avoiding potential pitfalls, as fast domain ownership changes combined with hidden domain states may pose challenges to operational and research communities.\n\nTitle: Tracing Information Flows Between Ad Exchanges Using Retargeted Ads\nContent: Numerous surveys have shown that Web users are concerned about the loss of privacy associated with online tracking. Alarmingly, these surveys also reveal that people are also unaware of the amount of data sharing that occurs between ad exchanges, and thus underestimate the privacy risks associated with online tracking.In reality, the modern ad ecosystem is fueled by a flow of user data between trackers and ad exchanges. Although recent work has shown that ad exchanges routinely perform cookie matching with other exchanges, these studies are based on brittle heuristics that cannot detect all forms of information sharing, especially under adversarial conditions.In this study, we develop a methodology that is able to detect client-and server-side flows of information between arbitrary ad exchanges. Our key insight is to leverage retargeted ads as a tool for identifying information flows. Intuitively, our methodology works because it relies on the semantics of how exchanges serve ads, rather than focusing on specific cookie matching mechanisms. Using crawled data on 35,448 ad impressions, we show that our methodology can successfully categorize four different kinds of information sharing behavior between ad exchanges, including cases where existing heuristic methods fail.We conclude with a discussion of how our findings and methodologies can be leveraged to give users more control over what kind of ads they see and how their information is shared between ad exchanges.\n\nTitle: An Experience in Testing the Security of Real-World Electronic Voting Systems\nContent: Voting is the process through which a democratic society determines its government. Therefore, voting systems are as important as other well-known critical systems, such as air traffic control systems or nuclear plant monitors. Unfortunately, voting systems have a history of failures that seems to indicate that their quality is not up to the task. Because of the alarming frequency and impact of the malfunctions of voting systems, in recent years a number of vulnerability analysis exercises have been carried out against voting systems to determine if they can be compromised in order to control the results of an election. We have participated in two such large-scale projects, sponsored by the Secretaries of State of California and Ohio, whose goals were to perform the security testing of the electronic voting systems used in their respective states. As the result of the testing process, we identified major vulnerabilities in all of the systems analyzed. We then took advantage of a combination of these vulnerabilities to generate a series of attacks that would spread across the voting systems and would \u201csteal\u201d votes by combining voting record tampering with social engineering approaches. As a response to the two large-scale security evaluations, the Secretaries of State of California and Ohio recommended changes to improve the security of the voting process. In this paper, we describe the methodology that we used in testing the two real-world electronic voting systems we evaluated, the findings of our analysis, our attacks, and the lessons we learned.\n\nTitle: Large-Scale Analysis of Style Injection by Relative Path Overwrite.\nContent: Relative Path Overwrite (RPO) is a recent technique to inject style directives into sites even when no style sink or markup injection vulnerability is present. It exploits differences in how browsers and web servers interpret relative paths (i.e., path confusion) to make a HTML page reference itself as a stylesheet; a simple text injection vulnerability along with browsers\u00bb leniency in parsing CSS resources results in an attacker\u00bbs ability to inject style directives that will be interpreted by the browser. Even though style injection may appear less serious a threat than script injection, it has been shown that it enables a range of attacks, including secret exfiltration. In this paper, we present the first large-scale study of the Web to measure the prevalence and significance of style injection using RPO. Our work shows that around 9% of the sites in the Alexa Top 10,000 contain at least one vulnerable page, out of which more than one third can be exploited. We analyze in detail various impediments to successful exploitation, and make recommendations for remediation. In contrast to script injection, relatively simple countermeasures exist to mitigate style injection. However, there appears to be little awareness of this attack vector as evidenced by a range of popular Content Management Systems (CMSes) that we found to be exploitable.\n\n\n\nTitle: Improving Signature Testing through Dynamic Data Flow Analysis\nContent: The effectiveness and precision of network-based intru- sion detection signatures can be evaluated either by di- rect analysis of the signatures (if they are available) or by using black-box testing (if the system is closed-source). Recently, several techniques have been proposed to gen- erate test cases by automatically deriving variations (or mutations) of attacks. Even though these techniques have been useful in identifying \"blind spots\" in the signatures of closed-source, network-based intrusion detection systems, the generation of test cases is performed in a random, un- guided fashion. The reason is that there is no information available about the signatures to be tested. As a result, identifying a test case that is able to evade detection is dif- ficult. In this paper, we propose a novel approach to drive the generation of test cases by using the information gathered by analyzing the dynamic behavior of the intrusion detec- tion system. Our approach applies dynamic data flow anal- ysis techniques to the intrusion detection system to identify which parts of a network stream are used to detect an at- tack and how these parts are matched by a signature. The result of our analysis is a set of constraints that is used to guide the black-box testing process, so that the mutations are applied to only those parts of the attack that are rele- vant for detection. By doing this, we are able to perform a more focused generation of the test cases and improve the process of identifying an attack variation that evades detection.\n\n## Task\nCompose an abstract for the title 'Game Of Registrars: An Empirical Analysis Of Post-Expiration Domain Name Takeovers' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Every day, hundreds of thousands of Internet domain names are abandoned by their owners and become available for re-registration. Yet, there appears to be enough residual value and demand from domain speculators to give rise to a highly competitive ecosystem of drop-catch services that race to be the first to re-register potentially desirable domain names in the very instant the old registration is deleted. To pre-empt the competitive (and uncertain) race to re-registration, some registrars sell their own customers' expired domains pre-release, that is, even before the names are returned to general availability.These practices are not without controversy, and can have serious security consequences. In this paper, we present an empirical analysis of these two kinds of post expiration domain ownership changes. We find that 10 % of all com domains are re-registered on the same day as their old registration is deleted. In the case of org, over 50 % of re-registrations on the deletion day occur during only 30 s. Furthermore, drop-catch services control over 75 % of accredited domain registrars and cause more than 80 % of domain creation attempts, but represent at most 9.5 % of successful domain creations. These findings highlight a significant demand for expired domains, and hint at highly competitive re-registrations.Our work sheds light on various questionable practices in an opaque ecosystem. The implications go beyond the annoyance of websites turned into \"Internet graffiti\" [26], as domain ownership changes have the potential to circumvent established security mechanisms."
      }
    ]
  },
  {
    "tid":"29",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Precise alias analysis for static detection of web application vulnerabilities\nContent: The number and the importance of web applications have increased rapidly over the last years. At the same time, the quantity and impact of security vulnerabilities in such applications have grown as well. Since manual code reviews are time-consuming, error-prone and costly, the need for automated solutions has become evident. In this paper, we address the problem of vulnerable web applications by means of static source code analysis. To this end, we present a novel, precise alias analysis targeted at the unique reference semantics commonly found in scripting languages. Moreover, we enhance the quality and quantity of the generated vulnerability reports by employing a novel, iterative two-phase algorithm for fast and precise resolution of file inclusions.We integrated the presented concepts into Pixy~\\cite{jovanovic06:pixy_short}, a high-precision static analysis tool aimed at detecting cross-site scripting vulnerabilities in PHP scripts. To demonstrate the effectiveness of our techniques, we analyzed three web applications and discovered 106 vulnerabilities. Both the high analysis speed as well as the low number of generated false positives show that our techniques can be used for conducting effective security audits.\n\nTitle: Static analysis for detecting taint-style vulnerabilities in web applications\nContent: The number and the importance of web applications have increased rapidly over the last years. At the same time, the quantity and impact of security vulnerabilities in such applications have grown as well. Since manual code reviews are time-consuming, error-prone and costly, the need for automated solutions has become evident. In this paper, we address the problem of vulnerable web applications by means of static source code analysis. More precisely, we use flow-sensitive, interprocedural and context-sensitive data flow analysis to discover vulnerable points in a program. In addition to the taint analysis at the core of our engine, we employ a precise alias analysis targeted at the unique reference semantics commonly found in scripting languages. Moreover, we enhance the quality and quantity of the generated vulnerability reports by employing an iterative two-phase algorithm for fast and precise resolution of file inclusions. The presented concepts are targeted at the general class of taint-style vulnerabilities and can be easily applied to the detection of vulnerability types such as SQL injection, cross-site scripting (XSS), and command injection. We implemented the presented concepts in Pixy, a high-precision static analysis tool aimed at detecting cross-site scripting and SQL injection vulnerabilities in PHP programs. To demonstrate the effectiveness of our techniques, we analyzed a number of popular, open-source web applications and discovered hundreds of previously unknown vulnerabilities. Both the high analysis speed as well as the low number of generated false positives show that our techniques can be used for conducting effective security audits.\n\nTitle: Preventing Input Validation Vulnerabilities in Web Applications through Automated Type Analysis\nContent: Web applications have become an integral part of the daily lives of millions of users. Unfortunately, web applications are also frequently targeted by attackers, and criticial vulnerabilities such as cross-site scripting and SQL injection are still common. As a consequence, much effort in the past decade has been spent on mitigating web application vulnerabilities. Current techniques focus mainly on sanitization: either on automated sanitization, the detection of missing sanitizers, the correctness of sanitizers, or the correct placement of sanitizers. However, these techniques are either not able to prevent new forms of input validation vulnerabilities such as HTTP Parameter Pollution, come with large runtime overhead, lack precision, or require significant modifications to the client and/or server infrastructure. In this paper, we present IPAAS, a novel technique for preventing the exploitation of cross-site scripting and SQL injection vulnerabilities based on automated data type detection of input parameters. IPAAS automatically and transparently augments otherwise insecure web application development environments with input validators that result in significant and tangible security improvements for real systems. We implemented IPAAS for PHP and evaluated it on five real-world web applications with known cross-site scripting and SQL injection vulnerabilities. Our evaluation demonstrates that IPAAS would have prevented 83% of SQL injection vulnerabilities and 65% of cross-site scripting vulnerabilities while incurring no developer burden.\n\nTitle: ZigZag: Automatically Hardening Web Applications Against Client-side Validation Vulnerabilities\nContent: Modern web applications are increasingly moving program code to the client in the form of JavaScript. With the growing adoption of HTML5 APIs such as postMessage, client-side validation (CSV) vulnerabilities are consequently becoming increasingly important to address as well. However, while detecting and preventing attacks against web applications is a well-studied topic on the server, considerably less work has been performed for the client. Exacerbating this issue is the problem that defenses against CSVs must, in the general case, fundamentally exist in the browser, rendering current server-side defenses inadequate. In this paper, we present ZigZag, a system for hardening JavaScript-based web applications against clientside validation attacks. ZigZag transparently instruments client-side code to perform dynamic invariant detection on security-sensitive code, generating models that describe how - and with whom - client-side components interact. ZigZag is capable of handling templated JavaScript, avoiding full re-instrumentation when JavaScript programs are structurally similar. Learned invariants are then enforced through a subsequent instrumentation step. Our evaluation demonstrates that ZigZag is capable of automatically hardening client-side code against both known and previously-unknown vulnerabilities. Finally, we show that ZigZag introduces acceptable overhead in many cases, and is compatible with popular websites drawn from the Alexa Top 20 without developer or user intervention.\n\nTitle: Using static program analysis to aid intrusion detection\nContent: The Internet, and in particular the world-wide web, have become part of the everyday life of millions of people. With the growth of the web, the demand for on-line services rapidly increased. Today, whole industry branches rely on the Internet to do business. Unfortunately, the success of the web has recently been overshadowed by frequent reports of security breaches. Attackers have discovered that poorly written web applications are the Achilles heel of many organizations. The reason is that these applications are directly available through firewalls and are often developed by programmers who focus on features and tight schedules instead of security In previous work, we developed an anomaly-based intrusion detection system that uses learning techniques to identify attacks against web-based applications. That system focuses on the analysis of the request parameters in client queries, but does not take into account any information about the protected web applications themselves. The result are imprecise models that lead to more false positives and false negatives than necessary In this paper, we describe a novel static source code analysis approach for PHP that allows us to incorporate information about a web application into the intrusion detection models. The goal is to obtain a more precise characterization of web request parameters by analyzing their usage by the program. This allows us to generate more precise intrusion detection models. In particular, our analysis allows us to determine the names of request parameters expected by a program and provides information about their types, structure, or even concrete value sets. Our experimental evaluation demonstrates that the information derived statically from web applications closely characterizes the parameter values observed in real-world traffic\n\n## Task\nCompose an abstract for the title 'Pixy: A Static Analysis Tool for Detecting Web Application Vulnerabilities (Short Paper)' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The number and the importance of Web applications have increased rapidly over the last years. At the same time, the quantity and impact of security vulnerabilities in such applications have grown as well. Since manual code reviews are time-consuming, error-prone and costly, the need for automated solutions has become evident. In this paper, we address the problem of vulnerable Web applications by means of static source code analysis. More precisely, we use flow-sensitive, interprocedural and context-sensitive data flow analysis to discover vulnerable points in a program. In addition, alias and literal analysis are employed to improve the correctness and precision of the results. The presented concepts are targeted at the general class of taint-style vulnerabilities and can be applied to the detection of vulnerability types such as SQL injection, cross-site scripting, or command injection. Pixy, the open source prototype implementation of our concepts, is targeted at detecting cross-site scripting vulnerabilities in PHP scripts. Using our tool, we discovered and reported 15 previously unknown vulnerabilities in three web applications, and reconstructed 36 known vulnerabilities in three other web applications. The observed false positive rate is at around 50% (i.e., one false positive for each vulnerability) and therefore, low enough to permit effective security audits."
      }
    ]
  },
  {
    "tid":"30",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Vector Similarity Measure for Type-1 Fuzzy Sets\nContent: Comparing the similarity between two fuzzy sets (FSs) is needed in many applications. The focus herein is linguistic approximation using type-1 (T1) FSs, i.e. associating a T1 FS Awith a linguistic label from a vocabulary. Because each label is represented by an T1 FS Bi, there is a need to compare the similarity of Aand Bito find the Bimost similar to A. In this paper, a vector similarity measure (VSM) is proposed for T1 FSs, whose two elements measure the similarity in shape and proximity, respectively. A comparative study shows that the VSM gives best results. Additionally, the VSM can be easily extended to interval type-2 FSs.\n\nTitle: A Vector Similarity Measure for Interval Type-2 Fuzzy Sets\nContent: Fuzzy logic is frequently used in computing with words (CWW). When input words to a CWW engine are modeled by interval type-2 fuzzy sets (IT2 FSs), the CWW engine's output can also be an IT2 FS, A tilde, which needs to be mapped to a linguistic label so that it can be understood. Because each linguistic label is represented by an IT2 FS Bi, there is a need to compare the similarity of A tilde and B tildei to find the B tildei most similar to A tilde. In this paper, a vector similarity measure (VSM) is proposed for IT2 FSs, whose two elements measure the similarity in shape and proximity, respectively. A comparative study shows that the VSM gives more reasonable results than all other existing similarity measures for IT2 FSs.\n\nTitle: A vector similarity measure for linguistic approximation: Interval type-2 and type-1 fuzzy sets\nContent: Fuzzy logic is frequently used in computing with words (CWW). When input words to a CWW engine are modeled by interval type-2 fuzzy sets (IT2 FSs), the CWW engine's output can also be an IT2 FS, A~, which needs to be mapped to a linguistic label so that it can be understood. Because each linguistic label is represented by an IT2 FS B~\"i, there is a need to compare the similarity of A~ and B~\"i to find the B~\"i most similar to A~. In this paper, a vector similarity measure (VSM) is proposed for IT2 FSs, whose two elements measure the similarity in shape and proximity, respectively. A comparative study shows that the VSM gives more reasonable results than all other existing similarity measures for IT2 FSs for the linguistic approximation problem. Additionally, the VSM can also be used for type-1 FSs, which are special cases of IT2 FSs when all uncertainty disappears.\n\nTitle: Efficient algorithms for computing a class of subsethood and similarity measures for interval type-2 fuzzy sets\nContent: Subsethood and similarity measures are important concepts in fuzzy set (FS) theory. There are many different definitions of them, for both type-1 (T1) FSs and interval type-2 (IT2) FSs. In this paper, Rickard et al.'s definition of IT2 FS subsethood measure, extended from Kosko's T1 FS subsethood measure using the Representation Theorem, and Nguyen and Kreinovich's IT2 FS similarity measure, extended from the Jaccard similarity measure for T1 FSs, are introduced. Efficient algorithms for computing them are also proposed. Simulations demonstrate that our proposed algorithms outperform existing algorithms in the literature.\n\nTitle: A comparative study of ranking methods, similarity measures and uncertainty measures for interval type-2 fuzzy sets\nContent: Ranking methods, similarity measures and uncertainty measures are very important concepts for interval type-2 fuzzy sets (IT2 FSs). So far, there is only one ranking method for such sets, whereas there are many similarity and uncertainty measures. A new ranking method and a new similarity measure for IT2 FSs are proposed in this paper. All these ranking methods, similarity measures and uncertainty measures are compared based on real survey data and then the most suitable ranking method, similarity measure and uncertainty measure that can be used in the computing with words paradigm are suggested. The results are useful in understanding the uncertainties associated with linguistic terms and hence how to use them effectively in survey design and linguistic information processing.\n\n## Task\nCompose an abstract for the title 'Similarity Measures for Closed General Type-2 Fuzzy Sets: Overview, Comparisons, and a Geometric Approach' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The similarity between two fuzzy sets (FSs) is an important concept in fuzzy logic. As the research interest on general type-2 (GT2) FSs has increased recently, many similarity measures for them have also been proposed. This paper gives a comprehensive overview of existing similarity measures for GT2 FSs, points out their limitations, and, by using an intuitive geometric explanation, proposes a Jaccard similarity measure for GT2 FSs that is an extension of the popular Jaccard similarity measure for type-1 and interval type-2 FSs. The fundamental difference between the proposed Jaccard similarity measure for GT2 FSs and all existing similarity measures is that the Jaccard similarity measure considers the overall geometries of two GT2 FSs and does not depend on a specific representation of the GT2 FSs, whereas all existing similarity measures for GT2 FSs depend either on the vertical slice representation or the \n<inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$\\alpha$</tex-math></inline-formula>\n-plane representation. We show that the Jaccard similarity measure for GT2 FSs satisfies four properties of a similarity measure and demonstrate its reasonableness using two examples."
      }
    ]
  },
  {
    "tid":"31",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A New Fuzzy-based Cluster-Head Selection System for WSNs\nContent: Cluster formation and cluster head selection are important problems in sensor network applications and can drastically affect the network's communication energy dissipation. However, selecting of the cluster head is not easy in different environments which may have different characteristics. In order to deal with this problem, we have proposed a power reduction algorithm for sensor networks based on fuzzy logic and number of neighbor nodes. In this paper, we propose a new fuzzy-based cluster-head selection system to improve the performance of the previous system. The proposed system in different from previous system uses 4 input linguistic parameter for cluster-head decision. We evaluate the proposed system by simulations and show that the proposed system has a good cluster-head selection.\n\nTitle: A Fuzzy-Based Cluster-Head Selection System forWSNs Considering Different Parameters\nContent: Cluster formation and cluster head selection are important problems in sensor network applications and can drastically affect the network's communication energy dissipation. However, selecting the cluster head is not easy in different environments which may have different characteristics. In order to deal with this problem, we have proposed a power reduction algorithm for sensor networks based on fuzzy logic and node movement. The proposed system different from our previous systems uses 4 input linguistic parameters: Remaining Power of Sensor (RPS), Degree of Number of Neighbor Nodes (D3N), Distance from Cluster Centroid (DCC) and Sensor Speed (SS) for cluster-head decision. By considering the moving speed of the sensor we are able to predict if the node will leave the cluster. We evaluate the proposed system by simulations and show that it has a good cluster-head selection.\n\nTitle: An Intelligent Fuzzy-Based Cluster Head Selection System for Wireless Sensor Networks and Its Performance Evaluation\nContent: Sensor networks supported by recent technological advances in low power wireless communications along with silicon integration of various functionalities are emerging as a critically important computer class that enable novel and low cost applications. There are many fundamental problems that sensor networks research will have to address in order to ensure a reasonable degree of cost and system quality. Cluster formation and cluster head selection are important problems in sensor network applications and can drastically affect the network\u2019scommunication energy dissipation. However, selecting of the cluster head is not easy in different environments which may have different characteristics. In our previous work, in order to deal with this problem, we proposed a power reduction algorithm for sensor networks based on fuzzy logic and number of neighbor nodes. We call this algorithm F3N. In this paper, we implement a simulation system for clustering algorithms in sensor networks. We evaluate LEACH and F3N by some simulation results. Presently, we have implemented LEACH algorithm in NS-2. However, F3N is implemented in MATLAB. We are working to implement also F3N system in NS-2 in order to compare their performance.\n\nTitle: Evaluation of an Intelligent Fuzzy-Based Cluster Head Selection System for WSNs Using Different Parameters\nContent: Cluster formation and cluster head selection are important problems in sensor network applications and can drastically affect the network's communication energy dissipation. However, selecting of the cluster head is not easy in different environments which may have different characteristics. In our previous work, in order to deal with this problem, we proposed a power reduction algorithm for sensor networks based on fuzzy logic and number of neighbour nodes. We call this algorithm F3N. In this paper, we evaluate F3N and LEACH by many simulation results. We evaluate the performance of our proposed system for tree different parameters: Remaining Battery Power of Sensor (RPS), Degree of Number of Neighbour Nodes (D3N), and Distance from Cluster Centroid (DCC). From the simulation results, we found that the probability of a sensor node to be a cluster head is increased with increase of number of neighbour nodes and remained battery power and is decreased with the increase of distance from the cluster centroid.\n\nTitle: A Fuzzy-Based Cluster-Head Selection System for WSNs: A Comparison Study for Static and Mobile Sensors\nContent: Cluster formation and cluster head selection are important problems in sensor network applications and can drastically affect the network's communication energy dissipation. However, selecting the cluster head is not easy in different environments which may have different characteristics. In order to deal with this problem, we have proposed a power reduction algorithm for sensor networks based on fuzzy logic and node movement. The proposed system uses 4 input linguistic parameters: Remaining Power of Sensor (RPS), Degree of Number of Neighbor Nodes (D3N), Distance from Cluster Centroid (DCC) and Sensor Speed (SS) for cluster-head decision. We consider for evaluation two cases: when sensors are static and when sensors move. By considering the moving speed of the sensor we are able to predict if the node will leave the cluster. We evaluate the proposed system by simulations and show that it has a good cluster-head selection.\n\n## Task\nCompose an abstract for the title 'A Fuzzy-Based Cluster-Head Selection System for WSNs Considering Sensor Node Movement' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Cluster formation and cluster head selection are important problems in sensor network applications and can drastically affect the network's communication energy dissipation. However, selecting of the cluster head is not easy in different environments which may have different characteristics. In order to deal with this problem, we propose a power reduction algorithm for sensor networks based on fuzzy logic and node movement. The proposed system uses 3 input linguistic parameters: remaining battery power of sensor, degree of number of neighbor nodes and sensor speed for cluster-head decision. By considering the moving speed of the sensor node, we are able to find the nodes which leave the cluster. We evaluate the proposed system by simulations and show that the proposed system has a good cluster-head selection."
      }
    ]
  },
  {
    "tid":"32",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Joint Iterative Transmit/Receive Fde & Fdic For Single-Carrier Block Transmissions\nContent: In this paper, we propose a novel iterative transmit/receive equalization technique for single-carrier (SC) block transmission in a severe frequency-selective fading channel. Iterative frequency-domain inter-symbol interference (ISI) cancellation (FDIC) is introduced to the previously proposed joint iterative transmit/receive frequency-domain equalization (FDE) based on the minimum mean square error (MMSE) criterion. 1-tap FDE is employed at the transmitter. At the receiver, a 1-tap FDE and FDIC are jointly used and they are updated in an iterative manner. The transmit FDE weight is derived based on the MMSE criterion by taking into account the reduction of residual ISI in the receiver. To derive the weight, the transmitter assumes that the receiver can partially reduce the residual ISI after the FDIC. We conduct a computer simulation to investigate the achievable bit error rate (BER) performance to confirm the effectiveness of our proposed technique.\n\nTitle: Joint Transmit/Receive MMSE-FDE for MIMO Analog Network Coding in Single-Carrier Bi-Directional Relay Communications.\nContent: In this paper, we propose a joint transmit/receive frequency-domain equalization (FDE) based on minimum mean square error (MMSE) criterion for multi-input multi-output (MIMO) analog network coding (ANC) in single-carrier (SC) bi-directional relay communications. In the proposed scheme, the relay station (RS) equipped with multiple antennas carries out antenna diversity and one-tap transmit FDE, and the base station (BS) and mobile terminal (MT) receivers, both equipped with single antenna, carry out one-tap receive FDE only. The FDE weights at RS, BS and MT are jointly optimized so as to minimize the end-to-end mean square error (MSE). We evaluate, by computer simulations, the bit error rate (BER) performance when using the proposed scheme, and discuss how the transmit FDE at RS operates and affects the BER performance.\n\nTitle: Joint Mmse-Fde & Spectrum Combining For A Broadband Single-Carrier Transmission In The Presence Of Timing Offset\nContent: Frequency-domain equalization (FOE) based on minimum mean square error (MMSE) is considered as a promising equalization technique for a broadband single-carrier (SC) transmission. When a square-root Nyquist filter is used at a transmitter and receiver to limit the signal bandwidth, the presence of timing offset produces the inter-symbol interference (ISI) and degrades the bit error rate (BER) performance using MMSE-FDE. In this paper, we discuss the mechanism of the BER performance degradation in the presence of timing offset. Then, we propose joint MMSE-FDE & spectrum combining which can make use the excess bandwidth introduced by transmit filter to achieve larger frequency diversity gain while suppressing the negative effect of the timing offset.\n\nTitle: Joint iterativeTx/Rx MMSE-FDE and ISICancellation for single-carrier hybrid ARQ with chase combining\nContent: We propose a joint iterative transmit/receive (Tx/Rx) minimum mean square error (MMSE) frequency-domain equalization (FDE) and intersymbol interference cancellation (ISIC) suitable for single-carrier (SC) hybrid automatic repeat request (HARQ) with chase combining (CC). In the proposed scheme, a one-tap transmit MMSE-FDE and an iterative one-tap receive MMSE-FDE and ISIC are employed. Every time the retransmitted packet is received, a series of receive MMSE-FDE, antenna diversity combining, packet combining, and ISIC is carried out in an iterative manner by using updated receive MMSE-FDE weights in each iteration. The transmit MMSE-FDE weight is determined based on the prediction of the degree of residual ISI after the iteration process in a receiver. We theoretically derive a set of transmit and receive MMSE-FDE weights. We show by computer simulation that joint iterative Tx/Rx MMSE-FDE and ISIC achieves better packet error rate (PER) and higher throughput of SC-HARQ than the conventional Rx MMSE-FDE and ISIC.\n\nTitle: Selective Mapping For Broadband Single-Carrier Transmission Using Joint Tx/Rx Mmse-Fde\nContent: Joint transmit/receive frequency-domain equalization based on minimum mean-square error criterion (joint Tx/Rx MMSE-FDE) is a promising frequency-domain-based technique suppressing inter-symbol interference (ISI) in broadband single-carrier (SC) transmission. However, it changes the frequency-domain spectrum shape and hence increases the peak-to-average power ratio (PAPR) of transmit signal. In this paper, we introduce the selective mapping (SLM), which is typically used in orthogonal frequency division multiplexing (OFDM), to reduce PAPR of SC transmission using joint Tx/Rx MMSE-FDE. The SLM is applied to the frequency-domain SC signal after applying the transmit FDE (Tx-FDE) and prior to inverse fast Fourier transform (IFFT). It is shown, by computer simulation, that the SLM can reduce PAPR without significant effect on bit-error rate (BER). Performance of SLM is also evaluated in various phase-rotation sequence types, in both deterministic and random points of views, and number of candidates. Computational complexity of the proposed transmission scheme employing SLM algorithm is also discussed.\n\n## Task\nCompose an abstract for the title 'Joint Cooperative-Transmit and Receive FDE for Single-Carrier Incremental Relaying' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose joint cooperative-transmit/receive frequency-domain equalization (cooperative-Tx/Rx FDE) with incremental relaying (IR) for broadband single-carrier (SC) transmission under the total and individual transmit power constraint at a source node ( S ) and a relay node ( R ). We derive the optimum cooperative-Tx/Rx FDE weights to minimize the mean square error (MSE) after packet combining at a destination node (\\ssr D ). We show that the optimum cooperative-Tx FDE weights allocate the transmit power in the minimum MSE (MMSE) sense in the frequency domain and in the maximal ratio transmission (MRT) sense in the spatial domain. To relax the condition that the complete channel state information (CSI) needs to be shared among all nodes, selection-based suboptimum cooperative-Tx FDE weights are derived. Computer simulation verifies the effectiveness of the proposed schemes and shows that the 5%-outage throughput of the system can be increased by around 30% over the conventional IR with only Rx FDE."
      }
    ]
  },
  {
    "tid":"33",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A differential fault attack on the grain family of stream ciphers\nContent: In this paper we study a differential fault attack against the Grain family of stream ciphers. The attack works due to certain properties of the Boolean functions and corresponding choices of the taps from the LFSR. The existing works, by Berzati et al. (2009) and Karmakar et al. (2011), are applicable only on Grain-128 exploiting certain properties of the combining Boolean function h. That idea could not easily be extended to the corresponding Boolean function used in Grain v1. Here we show that the differential fault attack can indeed be efficiently mounted for the Boolean function used in Grain v1. In this case we exploit the idea that there exists certain suitable \u03b1 such that $h(\\mathbf{x}) + h({\\mathbf x} + \\mathbf{\\alpha})$ is linear. In our technique, we present methods to identify the fault locations and then construct set of linear equations to obtain the contents of the LFSR and the NFSR. As a countermeasure to such fault attack, we provide exact design criteria for Boolean functions to be used in Grain like structure.\n\nTitle: A differential fault attack on grain-128a using MACs\nContent: The 32-bit MAC of Grain-128a is a linear combination of the first 64 and then the alternative keystream bits. In this paper we describe a successful differential fault attack on Grain-128a, in which we recover the Secret Key by observing the correct and faulty MACs of certain chosen messages. The attack works due to certain properties of the Boolean functions and corresponding choices of the taps from the LFSR. We present methods to identify the fault locations and then construct a set of linear equations to obtain the contents of the LFSR and the NFSR. Our attack requires less than 211 fault injections and invocations of less than 212 MAC generation routines.\n\nTitle: Improved differential fault attack on MICKEY 2.0.\nContent: In this paper we describe several ideas related to differential fault attack (DFA) on MICKEY 2.0, a stream cipher from eStream hardware profile. Using the standard assumptions for fault attacks, we first show that if the adversary can induce random single bit faults in the internal state of the cipher, then by injecting around \\(2^{16.7}\\) faults and performing \\(2^{32.5}\\) computations on an average, it is possible to recover the entire internal state of MICKEY at the beginning of the key-stream generation phase. We further consider the scenario where the fault may affect more than one (at most three) neighboring bits and in that case we require around \\(2^{18.4}\\) faults on an average to mount the DFA. We further show that if the attacker can solve multivariate equations (say, using SAT solvers) then the attack can be carried out using around \\(2^{14.7}\\) faults in the single-bit fault model and \\(2^{16.06}\\) faults for the multiple-bit scenario\n\nTitle: A differential fault attack on MICKEY 2.0\nContent: In this paper we present a differential fault attack on the stream cipher MICKEY 2.0 which is in eStream's hardware portfolio. While fault attacks have already been reported against the other two eStream hardware candidates Trivium and Grain, no such analysis is known for MICKEY. Using the standard assumptions for fault attacks, we show that if the adversary can induce random single bit faults in the internal state of the cipher, then by injecting around 216.7 faults and performing 232.5 computations on an average, it is possible to recover the entire internal state of MICKEY at the beginning of the key-stream generation phase. We further consider the scenario where the fault may affect at most three neighbouring bits and in that case we require around 218.4 faults on an average.\n\nTitle: Probabilistic signature based generalized framework for differential fault analysis of stream ciphers.\nContent: Differential Fault Attack (DFA) considers injection of faults and the most general set-up should take care of faults at random location and random time. Then one should be able to identify the exact location as well as the exact timing of the fault (including the multi bit ones) with the help of fault signatures. In this paper we solve the problem of DFA under a general frame-work, introducing the idea of probabilistic signatures. The method considers the Maximum Likelihood approach related to probability distributions. Our techniques subsume all the existing DFAs against the Grain family, MICKEY 2.0 and Trivium. In the process we provide improved fault attacks for all the versions of Grain family and also for MICKEY 2.0. Our generalized method successfully takes care of the cases where certain parts of the keystream bits are missing (this situation may arise for authentication purpose). In particular, we show that the unsolved problem of identifying the faults in random time for Grain 128a can be solved in this manner. Moreover, for MICKEY 2.0, our method not only provides improvement in fault identification probability but also reduces the required faults by 60 %, compared to the best known result.\n\n## Task\nCompose an abstract for the title 'Differential Fault Attack against Grain family with very few faults and minimal assumptions.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The series of published works, related to differential fault attack (DFA) against the Grain family, require quite a large number (hundreds) of faults and also several assumptions on the locations and the timings of the faults injected. In this paper, we present a significantly improved scenario from the adversarial point of view for DFA against the Grain family of stream ciphers. Our model is the most realistic one so far as it considers that the cipher has to be re-keyed only a few times and faults can be injected at any random location and at any random point of time, i.e., no precise control is needed over the location and timing of fault injections. We construct equations based on the algebraic description of the cipher by introducing new variables so that the degrees of the equations do not increase. In line of algebraic cryptanalysis, we accumulate such equations based on the fault-free and faulty key-stream bits and solve them using the SAT Solver Cryptominisat-2.9.5 installed with SAGE 5.7. In a few minutes we can recover the state of Grain v1, Grain-128 and Grain-128a with as little as 10, 4 and 10 faults respectively."
      }
    ]
  },
  {
    "tid":"34",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Augmenting a Web Server with QoS by Means of an Aspect-Oriented Architecture\nContent: The handling of Quality of Service (QoS) parameters on a web server is a concern that generally spreads across the several modules responsible for fetching and delivering contents as per incoming requests. In order to avoid tangling between QoS and communication-related concerns, this paper proposes the use of aspect-orientation to handle QoS on top of an existing web server, Jigsaw. We show how an aspect-based architecture can be effective for providing the web server with several improvements when processing incoming requests. QoS parameters have been handled by associating requests with priorities and by introducing into the web server checks on resource-usage and tasks to be executed. Suggested aspects are connected at compile-time to existing classes, hence keeping the QoS-enforcing code separated from web server modules.\n\nTitle: Enhancing applications with cloud services by means of aspects\nContent: In order to use remote cloud-based services, existing legacy applications need to be adapted, hence partially rewritten to include calls to such services and take decisions on the basis of the dynamic state of these services. This knowledge on cloud services makes applications run smoothly, however it also makes applications more complex. In this paper we propose a general software architecture, making use of aspect-orientation, that allows storage-related code within applications to be redefined in order to integrate applications with appropriate services available on several clouds. Our provided components allow integration while keeping cloud-and application-related concerns well separated, hence obtaining modularity. Additional components provide applications with enhanced features, such as monitoring the behaviour of cloud resources, connecting with several cloud providers, and generating replicas of most accessed contents. As a result, the availability of contents will be increased, i.e., in a given moment the most unloaded cloud service will be automatically selected and accessed.\n\nTitle: A QoS-Aware Architecture for Multimedia Content Provisioning in a GRID Environment\nContent: In this paper, we propose a mobile agent infras- tructure to support \"Quality of Service\" (QoS) parameters in multimedia content streaming. The infrastructure is able to face the issues concerning multimedia content transformation, in order to ensure that the client-side QoS is met. A Grid computing platform is exploited and content adaptation is per- formed through appropriate agents that are allocated as jobs and executed on Grid hosts. A location service is therefore devised and made available within the Grid system, which helps finding the hosts whose geographic position and network connections minimise the delays required to move the desired multimedia content from the storage to the processing site, and (after suitable transcoding) from this to the requesting client.\n\nTitle: Enhancing City Transportation Services Using Cloud Support.\nContent: Smart cities will provide enhanced monitoring of crucial infrastructure resources, connectivity to users and advanced information services. Thanks to gathered data the quality of traditional services and infrastructures will be improved, and further services can emerge, such as novel urban transportation services. This paper devises a solution that enforces the cooperation between mobile devices and cloud infrastructures with the aim to bring public transportation where the people need it. Thanks to smart phones, sensing user locations, a request for transportation vehicles can be sent to a cloud-based intelligence, which filters and serves requests according to available transport routes, and their adaptation to user needs. Then, the available transportation vehicles will be timely alerted to operate accordingly.\n\nTitle: Kaqudai: A Dependable Web Infrastructure Made Out of Existing Components\nContent: An internet application typically consists of a business logic complemented by additional concerns, such as communication, security and higher level features such as dependability. We advocate the use of aspect orientation as an advanced concern separation technology, ensuring higher modularity and reusability of code. In this paper we show how such an approach can be put into practice, by enhancing an existing web application with a dependability layer, addressing availability, reliability and data integrity concerns. Specifically, we have produced a set of aspects and other suitable components which can be smoothly integrated into an existing web server, endowing it with the ability to redirect service requests under excessive load. Redirection may target either cloud-powered lightweight replicas of the main web server, or publicly exposed browsing caches from clients. Experiments were conducted on the W3C Jigsaw web server.\n\n## Task\nCompose an abstract for the title 'Providing QoS strategies and cloud-integration to web servers by means of aspects' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The main responsibilities of a web server are to listen from the communication channel and to prepare replies to requests. Additional responsibilities include adapting processing activities, for example, through scheduling or request filtering, so as to satisfy Quality of Service QoS requirements. Typical QoS-related concerns address behavioural constraints e.g. response time bounds, satisfiable by scheduling the most urgent requests first and resource monitoring, for optimal use. Although such concerns are spread across several web server components, they should be handled separately from communication-related ones, for the sake of modularity."
      }
    ]
  },
  {
    "tid":"35",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: On augmenting database design-support environments to capture the geo-spatio-temporal data semantics\nContent: A database design-support environment supports a data analyst in eliciting, articulating, specifying and validating data-related requirements. Extant design-support environments--based on conventional conceptual models--do not adequately support applications that need to organize data based on time (e.g., accounting, portfolio management, personnel management) and/or space (e.g., facility management, transportation, logistics). For geo-spatio-temporal applications, it is left to database designers to discover, design and implement--on an ad-hoc basis--the temporal and geospatial concepts that they need to represent the miniworld. To elicit the geo-spatio-temporal data semantics, we characterize guiding principles for augmenting the conventional conceptual database design approach, present our annotation-based approach, and illustrate how our proposed approach can be instantiated via a proof-of-concept prototype. Via a proof-of-concept database design-support environment, we exemplify our annotation-based approach, and show how segregating \"what\" from \"when/where\" via annotations satisfies ontologic- and cognition-based requirements, dovetails with existing database design methodologies, results in upward-compatible conceptual as well as XML schemas, and provides a straightforward mechanism to extend extant design-support environments.\n\nTitle: DISTIL: A Design Support Environment for Conceptual Modeling of Spatio-temporal Requirements\nContent: We describe DISTIL (DIstributed design of SpaTIo-temporaL data), a web-based conceptual modeling prototype system that can help capture the semantics of spatio-temporal data. Via DISTIL, we describe an annotation-based approach that divides spatio-temporal conceptual design into two steps: first capture the current reality of an application using a conventional conceptual model without considering the spatial aspects, and only then annotate the schema with the spatio-temporal semantics of the application. A database development team can use DISTIL to capture and validate their spatio-temporal data requirements. Using DISTIL we demonstrate that the annotation-based approach for capturing spatio-temporal requirements is straightforward to implement, satisfies ontology-based and cognition-based requirements, and integrates seamlessly into the existing database design methodologies.\n\nTitle: Augmenting a Conceptual Model with Geospatiotemporal Annotations\nContent: While many real-world applications need to organize data based on space (e.g., geology, geomarketing, environmental modeling) and/or time (e.g., accounting, inventory management, personnel management), existing conventional conceptual models do not provide a straightforward mechanism to explicitly capture the associated spatial and temporal semantics. As a result, it is left to database designers to discover, design, and implement\u9a74on an ad hoc basis\u9a74the temporal and spatial concepts that they need. We propose an annotation-based approach that allows a database designer to focus first on nontemporal and nongeospatial aspects (i.e., \"what\u9a74) of the application and, subsequently, augment the conceptual schema with geospatiotemporal annotations (i.e., \"when\u9a74 and \"where\u9a74). Via annotations, we enable a supplementary level of abstraction that succinctly encapsulates the geospatiotemporal data semantics and naturally extends the semantics of a conventional conceptual model. An overarching assumption in conceptual modeling has always been that expressiveness and formality need to be balanced with simplicity. We posit that our formally defined annotation-based approach is not only expressive, but also straightforward to understand and implement.\n\nTitle: Modeling Spatial and Temporal Set-Based Constraints During Conceptual Database Design\nContent: From a database perspective, business constraints provide an accurate picture of the real world being modeled and help enforce data integrity. Typically, rules are gathered during requirements analysis and embedded in code during the implementation phase. We propose that the rules be explicitly modeled during conceptual design, and develop a framework for understanding and classifying spatiotemporal set-based (cardinality) constraints and an associated syntax. The constraint semantics are formally specified using first-order logic. Modeling rules in conceptual design ensures they are visible to designers and users and not buried in application code. The rules can then be semiautomatically translated into logical design triggers yielding productivity gains. Following the principles of design science research, we evaluate the framework's expressiveness and utility with a case study.\n\nTitle: Capturing Telic/Atelic Temporal Data Semantics: Generalizing Conventional Conceptual Models\nContent: Time provides context for all our experiences, cognition, and coordinated collective action. Prior research in linguistics, artificial intelligence, and temporal databases suggests the need to differentiate between temporal facts with goal-related semantics (i.e., telic) from those are intrinsically devoid of culmination (i.e., atelic). To differentiate between telic and atelic data semantics in conceptual database design, we propose an annotation-based temporal conceptual model that generalizes the semantics of a conventional conceptual model. Our temporal conceptual design approach involves: 1) capturing \"what\" semantics using a conventional conceptual model; 2) employing annotations to differentiate between telic and atelic data semantics that help capture \"when\" semantics; 3) specifying temporal constraints, specifically nonsequenced semantics, in the temporal data dictionary as metadata. Our proposed approach provides a mechanism to represent telic/atelic temporal semantics using temporal annotations. We also show how these semantics can be formally defined using constructs of the conventional conceptual model and axioms in first-order logic. Via what we refer to as the \"semantics of composition,\" i.e., semantics implied by the interaction of annotations, we illustrate the logical consequences of representing telic/atelic data semantics during temporal conceptual design.\n\n## Task\nCompose an abstract for the title 'Supporting User-Defined Granularities in a Spatiotemporal Conceptual Model' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Granularities are integral to spatial and temporal data. A large number of applications require storage of facts along with their temporal and spatial context, which needs to be expressed in terms of appropriate granularities. For many real-world applications, a single granularity in the database is insufficient. In order to support any type of spatial or temporal reasoning, the semantics related to granularities needs to be embedded in the database. Specifying granularities related to facts is an important part of conceptual database design because under-specifying the granularity can restrict an application, affect the relative ordering of events and impact the topological relationships. Closely related to granularities is indeterminacy, i.e., an occurrence time or location associated with a fact that is not known exactly. In this paper, we present an ontology for spatial granularities that is a natural analog of temporal granularities. We propose an upward-compatible, annotation-based spatiotemporal conceptual model that can comprehensively capture the semantics related to spatial and temporal granularities, and indeterminacy without requiring new spatiotemporal constructs. We specify the formal semantics of this spatiotemporal conceptual model via translation to a conventional conceptual model. To underscore the practical focus of our approach, we describe an on-going case study. We apply our approach to a hydrogeologic application at the United States Geologic Survey and demonstrate that our proposed granularity-based spatiotemporal conceptual model is straightforward to use and is comprehensive."
      }
    ]
  },
  {
    "tid":"36",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: The Role of Environmental Information Disclosure Systems and Their Impact on Firm Performance\nContent: In response to the call for more research on impactful green IS, this paper examines one country-level environmental information disclosure system (EIDs), the U.S. Toxic Release Inventory (TRI), as a strategic tool or infrastructure of green IS and empirically tests the impact of environmental performance on financial performance in the chemical industry using both real financial and forward-looking measures. In particular, the study explores EDIs-inspired environmental managerial efforts and their influence on firms' financial performance. The study's finding that firms with better environmental performance also show greater cost competitiveness challenges the view that environmental managerial activities constitute an additional burden irrespective of firms' business operations. Instead, it suggests, these can be innovative activities that improve a firm's production efficiency, material resource management, and financial performance and sustainability.\n\nTitle: The Impact of EDI Controls on the Relationship Between EDI Implementation and Performance\nContent: The evaluation of EDI controls is important in ensuring the high performance of an EDI system. Although penetration level is strongly associated with performance, the benefits derived arise from the usage level of EDI controls. This suggests that EDI controls affect the relationship between EDI implementation and performance. Research hypotheses have been empirically tested using responses from Korean companies that have adopted EDI. It has been shown that the integration and utilization of EDI affects its performance. When EDI is highly integrated, formal, informal, and automated controls must be enhanced in order to provide high performance. High utilization affects performance only when automated controls are completely implemented. Empirical analysis confirms that EDI controls affect the relationship between EDI implementation and performance.\n\nTitle: A Hybrid System using Multiple Cyclic Decomposition Methods and Neural Network Techniques for Point Forecast Decision Making\nContent: Data filtering methods are so much crucial to get good performance in time series forecasting. There are a few preprocessing methods (i.e. ARMA outputs as time domain filters, and Fourier transform or wavelet transform as time-frequency domain filters) for handling time series. In particular, the time-frequency domain filters describe the fractal structure of financial markets better than the time domain filters without frequency information. We study the issues of integrated methods of joint time- frequency analysis and neural network techniques as the application of multi-cyclic decomposition methods to the neural networks for short-term point forecast decision- making. The issues include the appropriate selection of neural network model architecture, for example, what type of neural network learning architecture is selected and what input size should be selected for our time series forecasting. We analyze these problems in particular with recurrent neural network learning and embedding dimension as chaos analysis. This study is also applied to a case study of daily Korean won / U.S. dollar exchange returns. Finally we suggest an integration framework for future research from our experimental results.\n\nTitle: Prioritization and selection of intellectual capital measurement indicators using analytic hierarchy process for the mobile telecommunications industry\nContent: Intellectual capital (IC) has prevailed as a measure of core competency and competitive advantage which explains the gap between the market value and book value of an organization at a time of decreasing usefulness of current financial reporting. In spite of the importance of IC management (ICM), few applicable ICM methodologies have been addressed. There has been no basis model for IC statements, nor bottom-line indicators of the value of IC. The selection of effective IC indicators is a major task of the companies that are preparing IC reports. This paper has proposed a decision model based on the analysis of the conceptual framework of the qualitative characteristics of financial information and an examination of information quality of the information system. The application of the analytic hierarchy process makes it possible to extract weights for setting the priority among criteria in the mobile telecommunications industry. During the last decade, this industry has experienced a dramatic growth in its intellectual value, and the value of IC seems to have had a major impact on the value of the mobile telecommunications companies. Based on specified criteria and weighting, this paper presents the results of a case study illustrating the results of selected indicators from candidate indicators in the mobile telecommunications company.\n\nTitle: An Intelligent Clustering Forecasting System Based on Change-Point Detection and Artificial Neural Networks: Application to Financial Economics\nContent: Suggests a new clustering forecasting system to integrate change-point detection and artificial neural networks. The basic concept of the proposed model is to obtain intervals divided by change points, to identify them as change-point groups, and to involve them in the forecasting model. The proposed models consist of two stages. The first stage (the clustering neural network modeling stage) detects successive change points in a data set and forecasts the change-point group with backpropagation neural networks (BPN). In this stage, three change-point detection methods are applied and compared: (1) the parametric method, (2) the nonparametric approach, and (3) the model-based approach. The next stage is to forecast the final output with a BPN. Through an application to financial economics, we compare the proposed models with a neural network model alone and, in addition, determine which of the three change-point detection methods can perform better. This article then examines the predictability of the integrated neural network model based on change-point detection.\n\n## Task\nCompose an abstract for the title 'Performance measure of information systems (IS) in evolving computing environments: an empirical investigation' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper argues that the contingent evaluation approach can help an organization to evaluate the impact of information systems (IS) on business performance. The structure of IS is one of the most important factors in applying the contingent approach, because it reflects the evolution of the computing environment and is also aligned with business objectives and strategies. We empirically derived four taxonomics of IS structure and tested the relationships between the derived typologies and the prioritized or adopted measures of their performance. Firms with different IS structure were found to consider some measures to be more appropriate than others. The empirical results support the idea that the contingent measurement approach has the potential to be an even more useful framework if it incorporates more contingency factors, such as environmental, organizational contexts, and other characteristics."
      }
    ]
  },
  {
    "tid":"37",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Fast and Flexible Clustering Algorithm Using Binary Discretization\nContent: We present in this paper a new clustering algorithm for multivariate data. This algorithm, called BOOL (Binary coding Oriented clustering), can detect arbitrarily shaped clusters and is noise tolerant. BOOL handles data using a two-step procedure: data points are first discretized and represented as binary words, clusters are then iteratively constructed by agglomerating smaller clusters using this representation. This latter step is carried out with linear complexity by sorting such binary representations, which results in dramatic speedups when compared with other techniques. Experiments show that BOOL is faster than K-means, and about two to three orders of magnitude faster than two state-of-the-art algorithms that can detect non-convex clusters of arbitrary shapes. We also show that BOOL's results are robust to changes in parameters, whereas most algorithms for arbitrarily shaped clusters are known to be overly sensitive to such changes. The key to the robustness of BOOL is the hierarchical structure of clusters that is introduced automatically by increasing the accuracy of the discretization.\n\nTitle: The Coding Divergence for Measuring the Complexity of Separating Two Sets\nContent: In this paper we integrate two essential processes, discretization of continuous data and learning of a model that explains them, towards fully computational machine learning from continuous data. Discretization is fundamental for machine learning and data mining, since every continuous datum; e.g., a real-valued datum obtained by observation in the real world, must be discretized and converted from analog (continuous) to digital (discrete) form to store in databases. However, most machine learning methods do not pay attention to the situation; i.e., they use digital data in actual applications on a computer whereas assume analog data (usually vectors of real numbers) theoretically. To bridge the gap, we propose a novel measure of the difference between two sets of data, called the coding divergence, and unify two processes discretization and learning computationally. Discretization of continuous data is realized by a topological mapping (in the sense of mathematics) from the d-dimensional Euclidean space R-d into the Cantor space Sigma(omega), and the simplest model is learned in the Cantor space, which corresponds to the minimum open set separating the given two sets of data. Furthermore, we construct a classifier using the divergence, and experimentally demonstrate robust performance of it. Our contribution is not only introducing a new measure from the computational point of view, but also triggering more interaction between experimental science and machine learning.\n\nTitle: Efficient Mining Of Closed Tree Patterns From Large Tree Databases With Subtree Constraint\nContent: Mining frequent tree patterns from tree databases has practical importance in domains like Web mining, Bioinformatics, and so on. Although there have been algorithms on efficient tree mining, these algorithms are often lack of the interpretability in that they often produce a huge number of patterns, most of which are meaningless to users. This paper aims at both demands, one with respect to computational cost, which is efficient generation of tree patterns, and another one with respect to the interpretability. This task requires an efficient method to incorporate the users' needs into mining process.We propose a new top-down method for mining unordered closed tree patterns from a database of trees such that every mined pattern must contain a common piece of information in the form of a tree specified by the user. This type of mining is called mining with subtree constraint which would be useful, for example, in Web mining and Bioinformatics, where users want to extract common patterns around some given information from original data. The proposed algorithm is tested and compared with a state-of-the-art tree mining algorithm on real and artificial datasets with very good results.\n\nTitle: Incremental mining of closed frequent subtrees\nContent: We study the problem of mining closed frequent subtrees from tree databases that are updated regularly over time. Closed frequent subtrees provide condensed and complete information for all frequent subtrees in the database. Although mining closed frequent subtrees is in general faster than mining all frequent subtrees, this is still a very time consuming process, and thus it is undesirable to mine from scratch when the change to the database is small. The set of previous mined closed subtrees should be reused as much as possible to compute new emerging subtrees. We propose, in this paper, a novel and efficient incremental mining algorithm for closed frequent labeled ordered trees. We adopt a divide-and-conquer strategy and apply different mining techniques in different parts of the mining process. The proposed algorithm requires no additional scan of the whole database while its memory usage is reasonable. Our experimental study on both synthetic and real-life datasets demonstrates the efficiency and scalability of our algorithm.\n\nTitle: Learning figures with the Hausdorff metric by fractals\u2014towards computable binary classification\nContent: We present learning of figures, nonempty compact sets in Euclidean space, based on Gold's learning model aiming at a computable foundation for binary classification of multivariate data. Encoding real vectors with no numerical error requires infinite sequences, resulting in a gap between each real vector and its discretized representation used for the actual machine learning process. Our motivation is to provide an analysis of machine learning problems that explicitly tackles this aspect which has been glossed over in the literature on binary classification as well as in other machine learning tasks such as regression and clustering. In this paper, we amalgamate two processes: discretization and binary classification. Each learning target, the set of real vectors classified as positive, is treated as a figure. A learning machine receives discretized vectors as input data and outputs a sequence of discrete representations of the target figure in the form of self-similar sets, known as fractals. The generalization error of each output is measured by the Hausdorff metric. Using this learning framework, we reveal a hierarchy of learnable classes under various learning criteria in the track of traditional analysis based on Gold's learning model, and show a mathematical connection between machine learning and fractal geometry by measuring the complexity of learning using the Hausdorff dimension and the VC dimension. Moreover, we analyze computability aspects of learning of figures using the framework of Type-2 Theory of Effectivity (TTE).\n\n## Task\nCompose an abstract for the title 'The minimum code length for clustering using the gray code' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We propose new approaches to exploit compression algorithms for clustering numerical data. Our first contribution is to design a measure that can score the quality of a given clustering result under the light of a fixed encoding scheme. We call this measure the Minimum Code Length (MCL). Our second contribution is to propose a general strategy to translate any encoding method into a cluster algorithm, which we call COOL (COding-Oriented cLustering). COOL has a low computational cost since it scales linearly with the data set size. The clustering results of COOL is also shown to minimize MCL. To illustrate further this approach, we consider the Gray Code as the encoding scheme to present GCOOL. G-COOL can find clusters of arbitrary shapes and remove noise. Moreover, it is robust to change in the input parameters; it requires only two lower bounds for the number of clusters and the size of each cluster, whereas most algorithms for finding arbitrarily shaped clusters work well only if all parameters are tuned appropriately. G-COOL is theoretically shown to achieve internal cohesion and external isolation and is experimentally shown to work well for both synthetic and real data sets."
      }
    ]
  },
  {
    "tid":"38",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Sixth international workshop on traceability in emerging forms of software engineering: (TEFSE 2011)\nContent: The Sixth International Workshop on Traceability in Emerging Forms of Software Engineering (TEFSE 2011) will bring together researchers and practitioners to examine the challenges of recovering and maintaining traceability for the myriad forms of software engineering artifacts, ranging from user needs to models to source code. The objective of the 6th edition of TEFSE is to build on the work the traceability research community has completed in identifying the open traceability challenges. In particular, it is intended to be a working event focused on discussing the main problems related to software artifact traceability and propose possible solutions for such problems. Moreover, the workshop also aims at identifying key issues concerning the importance of maintaining the traceability information during software development, to further improve the cooperation between academia and industry and to facilitate technology transfer.\n\nTitle: TraceLab: an experimental workbench for equipping researchers to innovate, synthesize, and comparatively evaluate traceability solutions\nContent:  TraceLab is designed to empower future traceability research, through facilitating innovation and creativity, increasing collaboration between researchers, decreasing the startup costs and effort of new traceability research projects, and fostering technology transfer. To this end, it provides an experimental environment in which researchers can design and execute experiments in TraceLab's visual modeling environment using a library of reusable and user-defined components. TraceLab fosters research competitions by allowing researchers or industrial sponsors to launch research contests intended to focus attention on compelling traceability challenges. Contests are centered around specific traceability tasks, performed on publicly available datasets, and are evaluated using standard metrics incorporated into reusable TraceLab components. TraceLab has been released in beta-test mode to researchers at seven universities, and will be publicly released via CoEST.org in the summer of 2012. Furthermore, by late 2012 TraceLab's source code will be released as open source software, licensed under GPL. TraceLab currently runs on Windows but is designed with cross platforming issues in mind to allow easy ports to Unix and Mac environments. \n\n\n\nTitle: Supporting and Accelerating Reproducible Research in Software Maintenance Using TraceLab Component Library\nContent: Research studies in software maintenance are notoriously hard to reproduce due to lack of datasets, tools, implementation details (e.g., parameter values, environmental settings) and other factors. The progress in the field is hindered by the challenge of comparing new techniques against existing ones, as researchers have to devote a lot of their resources to the tedious and error-prone process of reproducing previously introduced approaches. In this paper, we address the problem of experiment reproducibility in software maintenance and provide a long term solution towards ensuring that future experiments will be reproducible and extensible. We conducted a mapping study of a number of representative maintenance techniques and approaches and implemented them as a library of experiments and components that we make publicly available with TraceLab, called the Component Library. The goal of these experiments and components is to create a body of actionable knowledge that would (i) facilitate future research and would (ii) allow the research community to contribute to it as well. In addition, to illustrate the process of using and adapting these techniques, we present an example of creating new techniques based on existing ones, which produce improved results.\n\nTitle: Supporting and accelerating reproducible empirical research in software evolution and maintenance using TraceLab Component Library\nContent: Research studies in software maintenance are notoriously hard to reproduce due to lack of datasets, tools, implementation details (e.g., parameter values, environmental settings) and other factors. The progress in the field is hindered by the challenge of comparing new techniques against existing ones, as researchers have to devote a large portion of their resources to the tedious and error-prone process of reproducing previously introduced approaches. In this paper, we address the problem of experiment reproducibility in software maintenance and provide a long-term solution towards ensuring that future experiments will be reproducible and extensible. We conducted a preliminary mapping study of a number of representative maintenance techniques and approaches and implemented them as a set of experiments and a library of components that we make publicly available with TraceLab, called the Component Library. The goal of these experiments and components is to create a body of actionable knowledge that would (i) facilitate future research and (ii) allow the research community to contribute to it as well. In addition, to illustrate the process of using and adapting these techniques, we present an example of creating new techniques based on existing ones, in order to produce improved results.\n\nTitle: Grand challenges, benchmarks, and TraceLab: developing infrastructure for the software traceability research community.\nContent: The challenges of implementing successful and cost-effective traceability have created a compelling research agenda that has addressed a broad range of traceability related issues, ranging from qualitative studies of traceability users in industry to very technical and quantitative studies. Unfortunately, advances are hampered by the significant time and effort needed to establish a traceability research environment and to perform comparative evaluations of new results against existing baselines. In this panel we discuss ongoing efforts by members of the Center of Excellence for Software Traceability (CoEST) to define the Grand Challenges of Traceability, develop benchmarks, and to construct TraceLab, an extensible and scalable visual environment for designing and executing a broad range of traceability experiments.\n\n## Task\nCompose an abstract for the title '5th international workshop on Traceability in Emerging Forms of Software Engineering (TEFSE 2009)' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Traceability of Emerging Forms of Software Engineering (TEFSE) 2009 will bring together researchers and practitioners to examine the challenges of recovering and maintaining traceability for the myriad forms of software engineering, from user needs to models to source code. In the 2007 instalment, TEFSE focused on the grand challenges of traceability. The 2009 instalment will focus on these and other emerging challenges in traceability."
      }
    ]
  },
  {
    "tid":"39",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Optimal nonmyopic value of information in graphical models: efficient algorithms and theoretical limits\nContent: Many real-world decision making tasks require us to choose among several expensive observations. In a sensor network, for example, it is important to select the subset of sensors that is expected to provide the strongest reduction in uncertainty. It has been general practice to use heuristic-guided procedures for selecting observations. In this paper, we present the first efficient optimal algorithms for selecting observations for a class of graphical models containing Hidden Markov Models (HMMs). We provide results for both selecting the optimal subset of observations, and for obtaining an optimal conditional observation plan. For both problems, we present algorithms for the filtering case, where only observations made in the past are taken into account, and the smoothing case, where all observations are utilized. Furthermore we prove a surprising result: In most graphical models tasks, if one designs an efficient algorithm for chain graphs, such as HMMs, this procedure can be generalized to polytrees. We prove that the value of information problem is NPPP-hard even for discrete polytrees. It also follows from our results that even computing conditional entropies, which are widely used to measure value of information, is a #P-complete problem on polytrees. Finally, we demonstrate the effectiveness of our approach on several real-world datasets.\n\nTitle: Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies\nContent: When monitoring spatial phenomena, which can often be modeled as Gaussian processes (GPs), choosing sensor locations is a fundamental task. There are several common strategies to address this task, for example, geometry or disk models, placing sensors at the points of highest entropy (variance) in the GP model, and A-, D-, or E-optimal design. In this paper, we tackle the combinatorial optimization problem of maximizing the mutual information between the chosen locations and the locations which are not selected. We prove that the problem of finding the configuration that maximizes mutual information is NP-complete. To address this issue, we describe a polynomial-time approximation that is within (1-1/e) of the optimum by exploiting the submodularity of mutual information. We also show how submodularity can be used to obtain online bounds, and design branch and bound search procedures. We then extend our algorithm to exploit lazy evaluations and local structure in the GP, yielding significant speedups. We also extend our approach to find placements which are robust against node failures and uncertainties in the model. These extensions are again associated with rigorous theoretical approximation guarantees, exploiting the submodularity of the objective function. We demonstrate the advantages of our approach towards optimizing mutual information in a very extensive empirical study on two real-world data sets.\n\nTitle: Near-Optimal Bayesian Active Learning with Noisy Observations\nContent: We tackle the fundamental problem of Bayesian active learning with noise,\nwhere we need to adaptively select from a number of expensive tests in order to\nidentify an unknown hypothesis sampled from a known prior distribution. In the\ncase of noise-free observations, a greedy algorithm called generalized binary\nsearch (GBS) is known to perform near-optimally. We show that if the\nobservations are noisy, perhaps surprisingly, GBS can perform very poorly. We\ndevelop EC2, a novel, greedy active learning algorithm and prove that it is\ncompetitive with the optimal policy, thus obtaining the first competitiveness\nguarantees for Bayesian active learning with noisy observations. Our bounds\nrely on a recently discovered diminishing returns property called adaptive\nsubmodularity, generalizing the classical notion of submodular set functions to\nadaptive policies. Our results hold even if the tests have non-uniform cost and\ntheir noise is correlated. We also propose EffECXtive, a particularly fast\napproximation of EC2, and evaluate it on a Bayesian experimental design problem\ninvolving human subjects, intended to tease apart competing economic theories\nof how people make decisions under uncertainty.\n\nTitle: Near-optimal Nonmyopic Value of Information in Graphical Models\nContent: A fundamental issue in real-world systems, such as sen- sor networks, is the selection of observations which most effectively reduce uncertainty. More specifically, we address the long standing problem of nonmyopically selecting the most informative subset of variables in a graphical model. We present the first efficient random- ized algorithm providing a constant factor (1 1/e \") approximation guarantee for any \" > 0 with high con- fidence. The algorithm leverages the theory of submod- ular functions, in combination with a polynomial bound on sample complexity. We furthermore prove that no polynomial time algorithm can provide a constant factor approximation better than (1 1/e) unless P = NP. Finally, we provide extensive evidence of the effective- ness of our method on two complex real-world datasets.\n\nTitle: Near Optimal Bayesian Active Learning for Decision Making.\nContent: How should we gather information to make effective decisions? We address Bayesian active learning and experimental design problems, where we sequentially select tests to reduce uncertainty about a set of hypotheses. Instead of minimizing uncertainty per se, we consider a set of overlapping decision regions of these hypotheses. Our goal is to drive uncertainty into a single decision region as quickly as possible. We identify necessary and sufficient conditions for correctly identifying a decision region that contains all hypotheses consistent with observations. We develop a novel Hyperedge Cutting (HEC) algorithm for this problem, and prove that is competitive with the intractable optimal policy. Our efficient implementation of the algorithm relies on computing subsets of the complete homogeneous symmetric polynomials. Finally, we demonstrate its effectiveness on two practical applications: approximate comparison-based learning and active localization using a robot manipulator.\n\n## Task\nCompose an abstract for the title 'Efficient Partial Monitoring with Prior Information.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Partial monitoring is a general model for online learning with limited feedback: a learner chooses actions in a sequential manner while an opponent chooses outcomes. In every round, the learner suffers some loss and receives some feedback based on the action and the outcome. The goal of the learner is to minimize her cumulative loss. Applications range from dynamic pricing to label-efficient prediction to dueling bandits. In this paper, we assume that we are given some prior information about the distribution based on which the opponent generates the outcomes. We propose BPM, a family of new efficient algorithms whose core is to track the outcome distribution with an ellipsoid centered around the estimated distribution. We show that our algorithm provably enjoys near-optimal regret rate for locally observable partial-monitoring problems against stochastic opponents. As demonstrated with experiments on synthetic as well as real-world data, the algorithm outperforms previous approaches, even for very uninformed priors, with an order of magnitude smaller regret and lower running time."
      }
    ]
  },
  {
    "tid":"40",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Adaptive thermal management for portable system batteries by forced convection cooling\nContent: Cycle life of a battery largely varies according to the battery operating conditions, especially the battery temperature. In particular, batteries age much faster at high temperature. Extensive experiments have shown that the battery temperature varies dramatically during continuous charge or discharge process. This paper introduces a forced convection cooling technique for the batteries that power a portable system. Since the cooling fan is also powered by the same battery, it is critical to develop a highly effective, low power-consuming solution. In addition, there is a fundamental tradeoff between the service time of a battery equipped with fans and the cycle life of the same battery. In particular, as the fan speed is increased, the power dissipated by the fan goes up and hence the full charge capacity of the battery is lost at a faster rate, but at the same time, the battery temperature remains lower and hence the battery longevity increases. This is the first work that formulates the adaptive thermal management problem for batteries (ATMB) in portable systems and provides a systematic solution for it. A hierarchical algorithm combining reinforcement learning at the lower level and dynamic programming at the upper level is proposed to derive the ATMB policy.\n\nTitle: Power supply and consumption co-optimization of portable embedded systems with hybrid power supply\nContent: Energy efficiency has always been an important design criterion for portable embedded systems. To compensate for the shortcomings of electrochemical batteries such as low power density, limited cycle life, and the rate capacity effect, supercapacitors have been employed as complementary power supplies for electrochemical batteries, i.e., hybrid power supplies comprised of batteries and supercapacitors have been proposed. In this work, we consider a portable embedded system with a hybrid power supply and executing periodic real-time tasks. We perform system power management from both the power supply side and the power consumption side to maximize the system service time. Specifically, we use feedback control for maintaining the supercapacitor energy at a certain level by regulating the discharging current of the battery, such that the supercapacitor has the capability to buffer the load current fluctuation. At the power consumption side, we perform task scheduling to assist supercapacitor energy maintenance. Experimental results demonstrate that the proposed joint optimization framework of task scheduling and power supply control successfully prolongs the total service time by up to 57%.\n\nTitle: Charge Allocation in Hybrid Electrical Energy Storage Systems\nContent: hybrid electrical energy storage (HEES) system consists of multiple banks of heterogeneous electrical energy storage (EES) elements placed between a power source and some load devices and providing charge storage and retrieval functions. For an HEES system to perform its desired functions of 1) reducing electricity costs by storing electricity obtained from the power grid at off-peak times when its price is lower, for use at peak times instead of electricity that must be bought then at higher prices, and 2) alleviating problems, such as excessive power fluctuation and undependable power supply, which are associated with the use of large amounts of renewable energy on the grid, appropriate charge management policies must be developed in order to efficiently store and retrieve electrical energy while attaining performance metrics that are close to the respective best values across the constituent EES banks in the HEES system. This paper is the first to formally describe the global charge allocation problem in HEES systems, namely, distributing a specified level of incoming power to a subset of destination EES banks so that maximum charge allocation efficiency is achieved. The problem is formulated as a mixed integer nonlinear program with the objective function set to the global charge allocation efficiency and the constraints capturing key requirements and features of the system such as the energy conservation law, power conversion losses in the chargers, the rate capacity, and self-discharge effects in the EES elements. A rigorous algorithm is provided to obtain near-optimal charge allocation efficiency under a daily charge allocation schedule. A photovoltaic array is used as an example of the power source for the charge allocation process and a heuristic is provided to predict the solar radiation level with a high accuracy. Simulation results using this photovoltaic cell array and a representative HEES system demonstrate up to 25% gain in the charge allocation efficiency by employing the proposed algorithm.\n\nTitle: Multiple-source and multiple-destination charge migration in hybrid electrical energy storage systems\nContent: Hybrid electrical energy storage (HEES) systems consist of multiple banks of heterogeneous electrical energy storage (EES) elements that are connected to each other through the Charge Transfer Interconnect. A HEES system is capable of providing an electrical energy storage means with very high performance by taking advantage of the strengths (while hiding the weaknesses) of individual EES elements used in the system. Charge migration is an operation by which electrical energy is transferred from a group of source EES elements to a group of destination EES elements. It is a necessary process to improve the HEES system's storage efficiency and its responsiveness to load demand changes. This paper is the first to formally describe a more general charge migration problem, involving multiple sources and multiple destinations. The multiple-source, multiple-destination charge migration optimization problem is formulated as a nonlinear programming (NLP) problem where the goal is to deliver a fixed amount of energy to the destination banks while maximizing the overall charge migration efficiency and not depleting the available energy resource of the source banks by more than a given percentage. The constraints for the optimization problem are the energy conservation relation and charging current constraints to ensure that charge migration will meet a given deadline. The formulation correctly accounts for the efficiency of chargers, the rate capacity effect of batteries, self-discharge currents and internal resistances of EES elements, as well as the terminal voltage variation of EES elements as a function of their state of charges (SoC's). An efficient algorithm to find a near-optimal migration control policy by effectively solving the above NLP optimization problem as a series of quasi-convex programming problems is presented. Experimental results show significant gain in migration efficiency up to 35%.\n\nTitle: Dynamic thermal management in mobile devices considering the thermal coupling between battery and application processor\nContent: The thermal management is a crucial design problem for mobile devices because it greatly affects not only the device reliability, but also the leakage energy consumption. Conventional dynamic thermal management (DTM) techniques work well for the computer systems. However, due to the limitation of the physical space in mobile devices, the thermal coupling effect between the major heat generation components, such as the application processor (AP) and the battery, plays an important role in determining the temperature inside the mobile device package. Due to this effect, the thermal behavior of one part is no longer independent of the other, but is affected by the temperature of other parts. This is the first work that quantitatively characterizes the thermal coupling between the battery and AP and presents a predictive DTM for mobile devices considering this effect. Simulation results show that the proposed DTM method significantly reduces the thermal violations for the target mobile devices.\n\n\n\n## Task\nCompose an abstract for the title 'Joint Charge and Thermal Management for Batteries in Portable Systems With Hybrid Power Sources.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper introduces a joint charge and thermal management problem for batteries in a battery-supercapacitor hybrid power source of a portable system, which has been equipped with a forced convection cooling technique, such as a fan. A key consideration in such a system is that the battery aging depends strongly on the battery temperature, which is in turn a function of the workload running on th..."
      }
    ]
  },
  {
    "tid":"41",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Urban Object Recognition From Informative Local Features\nContent: Autonomous mobile agents require object recognition for high level interpretation and localization in complex scenes. In urban environments, recognition of buildings might play a dominant role in robotic systems that need object based navigation, that take advantage of visual feedback and multimodal information for self-localization, or that enable association to related information from the identified semantics. We present a new method - the informative local features approach - based on an information theoretic saliency measure that is rapidly derived from a local Parzen window density estimation in feature subspace. From the learning of a decision tree based mapping to informative features, it enables attentive access to discriminative information and thereby significantly speeds up the recognition process. This approach is highly robust with respect to severe degrees of partial occlusion, noise, and tolerant to some changes in scale and illumination. We present performance evaluation on our publicly available reference object database (TSG-20) that demonstrates the efficiency of this approach, case wise even outperforming the SIFT feature approach [1]. Building recognition will be advantageous in various application domains, such as, mobile mapping, unmanned vehicle navigation, and systems for car driver assistance.\n\nTitle: Building detection from mobile imagery using informative SIFT descriptors\nContent: We propose reliable outdoor object detection on mobile phone imagery from off-the-shelf devices. With the goal to provide both robust object detection and reduction of computational complexity for situated interpretation of urban imagery, we propose to apply the 'Informative Descriptor Approach' on SIFT features (i-SIFT descriptors). We learn an attentive matching of i-SIFT keypoints, resulting in a significant improvement of state-of-the-art SIFT descriptor based keypoint matching. In the off-line learning stage, firstly, standard SIFT responses are evaluated using an information theoretic quality criterion with respect to object semantics, rejecting features with insufficient conditional entropy measure, producing both sparse and discriminative object representations. Secondly, we learn a decision tree from the training data set that maps SIFT descriptors to entropy values. The key advantages of informative SIFT (i-SIFT) to standard SIFT encoding are argued from observations on performance complexity, and demonstrated in a typical outdoor mobile vision experiment on the MPG-20 reference database.\n\nTitle: An Attentive Machine Interface Using Geo-Contextual Awareness for Mobile Vision Tasks\nContent: The presented work settles attention in the architecture of ambient intelligence, in particular, for the application of mobile vision tasks in multimodal interfaces. A major issue for the performance of these services is uncertainty in the visual information which roots in the requirement to index into a huge amount of reference images. We propose a system implementation --the Attentive Machine Interface (AMI) --that enables contextual processing of multi-sensor information in a probabilistic framework, for example to exploit contextual information from geo-services with the purpose to cut down the visual search space into a subset of relevant object hypotheses. We present a proof-of-concept with results from bottom-up information processing from experimental tracks and image capture in an urban scenario, extracting object hypotheses in the local context from both (i) mobile image based appearance and (ii) GPS based positioning, and verify performance in recognition accuracy ( 10%) using Bayesian decision fusion. Finally, we demonstrate that top-down information processing --geo-information priming the recognition method in feature space --can yield even better results ( 13%) and more economic computing, verifying the advantage of multi-sensor attentive processing in multimodal interfaces.\n\nTitle: Visual object detection from mobile phone imagery for context awareness\nContent: We describe a system which proposes a solution for multi-sensor object awareness and positioning to enable stable location awareness for a mobile service in urban areas. The system offers technology of outdoors vision based object recognition that will extend state-of-the-art location and context aware services towards object based awareness in urban environments. In the proposed application scenario, tourist pedestrians are equipped with a GPRS or UMTS capable camera-phone. They are interested whether their field of view contains tourist sights that would point to more detailed information. Multimedia type data about related history might be explored by a mobile user who is intending to learn within the urban environment. Ambient learning is in this way achieved by pointing the device towards an urban sight, capturing an image, and consequently getting information about the object on site and within the focus of attention, i.e., the user's current field of view. The described mobile system offers multiple opportunities for application in both mobile business and commerce, and is currently developed as an industrial prototype.\n\nTitle: Dynamic Multi-Cue Information Fusion for Robust Detection of Traffic Infrastructure\nContent: Visual object detection using single cue information has been successfully applied in various tasks, in par- ticular for near range recognition. While robust clas- sification and probabilistic representation enhance 2D pattern recognition performance, they are 'per se' re- stricted due to the limited information content of single cues. The contribution of this work is to demonstrate performance improvement using multi-cue information integrated within a probabilistic framework. 2D and 3D visual information naturally complement one an- other, each information source providing evidence for the occurrence of the object of interest. We demon- strate preliminary work describing Bayesian decision fusion for object detection and illustrate the method by robust detection of traffic infrastructure.\n\n## Task\nCompose an abstract for the title 'A Mobile Vision System for Urban Detection with Informative Local Descriptors' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We present a computer vision system for the detection and identification of urban objects from mobile phone imagery, e.g., for the application of tourist information services. Recognition is based on MAP decision making over weak object hypotheses from local descriptor responses in the mobile imagery. We present an improvement over the standard SIFT key detector [7] by selecting only informative (i-SIFT) keys for descriptor matching. Selection is applied first to reduce the complexity of the object model and second to accelerate detection by selective filtering. We present results on the MPG-20 mobile phone imagery with severe illumination, scale and viewpoint changes in the images, performing with \u2248 98% accuracy in identification, efficient (100%) background rejection, efficient (0%) false alarm rate, and reliable quality of service under extreme illumination conditions, significantly improving standard SIFT based recognition in every sense, providing - important for mobile vision - runtimes which are \u2248 8 (\u224824) times faster for the MPG-20 (ZuBuD) database."
      }
    ]
  },
  {
    "tid":"42",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A hierarchical federated integration architecture for collaborative product development\nContent: Collaborative product development CPD is concerned with various experience, knowledge, teams, tools and processes. Since simulation is an important stage of CPD, high level architecture HLA has been successfully adopted as basic architecture of CPD systems. At the same time, HLA-based federated CPD systems also have some difficulties. Among these, effective communication and cooperation among these disparate parties are key challenges. In this article, a hierarchical federated integration architecture for heterogeneous information systems is proposed. Different from the tightly coupled collaborative systems for fixed-scenario collaborative jobs, the proposed architecture is more suitable for dynamic, loosely coupled and multi-objective collaborations. Under this architecture, cooperation individuals are the projections ambassadors of real subsystems on given application domains. The control scope of subsystems and the management scope of collaboration can be different in order to avoid exploring too many details about the subsystems and to maintain their independency. The owners of subsystems can fully control their own subsystems and share their resources as they wish. The failure of one subsystem does not affect collaboration among other subsystems, and each subsystem can work independently when the integration system fails. Moreover, hierarchical collaborations are achieved by introducing the concepts of system federation and application federation, which address the collaboration issues at the physical level and the logical level separately. A real industrial application system has been designed and implemented using the proposed architecture.\n\nTitle: Ontology maintenance in high level architecture federation development and execution process\nContent: This paper presents an ontology maintenance approach aiming at dynamically maintaining the federation collaboration ontology in High Level Architecture HLA federation development and execution process, with the objective to provide guidelines on how to use, organize and manipulate federation collaboration ontology to meet various requirements. The proposed approach includes two algorithms: ontology maintenance + and ontology maintenance -, corresponding to federate joining and resigning. It adopts an axiom-based deduction strategy and takes heavy-weighted ontologies into consideration. It can find all the explicit and derived inter-ontology relations. A case study shows that the proposed approach has great potential to improve the efficiency of federation development and execution processes, reduce the workload for adaptive adjustment of existing platforms, and enhance the applicability and flexibility of HLA based collaborative product development systems.\n\nTitle: Ontology Fusion in High-Level-Architecture-Based Collaborative Engineering Environments\nContent: In high-level-architecture (HLA)-based distributed heterogeneous collaborative engineering environments (CEEs), the construction of federation object model files is time consuming. This paper presents an ontology fusion approach aiming at establishing a common understanding in such collaborative environments. The proposed approach has three steps: ontology mapping, ontology alignment, and ontology merging. Ontology mapping employs a top-down approach to explore all bridge relations between two terms from different ontologies based on bridge axioms and deduction rules. Ontology alignment adopts a bottom-up approach to discover implicit bridge relations between two terms from different domain ontologies based on equivalent inference. Ontology merging generates a new collaboration ontology from discovered equivalent bridge relations. It adopts an axiom-based ontology fusion strategy and takes heavy-weighted ontologies into consideration. It can find all the explicit and derived interontology relations. In a typical CEE, the proposed approach has a great potential to improve the efficiency of preparation for HLA-based collaborative engineering processes, reduce the work load for adaptive adjustment of existing platforms, and enhance the reusability and flexibility of CEEs. A case study has been conducted to validate the feasibility of the proposed approach.\n\nTitle: Ontology fusion in HLA-based collaborative product development\nContent: This paper presents a novel ontology fusion approach which aims to establish a mutual understanding among HLA (High Level Architecture) -based distributed heterogeneous collaborative product development systems. The approach includes three steps: ontology mapping, ontology alignment and ontology merging. It adopts an axiom-based deduction ontology fusion strategy, and takes heavy weighted ontologies into consideration. It can find all the explicit and derived inter-ontology relations, and furthermore it reaches the active upper bounds of implicit equivalent inter-ontology relations searching. The proposed approach has great potential to improve the efficiency of preparation for HLA-based collaborative product development, reduce the work load for adaptive adjustment of ever-existing platforms, and enhance the applicability and flexibility of collaborative development systems.\n\nTitle: Ontology-based interoperation model of collaborative product development\nContent: The reuse of existing systems is an important objective of High Level Architecture (HLA) based collaborative product development systems. However, in order to reuse an existing system, its interoperation interface has to be modified so as to comply with the objective and interaction representations defined in a corresponding Federation Object Model (FOM). Such modifications imply added time and effort, which diminishes the efficiency of system reuse in collaborative product development. This paper presents a heavy-weighted ontology-based construction method for interoperation models to support the reuse of subsystems in various collaborative contexts. In this method ontologies are used to specify the semantics of object classes and interaction classes in subsystems in a formal and computer readable fashion. In doing so, a Formal Concept Analysis (FCA) like construction method is introduced to establish the original interoperation ontology from scratch. An automatic transforming method from Simulation Object Model (SOM) into interoperation ontology is also described to make existing HLA based systems easy to adopt this approach. Then a consistency verification method is introduced to guarantee the consistency of the interoperation ontologies. A case study is used to demonstrate the feasibility of the proposed method. As a human-friendly modeling method, compared with existing interoperation modeling methods the proposed method is more flexible, efficient and reliable.\n\n## Task\nCompose an abstract for the title 'Ontology maintenance in a hierarchical federated collaborative product development environment' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper presents a novel approach aiming to dynamically maintain the collaboration ontology in the execution of an ontology-based federated collaborative product development system when a federate joins or has resigned from a given federation. The proposed approach includes two algorithms: ontology maintenance (+) and ontology maintenance (-), corresponding to joining and resigning situations. It adopts an axiom-based deduction ontology fusion strategy, and takes heavy-weighted ontologies into consideration. It can find all the explicit and derived inter-ontology relations, and furthermore it reaches the active upper bounds of implicit equivalent inter-ontology relations searching. This paper also discusses some implementation issues on the basis of TH_RTI, a RTI (Run Time Infrastructure) version developed by National CIMS ERC, Tsinghua University. The proposed approach has great potential to improve the efficiency of ontology-based federated collaboration executions, reduce the work load for adaptive adjustment of ever-existing platforms, and enhance the applicability and flexibility of collaborative product development systems."
      }
    ]
  },
  {
    "tid":"43",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Integrating text chunking with mixture hidden markov models for effective biomedical information extraction\nContent: This paper presents a new information extraction (IE) technique, KXtractor, which integrates a text chunking technique with Mixture Hidden Markov Models (MiHMM). KXtractor is differentiated from other approaches in that (a) it overcomes the problem of the single Part-Of-Speech (POS) HMMs with modeling the rich representation of text where features overlap among state units such as word, line, sentence, and paragraph. By incorporating sentence structures into the learned models, KXtractor provides better extraction accuracy than the single POS HMMs do. (b) It resolves the issues with the traditional HMMs for IE that operate only on the semi-structured data such as HTML documents and other text sources in which language grammar does not play a pivotal role. We compared KXtractor with three IE techniques: 1) RAPIER, an inductive learning-based machine learning system, 2) a Dictionary-based extraction system, and 3) single POS HMM. Our experiments showed that KXtractor outperforms these three IE systems in extracting protein-protein interactions. In our experiments, F-measure for KXtractor was higher than ones for RAPIER, a dictionary-based system, and single POS HMM respectively by 16.89%, 16.28%, and 8.58%. In addition, both precision and recall of KXtractor are higher than those systems.\n\nTitle: An automatic unsupervised querying algorithm for efficient information extraction in biomedical domain\nContent: In the domain of bioinformatics, extracting a relation such as protein-protein interations from a large database of text documents is a challenging task. One major issue with biomedical information extraction is how to efficiently digest the sheer size of unstructured biomedical data corpus. Often, among these huge biomedical data, only a small fraction of the documents contain information that is relevant to the extraction task. We propose a novel query expansion algorithm to automatically discover the characteristics of documents that are useful for extraction of a target relation. Our technique introduces a hybrid query re-weighting algorithm combining the modified Robertson Sparck-Jones query ranking algorithm with a keyphrase extraction algorithm. Our technique also adopts a novel query translation technique that incorporates POS categories to query translation. We conduct a series of experiments and report the experimental results. The results show that our technique is able to retrieve more documents that contain protein-protein pairs from MEDLINE as iteration increases. Our technique is also compared with SLIPPER, a supervised rule-based query expansion technique. The results show that our technique outperforms SLIPPER from 17.90% to 29.98 better in four iterations.\n\nTitle: Data exploration and knowledge discovery in a patient wellness tracking (PWT) system at a nurse-managed health services center\nContent: This paper describes our ongoing research on data exploration and knowledge discovery in a patient wellness tracking (PWT) information system developed for a nurse-managed community health center. The center employs an innovative and transdisciplinary care model that fully integrates behavioral and various wellness services into primary care to form a team approach. We have developed the PWT system that integrates clinical data collected in an electronic medical record (EMR) system with the data generated by a spectrum of healthy living programs and wellness services. While data is being collected rapidly in large volumes, it is imperative to develop effective tools in helping clinicians explore data and discover knowledge. In this paper, we present (1) an exploratory data browser based on information content in information theory for searching granularity patient data, and (2) a knowledge discovery component based on probabilistic graphical models for diagnosis, prognosis, and revealing clinical cause-effect interactions.\n\nTitle: KXtractor: an effective biomedical information extraction technique based on mixture hidden markov models\nContent: We present a novel information extraction (IE) technique, KXtractor, which combines a text chunking technique and Mixture Hidden Markov Models (MiHMM). KXtractor overcomes the problem of the single Part-Of-Speech (POS) HMMs with modeling the rich representation of text where features overlap among state units such as word, line, sentence, and paragraph. KXtractor also resolves issues with the traditional HMMs for IE that operate only on the semi-structured data such as HTML documents and other text sources in which language grammar does not play a pivotal role. We compared KXtractor with three IE techniques: 1) RAPIER, an inductive learning-based machine learning system, 2) a Dictionary-based extraction system, and 3) single POS HMM. Our experiments showed that KXtractor outperforms these three IE systems in extracting protein-protein interactions. In our experiments, the F-measure for KXtractor was higher than for RAPIER, a dictionary-based system, and single POS HMM respectively by 16.89%, 16.28%, and 8.58%. In addition, both precision and recall of KXtractor are higher than those systems.\n\nTitle: Integration of semantic-based bipartite graph representation and mutual refinement strategy for biomedical literature clustering\nContent: We introduce a novel document clustering approach that overcomes those problems by combining a semantic-based bipartite graph representation and a mutual refinement strategy. The primary contributions of this paper are the following. First, we introduce a new representation of documents using a bipartite graph between documents and co-occurrence concepts in the documents. Second, we show how to enhance clustering quality by applying the mutual refinement strategy to the initial clustering results. Third, through the experiments on MEDLINE documents, we show that our integrated method significantly enhances cluster quality and clustering reliability compared to existing clustering methods. Our approach improves on the average 29.5 cluster quality and 26.3 clustering reliability, in terms of misclassification index, over Bisecting K-means with the best parameters.\n\n## Task\nCompose an abstract for the title 'Temporal rule induction for clinical outcome analysis' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Clinical outcomes analysis normally covers a particular time period. The sample under study is constantly changing as patients are censored, leave the study or die. In this paper, we present a novel data mining approach to mine temporal rules that reflect characteristics of outcomes analysis. We apply our temporal rule induction algorithm to a set of cancer patients, clinical records that were prospectively collected for 20 years. We analyse clinical data not only based on the static event, such as local recurrence for survival analysis, but also based on the temporal event with censored data for each time unit. The rules extracted from our temporal rule induction algorithm are compared to results from statistical analysis. The importance of this paper is that this novel temporal rule induction algorithm provides valuable insights for clinical data assessment and complements traditional statistical analysis."
      }
    ]
  },
  {
    "tid":"44",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Tighter low-rank approximation via sampling the leveraged element\nContent: In this work, we propose a new randomized algorithm for computing a low-rank approximation to a given matrix. Taking an approach different from existing literature, our method first involves a specific biased sampling, with an element being chosen based on the leverage scores of its row and column, and then involves weighted alternating minimization over the factored form of the intended low-rank matrix, to minimize error only on these samples. Our method can leverage input sparsity, yet produce approximations in spectral (as opposed to the weaker Frobenius) norm; this combines the best aspects of otherwise disparate current results, but with a dependence on the condition number \u03ba = \u03c31/\u03c3r. In particular we require O(nnz(M)+n\u03ba2r5/\u03b52) computations to generate a rank-r approximation to M in spectral norm. In contrast, the best existing method requires O(nnz(M)+nr2/\u03b54) time to compute an approximation in Frobenius norm. Besides the tightness in spectral norm, we have a better dependence on the error \u03b5. Our method is naturally and highly parallelizable. Our new approach enables two extensions that are interesting on their own. The first is a new method to directly compute a low-rank approximation (in efficient factored form) to the product of two given matrices; it computes a small random set of entries of the product, and then executes weighted alternating minimization (as before) on these. The sampling strategy is different because now we cannot access leverage scores of the product matrix (but instead have to work with input matrices). The second extension is an improved algorithm with smaller communication complexity for the distributed PCA setting (where each server has small set of rows of the matrix, and want to compute low rank approximation with small amount of communication with other servers).\n\nTitle: Solving a Mixture of Many Random Linear Equations by Tensor Decomposition and Alternating Minimization.\nContent: We consider the problem of solving mixed random linear equations with $k$ components. This is the noiseless setting of mixed linear regression. The goal is to estimate multiple linear models from mixed samples in the case where the labels (which sample corresponds to which model) are not observed. We give a tractable algorithm for the mixed linear equation problem, and show that under some technical conditions, our algorithm is guaranteed to solve the problem exactly with sample complexity linear in the dimension, and polynomial in $k$, the number of components. Previous approaches have required either exponential dependence on $k$, or super-linear dependence on the dimension. The proposed algorithm is a combination of tensor decomposition and alternating minimization. Our analysis involves proving that the initialization provided by the tensor method allows alternating minimization, which is equivalent to EM in our setting, to converge to the global optimum at a linear rate.\n\nTitle: Matrix Completion With Column Manipulation: Near-Optimal Sample-Robustness-Rank Tradeoffs\nContent: This paper considers the problem of matrix completion when some number of the columns are completely and arbitrarily corrupted, potentially by a malicious adversary. It is well known that standard algorithms for matrix completion can return arbitrarily poor results, if even a single column is corrupted. One direct application comes from robust collaborative filtering. Here, some number of users are so-called manipulators who try to skew the predictions of the algorithm by calibrating their inputs to the system. In this paper, we develop an efficient algorithm for this problem based on a combination of a trimming procedure and a convex program that minimizes the nuclear norm and the $ell _{1,2}$ norm. Our theoretical results show that given a vanishing fraction of observed entries, it is nevertheless possible to complete the underlying matrix even when the number of corrupted columns grows. Significantly, our results hold without any assumptions on the locations or values of the observed entries of the manipulated columns. Moreover, we show by an information-theoretic argument that our guarantees are nearly optimal in terms of the fraction of sampled entries on the authentic columns, the fraction of corrupted columns, and the rank of the underlying matrix. Our results therefore sharply characterize the tradeoffs between sample, robustness, and rank in matrix completion.\n\nTitle: Learning a Compressed Sensing Measurement Matrix via Gradient Unrolling.\nContent: Linear encoding of sparse vectors is widely popular, but is commonly data-independent -- missing any possible extra (but a-priori unknown) structure beyond sparsity. In this paper we present a new method to learn linear encoders that adapt to data, while still performing well with the widely used $ell_1$ decoder. The convex $ell_1$ decoder prevents gradient propagation as needed in standard gradient-based training. Our method is based on the insight that unrolling the convex decoder into $T$ projected subgradient steps can address this issue. Our method can be seen as a data-driven way to learn a compressed sensing measurement matrix. We compare the empirical performance of 10 algorithms over 6 sparse datasets (3 synthetic and 3 real). Our experiments show that there is indeed additional structure beyond sparsity in the real datasets. Our method is able to discover it and exploit it to create excellent reconstructions with fewer measurements (by a factor of 1.1-3x) compared to the previous state-of-the-art methods. We illustrate an application of our method in learning label embeddings for extreme multi-label classification. Our experiments show that our method is able to match or outperform the precision scores of SLEEC, which is one of the state-of-the-art embedding-based approaches for extreme multi-label learning.\n\nTitle: Finding Low-Rank Solutions via Nonconvex Matrix Factorization, Efficiently and Provably\nContent: A rank-r matrix X is an element of R-mxn can be written as a product UVT, where U is an element of R(mxr )and V is an element of R-nxr. One could exploit this observation in optimization: e.g., consider the minimization of a convex function f(X) over rank-r matrices, where the set of low-rank matrices is modeled via UVT . Though such parameterization reduces the number of variables and is more computationally efficient (of particular interest is the case r << min{m, n}), it comes at a cost: f(UVT) becomes a nonconvex function w.r.t. U and V. We study such parameterization on generic convex objectives f and focus on firstorder, gradient descent algorithms. We propose the bifactored gradient descent (BFGD) algorithm, an efficient first-order method that operates directly on the U, V factors. We show that when f is (restricted) smooth, BFGD has local sublinear convergence; when f is both (restricted) smooth and (restricted) strongly convex, it has local linear convergence. For several applications, we provide simple and efficient initialization schemes that provide initial conditions, good enough for the above convergence results to hold, globally. Extensive experimental results support our arguments that BFGD is an efficient and accurate nonconvex method, compared to state-of-the-art approaches.\n\n## Task\nCompose an abstract for the title 'A New Sampling Technique for Tensors.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"  In this paper we propose new techniques to sample arbitrary third-order tensors, with an objective of speeding up tensor algorithms that have recently gained popularity in machine learning. Our main contribution is a new way to select, in a biased random way, only $O(n^{1.5}/\\epsilon^2)$ of the possible $n^3$ elements while still achieving each of the three goals: \\\\ {\\em (a) tensor sparsification}: for a tensor that has to be formed from arbitrary samples, compute very few elements to get a good spectral approximation, and for arbitrary orthogonal tensors {\\em (b) tensor completion:} recover an exactly low-rank tensor from a small number of samples via alternating least squares, or {\\em (c) tensor factorization:} approximating factors of a low-rank tensor corrupted by noise. \\\\ Our sampling can be used along with existing tensor-based algorithms to speed them up, removing the computational bottleneck in these methods. "
      }
    ]
  },
  {
    "tid":"45",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A wavelet-based method for multiscale tomographic reconstruction\nContent: The authors represent the standard ramp filter operator of the filtered-back-projection (FBP) reconstruction in different bases composed of Haar and Daubechies compactly supported wavelets. The resulting multiscale representation of the ramp-filter matrix operator is approximately diagonal. The accuracy of this diagonal approximation becomes better as wavelets with larger numbers of vanishing moments are used. This wavelet-based representation enables the authors to formulate a multiscale tomographic reconstruction technique in which the object is reconstructed at multiple scales or resolutions. A complete reconstruction is obtained by combining the reconstructions at different scales. The authors' multiscale reconstruction technique has the same computational complexity as the FBP reconstruction method. It differs from other multiscale reconstruction techniques in that (1) the object is defined through a one-dimensional multiscale transformation of the projection domain, and (2) the authors explicitly account for noise in the projection data by calculating maximum a posteriori probability (MAP) multiscale reconstruction estimates based on a chosen fractal prior on the multiscale object coefficients. The computational complexity of this maximum a posteriori probability (MAP) solution is also the same as that of the FBP reconstruction. This result is in contrast to commonly used methods of statistical regularization, which result in computationally intensive optimization algorithms.\n\nTitle: Tomographic reconstruction and estimation based on multiscale natural-pixel bases.\nContent: We use a natural pixel-type representation of an object, originally developed for incomplete data tomography problems, to construct nearly orthonormal multiscale basis functions. The nearly orthonormal behavior of the multiscale basis functions results in a system matrix, relating the input (the object coefficients) and the output (the projection data), which is extremely sparse. In addition, the coarsest scale elements of this matrix capture any ill conditioning in the system matrix arising from the geometry of the imaging system. We exploit this feature to partition the system matrix by scales and obtain a reconstruction procedure that requires inversion of only a well-conditioned and sparse matrix. This enables us to formulate a tomographic reconstruction technique from incomplete data wherein the object is reconstructed at multiple scales or resolutions. In case of noisy projection data we extend our multiscale reconstruction technique to explicitly account for noise by calculating maximum a posteriori probability (MAP) multiscale reconstruction estimates based on a certain self-similar prior on the multiscale object coefficients. The framework for multiscale reconstruction presented can find application in regularization of imaging problems where the projection data are incomplete, irregular, and noisy, and in object feature recognition directly from projection data.\n\nTitle: Efficient multiresolution counterparts to variational methods for surface reconstruction\nContent: alytical means of formulating the problem and as a means of determining a solution. Formulations of this type lead directly to Euler-Lagrange (5) partial differential equations (PDEs) to be solved in order to obtain the desired reconstructions. Except in those specific cases where the surface model and the measure- ment statistics are homogeneous, permitting FFT techniques to be applied, the solution of these equations can be a significant computational task. Moreover, the calculation of reconstruction error covariances (20, 31, 32, 37, 38) for such approaches are, for all practical purposes, completely infeasible, as their computa- tion corresponds in essence to the calculation of the full inverse of the partial differential operator arising from the variational problem. The research described in this paper is motivated by a class of problems for which FFT techniques are inapplicable and for which the solution of the Euler-Lagrange equations via vari- ational approaches is impractical: we are interested in large estimation problems having spatially varying models, possibly sparse measurements or measurements of varying quality and resolution, and for which estimation error statistics are required. In this paper we describe an alternative multiscale approach to surface reconstruction that overcomes these computational dif- ficulties. In our approach we take advantage of the dual interpre- tation of variational problems as statistical estimation problems; in particular, a variational problem with quadratic costs (i.e., a least squares problem) may be interpreted as a Gaussian sta- tistical estimation problem. This point separates the approach of this paper from those of the multigrid (14, 27, 35, 39) and preconditioning (2, 3, 7, 8, 33, 43) literatures:\n\nTitle: Tomographic reconstruction of polygons from knot location and chord length measurements\nContent: In this work, we develop statistically based algorithms to reconstruct binary polygonal objects from sparse and noisy tomographic-based observation data. Traditional approaches to the reconstruction of geometric objects from projection data often lead to highly nonlinear estimation problems. To avoid the difficulties associated with such nonlinear problems, we first examine the problem of reconstruction of an object based on knot location measurements, i.e., measurements of the locations of abrupt change in the projections. The ties between this problem and that of multitarget radar tracking enable us to develop a sequential hypothesis-testing algorithm requiring only the solution of a series of linear estimation problems. In particular, data association hypotheses are generated, under each of which the inversion is linear. The complexity of the association possibilities are kept in check through the use of constraints on the reconstruction imposed by the tomography problem. The solution of this first problem is then used as an initialization to a more complete reconstruction which, while utilizing all the projection data, is nonlinear. We demonstrate that the estimates provided by the first, efficient algorithm are of good quality on their own, and, when combined with a fully nonlinear inversion, produce excellent object estimates.\n\nTitle: A hierarchical algorithm for limited-angle reconstruction\nContent: The authors describe and demonstrate a hierarchical reconstruction algorithm for use in noisy and limited-angle or sparse-angle tomography. The algorithm estimates the object's mass, center of mass, and convex hull from the available projections, and uses this information, along with fundamental mathematical constraints, to estimate a full set of smoothed projections. The mass and center of mass are estimated using a maximum-likelihood (ML) estimator derived from the principles of consistency of the Radon transform. The convex hull estimate is produced by first estimating the positions of support lines of the object from each available projection and then estimating the overall convex hull using ML or maximum a posteriori (MAP) techniques. The position of two support lines from a single projection is estimated using either a generalized likelihood ratio technique for estimating jumps in linear systems or a support-width penalty method that uses Akaike's model-order estimation technique\n\n## Task\nCompose an abstract for the title 'A moment-based variational approach to tomographic reconstruction.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We describe a variational framework for the tomographic reconstruction of an image from the maximum likelihood (ML) estimates of its orthogonal moments. We show how these estimated moments and their (correlated) error statistics can be computed directly, and in a linear fashion from given noisy and possibly sparse projection data. Moreover, thanks to the consistency properties of the Radon transform, this two-step approach (moment estimation followed by image reconstruction) can be viewed as a statistically optimal procedure. Furthermore, by focusing on the important role played by the moments of projection data, we immediately see the close connection between tomographic reconstruction of nonnegative valued images and the problem of nonparametric estimation of probability densities given estimates of their moments. Taking advantage of this connection, our proposed variational algorithm is based on the minimization of a cost functional composed of a term measuring the divergence between a given prior estimate of the image and the current estimate of the image and a second quadratic term based on the error incurred in the estimation of the moments of the underlying image from the noisy projection data. We show that an iterative refinement of this algorithm leads to a practical algorithm for the solution of the highly complex equality constrained divergence minimization problem. We show that this iterative refinement results in superior reconstructions of images from very noisy data as compared with the classical filtered back-projection (FBP) algorithm."
      }
    ]
  },
  {
    "tid":"46",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Towards Optimized Fine-Grained Pricing of IaaS Cloud Platform\nContent: Although many pricing schemes in IaaS platform are already proposed with pay-as-you-go and subscription/spot market policy to guarantee service level agreement, it is still inevitable to suffer from wasteful payment because of coarsegrained pricing scheme. In this paper, we investigate an optimized fine-grained and fair pricing scheme. Two tough issues are addressed: (1) the profits of resource providers and customers often contradict mutually; (2) VM-maintenance overhead like startup cost is often too huge to be neglected. Not only can we derive an optimal price in the acceptable price range that satisfies both customers and providers simultaneously, but we also find a best-fit billing cycle to maximize social welfare (i.e., the sum of the cost reductions for all customers and the revenue gained by the provider). We carefully evaluate the proposed optimized fine-grained pricing scheme with two large-scale real-world production traces (one from Grid Workload Archive and the other from Google data center). We compare the new scheme to classic coarse-grained hourly pricing scheme in experiments and find that customers and providers can both benefit from our new approach. The maximum social welfare can be increased up to 72:98% and 48:15% with respect to DAS-2 trace and Google trace respectively.\n\nTitle: Synchronization-Aware Scheduling for Virtual Clusters in Cloud\nContent: Due to high flexibility and cost-effectiveness, cloud computing is increasingly being explored as an alternative to local clusters by academic and commercial users. Recent research already confirmed the feasibility of running tightly-coupled parallel applications with virtual clusters. However, such types of applications suffer from significant performance degradation, especially as the overcommitment is common in cloud. That is, the number of executable Virtual CPUs (VCPUs) is often larger than that of available Physical CPUs (PCPUs) in the system. The performance degradation is mainly due to the fact that the current Virtual Machine Monitors (VMMs) are unaware of the synchronization requirements of the VMs which are running parallel applications. In this paper, There are two key contributions. (1) We propose an autonomous synchronization-aware VM scheduling (SVS) algorithm, which can effectively mitigate the performance degradation of tightly-coupled parallel applications running atop them in overcommitted situation. (2) We integrate the SVS algorithm into Xen VMM scheduler, and rigorously implement a prototype. We evaluate our design on a real cluster environment with NPB benchmark and real-world trace. Experiments show that our solution attains better performance for tightly-coupled parallel applications than the state-of-the-art approaches like Xen\u2019s Credit scheduler, balance scheduling, and hybrid scheduling.\n\nTitle: SMU: Towards Cloud Oriented Service MashUp\nContent: Based on increasing popularity of cloud computing, social computing and web 2.0 technology, Internet resources are extremely increasing. How to provide service invoking interfaces ceaselessly while minimizing the cost of service development, which can meet the growing needs of end users, becomes a challenging issue for service providers. Meanwhile, service mashup technology is getting more attention in both enterprise and academia for building new end users applications fast in the complex and heterogeneous network environment. Therefore, we propose a new method of service mashup with the advantages of the cloud, grid, web services and other technologies. We develop cloud oriented service mashup system prototype (SMU). In SMU, we support the service information interaction, classification, and process during the procedure of service mashup to meet various needs of the multi-level and the multi-role of service applications. Our experiments show that SMU can reach the purpose of building personal service applications quickly and easily.\n\nTitle: Cranduler: a dynamic and reusable scheduler for cloud infrastructure service\nContent: As an import trend of cyberspace in the future, cloud computing has attracted much attention from the IT industry. Many research institutions and companies have launched their own cloud platforms, which have virtual machine schedulers to manage the infrastructure resource pool. The virtual machine scheduling modules in these platforms are built in the platform and it is hard for developers to re-program. Since developers cannot design and implement special policies in the platform, the flexibility of the virtual machine scheduler is poor. Furthermore, the schedule architecture which has a firm and unchangeable interface is designed and customized for one kind of cloud platform. It leads to poor portability. To target the problems above, this paper presents a dynamic and reusable scheduling system for cloud infrastructure service, called Cranduler, which introduces the advantages of cluster schedulers to the virtual machine scheduling in cloud infrastructure. The scheduling policies of Cranduler could be dynamically configured by developers. Developers can easily insert the custom policy. In addition, Cranduler provides a set of unified interfaces to the cloud platform, which make the system easily access resources from different cloud platforms and be reused in different cloud platforms.\n\nTitle: Towards Truly Elastic Distributed Graph Computing in the Cloud.\nContent: Elasticity is very important to the scale-out distributed systems running on today's large-scale multi-tenant clouds, regardless public or private. An elastic distributed data processing system must have the capability of: 1 dynamically balancing the computing load among workers due to their performance heterogeneity and dynamicity; 2 fast recovering the lost memory state of failure workers with acceptable overheads during the regular execution. Unfortunately, we found that the design of the state-of-the-art distributed graph computing system only works well in small sized dedicated clusters. We implement a distributed graph computing prototype, X-Graph, and demonstrate the capabilities of being elastic in three ways. First, we present menger, a novel two-level graph partition framework, which further splits one worker-level partition into several sub-partitions as the basic migration units, and each has the \\\"migration affinity\\\" with one of the other workers. Second, we implement a dynamical load balancer based on menger, which prefers the worker that has the affinity of the sub-partition to be migrated as the destination, and completely avoids the costly sophistical graph re-partitioning algorithms. Third, we implement a differentiated replication frame-work, which supports parallel recovery for lost partitions just like general-purpose dataflow systems.\n\n## Task\nCompose an abstract for the title 'Developing an optimized application hosting framework in Clouds' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"A Cloud system, which is also often called a data center, may host a large number of application servers (termed as applications for short in this paper) that consume various types of resource, such as CPU, memory, and I/O. In such a scenario, the application placement strategy, which specifies how to host applications across the physical servers in the Cloud, is critical to achieve good system- and/or client-oriented performance. The existing application placement algorithms in literature assume that the consumption of system resources is proportional to the level of workloads submitted to the system. In this paper, we reveal that it may not be the case in some circumstances. Based on this observation, we design and implement an application placement framework, called EAPAC, for Clouds. The developed framework is able to achieve the optimized performance by judiciously allocating to the applications a proper mixture of different types of application request as well as an appropriate number of requests in each type. Further, we investigate the issue of resource conflicts among different applications when there exist concurrent requests in the system. We have conducted extensive experiments to evaluate the performance of the developed framework. The experiment results show that compared with the existing method in literature, EAPAC can improve the performance by 35% in terms of the reply rate."
      }
    ]
  },
  {
    "tid":"47",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Distributed location and lifetime biased clustering for large scale wireless sensor network\nContent: Recent research in wireless sensor networks have shown in most of the WSN applications node positions are often known in priori, to be able to effectively assimilate data from the WSN deployment. Also expected lifetime of the network, ie. for how long the deployment should work, is often an important specification for a particular deployment. In this paper we proposed two novel protocols, we call, Location and expected Lifetime Biased Clustering (LeLBC) and a modification of it, with fully localized intra cluster chaining (LeLBC-ICC). Both the protocols utilize the location information and network lifetime requirement as the knowledge for scheduling cluster head selection expeditiously. Experiment results have shown that LeLBC outperforms widely quoted non deterministic cluster based protocol LEACH, while LeLBC-ICC gives comparable results with the near optimal solution PEGASIS. Both the protocols use only localized information and maximum numbers of nodes remain alive during entire lifetime of the network.\n\nTitle: An efficient decentralized rekeying scheme to secure hierarchical geographic multicast routing in wireless sensor networks\nContent: Wireless Sensor Networks consist of several autonomous sensors deployed in a distributed fashion for different purposes like that of wildlife tracing, detection of intruders, environment monitoring etc. Efficient multicast is necessary to scale down the exchange of messages for group communication in sensor networks. However incorporating security in wireless sensor group communication is a huge challenge because of the limited resource availability. In this paper, a decentralized rekeying mechanism based on Logical Key Hierarchy is proposed to secure the efficient Hierarchical Geographic Multicast Routing without affecting its actual performance. The proposed protocol, Secure Hierarchical Geographic Multicast Routing (SHGMR) meets all the security requirements necessary for any kind of secure group communication. This protocol is also efficient in terms of scalability as it uses only O(log2nmax) message transmissions, where nmax is the size of the largest subgroup among all the HGMR cells.\n\nTitle: ECS: An enhanced carrier sensing mechanism for wireless ad hoc networks\nContent: In wireless ad hoc networks, whenever a node overhears a frame, the node should defer its transmission to prevent the interference with the ongoing transmission. The exact duration value by which the node should defer is contained in the frame. However, due to the wireless transmission errors and due to the fact that the carrier sensing range is normally greater than the transmission range, a frame overheard by a node may not always be interpretable and thus the node cannot get the precise duration value by which the node should defer. Therefore, an important issue is that whenever a node detects an erroneous frame, how long the node should defer for. In the current IEEE 802.11 standards, the node will always defer the transmission by a fixed duration (represented by EIFS). We show that this duration is sometimes smaller and sometimes larger than the desired period by which the transmission should be deferred, and it leads to substantial unfairness and throughput degradation. We propose an enhanced carrier sensing (ECS) scheme, which distinguishes among the type of the erroneous frames based on their lengths and defers the transmission accordingly. Simulation results show that the ECS improves the fairness as well as the throughput substantially. To the best of our knowledge, this is the first work that focuses on the carrier sensing mechanism to improve the performance of IEEE 802.11.\n\nTitle: Energy-Efficient Data Gathering for Road-Side Sensor Networks Ensuring Reliability and Fault-Tolerance\nContent: Data gathering or converge cast is one of the most popular applications of road side sensor network where the data sensed from the road are accumulated in the road side gateways or sinks for traffic monitoring purpose. The required delay sensitivity and reliability of the application as well as the scarcity of sensor resources make the task challenging. In this paper, a novel tree based data gathering scheme has been proposed exploiting the strip like structure of the road network. Sensor nodes are distributed in several virtual blocks along the road and a converge cast tree is constructed selecting one active node from each block. Implementation of efficient scheduling assures both the coverage and critical power savings of sensor nodes. The network connectivity is guaranteed throughout by the proposed tree maintenance module that handles the sensor node joining and leaving events. Simulation results show that the tree maintenance overhead in terms of both delay and control message communication is nominal.\n\nTitle: A Tree-Based Local Repairing Approach for Increasing Lifetime of Query Driven WSN\nContent: Query driven Broadcast through wireless sensor nodes also leads to the domain of event driven converge cast. A query-response based application in Wireless Sensor Networks(WSN) demands the correct delivery of data message at each sensor node. A Breadth-First Search(BFS) tree rooted at the base station offers shortest path traversal for each data message which utilizes the sensor resources efficiently. Resource constrained sensor nodes are highly prone to sudden crash. So the application demands a quick and smart approach to repair the tree when a node dies. In this paper a novel scheme has been proposed to locally repair the tree with constant round of message transmissions. Each node piggybacks a few bytes of extra information along with each query and response messages. Based on these piggybacked values each node calculates its alternate parent. When a parent node fails, its children can contact their respective alternate parents immediately to establish an alternate path to the root. Reduced communication cost in terms of extra message transmissions saves battery power at each node. Efficient query-response message handler ensures the correct delivery of messages. Fast repairing offers good Quality of Service(QoS). Simulation result shows that no message is lost except the one holding by the crashed node.\n\n## Task\nCompose an abstract for the title 'CRP: Cluster head Re-election Protocol for heterogeneous wireless sensor networks.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Sensor nodes in a network consume energy in a non-uniform manner. Designing clustering protocols that are heterogeneity aware is still an open issue in sensor networks. This work focuses on hierarchically clustered heterogeneous sensor networks. Sensor nodes organize themselves in self organized groups called clusters. Each cluster consists of a cluster head and some member nodes. The sensor network consists of nodes of two different energy levels. We show that current heterogeneity aware protocols (such as SEP [1], DEEC [2] etc.) are unable to distribute the usage of energy amongst the sensor nodes uniformly. We propose CRP, Cluster Re-election Protocol, that is heterogeneity aware and enhances the network stability period (period before which the first node dies) over current protocols. We show, by simulation, that CRP improves network stability by reducing the variation in the residual energy of sensor nodes. Therefore. CRP is able to exploit the heterogeneity present in a sensor network in an efficient manner."
      }
    ]
  },
  {
    "tid":"48",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Large-scale, AST-based API-usage analysis of open-source Java projects\nContent: Research on API migration and language conversion can be informed by empirical data about API usage. For instance, such data may help with designing and defending mapping rules for API migration in terms of relevance and applicability. We describe an approach to large-scale API-usage analysis of open-source Java projects, which we also instantiate for the Source-Forge open-source repository in a certain way. Our approach covers checkout, building, tagging with metadata, fact extraction, analysis, and synthesis with a large degree of automation. Fact extraction relies on resolved (type-checked) ASTs. We describe a few examples of API-usage analysis; they are motivated by API migration. These examples are concerned with analysing API footprint (such as the numbers of distinct APIs used in a project), API coverage (such as the percentage of methods of an API used in a corpus), and framework-like vs. class-library-like usage.\n\nTitle: Recovering grammar relationships for the Java Language Specification\nContent: Grammar convergence is a method that helps in discovering relationships between different grammars of the same language or different language versions. The key element of the method is the operational, transformation-based representation of those relationships. Given input grammars for convergence, they are transformed until they are structurally equal. The transformations are composed from primitive operators; properties of these operators and the composed chains provide quantitative and qualitative insight into the relationships between the grammars at hand. We describe a refined method for grammar convergence, and we use it in a major study, where we recover the relationships between all the grammars that occur in the different versions of the Java Language Specification (JLS). The relationships are represented as grammar transformation chains that capture all accidental or intended differences between the JLS grammars. This method is mechanized and driven by nominal and structural differences between pairs of grammars that are subject to asymmetric, binary convergence steps. We present the underlying operator suite for grammar transformation in detail, and we illustrate the suite with many examples of transformations on the JLS grammars. We also describe the extraction effort, which was needed to make the JLS grammars amenable to automated processing. We include substantial metadata about the convergence process for the JLS so that the effort becomes reproducible and transparent.\n\nTitle: ICFP 2004: Scrap More Boilerplate: Reflection, Zips, and Generalised Casts\nContent: Writing boilerplate code is a royal pain. Generic programming promises to alleviate this pain by allowing the programmer to write a generic \\\"recipe\\\" for boilerplate code, and use that recipe in many places. In earlier work we introduced the \\\"Scrap your boilerplate\\\" approach to generic programming, which exploits Haskell's existing type-class mechanism to support generic transformations and queries. This paper completes the picture. We add a few extra \\\"introspective\\\" or \\\"reflective\\\" facilities, that together support a rich variety of serialisation and de-serialisation. We also show how to perform generic \\\"zips\\\", which at first appear to be somewhat tricky in our framework. Lastly, we generalise the ability to over-ride a generic function with a type-specific one. All of this can be supported in Haskell with independently-useful extensions: higher-rank types and type-safe cast. The GHC implementation of Haskell readily derives the required type classes for user-defined data types.\n\nTitle: Typed Generic Traversal With Term Rewriting Strategies\nContent: A typed model of strategic term rewriting is developed. The key innovation is that generic traversal is covered. To this end, we define a typed rewriting calculus S\u2032\u03b3. The calculus employs a many-sorted type system extended by designated generic strategy types \u03b3. We consider two generic strategy types, namely the types of type-preserving and type-unifying strategies. S\u2032\u03b3 offers traversal combinators to construct traversals or schemes thereof from many-sorted and generic strategies. The traversal combinators model different forms of one-step traversal, that is, they process the immediate subterms of a given term without anticipating any scheme of recursion into terms. To inhabit generic types, we need to add a fundamental combinator to lift a many-sorted strategy s to a generic type \u03b3. This step is called strategy extension. The semantics of the corresponding combinator states that s is only applied if the type of the term at hand fits, otherwise the extended strategy fails. This approach dictates that the semantics of strategy application must be type-dependent to a certain extent. Typed strategic term rewriting with coverage of generic term traversal is a simple but expressive model of generic programming. It has applications in program transformation and program analysis.\n\nTitle: Automated co-evolution of GMF editor models\nContent: The Eclipse Graphical Modeling (GMF) Framework provides the major approach for implementing visual languages on top of the Eclipse platform. GMF relies on a family of modeling languages to describe abstract syntax, concrete syntax as well as other aspects of the visual language and its implementation in an editor. GMF uses a model-driven approach to map the different GMF models to Java code. The framework, as it stands, lacks support for evolution. In particular, there is no support for propagating changes from the domain model (i.e., the abstract syntax of the visual language) to other editor models. We analyze the resulting co-evolution challenge, and we provide a solution by means of GMF model adapters, which automate the propagation of domain-model changes. These GMF model adapters are special model-to-model transformations that are driven by difference models for domain-model changes.\n\n## Task\nCompose an abstract for the title 'JavaGI: generalized interfaces for java' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"JavaGI is an experimental language that extends Java 1.5 by generalizing the interface concept to incorporate the essential features of Haskell's type classes. In particular, generalized interfaces cater for retroactive and constrained interface implementations, binary methods, static methods in interfaces, default implementations for interface methods, interfaces over families of types, and existential quantification for interface-bounded types. As a result, many anticipatory uses of design patterns such as Adapter, Factory, and Visitor become obsolete; several extension and integration problems can be solved more easily. JavaGI's interface capabilities interact with subtyping (and subclassing) in interesting ways that go beyond type classes. JavaGI can be translated to Java 1.5. Its formal type system is derived from Featherweight GJ."
      }
    ]
  },
  {
    "tid":"49",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Efficient routing in intermittently connected mobile networks: the single-copy case\nContent: Intermittently connected mobile networks are wireless networks where most of the time there does not exist a complete path from the source to the destination. There are many real networks that follow this model, for example, wildlife tracking sensor networks, military networks, vehicular ad hoc networks (VANETs), etc. In this context, conventional routing schemes would fail, because they try to establish complete end-to-end paths, before any data is sent. To deal with such networks researchers have suggested to use flooding-based routing schemes. While flooding-based schemes have a high probability of delivery, they waste a lot of energy and suffer from severe contention which can significantly degrade their performance. With this in mind, we look into a number of \"single-copy\" routing schemes that use only one copy per message, and hence significantly reduce the resource requirements of flooding-based algorithms. We perform a detailed exploration of the single-copy routing space in order to identify efficient single-copy solutions that (i) can be employed when low resource usage is critical, and (ii) can help improve the design of general routing schemes that use multiple copies. We also propose a theoretical framework that we use to analyze the performance of all single-copy schemes presented, and to derive upper and lower bounds on the delay of any scheme.\n\nTitle: Spray and wait: an efficient routing scheme for intermittently connected mobile networks\nContent: Intermittently connected mobile networks are sparse wireless networks where most of the time there does not exist a complete path from the source to the destination. These networks fall into the general category of Delay Tolerant Networks. There are many real networks that follow this paradigm, for example, wildlife tracking sensor networks, military networks, inter-planetary networks, etc. In this context, conventional routing schemes would fail.To deal with such networks researchers have suggested to use flooding-based routing schemes. While flooding-based schemes have a high probability of delivery, they waste a lot of energy and suffer from severe contention, which can significantly degrade their performance. Furthermore, proposed efforts to significantly reduce the overhead of flooding-based schemes have often be plagued by large delays. With this in mind, we introduce a new routing scheme, called Spray and Wait, that \"sprays\" a number of copies into the network, and then \"waits\" till one of these nodes meets the destination.Using theory and simulations we show that Spray and Wait outperforms all existing schemes with respect to both average message delivery delay and number of transmissions per message delivered; its overall performance is close to the optimal scheme. Furthermore, it is highly scalable retaining good performance under a large range of scenarios, unlike other schemes. Finally, it is simple to implement and to optimize in order to achieve given performance goals in practice.\n\nTitle: Single-Copy Routing in Intermittently Connected Mobile Networks\nContent: Intermittently connected mobile networks are wireless networks where most of the time there does not exist a complete path from source to destination, or such a path is highly unstable and may break soon after it has been discovered. In this context, conventional routing schemes would fail. To deal with such networks we propose the use of an opportunistic hop-by-hop routing model. According to the model, a series of independent, local forwarding decisions are made, based on current connectivity and predictions of future connectivity information diffused through nodes' mobility. The important issue here is how to choose an appropriate next hop. To this end, we propose and analyze via theory and simulations a number of routing algorithms. The champion algorithm turns out to be one that combines the simplicity of a simple random policy, which is efficient in finding good leads towards the destination, with the sophistication of utility-based policies that efficiently follow good leads. We also state and analyze the performance of an oracle-based optimal algorithm, and compare it to the online approaches. The metrics used in the comparison are the average message delivery delay and the number of transmissions per message delivered. I. INTRODUCTION\n\nTitle: Fundamental Mobility Properties for Realistic Performance Analysis of Intermittently Connected Mobile Networks\nContent: Traditional mobile ad hoc routing protocols fail to deliver any data in Intermittently Connected Mobile Ad Hoc Networks (ICMN's) because of the absence of complete end-to-end paths in these networks. To overcome this issue, researchers have proposed to use node mobility to carry data around the network. These schemes are referred to as mobility-assisted routing schemes. A mobility-assisted routing scheme forwards data only when appropriate relays meet each other. The time it takes for them to first meet each other is referred to as the meeting time. The time duration they remain in contact with each other is called the contact time. If they fail to exchange the packet during the contact time (due to contention in the network), then they have to wait till they meet each other again. This time duration is referred to as the inter meeting time. A realistic performance analysis of any mobility-assisted routing scheme requires a knowledge of the statistics of these three quantities. These quantities vary largely depending on the mobility model at hand. This paper studies these three quantities for the three most popularly used mobility models: random direction, random waypoint and random walk models. Hence, this work allows for a realistic performance analysis of any routing scheme under any of these three mobility models.\n\nTitle: Spray and Focus: Efficient Mobility-Assisted Routing for Heterogeneous and Correlated Mobility\nContent: Intermittently connected mobile networks are wireless networks where most of the time there does not exist a complete path from the source to the destination. There are many real networks that follow this model, for example, wildlife tracking sensor networks, military networks, vehicular ad hoc networks (VANETs), etc. To deal with such networks researchers have suggested to use controlled replication or \"spraying\" methods that can reduce the overhead of flooding-based schemes by distributing a small number of copies to only a few relays. These relays then \"look\" for the destination in parallel as they move into the network. Although such schemes can perform well in scenarios with high mobility (e.g. VANETs), they struggle in situations were mobility is slow and correlated in space and/or time. To route messages efficiently in such networks, we propose a scheme that also distributes a small number of copies to few relays. However, each relay can then forward its copy further using a single-copy utility-based scheme, instead of naively waiting to deliver it to the destination itself. This scheme exploits all the advantages of controlled replication, but is also able to identify appropriate forwarding opportunities that could deliver the message faster. Simulation results for traditional mobility models, as well as for a more realistic \"community-based\" model, indicate that our scheme can reduce the delay of existing spraying techniques up to 20 times in some scenarios.\n\n## Task\nCompose an abstract for the title 'Efficient Routing in Intermittently Connected Mobile Networks: The Multiple-Copy Case' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Intermittently connected mobile networks are wireless networks where most of the time there does not exist a complete path from the source to the destination. There are many real networks that follow this model, for example, wildlife tracking sensor networks, military networks, vehicular ad hoc networks, etc. In this context, conventional routing schemes fail, because they try to establish complete end-to-end paths, before any data is sent. To deal with such networks researchers have suggested to use flooding-based routing schemes. While flooding-based schemes have a high probability of delivery, they waste a lot of energy and suffer from severe contention which can significantly degrade their performance. Furthermore, proposed efforts to reduce the overhead of flooding-based schemes have often been plagued by large delays. With this in mind, we introduce a new family of routing schemes that \"spray\" a few message copies into the network, and then route each copy independently towards the destination. We show that, if carefully designed, spray routing not only performs significantly fewer transmissions per message, but also has lower average delivery delays than existing schemes; furthermore, it is highly scalable and retains good performance under a large range of scenarios. Finally, we use our theoretical framework proposed in our 2004 paper to analyze the performance of spray routing. We also use this theory to show how to choose the number of copies to be sprayed and how to optimally distribute these copies to relays."
      }
    ]
  },
  {
    "tid":"50",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Coupling Transparency and Visibility: a Translucent Middleware Approach for Positioning System Integration and Management (PoSIM)\nContent: The diffusion of wireless terminals with multiple communication interfaces and the proliferation of heterogeneous positioning techniques opens new possibilities for Location Based Services (LBSs), even if potentially complicating their development In particular, LBSs can significantly benefit from middleware supports to uniformly access the set of positioning systems available at their wireless clients and to dynamically choose the most suitable one depending on application/execution context, also by fusing concurrent positioning data from multiple sources. The paper presents our PoSIM middleware for the integrated and synergic access/control of heterogeneous positioning systems in a highly flexible way. PoSIM provides LBSs with two differentiated levels of visibility for positioning management: a transparent way based on high-level context-based policies, and a fully-aware access to advanced functions/configurations, mediated and uniformed by the middleware independently of the underlying positioning solution. In particular, the paper mainly concentrates on the description of the PoSIM architecture and of its API, by pointing out our primary design and implementation choices.\n\nTitle: Context-Aware Middleware for Reliable Multi-hop Multi-path Connectivity\nContent: The widespread diffusion of portable devices with multiple wireless interfaces, e.g., UMTS/GPRS, IEEE 802.11, and/or Bluetooth, is enabling multi-homing and multi-channel scenarios, possibly made up by multi-hop cooperative paths towards the traditional Internet infrastructure. There is the need for novel middleware supports, aware of innovative context information, to select and dynamically re-configure the most suitable interfaces and connectivity providers for each client application. In particular, novel middlewares should effectively exploit concise and lightweight context indicators about expected node mobility, path throughput, and energy availability to take proper connectivity management decisions at session startup and to promptly re-configure them with limited overhead at runtime. Here, we present how our MMHC middleware originally uses mobility/throughput/energy context to manage connectivity opportunities effectively, i) by filtering out connectivity opportunities that are considered insufficiently reliable, and ii) by carefully evaluating the residual candidates in two distinguished local/global management phases to achieve the most suitable tradeoff between promptness and management costs.\n\nTitle: Context-aware middleware for resource management in the wireless Internet\nContent: The provisioning of Web services over the wireless Internet introduces novel challenging issues for service design and implementation: from user/terminal mobility during service execution, to wide heterogeneity of portable access devices and unpredictable modifications in accessible resources. In this scenario, there are frequent provision-time changes in the context, defined as the logical set of accessible resources depending on client location, access terminal capabilities, and system/service management policies. The development of context-dependent services requires novel middlewares with full context visibility. We propose a middleware for context-aware resource management, called CARMEN, capable of supporting the automatic reconfiguration of wireless Internet services in response to context changes without any intervention on the service logic. CARMEN determines the context on the basis of metadata, which include declarative management policies and profiles for user preferences, terminal capabilities, and resource characteristics. In addition, CARMEN exploits the mobile agent technology to implement mobile middleware components that follow the provision-time movement of clients to support locally their customized service access. The proposed middleware shows how metadata and mobile agents can favor component reusability and automatic service reconfiguration, by reducing the development/ deployment complexity.\n\nTitle: Context-Aware Multimedia Middleware Solutions for Counteracting IEEE 802.11 Performance Anomaly\nContent: Multimedia provisioning over Wi-Fi networks is a complex task that should consider also novel issues stemming from the specific behavior of IEEE 802.11 protocols. In particular, we claim the need for visibility of context data about the IEEE 802.11 performance anomaly, i.e., the situation where even a single node located at the borders of the coverage area of a Wi-Fi access point produces a relevant degradation in the connectivity quality of all other nodes in the area. The paper proposes a novel application-level middleware that counteracts IEEE 802.11 anomaly without imposing any modification in standard Wi-Fi protocols, thus permitting to maintain the current wide base of installed equipment. Our middleware portably detects anomaly situations via decentralized standard mechanisms available at clients; anomaly awareness is used to promptly react with application-level management operations (flow quality downscaling and traffic shaping) that both preserve the goodput at nodes in well-covered areas and minimize quality degradations at clients generating the anomaly. The reported experimental results point out the feasibility of application-level middleware approaches also in the challenging multimedia area.\n\nTitle: COSMOS: A Context-Centric Access Control Middleware for Mobile Environments\nContent: User/terminal mobility during service provisioning and high heterogeneity of wireless portable devices identify novel challenges for service delivery in ubiquitous pervasive environments. An emerging architecture solution in the wireless Internet is to have middleware components (mobile proxies) over the fixed network that follow the movements and act on behalf of the limited wireless clients. It is crucial that mobile proxies have full visibility of their context, i.e., the set of available and relevant resources depending on access control rules, client location, user preferences, privacy requirements, terminal characteristics, and current state of hosting environments. The paper presents the design and implementation of a context-centric access control middleware, called COSMOS, for the wireless Internet. COSMOS dynamically determines the contexts of mobile proxies, and effectively rules the access to them, by taking into account different types of metadata (user profiles and system/user-level authorization policies), expressed at a high level of abstraction and cleanly separated from the service logic. The paper also shows how COSMOS facilitates the development of articulated access control strategies in the case study of a context-dependent movie-info service deployed over IEEE 802.11 network localities.\n\n## Task\nCompose an abstract for the title 'The PoSIM middleware for translucent and context-aware integrated management of heterogeneous positioning systems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The widespread availability of devices with multiple wireless interfaces and of heterogeneous positioning techniques open new market potentials for Location Based Services (LBSs), even if complicating their development. The paper claims the need for novel middlewares to manage dynamically retrieved client-side positioning systems in a synergic way and depending on context, i.e., LBS requirements, user preferences, device characteristics, and overall system state. To that purpose, we have designed and implemented PoSIM, a context-aware middleware for the synergic exploitation and control of heterogeneous positioning systems that facilitates the development and portability of LBSs. PoSIM is translucent, i.e., can provide LBS developers with differentiated visibility of data characteristics and control possibilities, thus dynamically adapting to application-specific deployment requirements and enabling cross-layer management decisions. The paper describes the translucent PoSIM architecture, some primary implementation insights, and how to practically use it to simplify LBS development via events, filters, and policies."
      }
    ]
  },
  {
    "tid":"51",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A High-Level Signal Integrity Fault Model and Test Methodology for Long On-Chip Interconnections\nContent: In this paper, considering the interconnection topology information, an abstract model and a new test pattern generation method of signal integrity problems on interconnects are proposed. In addition, previous SPICE-based pattern generation methods are too complex and time consuming to generate test patterns for signal integrity faults. To more accurately detect signal integrity defects on practical on-chip interconnection lines and avoid time consuming for interconnection analysis, in this paper, we propose a new high-level signal integrity fault model to estimate noise effects based on process variation and interconnect signal transition. Experimental results show that the proposed signal integrity fault model is more exact for long interconnects than previous approaches. In addition, the proposed method is much faster than the SPICE-based pattern generation method.\n\nTitle: A new maximal diagnosis algorithm for interconnect test\nContent: Interconnect test for highly integrated environments becomes more important in terms of its test time and a complete diagnosis, as the complexity of the circuit increases. Since the board-level interconnect test is based on boundary scan technology, it takes a long test time to apply test vectors serially through a long scan chain. Complete diagnosis is another important issue. Since the board-level test is performed for repair, noticing the faulty position is an essential element of any interconnect test. Generally, the interconnect test algorithms that need a short test time cannot perform the complete diagnosis and the algorithms that perform the complete diagnosis need a lengthy test time. To overcome this problem, a new interconnect test algorithm is developed. The new algorithm can provide the complete diagnosis of all faults with a shorter test time compared to the previous algorithms.\n\nTitle: A Novel Massively Parallel Testing Method Using Multi-Root for High Reliability\nContent: Wafer testing (wafer sort) is used in the semiconductor industry to reduce test costs. High parallelism is important to reduce test application time. However, increasing parallelism is becoming more difficult because the elements that drive test costs are increases in the pin count of the system on chip (SOC), and required automated test equipment (ATE) channels. While the need for parallelism has been growing, a reliability problem in which fault distribution causes good devices under test (DUT) to be improperly tested is becoming a concern. To achieve high parallelism and reliability, we propose a novel, massively parallel testing method using multi-root. In addition, we develop a test setup for setting the root-DUT location and the path of all DUTs. Using the proposed wafer testing method, test data can be transferred between the ATE and dies using multi-roots. The experimental results using ITC 02 SOC benchmarks show that the proposed method can reduce test costs up to 90%, and achieve nearly 94.84% test reliability without affecting yield.\n\nTitle: A TSV test structure for simultaneously detecting resistive open and bridge defects in 3D-ICs\nContent: After the 3D stacking process, TSV-based 3D-ICs are required to perform the post-bond testing in order to detect TSV faults or device functional defects. To detect the resistive open and bridge defects, various effective TSV testing techniques have been studied. At an early stage of TSV manufacturing, it is important to consider that the TSV testing is required not only determining whether each TSV is defective or non-defective, but also digitizing the fault degree into the TSV resistance value during the silicon debugging. In this paper, we propose a new TSV test structure for simultaneously detecting the resistive open and bridge defects with supporting the debug mode to analysis the characteristic of the specific TSV. It can highly reduce the test time by detecting TSV defects at the same time without compromising test quality.\n\nTitle: Interleaving Test Algorithm for Subthreshold Leakage-Current Defects in DRAM Considering the Equal Bit Line Stress\nContent: Since the minimum feature size of dynamic RAM has been down-scaled, several studies have been carried out to determine ways to protect cell data from leakage current in many areas. In the field of testing, more appropriate test algorithms are required to detect weak cells with leakage-current sources. In this paper, we propose an interleaving test algorithm that takes into account the equal bit-line stress regardless of the cell location. The proposed test algorithm allows screening of weak cells that cannot hold cell data due to the subthreshold leakage current. During the stress period, the algorithm can also detect other leakage currents. This paper presents the maximum stress differences according to the cell location, and determines the influence of the refresh operation on the maximum stress time. Therefore, this paper suggests a correlation between the refresh and read time to give maximum stress time.\n\n## Task\nCompose an abstract for the title 'MDSI: Signal Integrity Interconnect Fault Modeling and Testing for SoCs' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Unacceptable loss of signal integrity may cause permanent or intermittent harm to the functionality and performance of SoCs. In this paper, we present an abstract model and a new test pattern generation method of signal integrity problems on interconnects. This approach is achieved by considering the effects for testing inputs and parasitic RLC elements of interconnects. We also develop a framework to deal with arbitrary interconnection topology. Experimental results show that the proposed signal integrity fault model is more exact and more powerful for long interconnects than previous approaches."
      }
    ]
  },
  {
    "tid":"52",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Enhancement of residual echo for improved acoustic echo cancellation\nContent: This paper investigates the use of a signal enhancement technique, namely a noise suppressing nonlinearity, on the adaptive filter error in order to increase the stability and the performance of acoustic echo cancellation (AEC) when there is a continuous distortion to the acoustic echo signal. The algorithm presented here differs from others in that the enhancement of signal is done in the adaptation loop, rather than as a post-processing technique for further reduction of residual echo in the signal, and that the resulting nonlinearity for the cancellation error is formulated as a solution to the signal enhancement problem. Combining the nonlinear error suppression method with NLMS and other adaptive step-size algorithms based on NLMS shows an improvement of between 5 to 15 dB in the average ERLE for additive white noise and around 2 dB for speech coding distortion when a simulated acoustic echo is used. The reduction of the misalignment of 5 dB or more for both noise cases can be expected. The technique is shown to be beneficial also with a real acoustic echo. The new method is seen as a viable technique for improving the existing AEC algorithms when the acoustic echo is corrupted by linear distortion in the form of additive noise or by nonlinear distortion in the form of speech coding.\n\nTitle: Towards robust acoustic echo cancellation during double-talk and near-end background noise via enhancement of residual echo\nContent: This paper examines the technique of using a noise suppressing nonlinearity in the adaptive filter error feedback loop of the acoustic echo canceler (AEC) based on the least mean square (LMS) algorithm when there are both double-talk and white background noise at the near-end. By combining the previously introduced noise suppressing technique with a compressive nonlinearity derived from the theory of robust statistics, consistently better results are obtained during double-talk as well as during single-talk when compared to the traditional approach of using only the compressive nonlinearity. It is shown that a compressive form of noise reducing nonlinearity can be derived also from the signal enhancement point of view when the noise probability density (pdf) is tailed more heavily and has a higher kurtosis than the Gaussian pdf. A combination of such a noise compressing nonlinearity and a noise suppressing nonlinearity is capable of producing results that are similar to that of the robust statistics approach during double-talk along with an added benefit of increased robustness during single-talk when there is only the background noise.\n\nTitle: Acoustic echo cancellation based on independent component analysis and integrated residual echo enhancement\nContent: This paper examines the technique of using a memoryless noise-suppressing nonlinearity in the adaptive filter error feedback-loop of an acoustic echo canceler (AEC) based on normalized least-mean square (NLMS) when there is an additive noise at the near-end. It will be shown that introducing the nonlinearity to ldquoenhancerdquo the filter estimation error is well-founded in the information-theoretic sense and has a deep connection to the independent component analysis (ICA). The paradigm of AEC as a problem that can be approached by ICA leads to new algorithmic possibilities beyond the conventional LMS family of techniques. In particular, a right combination of the error enhancement procedure and a properly implemented regularization procedure enables the AEC to be performed recursively and continuously in the frequency domain when there are both ambient noise and double-talk even without the double-talk detection (DTD) or the voice activity detection (VAD) procedure.\n\nTitle: On the performance of the robust acoustic echo cancellation system with decorrelation by sub-band resampling\nContent: This paper examines the effect of inter-channel decorrelation by sub-band resampling (SBR) on the performance of the robust acoustic echo cancellation (AEC) system based on the residual echo enhancement technique. Due to the flexibility of SBR, the decorrelation performance as measured by the coherence can be matched with other conventional decorrelation procedures. Given the same degree of decorrelation, we have shown previously that SBR achieves superior audio quality compared to other procedures. We show in this paper that SBR also provides higher stereophonic AEC performance in a very noisy condition, where the performance is evaluated by decomposing the true echo return loss enhancement and the misalignment per sub-band to better demonstrate the superiority of our decorrelation procedure over other methods.\n\nTitle: A system approach to residual echo suppression in robust hands-free teleconferencing\nContent: This paper presents a system approach to the residual echo suppression (RES) problem in a noisy acoustic environment. We propose a method that takes advantage of our existing robust acoustic echo cancellation system in order to obtain a residual echo estimate that closely resembles the true, noise-free residual echo. To achieve improved RES during strong near-end interference (e.g., double talk), a psychoacoustic postfilter is also used. The simulation results show that our RES based on the system approach outperforms a conventional estimation method. Comparing the postfiltered output to the unprocessed one indicates that our proposed RES approach can raise the PESQ score by more than half a point.\n\n## Task\nCompose an abstract for the title 'Enhancement of Residual Echo for Robust Acoustic Echo Cancellation' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper examines the technique of using a noise-suppressing nonlinearity in the adaptive filter error feedback-loop of an acoustic echo canceler (AEC) based on the least mean square (LMS) algorithm when there is an interference at the near end. The source of distortion may be linear, such as local speech or background noise, or nonlinear due to speech coding used in the telecommunication networks. Detailed derivation of the error recovery nonlinearity (ERN), which \u201cenhances\u201d the filter estimation error prior to the adaptation in order to assist the linear adaptation process, will be provided. Connections to other existing AEC and signal enhancement techniques will be revealed. In particular, the error enhancement technique is well-founded in the information-theoretic sense and has strong ties to independent component analysis (ICA), which is the basis for blind source separation (BSS) that permits unsupervised adaptation in the presence of multiple interfering signals. The single-channel AEC problem can be viewed as a special case of semi-blind source separation (SBSS) where one of the source signals is partially known, i.e., the far-end microphone signal that generates the near-end acoustic echo. The system approach to robust AEC will be motivated, where a proper integration of the LMS algorithm with the ERN into the AEC \u201csystem\u201d allows for continuous and stable adaptation even during double talk without precise estimation of the signal statistics. The error enhancement paradigm encompasses many traditional signal enhancement techniques and opens up an entirely new avenue for solving the AEC problem in a real-world setting."
      }
    ]
  },
  {
    "tid":"53",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Supporting Fault-Tolerance in Presence of In-Situ Analytics.\nContent: In-situ analytics have recently emerged as a promising approach to reduce I/O, network, and storage congestion for scientific data analysis. At the same time, Mean Time To Failure (MTTF) has also continuously decreased for HPC clusters, making failures during the execution of a simulation more likely. It is difficult to apply Checkpointing and Restart (C/R) to a simulation executed with in-situ analytics, because we not only need to store analytic and simulation states, but also need to ensure consistency between them. To solve the above problem, we present a novel solution to apply C/R approach. The solution exploits a common property of analytics applications, which is that they fit the reduction-style processing pattern. Based on the observation that the global reduction result is correct if all local reduction objects have been processed, only global reduction objects and progress of the reduction procedure is checkpointed, together with an application-level checkpoint of the simulation. We also discuss the specific details for handling time-sharing and space sharing modes, which are suitable for multi-core and many-core environments, respectively. We have evaluated our approach in the context of Smart middleware for developing in-situ analytics, which provides a Map-Reduce like API. We demonstrate the low overheads of our approach with different scientific simulations and analytics tasks.\n\nTitle: Supporting fault-tolerance in streaming grid applications\nContent: This paper considers the problem of supporting and efciently implementing fault-tolerance for tightly-coupled and pipelined applications, especially streaming applications, in a grid environment. We provide an alternative to basic checkpointing and use the notion of Light-weight Summary Structure(LSS) to enable efcient failure-recovery. The idea behind LSS is that at certain points during the execution of a processing stage, the state of the program can be summarized by a small amount of memory. This allows us to store copies of LSS for enabling failure-recovery, which causes low overhead fault-tolerance. Our work can be viewed as an optimization and adaptation of the idea of application-level checkpointing to a different execution environment, and for a different class of applications. Our implementation and evaluation of LSS based failure-recovery has been in the context of the GATES (Grid- based AdapTive Execution on Streams) middleware. An observation we use for providing very low overhead support for fault-tolerance is that algorithms analyzing data streams are only allowed to take a single pass over data, which means they only perform approximate processing. Therefore, we believe that in supporting fault-tolerant execution for these applications, it is acceptable to not analyze a small number of packets of data during failure-recovery. We show how we perform failure-recovery and also demonstrate how we could use additional buffers to limit data loss during the recovery procedure. We also present an efcient algorithm for allocating a new computation resource for failure-recovery at runtime. We have extensively evaluated our implementation using three stream data processing applications, and shown that the use of LSS allows effective and low-overhead failure-recovery.\n\nTitle: Supporting fault-tolerance for time-critical events in distributed environments\nContent: In this paper, we consider the problem of supporting fault tolerance for adaptive and time-critical applications in heterogeneous and unreliable grid computing environments. Our goal for this class of applications is to optimize a user-specified benefit function while meeting the time deadline. Our first contribution in this paper is a multi-objective optimization algorithm for scheduling the application onto the most efficient and reliable resources. In this way, the processing can achieve the maximum benefit while also maximizing the success-rate, which is the probability of finishing execution without failures. However, for the cases where failures do occur, we have developed a hybrid failure-recovery scheme to ensure that the application can complete within the pre-specified time interval. Our experimental results show that our scheduling algorithm can achieve better benefit when compared to several heuristics-based greedy scheduling algorithms, while still having a negligible overhead. Benefit is further improved when we apply the hybrid failure recovery scheme, and the success-rate becomes 100%.\n\nTitle: Indexing and Parallel Query Processing Support for Visualizing Climate Datasets\nContent: With increasing emphasis on analysis of large-scale scientific data, and with growing dataset sizes, a number of new challenges are arising. Particularly, novel data management solutions are needed, which can work together with the existing tools. This paper examines indexing support for supporting high-level queries (primarily those for sub setting) on array-based scientific datasets. This work is motivated by the limitations arising in visualizing climate datasets (stored in Net CDF), using tools like Para View. We have developed a new indexing strategy, which can help support a variety of sub setting queries over these datasets, including those requiring sub setting over dimensions/coordinates and those involving variable values. Our approach is based on bitmaps, but involves use of two-level indices and careful partitioning, based on query profiles. We also show how our indexing support can be used for sub setting operations executed in parallel. We compare our solutions against a number of other solutions, and demonstrate that our method is more effective.\n\nTitle: A General Interprocedural Framework for Placement of Split-Phase Large Latency Operations\nContent: Overlapping split-phase large latency operations with computations is a standard technique for improving performance on modern architectures. In this paper, we present a general interprocedural technique for overlapping such accesses with computation. We have developed an Interprocedural Balanced Code Placement (IBCP) framework, which performs analysis on arbitrary recursive procedures and arbitrary control flow and replaces synchronous operations with a balanced pair of asynchronous operations. We have evaluated this scheme in the context of overlapping I/O operations with computation. We demonstrate how this analysis is useful for applications which perform frequent and large accesses to disks, including applications which snapshot or checkpoint their computations or out-of-core applications.\n\n## Task\nCompose an abstract for the title 'A Fault-Tolerant Environment For Large-Scale Query Processing' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"As datasets are increasing in size, the data management and processing needs are being met with added parallelism, i.e, by involving more nodes and/or cores in the system. This, in turn, is increasing the chances of failures during processing. In this paper, we present the design and implementation of a fault-tolerant environment for processing queries on large scientific dataset. Our systems meet the following three requirements that we consider essential for any such environment: 1) high efficiency of execution of a particular data analysis task or query, when there are no failures, 2) ability to handle failure of up to a certain number of nodes, and 3) only a modest slowdown in processing times of data analysis task or a query when there are failures. We address these challenges by developing a new data replication scheme, which we refer to as subchunk or subpartition replication. Our system currently supports two types of queries: range queries on spatial data and aggregation queries on point datasets, but the underlying ideas can be extended to other query types as well. Our extensive evaluation shows that we can handle single and rack failures with only modest slowdowns, and particularly, clearly outperform the traditional (chunk or partition replication) schemes."
      }
    ]
  },
  {
    "tid":"54",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Research on Grid-Based Traffic Simulation Platform\nContent: With the improvement of the traffic complexity extent, the solution of traffic problems becomes more and more difficult in the reality, so traffic simulation is an effective method for analyzing the traffic states and problems. As the simulation area becomes larger and more complicate, which requires large computing and storage resources, it can't be resolved by the traditional computing technology. Due to grid technology's advantages on such aspects, a new simulation architecture named GHA, which implements the HLA 's component as grid service and combined with the agent technology is presented in this paper. Thus the simulation architecture can offer high-capable computing resources to solve the complex traffic issues with great expansibility and flexibility; moreover, it also makes solid foundation to the authenticity of traffic simulation by adopting agent model the character of traffic entity. At last, a traffic simulation platform is implemented based on GHA and the performance tests are given\n\nTitle: Research on modeling of complicate traffic simulation system\nContent: With the increasing of traffic complexity, traffic simulation becomes an important approach to deal with the complicated traffic problems; meanwhile, system modeling plays a more and more important role in the simulation systems. Cellular automata provide a simple discrete deterministic mathematical model for physical, biological, and computational systems and are shown to be capable of complicated behavior and generate complex and random patterns, which are very suitable for the description of complicate traffic environment [7]. A simulation model based on agent technology, HLA/RTI technology and expanded cellular automaton is presented in this paper. The simulation model makes the platform expandable and flexible, at the same time, it can provide high-capable computing resources to solve the complex traffic issues. In the traffic entity model aspects, the expanded cellular automata and agent technology were adopted to model the behaviors of passengers, vehicles, traffic signal lights and so on. The optimal scheme for evacuation of traffic disaster, superintendence of large scale activities and design of traffic environment will be obtained through the simulation model.\n\nTitle: Predicting Multi-step Citywide Passenger Demands Using Attention-based Neural Networks.\nContent: Predicting passenger pickup/dropoff demands based on historical mobility trips has been of great importance towards better vehicle distribution for the emerging mobility-on-demand (MOD) services. Prior works focused on predicting next-step passenger demands at selected locations or hotspots. However, we argue that multi-step citywide passenger demands encapsulate both time-varying demand trends and global statuses, and hence are more beneficial to avoiding demand-service mismatching and developing effective vehicle distribution/scheduling strategies. In this paper, we propose an end-to-end deep neural network solution to the prediction task. We employ the encoder-decoder framework based on convolutional and ConvLSTM units to identify complex features that capture spatiotemporal influences and pickup-dropoff interactions on citywide passenger demands. A novel attention model is incorporated to emphasize the effects of latent citywide mobility regularities. We evaluate our proposed method using real-word mobility trips (taxis and bikes) and the experimental results show that our method achieves higher prediction accuracy than the adaptations of the state-of-the-art approaches.\n\n\n\nTitle: A Context-aware Modeling Framework for Pervasive Applications\nContent: It is a challenge to recommend Web services under multiple contexts. To address this challenge, we propose a context-aware collaborative filtering (CaCF) approach for service recommendation. Three types of contextual information, i.e. time, location and interest of user, are considered. In this approach, users' interests are extracted from service invocation records and represented as term-weight vectors. Neighbors are chosen according to the Cosine similarities of these vectors. Then, neighbors are filtered into close neighbors by location and time. At last, these close neighbors recommend service to a target user. We evaluate our method through comparing with other service recommendation approaches. The experimental results show that it achieves better precision and satisfaction rate than other two methods.\n\nTitle: Architecture Reconfiguration in Pervasive Environment: An AADL-based Approach\nContent: In the pervasive environment, in order to adapt to dynamic environments and changeable requirements, the architecture reconfiguration has drawn great attention, which includes structural, behavioral, and configuration aspects. Unfortunately, traditional architecture description methods are always static, and as such fail to completely specify the reconfiguration. In this paper, AADL (Architecture Analysis and Design Language) is applied to describe the dynamic architecture. First, with features of AADL, the architecture is specified from a structural, a behavioral and a configuration view. Second, the behavioral specification is applied to trigger the mode transition to implement the architecture reconfiguration. Moreover, the reconfiguration improves the architecture such that it satisfies the reliability property requirement. The application on a smart treatment room in the special education is given out as a running example to illustrate the feasibility of our approach.\n\n## Task\nCompose an abstract for the title 'Towards Building an Intelligent Traffic Simulation Platform' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Traffic congestion has become a major concern for many cities throughout the world. Simulations provide useful tools for engineer to plan traffic systems and government to make decisions. The microscopic traffic simulation approach defines the behavior of each interactive object in the traffic network, such as a vehicle and a traffic light through an individual model so that we can observe the detail traffic information of a scenario. Since microscopic model requires large computational power and data storage power, new simulation system architecture is needed. In the paper, the background of this research work is introduced. The simulation platform which combines grid, agent and HLA is given. Some considerations about microscopic traffic simulation models and technical issues of this platform are also discussed."
      }
    ]
  },
  {
    "tid":"55",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Allocating spatially distributed tasks in large, dynamic robot teams\nContent: For an interesting class of emerging applications, a large robot team will need to distributedly allocate many more tasks than there are robots, with dynamically appearing tasks and a limited ability to communicate. The LA-DCOP algorithm can conceptually handle both large-scale problems and multiple tasks per robot, but has key limitations when allocating spatially distributed tasks. In this paper, we extend LA-DCOP with several alternative acceptance rules for robots to determine whether to take on an additional task, given the interaction with the tasks it has already committed to. We show that these acceptance rules dramatically outperform a naive LA-DCOP implementation. In addition, we developed a technique that lets the robots use completely local knowledge to adjust their task acceptance criteria to get the best possible performance at a given communication bandwidth level.\n\nTitle: Interacting with team oriented plans in multi-robot systems.\nContent: Team oriented plans have become a popular tool for operators to control teams of autonomous robots to pursue complex objectives in complex environments. Such plans allow an operator to specify high level directives and allow the team to autonomously determine how to implement such directives. However, the operators will often want to interrupt the activities of individual team members to deal with particular situations, such as a danger to a robot that the robot team cannot perceive. Previously, after such interrupts, the operator would usually need to restart the team plan to ensure its success. In this paper, we present an approach to encoding how interrupts can be smoothly handled within a team plan. Building on a team plan formalism that uses Colored Petri Nets, we describe a mechanism that allows a range of interrupts to be handled smoothly, allowing the team to efficiently continue with its task after the operator intervention. We validate the approach with an application of robotic watercraft and show improved overall efficiency. In particular, we consider a situation where several platforms should travel through a set of pre-specified locations, and we identify three specific cases that require the operator to interrupt the plan execution: (i) a boat must be pulled out; (ii) all boats should stop the plan and move to a pre-specified assembly position; (iii) a set of boats must synchronize to traverse a dangerous area one after the other. Our experiments show that the use of our interrupt mechanism decreases the time to complete the plan (up to 48 % reduction) and decreases the operator load (up to 80 % reduction in number of user actions). Moreover, we performed experiments with real robotic platforms to validate the applicability of our mechanism in the actual deployment of robotic watercraft.\n\nTitle: Scalable target detection for large robot teams\nContent: In this paper, we present an asynchronous display method, coined image queue, which allows operators to search through a large amount of data gathered by autonomous robot teams. We discuss and investigate the advantages of an asynchronous display for foraging tasks with emphasis on Urban Search and Rescue. The image queue approach mines video data to present the operator with a relevant and comprehensive view of the environment in order to identify targets of interest such as injured victims. It fills the gap for comprehensive and scalable displays to obtain a network-centric perspective for UGVs. We compared the image queue to a traditional synchronous display with live video feeds and found that the image queue reduces errors and operator's workload. Furthermore, it disentangles target detection from concurrent system operations and enables a call center approach to target detection. With such an approach we can scale up to very large multi-robot systems gathering huge amounts of data that is then distributed to multiple operators.\n\nTitle: A prototype infrastructure for distributed robot-agent-person teams\nContent: Effective coordination of robots, agents and people promises to improve the safety, robustness and quality with which shared goals are achieved by harnessing the highly heterogeneous entities' diverse capabilities. Proxy-based integration architectures are emerging as a standard method for coordinating teams of heterogeneous entities. Such architectures are designed to meet imposing challenges such as ensuring that the diverse capabilities of the group members are effectively utilized, avoiding miscoordination in a noisy, uncertain environment and reacting flexibly to changes in the environment. However, we contend that previous architectures have gone too far in taking coordination responsibility away from entities and giving it to proxies. Our goal is to create a proxy-based integration infrastructure where there is a beneficial symbiotic relationship between the proxies and the team members. By leveraging the coordination abilities of both proxies and socially capable team members the quality of the coordination can be improved. We present two key new ideas to achieve this goal. First, coordination tasks are represented as explicit roles, hence the responsibilities not the actions are specified, thus allowing the team to leverage the coordination skills of the most capable team members. Second, building on the first idea, we have developed a novel role allocation and reallocation algorithm. These ideas have been realized in a prototype software proxy architecture and used to create heterogeneous teams for an urban disaster recovery domain. Using the rescue domain as a testbed, we have experimented with the role allocation algorithm and observed results to support the hypothesis that leveraging the coordination capabilities of people can help the performance of the team.\n\nTitle: Decentralized prioritized planning in large multirobot teams\nContent: In this paper, we address the problem of distributed path planning for large teams of hundreds of robots in constrained environments. We introduce two distributed prioritized planning algorithms: an efficient, complete method which is shown to converge to the centralized prioritized planner solution, and a sparse method in which robots discover collisions probabilistically. Planning is divided into a number of iterations, during which every robot simultaneously and independently computes a planning solution based on other robots' path information from the previous iteration. Paths are exchanged in ways that exploit the cooperative nature of the team and a statistical phenomenon known as the \u201cbirthday paradox\u201d. Performance is measured in simulated 2D environments with teams of up to 240 robots. We find that in moderately constrained environments, these methods generate solutions of similar quality to a centralized prioritized planner, but display interesting communication and planning time characteristics.\n\n## Task\nCompose an abstract for the title 'Real-world testing of a multi-robot team' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Multi-robot systems (MRS) have received a great deal of attention recently due to their potential to address complex distributed tasks such as environmental monitoring, search and rescue, agriculture, and security [3, 4, 5, 1, 2]. One specific type of multi-robot system that has significant near term promise is fleets of autonomous watercraft for applications such as flood response, water monitoring and bathymetry. Small watercraft are an attractive option for real world multi-robot systems because some of the most critical robotic problems are minimized on water - movement is relatively simple and dangers are relatively low."
      }
    ]
  },
  {
    "tid":"56",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Energy-efficient topology control for three-dimensional sensor networks\nContent: Topology control in sensor networks has been heavily studied recently. Different geometric topologies were proposed to be the underlying network topologies to achieve the sparseness of the communication networks or to guarantee the package delivery of specific routing methods. However, most of the proposed topology control algorithms were only applied to Two-Dimensional (2D) networks where all sensor nodes are distributed in a 2D plane. In practice, the sensor networks are often deployed in 3D space, such as sensor nodes in a forest. This paper seeks to investigate efficient topology control protocols for 3D sensor networks. In our new protocols, we extend several 2D geometric topologies to 3D case, and propose some new 3D Yao-based topologies for sensor networks. We also prove several properties (e.g. bounded degree and constant power stretch factor) for them in 3D space. The simulation results confirm our theoretical proofs for these proposed 3D topologies.\n\nTitle: Distributed Multi-Actuator Control for Workload Balancing in Wireless Sensor and Actuator Networks\nContent: This technical note presents strategies for systematic ways to control mobile actuators in distributed wireless sensor and actuator networks. The efficiency, responsiveness, and service lifetime of the mobile actuators depend on the allocation of the workload to each individual actuator. Starting from an initial deployment, the proposed distributed multi-actuator control algorithm computes the load in each partition and uses the load imbalance as a virtual force to dynamically move the actuators until a balanced distribution is achieved. The load in each partition for an individual actuator includes both the total travel cost to visit or contact all sensors in the partition and the cost of servicing them. Finding the minimum workload allocation to serve each partition is an NP-hard problem. Therefore, efficient heuristics are proposed to do the partition, plan the path of each actuator and calculate its workload. Simulation results demonstrate the effectiveness of the proposed strategies.\n\nTitle: Three-dimensional greedy routing in large-scale random wireless sensor networks\nContent: In this paper, we investigate how to design greedy routing to achieve sustainable and scalable in a large-scale three-dimensional (3D) sensor network. Several 3D position-based routing protocols were proposed to seek either delivery guarantee or energy-efficiency in 3D wireless networks. However, recent results [1,2] showed that there is no deterministic localized routing algorithm that guarantees either delivery of packets or energy-efficiency of its routes in 3D networks. In this paper, we focus on design of 3D greedy routing protocols which can guarantee delivery of packets and/or energy-efficiency of their paths with high probability in a randomly deployed 3D sensor network. In particular, we first study the asymptotic critical transmission radius for 3D greedy routing to ensure the packet delivery in large-scale random 3D sensor networks, then propose a refined 3D greedy routing protocol to achieve energy-efficiency of its paths with high probability. We also conduct extensive simulations to confirm our theoretical results.\n\nTitle: Efficient Fault Tolerant Topology Control for Three-Dimensional Wireless Networks\nContent: Fault tolerant topology control in wireless networks has been studied recently. In order to achieve both sparseness (i.e., the number of links is linear with the number of nodes) and fault tolerance (i.e., can survive certain level of node/link failures), different geometric topologies were proposed and used as the underlying network topologies for wireless networks. However, most of the existing topology control algorithms can only be applied to 2-dimensional (2D) networks where all nodes are distributed in a 2D plane. In practice, wireless networks may be deployed in 3-dimensional (3D) space, such as underwater wireless sensor networks in ocean or ad hoc networks in space. This paper seeks to investigate efficient fault tolerant topology control protocols for 3D wireless networks. Our new protocols not only guarantee the kappa-connectivity of the network, but also ensure the bounded node degree and constant power stretch factor. All of our proposed protocols are localized algorithms, which only use one-hop neighbor information and constant messages with small time complexity. Our simulation confirms our theoretical proofs for all proposed 3D topologies.\n\nTitle: Energy-balanced cooperative routing in multihop wireless networks\nContent: Cooperative communication (CC) allows multiple nodes to simultaneously transmit the same packet to the receiver so that the combined signal at the receiver can be correctly decoded. Since the CC can reduce the transmission power and extend the transmission coverage, it has been considered in minimum energy routing protocols to reduce the total energy consumption. However, previous research on cooperative routing only focuses on minimizing the total energy consumption from the source node to the destination node, which may lead to the unbalanced energy distribution among nodes. In this paper, we aim to study the impact of cooperative routing on balancing the energy distribution among nodes. By introducing a new routing scheme which carefully selects cooperative relay nodes and assigns their transmission power, our cooperative routing method can balance the remaining energy among neighboring nodes to maximize the lifetime of the network. Simulation results demonstrate that the proposed cooperative routing algorithm significantly balances the energy distribution and prolongs the lifetime of the network.\n\n## Task\nCompose an abstract for the title 'W3W: Energy Management of Hybrid Energy Supplied Sensors for Internet of Things.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The usage of hybrid energy supplied sensors in the Internet of Things has enabled longer lifetime of sensors and expanded scope of applications. These sensors can combine advantages of environmental energy harvesting techniques and wireless energy harvesting techniques. However, how to coordinate them is still a challenge and has not been studied extensively. In this article, we present a system based on mobile crowd wireless charging to manage energy of hybrid energy supplied sensors. When environmental energy is insufficient, the system will utilize smart devices carried by mobile users as chargers to provide wireless energy. We construct and study a W3W problem in the system: <underline>w</underline>hen to leverage mobile crowd wireless charging to support rechargeable sensors, <underline>w</underline>here to perform wireless energy transfer, and <underline>w</underline>hom to allocate and incentivize as chargers to maximize useful energy value over all sensors subject to a budget. In order to control the actual quality of wireless energy charging, we propose a design principle named task completion trustfulness. We consider offline and online conditions and design corresponding algorithms with incentive allocations. Extensive simulations are conducted to demonstrate the effectiveness of our algorithms, which also validates our theoretical results.\n\n"
      }
    ]
  },
  {
    "tid":"57",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Sentence Correction Incorporating Relative Position and Parse Template Language Models\nContent: Sentence correction has been an important emerging issue in computer-assisted language learning. However, existing techniques based on grammar rules or statistical machine translation are still not robust enough to tackle the common errors in sentences produced by second language learners. In this paper, a relative position language model and a parse template language model are proposed to complement traditional language modeling techniques in addressing this problem. A corpus of erroneous English-Chinese language transfer sentences along with their corrected counterparts is created and manually judged by human annotators. Experimental results show that compared to a state-of-the-art phrase-based statistical machine translation system, the error correction performance of the proposed approach achieves a significant improvement using human evaluation.\n\nTitle: Edit disfluency detection and correction using a cleanup language model and an alignment model\nContent: This investigation presents a novel approach to detecting and correcting the edit disfluency in spontaneous speech. Hypothesis testing using acoustic features is first adopted to detect potential interruption points (IPs) in the input speech. The word order of the cleanup utterance is then cleaned up based on the potential IPs using a class-based cleanup language model, the deletable region and the correction are aligned using an alignment model. Finally, log linear weighting is applied to optimize the performance. Using the acoustic features, the IP detection rate is significantly improved especially in recall rate. Based on the positions of the potential IPs, the cleanup language model and the alignment model are able to detect and correct the edit disfluency efficiently. Experimental results demonstrate that the proposed approach has achieved error rates of 0.33 and 0.21 for IP detection and edit word deletion, respectively\n\nTitle: Variable-length unit selection using LSA-based syntactic structure cost\nContent: The paper introduces a variable-length unit selection method for concatenative speech synthesis based on a syntactic structure based on latent semantic analysis (LSA). First, a probabilistic context free grammar (PCFG) based parser is used to construct the syntactic structure of the input text sentence. Second, the synthesizer selects the candidate units for each node of the syntactic structure. LSA is then adopted to estimate the syntactic cost between the target unit and the candidate units in the database. Finally, the concatenation of units with minimum cost is selected using a dynamic programming algorithm. Experimental results show that variable-length unit selection based on syntactic structure outperforms the synthesizer that does not consider syntactic structure. Also, the LSA-based syntactic cost provides a better estimation of substitution cost than that calculated only from acoustic features.\n\nTitle: Error recovery and sentence verification using statistical partial pattern tree for conversational speech\nContent: In this paper, in order to deal with the problems of disfluencies in conversational speech, a partial pattern tree (PPT) and a PPT-based statistical language model are proposed. A partial pattern is defined to represent a sub-sentence with a key-phrase and some optional/ functional phrases. The PPT is an integrated tree structure of the partial patterns generated from the training sentences and used to model the n-gram and grammatical constraints. In addition, a PPT merging algorithm is also proposed to reduce the number of partial patterns with similar syntactic structure by minimizing an objective cost function. Using the PPT, the undetected/misdetected errors due to disfluencies can be recovered. Finally, a sentence verification approach is proposed to re-rank the recovered sentences generated from the PPT. In order to assess the performance, a faculty name inquiry system with 2583 names has been implemented. The recognition accuracy of the system using the proposed PPT achieved 77.23%. We also contrasted this method with previous conventional approaches to show its superior performance.\n\nTitle: Model Generation of Accented Speech using Model Transformation and Verification for Bilingual Speech Recognition\nContent: Nowadays, bilingual or multilingual speech recognition is confronted with the accent-related problem caused by non-native speech in a variety of real-world applications. Accent modeling of non-native speech is definitely challenging, because the acoustic properties in highly-accented speech pronounced by non-native speakers are quite divergent. The aim of this study is to generate highly Mandarin-accented English models for speakers whose mother tongue is Mandarin. First, a two-stage, state-based verification method is proposed to extract the state-level, highly-accented speech segments automatically. Acoustic features and articulatory features are successively used for robust verification of the extracted speech segments. Second, Gaussian components of the highly-accented speech models are generated from the corresponding Gaussian components of the native speech models using a linear transformation function. A decision tree is constructed to categorize the transformation functions and used for transformation function retrieval to deal with the data sparseness problem. Third, a discrimination function is further applied to verify the generated accented acoustic models. Finally, the successfully verified accented English models are integrated into the native bilingual phone model set for Mandarin-English bilingual speech recognition. Experimental results show that the proposed approach can effectively alleviate recognition performance degradation due to accents and can obtain absolute improvements of 4.1%, 1.8%, and 2.7% in word accuracy for bilingual speech recognition compared to that using traditional ASR approaches, MAP-adapted, and MLLR-adapted ASR methods, respectively.\n\n## Task\nCompose an abstract for the title 'Word Order Correction For Language Transfer Using Relative Position Language Modeling' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Sentence correction has been an important and emerging issue in computer-assisted language learning. However, existing techniques based on grammar rules or statistical machine translation are still not robust enough to tackle the common incorrect word order errors in sentences produced by second language learners of Chinese. In this paper, a novel relative position. language model is proposed to address this problem, for which a corpus of erroneous English-Chinese language transfer sentences along with their corrected counterparts is created and manually judged by human annotators. Experimental results show that compared to a scoring approach based on an n-gram language model and a phrase-based machine translation system, the performance in terms of BLEU scores of the proposed approach achieved improvements of 20.3% and 26.5% for the correction of word order errors resulting from language transfer, respectively."
      }
    ]
  },
  {
    "tid":"58",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: New Heuristics and Lower Bounds for the Min-Max k -Chinese Postman Problem\nContent: Given an undirected edge-weighted graph and a depot node, postman problems are generally concerned with traversing the edges of the graph (starting and ending at the depot node) while minimizing the distance traveled. For the Min-Max k-Chinese Postman Problem (MM k- CPP) we have k 1 postmen and want to minimize the longest of the k tours. We present two new heuristics and improvement procedures for the MM k-CPP. Furthermore, we give three new lower bounds in order to assess the quality of the heuristics. Extensive computational results show that our algorithms outperform the heuristic of Frederickson et al.\n\nTitle: A Branch & Cut Algorithm for the Asymmetric Traveling Salesman Problem with Precedence Constraints\nContent: In this article we consider a variant of the classical asymmetric traveling salesman problem (ATSP), namely the ATSP in which precedence constraints require that certain nodes must precede certain other nodes in any feasible directed tour. This problem occurs as a basic model in scheduling and routing and has a wide range of applications varying from helicopter routing (Timlin, Master's Thesis, Department of Combinatorics and Optimization, University of Waterloo, 1989), sequencing in flexible manufacturing (Ascheuer et al., Integer Programming and Combinatorial Optimization, University of Waterloo, Waterloo, 1990, pp. 19\u201328&semi; Idem., SIAM Journal on Optimization, vol. 3, pp. 25\u201342, 1993), to stacker crane routing in an automatic storage system (Ascheuer, Ph.D. Thesis, Tech. Univ. Berlin, 1995). We give an integer programming model and summarize known classes of valid inequalities. We describe in detail the implementation of a branch&cut-algorithm and give computational results on real-world instances and benchmark problems from TSPLIB. The results we achieve indicate that our implementation outperforms other implementations found in the literature. Real world instances with more than 200 nodes can be solved to optimality within a few minutes of CPU-time. As a side product we obtain a branch&cut-algorithm for the ATSP. All instances in TSPLIB can be solved to optimality in a reasonable amount of computation time.\n\nTitle: A tabu search algorithm for the min-max k-Chinese postman problem\nContent: In this paper we present a tabu search algorithm for the min-max k-Chinese postman problem (MM k-CPP). Given an undirected edge-weighted graph and a distinguished depot node, the MM k-CPP consists of finding k 1 tours (starting and ending at the depot node) such that each edge is traversed by at least one tour and the length of the longest tour is minimized. A special emphasis is put on investigating the trade-off between running time effort and solution quality when applying different improvement procedures in the course of the neighborhood construction. Furthermore, different neighborhoods are analyzed. Extensive computational results show that the tabu search algorithm outperforms all known heuristics and improvement procedures.Scope and purpose Given a road network, the Chinese postman problem (CPP) is to find the shortest postman tour covering all the roads in the network. Applications of the CPP include road maintenance, garbage collection, mail delivery, etc. Since usually large road networks have to be serviced the work load must be distributed among k \u2265 2 vehicles. In contrast to the usual objective to minimize the total distance traveled by the k vehicles (k-CPP), for the min-max k-Chinese postman problem (MM k-CPP) the aim is to minimize the length of the longest of the k tours. This kind of objective is preferable when customers have to be served as early as possible. Furthermore, tours will be enforced to be more balanced resulting in a fair scheduling of tours. Although the CPP and the k-CPP are polynomially solvable, the MM k-CPP is NP-hard. Hence, we must rely on heuristics producing approximate solutions. The purpose of this paper is to present a tabu search algorithm for the MM k-CPP which outperforms all known heuristics. In many cases we obtained solutions which could be proved to be near-optimal or even optimal.\n\nTitle: Towards Efficient and Exact MAP-Inference for Large Scale Discrete Computer Vision Problems via Combinatorial Optimization\nContent: Discrete graphical models (also known as discrete Markov random fields) are a major conceptual tool to model the structure of optimization problems in computer vision. While in the last decade research has focused on fast approximative methods, algorithms that provide globally optimal solutions have come more into the research focus in the last years. However, large scale computer vision problems seemed to be out of reach for such methods. In this paper we introduce a promising way to bridge this gap based on partial optimality and structural properties of the underlying problem factorization. Combining these preprocessing steps, we are able to solve grids of size 2048x2048 in less than 90 seconds. On the hitherto unsolvable Chinese character dataset of Nowozin et. al we obtain provably optimal results in 56% of the instances and achieve competitive runtimes on other recent benchmark problems. While in the present work only generalized Potts models are considered, an extension to general graphical models seems to be feasible.\n\nTitle: Maximally Violated Mod-p Cuts for the Capacitated Vehicle-Routing Problem\nContent: This paper makes a contribution to the branch and cut approach to the capacitated vehicle-routing problem (CVRP). In the CVRP, the demands of a set of customers have to be met at minimum total travel cost using vehicles of identical capacity based at a single depot. The potential of maximally violated mod-p cutting-planes (Caprara et al. 2000) for the CVRP is investigated via a computational study. The foundation of the assessment is formed by classes of problem-specific constraints taken from the literature. In several separation algorithms for the CVRP, it is advantageous to shrink inclusionwise maximal minimum-weight cuts in support graphs as preprocessing. It is mentioned how a partition of the set of customers into such mincuts can be computed in a fast and elegant way using the mincut algorithm of Hao and Orlin (1994). Interestingly, maximally violated mod-p cuts, which are general-purpose cuts of Chv\u00e1tal-Gomory type, stand comparison with problem-specific cuts for the CVRP and they are clearly useful on top of such cuts. The first-time proven optimal solution of the CVRP instance B-n68-k9 is reported. The computation used a branching strategy with far lookahead and relied on maximally violated mod-p cuts. This paper on maximally violated cuts belongs to a set of papers originating from Applegate et al. (1995) where a separation of maximally violated combs for the traveling-salesman problem (TSP) is suggested.\n\n## Task\nCompose an abstract for the title 'Provably good solutions for the traveling salesman problem' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The determination of true optimum solutions of combinatorial optimization problems is seldomly required in practical applications. The majority of users of optimization software would be satisfied with solutions of guaranteed quality in the sense that it can be proven that the given solution is at most a few percent off an optimum solution. This paper presents a general framework for practical problem solving with emphasis on this aspect. A detailed discussion along with a report about extensive computational experiments is given for the traveling salesman problem."
      }
    ]
  },
  {
    "tid":"59",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: On the stability of indirect ZMP controller for biped robot systems\nContent: This paper proposes the indirect zero momentum position (ZMP) controller for biped robot systems and proves its disturbance input-to-state stability (ISS). The ZMP control has been used as a standard method for stable walking control of biped robot systems. Since the ZMP information consists of position and acceleration of the center of gravity (COG) for a biped robot system, the ZMP can be indirectly controlled by the motion of COG. In this paper, the reference COG planner is developed by solving the reference ZMP differential equation. The indirect ZMP controller is proposed to derive the desired motion of COG from the reference ZMP trajectory and the COG error (the difference between the reference and real COG). The ISS of the proposed indirect ZMP controller is proved for the simplified biped robot model. The robustness of the proposed indirect ZMP controller is shown in simulation.\n\nTitle: Optimal design and actuator sizing of redundantly actuated omni-directional mobile robots\nContent: Despite that omni-directional mobile robots have been employed popularly in several application areas, effort on optimal design of such mobile robots has been few in literature. Thus, this paper investigates the optimal design of omni-directional mobile robots. Particularly, optimal design parameters such as one or double offset distance of wheel mechanism and the wheel radius are identifed with respect to isotropic characteristic of mobile robots. In addition, the force transmission characteristics and actuator-sizing problem of mobile robots are investigated Analysis has been performed for three actuation sets. It is shown that the redundantly acruated mobile robot with three active caster wheels represents the best performance among them.\n\nTitle: Stable whole-body motion generation for humanoid robots to imitate human motions\nContent: This work presents a methodology to generate dynamically stable whole-body motions for a humanoid robot, which are converted from human motion capture data. The methodology consists of the kinematic and dynamical mappings for human-likeness and stability, respectively. The kinematic mapping includes the scaling of human foot and Zero Moment Point (ZMP) trajectories considering the geometric differences between a humanoid robot and a human. It also provides the conversion of human upper body motions using the method in. The dynamic mapping modifies the humanoid pelvis motion to ensure the movement stability of humanoid whole-body motions, which are converted from the kinematic mapping. In addition, we propose a simplified human model to obtain a human ZMP trajectory, which is used as a reference ZMP trajectory for the humanoid robot to imitate during the kinematic mapping. A human whole-body dancing motion is converted by the methodology and performed by a humanoid robot with online balancing controllers.\n\nTitle: An Ethercat-Based Real-Time Motion Control System In Mobile Robot Application\nContent: This paper presents a real-time motion control system using EtherCAT protocol and its application on a differential drive mobile robot. The motion controller is designed from open-source components consisting of dual kernel approach using standard Linux and real-time extension of Xenomai, and the EtherCAT Master stack, IgH. In order to validate feasibility of the real-time system, timing analysis between the master and the slaves is performed in terms of periodicity of the cyclic task, jitter, and in-control execution time as test metrics. Furthermore, we conducted a convolution based trajectory planning algorithm that considers the physical limits of the mobile robot to generate periodic velocity commands following a curved path. Encoder data from each wheels is evaluated to guarantee the accuracy of the motion control system in Cartesian space.\n\nTitle: An IEEE-1394 Based Real-time Robot Control System for Efficient Controlling of Humanoids\nContent: Modern sophisticated robots, e.g. humanoids, require significant real-time communication speed in their fieldbuses. Several existing field-bus technologies, like controller area network (CAN), Profibus etc., provide real-time performance and reliability, however the bandwidth is not enough to support the communication demand of the complex humanoids. In this research, we have addressed this critical issue by utilizing comparatively faster IEEE 1394 serial bus on commercial off-the-shelf hardware and Linux-based real-time software platforms. Since the lack of user-level communication protocols in IEEE 1394 standard is a major obstacle for designing a IEEE 1394 based control system, we have solved this issue by customizing the predefined IEEE 1394 packet formats. Moreover, various IEEE 1394 serial bus related features have been investigated for designing an efficient humanoid control system. In this work, we have followed the real-time modeling and analysis method using Unified Modeling Language (UML). A use case driven analysis of timing requirement and determination of real-time constrain have been shown to prove the usefulness of our approach. The ongoing implementation of this proposed system in our network based humanoid, MAHRU, has been demonstrated in both commercial RTOS RTLinux (Pro) and free OSS RTOS Xenomai. The performance measurement of basic IEEE 1394 read transaction has been shown in both platforms along with a comparison with similar experiment on CAN bus. This provides an estimation of performance improvement of a humanoid control system on IEEE 1394 serial bus over CAN bus. Moreover the results obtained from two different Linux-based RTOS platforms indicate their relative pros and cons. This also enables us to select the suitable real-time framework for designing an efficient IEEE 1394 based humanoid control system\n\n## Task\nCompose an abstract for the title 'A stable target-tracking control for unicycle mobile robots' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper deals with target tracking control of unicycle type mobile robots. Target tracking function is essential for autonomous robots such as guide robots, security guard robots, etc. In the field of mobile robot control, many control schemes for posture stabilization and trajectory tracking problem have been proposed. Target tracking control, however, cannot be achieved using these kinds of control laws. Therefore, a new global asymptotic stable controller for this problem is designed using the backstepping method. The stability of the system is proved using the Lyapunov function. Various simulation results validate the performance and theoretical analysis"
      }
    ]
  },
  {
    "tid":"60",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Performance evaluation of network mobility handover over future aeronautical data link\nContent: The aviation community is currently working on the standardization of data communication systems for the future air traffic management. In this context, International Civil Aviation Organization (ICAO) has initiated a work on standardization of an IPv6-based aeronautical telecommunications network and on future radio access technologies, respectively. In this paper, we integrate L-Band Digital Aeronautical Communications System Option 1 (L-DACS 1), which is one candidate for future radio access technologies, with realistic IPv6-based network layer functionality and analyze the effect of handover delay to the TCP performance. Realistic Frame Error Rate (FER) values obtained from an L-DACS 1 physical layer simulator, which uses a realistic aeronautical channel model, are used in the simulation experiments. In the first stage, we decreased layer 3 handover latency by removing the Duplicate Address Detection (DAD) procedure for address configuration. In the second stage, we introduced a Home Agent (HA)-buffering method, which is used to buffer the traffic (destined to the mobile node) during handover. Transmission completion time is the primary performance metric in our analysis. With the HA-buffering method, the transmission completion time is reduced by at least 10% for the transmission of 110kB of information over a wireless link with 31.5kbits^-^1 data rate.\n\nTitle: Schedulability Analysis In Time-Triggered Automotive Real-Time Systems\nContent: In embedded automotive real-time systems correct timing is required to effectively implement distributed applications, time-triggered communication, and integration of intelligent safety applications. We use formal schedulability analysis to validate the task timing for asynchronous real-time systems in an automotive context. Restricting the computational model to purely time-triggered task activation and applying application specific constraints enables us to perform exact response time analyses without pessimism. The analysis can be done efficiently since our computational model drastically limits the number of critical instant candidates to be considered for the worst-case scenario. Simulation results show that the original computational model incorporates a significant amount of response time overestimation and a high computational workload. The presented method can be applied in early design stages as well as in series development and helps to reduce development risks for electronic control units and embedded telematic systems.\n\nTitle: Analytical Test Effort Estimation for Multisensor Driver Assistance Systems\nContent: Modern driver assistance systems are often using a wide range of equipped sensors as primary data source. The reliability of each sensor is specified by the manufacturer and influences the system reliability distinctly. To prevent potential fatal system failures, diverse failure prevention mechanisms are included. Nevertheless, a certain level of reliability of the system has to be guaranteed to meet legal regulations. For this, extensive testing is required, which is costly due to the involved resources. To estimate and simulate the required test effort, an exact analytical method based on Markov Chains and an implementation realized in a common simulation framework is presented in this paper. It enables real automotive projects to estimate the test costs and simulate changes for various sensor setups.\n\nTitle: Towards IEEE 802.15.4e: A study of performance aspects\nContent: We discuss the applicability of IEEE 802.15.4 for application in industrial automation. Based on the specific requirements in this field, especially w.r.t. real-time operation, we analyzed the weaknesses of the standard protocol and proposed a novel MAC protocol that keeps the original PHY definition in order to work using available IEEE 802.15.4 chipsets. In earlier work, we analytically derived the worst case latency for using the improved protocol version in typical industrial setups. We now also implemented this protocol version in a simulation environment in order to show the typical behavior in the network taking into account typical channel conditions. We performed extensive simulation experiments that show the limitations of the standard protocol and that demonstrate the capabilities of the new protocol in a selected automation scenario. Our protocol variant is going to become the forthcoming IEEE 802.15.4e standard.\n\nTitle: Modeling and Analysis of Advanced Automotive ECU Architectures at Early Design Stages Using EMF and Model Transformation\nContent: In the automotive industry active safety functions are deployed to better protect the passengers and vulnerable road users in case of an accident. To do so, advanced system architectures which enable an OEM to integrate this new functionality safely and efficiently have to be developed. In order to evaluate different possible concepts at an early stage in the development, model-based approaches are feasible. We propose a methodical approach to evaluate architecture concepts with regard to standard analysis methods as well as application specific evaluation techniques. We use UML to define the meta-model structure of the system architecture, model the instances of the meta-model using EMF and a generated graphical editor, and perform evaluation operations with the help of an MDE framework. The tool chain allows for an easy modeling of system models at early design stages and performing application specific analysis techniques. To validate the feasibility of the approach we apply real-time schedule simulation to two example models.\n\n## Task\nCompose an abstract for the title 'Analysis Of Deficit Round Robin Scheduling For Future Aeronautical Data Link' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The aviation community is currently working on the standardization of data communication systems for the future air traffic management. In this context, the International Civil Aviation Organization (ICAO) has initiated a work on standardization of an IPv6-based aeronautical telecommunications network and on future radio access technologies, respectively. In this paper, we integrate L-Band Digital Aeronautical Communications System Option 1 (L-DACS 1), which is one candidate for future radio access technologies, with realistic IPv6-based network layer functionality and analyze Deficit Round Robin (DRR) with fragmentation algorithm for the forward and return link in L-DACS 1. Our analysis mainly covers two application domains: file transfer and real-time services. We show that DRR with fragmentation scheduler provides good performance results in terms of throughput, delay, and bandwidth fairness."
      }
    ]
  },
  {
    "tid":"61",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Energy optimal coding for wireless nanosensor networks\nContent: Wireless nanosensor networks (WNSNs), which consist of a lot of nanosensors with size of just a few hundred nanometers and are able to detect and sense new types of events at the nanoscale, are promising for a lot of unique applications like intrabody drug delivery systems, air pollution surveillance, etc. One important feature of WNSNs is that the nanosensors are highly energy-constrained, which makes it essential to develop energy efficient protocols for different layers of such networks. This paper focuses on a WNSN with on-off keying (OOK) modulation and explores the problem of transmission energy minimization in it. We first propose a general minimum transmission energy (MTE) coding scheme, which maps m-bit symbols into n-bit codewords with the least number of high-bits and thus results in the lowest energy consumption per symbol for any given m and n. We further determine the optimal setting of symbol length m and codeword length n in the MTE coding scheme so as to achieve the minimum energy consumption per data bit, which serves as the lower bound of transmission energy consumption in such WNSNs. Numerical results are provided to demonstrate the efficiency of the MTE coding scheme.\n\nTitle: Energy-Efficient Prefix-Free Codes for Wireless Nano-Sensor Networks Using OOK Modulation\nContent: Wireless nano-sensor networks (WNSNs), which consist of nano-sensors a few hundred nanometers in size with the capability to detect and sense new types of events in nano-scale, are promising for many unique applications such as air pollution surveillance. The nano-sensors of WNSNs are highly energy-constrained, which makes it essential to develop energy-efficient communication techniques in such networks. In this paper, we focus on WNSNs employing on-off keying (OOK) modulation, whereby transmission energy minimization corresponds to the minimization of average codeword weight (ACW). We formulate an integer nonlinear programming problem to construct prefix-free codes with minimum ACW under the constraint of average codeword length (ACL) so as to minimize the transmission energy consumption while guaranteeing the throughput larger than a preset desired value. In addition, two efficient algorithms, called binary tree based weight decreasing (BT-WD) algorithm and binary tree based length decreasing (BT-LD) algorithm, are presented for constructing low-ACW prefix-free codes. The effectiveness of the proposed algorithms is verified through simulations and comparisons with the exhaustive search method. Compared with the available fixed-length low-weight codes, the designed prefix-free variable-length codes allow us to not only control the throughput more flexibly but also achieve lower transmission energy consumption in the scenarios with low or medium bit error rates.\n\nTitle: Joint Design of Network Coding and Transmission Rate Selection for Multihop Wireless Networks\nContent: Network coding has the potential to greatly improve the throughput of wireless networks. In the current proposals for wireless network coding, network nodes transmit packets at a fixed transmission rate. It is notable, however, that by dynamically selecting the rate, we can effectively improve the node transmission efficiency. In this paper, we study the application of a rate-adaptive transmission mechanism in network-coding-based multihop wireless networks. In such networks, whether a coding solution is satisfactory or not depends not only on the number of involved native packets but on the packet loss probabilities of its intended next hops and its transmission time as well, both of which depend on the transmission rate. Therefore, we aim to jointly design the coding operation and rate selection to maximize the transmission efficiency. Specifically, we first describe and mathematically formulate the optimal packet coding and rate-selection problem. Then, we prove the NP-completeness of this optimization problem. Finally, we propose an efficient algorithm for finding good combinations of the coding solution and the transmission rate. Simulation results demonstrate that compared with the rate-fixed transmission, the rate-adaptive transmission based on our algorithm can significantly improve the node transmission efficiency.\n\nTitle: An Energy-Efficient Data Collection Scheme in Body Area Nanonetworks.\nContent: Wireless nanosensor networks (WNSNs) represent a class of emerging and important network model for perceiving information in the nanoscale. The extremely limited energy storage in nanosensors necessitates the design of energy-efficient protocols (e.g., data collection schemes) for such networks. This paper considers the design of energy-efficient data collection scheme in a WNSN operating in the human body, i.e., Body Area Nanonetwork (BANN). More specifically, this scheme involves a hierarchical (i.e., nanointerface-nanorouter-nanonode) collection structure with nanointerface receiving external requests and returning the requested data, nanorouter collecting data from nanonodes and nanonodes gathering data from human body. In the data collection process, each nanorouter first activates the nanonodes in a rectangle region centered at itself with length l and width W (i.e., wake-up region) and obtains the available energy level of each nanonode in this region. Each nanorouter then selects the nanonode with the largest available energy in the wake-up region to return the requested data. Finally, simulation results are provided to illustrate the performances of our scheme in terms of average available energy and path loss. These results show that better performances can be achieved when a smaller wake-up region is specified.\n\nTitle: A More Efficient Cope Architecture For Network Coding In Multihop Wireless Networks\nContent: Recently, a promising packet forwarding architecture COPE was proposed to essentially improve the throughput of multihop wireless networks, where each network node can intelligently encode multiple packets together and forward them in a single transmission. However, COPE is still in its infancy and has the following limitations: (1) COPE adopts the FIFO packet scheduling and thus does not provide different priorities for different types of packets. (1) COPE simply classifies all packets destined to the same nexthop into small-size or large-size virtual queues solutions. Such a queuing structure will lose some potential coding opportunities, because among packets destined to the same nexthop at most two packets (the head packets of small-size and large-size queues) will be examined in the coding process, regardless of the number of flows. (3) The coding algorithm adopted in COPE is fast but cannot always find good solutions. In order to address the above limitations, in this paper we first present a new queuing structure for COPE, which can provide more potential coding opportunities, and then propose a new packet scheduling algorithm for this queuing structure to assign different priorities to different types of packets. Finally, we propose an efficient coding algorithm to find appropriate packets for coding. Simulation results demonstrate that this new COPE architecture can further greatly improve the node transmission efficiency.\n\n## Task\nCompose an abstract for the title 'Optimal coding for transmission energy minimization in wireless nanosensor networks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Wireless nanosensor networks (WNSNs), which consist of nanosensors with the size of just a few hundred nanometers, can sense new types of events at nanoscale and thus are promising for lots of important applications, like intrabody drug delivery, air pollution surveillance, etc. Since nanosensors are highly energy-constrained, it is critical to adopt energy-efficient protocols for communications in WNSNs. This paper focuses on WNSNs adopting the on\u2013off keying modulator which transmits a signal for a high bit and keeps silence for a low bit and explores the optimal coding design for transmission energy minimization in such networks. Specifically, we focus on the design of low-weight code which maps m-bit symbols into n-bit codewords with fewer high bits. First, for given symbol length m and codeword length n, we determine the optimal codebook with the least average number of high bits per codeword and give the corresponding average codeword weight as a function of m and n. Then, we study the minimum transmission energy (MTE) code design problem with codeword length and codeword rate constraints and investigate how the transmission energy changes when m and n are varied. Finally, the corresponding optimal settings of m and n are determined for the MTE code design problem to achieve the minimum transmission energy consumption per data bit."
      }
    ]
  },
  {
    "tid":"62",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Pinball loss minimization for one-bit compressive sensing: Convex models and algorithms.\nContent: The one-bit quantization is implemented by one single comparator that operates at low power and a high rate. Hence one-bit compressive sensing (1bit-CS) becomes attractive in signal processing. When measurements are corrupted by noise during signal acquisition and transmission, 1bit-CS is usually modeled as minimizing a loss function with a sparsity constraint. The one-sided \u21131 loss and the linear loss are two popular loss functions for 1bit-CS. To improve the decoding performance on noisy data, we consider the pinball loss, which provides a bridge between the one-sided \u21131 loss and the linear loss. Using the pinball loss, two convex models, an elastic-net pinball model and its modification with the \u21131-norm constraint, are proposed. To efficiently solve them, the corresponding dual coordinate ascent algorithms are designed and their convergence is proved. The numerical experiments confirm the effectiveness of the proposed algorithms and the performance of the pinball loss minimization for 1bit-CS.\n\nTitle: Sequential minimal optimization for SVM with pinball loss.\nContent: To pursue the insensitivity to feature noise and the stability to re-sampling, a new type of support vector machine (SVM) has been established via replacing the hinge loss in the classical SVM by the pinball loss and was hence called a pin-SVM. Though a different loss function is used, pin-SVM has a similar structure as the classical SVM. Specifically, the dual problem of pin-SVM is a quadratic programming problem with box constraints, for which the sequential minimal optimization (SMO) technique is applicable. In this paper, we establish SMO algorithms for pin-SVM and its sparse version. The numerical experiments on real-life data sets illustrate both the good performance of pin-SVMs and the effectiveness of the established SMO methods.\n\nTitle: Support Vector Machine Classifier with Pinball Loss.\nContent: Traditionally, the hinge loss is used to construct support vector machine (SVM) classifiers. The hinge loss is related to the shortest distance between sets and the corresponding classifier is hence sensitive to noise and unstable for re-sampling. In contrast, the pinball loss is related to the quantile distance and the result is less sensitive. The pinball loss has been deeply studied and widely applied in regression but it has not been used for classification. In this paper, we propose a SVM classifier with the pinball loss, called pin-SVM, and investigate its properties, including noise insensitivity, robustness, and misclassification error. Besides, insensitive zone is applied to the pin-SVM and a sparse model is obtained. Compared to the SVM with the hinge loss, the proposed pin-SVM has the same computational complexity and enjoys noise insensitivity and re-sampling stability.\n\nTitle: A Rank-One Tensor Updating Algorithm for Tensor Completion\nContent: In this letter, we propose a rank-one tensor updating algorithm for solving tensor completion problems. Unlike the existing methods which penalize the tensor by using the sum of nuclear norms of unfolding matrices, our optimization model directly employs the tensor nuclear norm which is studied recently. Under the framework of the conditional gradient method, we show that at each iteration, solving the proposed model amounts to computing the tensor spectral norm and the related rank-one tensor. Because the problem of finding the related rank-one tensor is NP-hard, we propose a subroutine to solve it approximately, which is of low computational complexity. Experimental results on real datasets show that our algorithm is efficient and effective.\n\nTitle: Signal recovery for jointly sparse vectors with different sensing matrices.\nContent: In this paper, we study a sparse multiple measurement vector problem in which we need to recover a set of jointly sparse vectors from incomplete measurements. Most related studies assumed that all these measurements correspond to the same compressed sensing matrix. Differently, we allow that the measurements come from different sensing matrices. To deal with different matrices, we establish an algorithm via applying block coordinate descent and Majorization-Minimization techniques. The numerical examples demonstrate the effectiveness of this new algorithm, which allows us to design different matrices for better recovery performance. HighlightsAn algorithm for recovering jointly sparse vectors with different sensing matrices.The algorithm is based on block coordinate method and Majorization-Minimization method.Recovery performance can be improved when different sensing matrices are available.The experiments demonstrate the effectiveness of this new algorithm.\n\n## Task\nCompose an abstract for the title 'Pinball Loss Minimization for One-bit Compressive Sensing' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"  The one-bit quantization can be implemented by one single comparator, which operates at low power and a high rate. Hence one-bit compressive sensing (\\emph{1bit-CS}) becomes very attractive in signal processing. When the measurements are corrupted by noise during signal acquisition and transmission, 1bit-CS is usually modeled as minimizing a loss function with a sparsity constraint. The existing loss functions include the hinge loss and the linear loss. Though 1bit-CS can be regarded as a binary classification problem because a one-bit measurement only provides the sign information, the choice of the hinge loss over the linear loss in binary classification is not true for 1bit-CS. Many experiments show that the linear loss performs better than the hinge loss for 1bit-CS. Motivated by this observation, we consider the pinball loss, which provides a bridge between the hinge loss and the linear loss. Using this bridge, two 1bit-CS models and two corresponding algorithms are proposed. Pinball loss iterative hard thresholding improves the performance of the binary iterative hard theresholding proposed in [6] and is suitable for the case when the sparsity of the true signal is given. Elastic-net pinball support vector machine generalizes the passive model proposed in [11] and is suitable for the case when the sparsity of the true signal is not given. A fast dual coordinate ascent algorithm is proposed to solve the elastic-net pinball support vector machine problem, and its convergence is proved. The numerical experiments demonstrate that the pinball loss, as a trade-off between the hinge loss and the linear loss, improves the existing 1bit-CS models with better performances. "
      }
    ]
  },
  {
    "tid":"63",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Robust MIMO cognitive radio systems under temperature interference constraints\nContent: In cognitive radio (CR) systems, the primary users (PU) are protected by temperature interference constraints imposed on secondary users (SU). However, such limitations may be easily violated by SUs if perfect SU-to-PU channel state information (CSI) is not available at the secondary transmitters. In this paper, we propose a novel and distributed design of MIMO CR networks that is robust against imperfect SU-to-PU CSI. Specifically, we formulate the system design as a noncooperative game and robust global interference constraints are enforced via pricing; the prices are thus additional variables to be optimized. Building on the advanced and new theory of finite-dimensional variational inequalities (VI) in the complex domain, we analyze the proposed NE problem and devise alternative distributed algorithms along with their convergence properties.\n\nTitle: Design of Cognitive Radio Systems Under Temperature-Interference Constraints: A Variational Inequality Approach\nContent: The concept of cognitive radio (CR) has recently received great attention from the research community as a promising paradigm to achieve efficient use of the frequency resource by allowing the coexistence of licensed (primary) and unlicensed (secondary) users in the same bandwidth. In this paper, we propose a novel Nash equilibrium (NE) problem to model concurrent communications of cognitive secondary users who compete against each other to maximize their information rate. The formulation contains constraints on the transmit power (and possibly spectral masks) as well as aggregate interference tolerable at the primary users' receivers. The coupling among the strategies of the players due to the interference constraints presents a new challenge for the analysis of this class of Nash games that cannot be addressed using the game theoretical models proposed in the literature. For this purpose, we need the framework given by the more advanced theory of finite-dimensional variational inequalities (VI). This provides us with all the mathematical tools necessary to analyze the proposed NE problem (e.g., existence and uniqueness of the solution) and to devise alternative distributed algorithms along with their convergence properties.\n\nTitle: MIMO Cognitive Radio: A Game Theoretical Approach\nContent: The concept of cognitive radio (CR) has recently received great attention from the research community as a promising paradigm to achieve efficient use of the frequency resource by allowing the coexistence of licensed (primary) and unlicensed (secondary) users in the same bandwidth. In this paper we propose and analyze a totally decentralized approach, based on game theory, to design cognitive MIMO transceivers, who compete with each other to maximize their information rate. The formulation incorporates constraints on the transmit power as well as null and/or soft shaping constraints on the transmit covariance matrix, so that the interference generated by secondary users be confined within the temperature-interference limit required by the primary users. We provide a unified set of conditions that guarantee the uniqueness and global asymptotic stability of the Nash equilibrium of all the proposed games through totally distributed and asynchronous algorithms. Interestingly, the proposed algorithms overcome the main drawback of classical waterfilling based algorithms--the violation of the temperature-interference limit--and they have the desired features required for CR applications, such as low-complexity, distributed implementation, robustness against missing or outdated updates of the users, and fast convergence behavior.\n\nTitle: Robust MIMO Cognitive Radio Via Game Theory\nContent: Cognitive radio (CR) systems improve the spectral efficiency by allowing the coexistence in harmony of primary users (PUs), the legacy users, with secondary users (SUs). This coexistence is built on the premises that no SU can generate interference higher than some prescribed limits against PUs. The system design based on perfect channel state information (CSI) can easily end up violating the interference limits in a realistic situation where CSI may be imperfect. In this paper, we propose a robust design of CR systems, composed of multiple PUs and multiple noncooperative SUs, in either single-input single-output (SISO) frequency-selective channels or more general multiple-input multiple-output (MIMO) channels. We formulate the design of the SU network as a noncooperative game, where the SUs compete with each other over the resources made available by the PUs, by maximizing their own information rates subject to the transmit power and robust interference constraints. Following the philosophy of the worst-case robustness, we take explicitly into account the imperfectness of SU-to-PU CSI by adopting proper interference constraints that are robust with respect to the worst channel errors. Relying on the variational inequality theory, we study the existence and uniqueness properties of the Nash equilibria of the resulting robust games, and devise totally asynchronous and distributed algorithms along with their convergency properties. We also propose efficient numerical methods, based on decomposition techniques, to compute the robust transmit strategy for each SU.\n\nTitle: Competitive optimization of cognitive radio MIMO systems via game theory.\nContent: The concept of cognitive radio (CR) has recently received great attention from the researchers' community as a promising paradigm to achieve efficient use of the frequency resource by allowing the coexistence of licensed (primary) and unlicensed (secondary) users in the same bandwidth. In this paper, we propose a distributed approach based on game theory to design cognitive MIMO transceiver in hierarchical CR networks, where primary users establish null and/or soft shaping constraints on the transmit covariance matrix of secondary users, so that the interference generated by secondary users be confined within the interference-temperature limits. We formulate the resource allocation problem among secondary users as a strategic noncooperative game, where each transmit/receive pair competes against the others to maximize the information rate over his own MIMO channel, under transmit power and/or null/soft shaping constraints. We provide a unified set of conditions that guarantee the uniqueness and global asymptotic stability of the Nash equilibrium of all the proposed games through totally distributed and asynchronous algorithms. Interestingly, the proposed algorithms overcome the main drawback of classical waterfilling based algorithms--the violation of the interference-temperature limits--and they have many of the desired features required for cognitive radio applications, such as low-complexity, distributed nature, robustness against missing or outdated updates of the users, and fast convergence behavior.\n\n## Task\nCompose an abstract for the title 'Robust MIMO Cognitive Radio Systems Under Interference Temperature Constraints' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Cognitive Radio (CR) systems are built on the coexistence of primary users (PUs) and secondary users (SUs), the latter being allowed to share spectral resources with the PUs but under strict interference limitations. However, such limitations may easily be violated by SUs if perfect SU-to-PU channel state information (CSI) is not available at the secondary transmitters, which always happens in practice. In this paper, we propose a distributed design of MIMO CR networks under global interference temperature constraints that is robust (in the worst-case sense) against SU-to-PU channel uncertainties. More specifically, we consider two alternative formulations that are complementary to each other in terms of signaling and system performance, namely: a game-theoretical design and a social-oriented optimization. To study and solve the proposed formulations we hinge on the new theory of finite-dimensional variational inequalities (VI) in the complex domain and a novel parallel decomposition technique for nonconvex sum-utility problems with coupling constraints, respectively. A major contribution of this paper is to devise a new class of distributed best-response algorithms with provable convergence. The algorithms differ in computational complexity, convergence speed, communication overhead, and achievable performance; they are thus applicable to a variety of CR scenarios, either cooperative or non-cooperative, which allow the SUs to explore the trade-off between signaling and performance."
      }
    ]
  },
  {
    "tid":"64",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: zFTL: power-efficient data compression support for NAND flash-based consumer electronics devices\nContent: Flash translation layers play an important role in determining the storage performance and lifetime of NAND flash-based consumer electronics devices. In this paper, we present a flash translation layer called zFTL, which reduces the amount of data written to NAND flash memory by supporting on-line, transparent data compression based on the X-Match algorithm. To minimize compression overhead and power consumption, we also propose a novel prediction scheme that identifies incompressible data in advance without going through full compression. Our evaluations with five real-world workloads show that zFTL successfully enhances storage performance and lifetime by improving the write amplification factor (WAF) by a factor of 2.6 (geometric mean) compared to the case without compression support. In addition, we find that the proposed prediction scheme is effective in reducing power consumption by skipping compression for incompressible data.\n\nTitle: A group-based wear-leveling algorithm for large-capacity flash memory storage systems\nContent: Although NAND flash memory has become one of the most popular storage media for portable devices, it has a serious problem with respect to lifetime. Each block of NAND flash memory has a limited number of program/erase cycles, usually 10,000-100,000, and data in a block become unreliable after the limit. For this reason, distributing erase operations evenly across the whole flash memory media is an important concern in designing flash memory storage systems.In this paper, we propose a memory-efficient group-based wear-leveling algorithm. Our group-based algorithm achieves a small memory footprint by grouping several logically sequential blocks and managing only the summary information for each group. We also propose an effective group summary structure and a method to reduce unnecessary wear-leveling operations in order to enhance the wear-leveling performance. The evaluation results show that our group-based algorithm consumes only 8.75% of memory space compared to the previous scheme that manages per-block information, while showing roughly the same wear-leveling performance.\n\nTitle: Efficient Metadata Management for Flash File Systems\nContent: NAND flash memory becomes one of the most popular storage for portable embedded systems. Although many flash-aware file systems, such as JFFS2 and YAFFS2, were proposed, the large memory consumption and the longmount delay have been serious obstacles for large-capacity NAND flash memory. In this paper, we present a new flash-aware file system called DFFS (Direct Flash File System) which fetches only the needed metadata on demand from flash memory. In addition, DFFS employs two novel metadata management schemes, inode embedding scheme and hybrid inode indexing scheme, to improve the performance of metadata operations. Comprehensive evaluation results using microbenchmark, postmark, and Linux kernel compilation trace, show that DFFS has comparable performance to JFFS2 and YAFFS2, while achieving a small memory footprint and instant mount time.\n\nTitle: CFLRU: a replacement algorithm for flash memory\nContent: In most operating systems which are customized for disk-based storage system, the replacement algorithm concerns only the number of memory hits. However, flash memory has different read and write cost in the aspects of time and energy so the replacement algorithm with flash memory should consider not only the hit count but also the replacement cost caused by selecting dirty victims. The replacement cost of dirty page is higher than that of clean page with regard to both access time and energy consumption. In this paper, we propose the Clean-First LRU (CFLRU) replacement algorithm that exploits the characteristics of flash memory. CFLRU splits the LRU list into the working region and the clean-first region and adopts a policy that evicts clean pages preferentially in the clean-first region until the number of page hits in the working region is preserved in a suitable level. Using the trace-driven simulation, the proposed algorithm reduces the average replacement cost by 28.4% in swap system and by 26.2% in buffer cache, compared with LRU algorithm. We also implement the CFLRU algorithm in the Linux kernel and present some optimization issues.\n\nTitle: Accelerating external sorting via on-the-fly data merge in active SSDs\nContent: The concept of active SSDs (solid state drives) has been introduced in order to cope with the demands required to process the ever-increasing volumes of data. In active SSDs, some of the data-processing tasks are offloaded to SSDs, freeing host system resources and improving overall performance of data analysis. In this paper, we propose a novel active SSD architecture focused on improving the external sorting algorithm that is used extensively in data-intensive computing. By performing merge operations on-the-fly in active SSDs, our method can remove the extra data transfer and enhance the lifetime of SSDs. Our evaluation results on a real SSD platform indicate that the proposed scheme out-performs the traditional external sorting by up to 39%.\n\n## Task\nCompose an abstract for the title 'Zombie Chasing: Efficient Flash Management Considering Dirty Data in the Buffer Cache' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper presents a novel technique, called Zombie Chasing, for efficient flash management in solid state drives (SSDs). Due to the unique characteristics of NAND flash memory, SSDs need to accurately understand the liveness of the data stored in themselves. Recently, the TRIM command has been introduced to notify SSDs of dead data caused by file deletions, which otherwise could not be tracked by SSDs. This paper goes one step further and proposes a new liveness state, called the zombie state, to denote live data that will be dead shortly due to the corresponding dirty data in the buffer cache. We also devise new zombie-aware garbage collection algorithms which utilize the information about such zombie data inside SSDs. To evaluate Zombie Chasing, we implement zombie-aware garbage collection algorithms in the prototype SSD and modify the Linux kernel and the Oracle DBMS to deliver the information on the zombie data to the prototype SSD. Through comprehensive evaluations using our in-house micro-benchmark and the TPC-C benchmark, we observe that Zombie Chasing improves SSD performance effectively by reducing garbage collection overhead. Especially, our evaluation with the TPC-C benchmark on the Oracle DBMS shows that Zombie Chasing enhances the Transactions Per Second (TPS) value by up to 22% with negligible overhead."
      }
    ]
  },
  {
    "tid":"65",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Space---Time tradeoffs for subset sum: an improved worst case algorithm\nContent: The technique of Schroeppel and Shamir (SICOMP, 1981) has long been the most efficient way to trade space against time for the Subset Sum problem. In the random-instance setting, however, improved tradeoffs exist. In particular, the recently discovered dissection method of Dinur et al. (CRYPTO 2012) yields a significantly improved space---time tradeoff curve for instances with strong randomness properties. Our main result is that these strong randomness assumptions can be removed, obtaining the same space---time tradeoffs in the worst case. We also show that for small space usage the dissection algorithm can be almost fully parallelized. Our strategy for dealing with arbitrary instances is to instead inject the randomness into the dissection process itself by working over a carefully selected but random composite modulus, and to introduce explicit space---time controls into the algorithm by means of a \"bailout mechanism\".\n\nTitle: Fast monotone summation over disjoint sets\nContent: We study the problem of computing an ensemble of multiple sums where the summands in each sum are indexed by subsets of size p of an n-element ground set. More precisely, the task is to compute, for each subset of size q of the ground set, the sum over the values of all subsets of size p that are disjoint from the subset of size q. We present an arithmetic circuit that, without subtraction, solves the problem using O((np+nq)logn) arithmetic gates, all monotone; for constant p, q this is within the factor logn of the optimal. The circuit design is based on viewing the summation as a \"set nucleation\" task and using a tree-projection approach to implement the nucleation. Applications include improved algorithms for counting heaviest k-paths in a weighted graph, computing permanents of rectangular matrices, and dynamic feature selection in machine learning.\n\nTitle: Counting thin subgraphs via packings faster than meet-in-the-middle time\nContent: Vassilevska and Williams (STOC 2009) showed how to count simple paths on k vertices and matchings on k/2 edges in an n-vertex graph in time nk/2+O(1). In the same year, two different algorithms with the same runtime were given by Koutis and Williams (ICALP 2009), and Bj\u00f6rklund et al. (ESA 2009), via nst/2+O(1)-time algorithms for counting t-tuples of pairwise disjoint sets drawn from a given family of s-sized subsets of an n-element universe. Shortly afterwards, Alon and Gutner (TALG 2010) showed that these problems have \u03a9(n\u230ast/2\u230b) and \u03a9(n\u230ak/2\u230b) lower bounds when counting by color coding. Here we show that one can do better, namely, we show that the \\\"meet-in-the-middle\\\" exponent st/2 can be beaten and give an algorithm that counts in time n0.4547st+O(1) for t a multiple of three. This implies algorithms for counting occurrences of a fixed subgraph on k vertices and pathwidth p &Lt; k in an n-vertex graph in n0.4547k+2p+O(1) time, improving on the three mentioned algorithms for paths and matchings, and circumventing the color-coding lower bound.\n\nTitle: Circumspect Descent Prevails In Solving Random Constraint Satisfaction Problems\nContent: We study the performance of stochastic local search algorithms for random instances of the K-satisfiability (K-SAT) problem. We present a stochastic local search algorithm, ChainSAT, which moves in the energy landscape of a problem instance by never going upwards in energy. ChainSAT is a focused algorithm in the sense that it focuses on variables occurring in unsatisfied clauses. We show by extensive numerical investigations that ChainSAT and other focused algorithms solve large K-SAT instances almost surely in linear time, up to high clause-to-variable ratios a; for example, for K = 4 we observe linear-time performance well beyond the recently postulated clustering and condensation transitions in the solution space. The performance of ChainSAT is a surprise given that by design the algorithm gets trapped into the first local energy minimum it encounters, yet no such minima are encountered. We also study the geometry of the solution space as accessed by stochastic local search algorithms.\n\nTitle: Dense Subset Sum May Be the Hardest.\nContent: The SUBSET SUM problem asks whether a given set of n positive integers contains a subset of elements that sum up to a given target t. It is an outstanding open question whether the O*(2(n)(/2))-time algorithm for SUBSET SUM by Horowitz and Sahni [J. ACM 1974] can be beaten in the worst-case setting by a \"truly faster\", O*(2((0.5-delta)n))-time algorithm, with some constant delta > 0. Continuing an earlier work [STACS 2015], we study SUBSET SUM parameterized by the maximum bin size beta, defined as the largest number of subsets of the n input integers that yield the same sum. For every epsilon > 0 we give a truly faster algorithm for instances with beta <= 2((0.5- epsilon)n) as well as instances with beta >= 2(0)(.661n) Consequently, we also obtain a characterization in terms of the popular density parameter n/log(2)t: if all instances of density at least 1.003 admit a truly faster algorithm, then so does every instance. This goes against the current intuition that instances of density 1 are the hardest, and therefore is a step toward answering the open question in the affirmative. Our results stem from a novel combinatorial analysis of mixings of earlier algorithms for SUBSET SUM and a study of an extremal question in additive combinatorics connected to the problem of Uniquely Decodable Code Pairs in information theory.\n\n## Task\nCompose an abstract for the title 'Subset Sum in the Absence of Concentration.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We study the exact time complexity of the Subset Sum problem. Our focus is on instances that lack additive structure in the sense that the sums one can form from the subsets of the given integers are not strongly concentrated on any particular integer value. We present a randomized algorithm that runs in O(2(0.3399n) B-4) time on instances with the property that no value can arise as a sum of more than B different subsets of the n given integers."
      }
    ]
  },
  {
    "tid":"66",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Understanding the Intention of Information Contribution to Online Feedback Systems from Social Exchange and Motivation Crowding Perspectives\nContent: The online feedback system (OFS) has been touted to be an effective artifact for electronic word-of-mouth (EWOM). Accumulating sufficient detailed consumption information in the OFS is essential to the success of OFS. Yet, past research has focused on the effects of OFS on building trust and promoting sales and little knowledge about information provision to OFS has been developed. This study attempts to fill this gap by developing and testing a theoretical model to identify the possible antecedents that lead to the intention of consumers' information contribution to OFS. The model employs social exchange theory to identify benefit and cost factors influencing consumer intention, and motivation crowding theory to explore the moderating effects from environmental interventions that are embodied in OFS. Our preliminary results in general provide empirical support for the model. Practical implications are offered to OFS designers for system customization.\n\nTitle: An empirical study of the effects of interactivity on web user attitude\nContent: Despite the growing attention given to Web usability, little is understood as to what Web design features contribute to Web users' attitude, a major component of the usability of a Web site. This research investigates the effects of interactivity level on Web user's attitude towards commercial Web sites. It extends existing Web interface design and usability literature by empirically examining the critical roles of interactivity. Three Web sites with different levels of interactivity were compared in a controlled laboratory experiment. Three eighteen-person groups completed each treatment. The independent variable is the incremental levels of interactivity. The dependent variables are satisfaction, effectiveness, efficiency, value, and attitude towards the Web site. Results suggest that increased level of interactivity on a Web site have positive effects on user's perceived satisfaction, effectiveness, efficiency, value, and overall attitude towards a Web site. Implications for Web site designers and researchers are discussed.\n\nTitle: Conveying information effectively in a virtual world: Insights from synthesized task closure and media richness\nContent: Scholars and practitioners alike increasingly emphasize the importance of the virtual world as a new medium of communication. Key to the success of this digital medium is its ability to support information exchange when compared with face-to-face communication. Its potential is highlighted by the literature illustrating the inadequacy of traditional computer-mediated communication (CMC) tools, such as e-mail and video conferencing, to support communication among geographically dispersed coworkers. Many of the traditional CMC tools lack the needed support for effective information exchange to varying degrees. The emergence of a sophisticated virtual world, such as Second Life, has met this dearth. We draw on the theories of task closure and media richness to propose a parsimonious model of information exchange behavior in a virtual world context. Observations from a series of group-based project discussion sessions in face-to-face and virtual world settings, respectively, suggest that the information exchange between coworkers in both settings could be similar. Specifically, virtual coworkers might be able to achieve task closure (i.e., the complete transmission of intended work-related information) in the same way as their counterparts in the face-to-face context. \u00a9 2012 Wiley Periodicals, Inc.\n\nTitle: Students' participation intention in an online discussion forum: Why is computer-mediated interaction attractive?\nContent: Anecdotal evidence indicates that an online discussion forum may not be utilized to its full potential in enhancing the effectiveness and efficiency of teaching due to a lower than expected student participation rate. This paper seeks to identify the motivational behavioral factors influencing students' intention to participate in an online discussion forums (ODF). Drawing on the literature on social psychology and applying the theory of reasoned action, we develop a conceptual model of intention to participate in an online discussion forum and empirically test the hypotheses in a cross-sectional quantitative survey. The findings indicate that expectancy on hedonic outcome and utilitarian outcome and peer pressure positively influence the participation intention of students. Also, the perceived importance of learning positively moderates the relationship between utilitarian outcome expectancy and participation intention. Theoretical and practical implications of the findings are discussed.\n\nTitle: Direct and Indirect Information System Use: A Multimethod Exploration of Social Power Antecedents in Healthcare\nContent: AbstractWhen an organization introduces an information system IS, management often expects employees to utilize the system. However, rather than use the system directly, designated users may delegate some parts of their IS use to other users while assuming primary responsibility and accountability for the system use tasks. This behavior is called indirect IS use, a behavior that is acknowledged in some studies but not adequately scrutinized. This research distinguishes between direct and indirect IS use and proposes a model of their respective antecedents. The research was carried out in three stages. First, we conceptually distinguished direct and indirect IS use and drew on a social power lens to develop a theoretically driven research framework. Second, we contextually refined the research framework with a case study in a Confucian society. Third, we validated the research model through a survey of 213 Chinese physicians. Results revealed that direct and indirect IS use are partially substitutive, as demonstrated by the varying effects of social power and physiological power on these types of IS use. Further insights were gained by splitting the two forms of IS use based on the extent of clinical accountability risks and combining the two types of IS use based on their sum or relative amount. With the distinct relationships between social power and different forms of IS use, management could employ the appropriate strategies to influence the desired system usage behavior from its employees.\n\n## Task\nCompose an abstract for the title 'An empirical study of information contribution to online feedback systems: A motivation perspective' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Online feedback systems (OFSs) are increasingly available on online shopping websites; they allow consumers to post their ratings and consumption reviews for products. We employed motivation theory and a goal attainment perspective to model a set of motivating and inhibiting factors that could influence a consumer's intention to contribute to an OFS. Our experiment, which involved 168 university students, showed that a consumer's intention to contribute product reviews is influenced by perceived satisfaction gained in helping other consumers, perceived satisfaction gained in influencing the merchant, perceived probability of enhancing self-image, and perceived executional costs. In addition, the presence of an economic rewarding mechanism was found to promote a contribution when a consumer's perceived probability of enhancing self-image was relatively high or when perceived cognitive cost was relatively low. Implications of our findings are discussed."
      }
    ]
  },
  {
    "tid":"67",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: An Automated Method For Finding Curves Of Sulcal Fundi On Human Cortical Surfaces\nContent: We present a method for automatically finding curves representing the sulcal fundi on the human brain cortex. A flattened map of the cortical surface is used as the reference space in which the curves are modeled. The map is also used to transfer planar curves back to the cortical surface to extract sulcal fundal curves. Instead of modeling the curves by densely sampled landmark points, as it is done in the traditional active shape models, we model sulcal curves by a small number of anchor points that correspond to salient features, such as end points or points of intersections. The full sulcal curves connecting the anchor points are reconstructed by an extension of the fast marching method. Each anchor point carries a wavelet based attribute vector whose goal is to provide a distinctive morphological signature for the anchor point. This allows us to efficiently solve the problem in a low-dimensional space. Moreover, because each anchor point has this signature, and because anchor points are chosen to be salient features, the cost function defined in this low-dimensional space is presumed to have few local minima. Experimental results show that the sulcal curves extracted using the automatic method agrees well with the manually drawn sulcal curves.\n\nTitle: Mapping the Cerebral Sulci: Application to Morphological Analysis of the Cortex and to Non-rigid Registration\nContent: We propose a methodology for extracting parametric representations of the cerebral sulci from magnetic resonance images, and we consider its application to two medical imaging problems: quantitative morphological analysis and spatial normalization and registration of brain images. Our methodology is based on deformable models utilizing characteristics of the cortical shape. Specifically, a parametric representation of a sulcus is determined by the motion of an active contour along the medial surface of the corresponding cortical fold. The active contour is initialized along the outer boundary of the brain, and deforms toward the deep edge of a sulcus under the influence of an external force field restricting it to lie along the medial surface of the particular cortical fold. A parametric representation of the surface is obtained as the active contour traverses the sulcus. In this paper we present results of this methodology and its applications.\n\nTitle: Using a statistical shape model to extract sulcal curves on the outer cortex of the human brain.\nContent: A method for automated segmentation of major cortical sulci on the outer brain boundary is presented, with emphasis on automatically determining point correspondence and on labeling cortical regions. The method is formulated in a general optimization framework defined on the unit sphere, which serves as parametric domain for convoluted surfaces of spherical topology. A statistical shape model, which includes a network of deformable curves on the unit sphere, seeks geometric features such as high curvature regions and labels such features via a deformation process that is confined within a spherical map of the outer brain boundary. The limitations of the customary spherical coordinate system, which include discontinuities at the poles and nonuniform sampling, are overcome by defining the statistical prior of shape variation in terms of projections of landmark points onto corresponding tangent planes of the sphere. The method is tested against and shown to be as accurate as manually defined segmentations.\n\nTitle: Automatic classification of sulcal regions of the human brain cortex using pattern recognition\nContent: Parcellation of the cortex has received a great deal of attention in magnetic resonance (MR) image analysis, but its usefulness has been limited by time-consuming algorithms that require manual labeling. An automatic labeling scheme is necessary to accurately and consistently parcellate a large number of brains. The large variation of cortical folding patterns makes automatic labeling a challenging problem, which cannot be solved by deformable atlas registration alone. In this work, an automated classification scheme that consists of a mix of both atlas driven and data driven methods is proposed to label the sulcal regions, which are defined as the gray matter regions of the cortical surface surrounding each sulcus. The premise for this algorithm is that sulcal regions can be classified according to the pattern of anatomical features (e.g. supramarginal gyrus, cuneus, etc.) associated with each region. Using a nearest-neighbor approach, a sulcal region is classified as being in the same class as the sulcus from a set of training data which has the nearest pattern of anatomical features. Using just one subject as training data, the algorithm correctly labeled 83% of the regions that make up the main sulci of the cortex.\n\nTitle: Deformable registration of cortical structures via hybrid volumetric and surface warping.\nContent: Registration of cortical structures across individuals is a very important step for quantitative analysis of the human brain cortex. This paper presents a method for deformable registration of cortical structures across individuals, using hybrid volumetric and surface warping. In the first step, a feature-based volumetric registration algorithm is used to warp a model cortical surface to the individual's space. This step greatly reduces the variation between the model and individual, thus providing a good initialization for the next step of surface warping. In the second step, a surface registration method, based on matching geometric attributes, warps the model surface to the individual. Point correspondences are also established at this step. The attribute vector, as the morphological signature of surface, was designed to be as distinctive as possible, so that each vertex on the model surface can find its correspondence on the individual surface. Experimental results on both synthesized and real brain data demonstrate the performance of the proposed method in the registration of cortical structures across individuals.\n\n## Task\nCompose an abstract for the title 'Finding parametric representations of the cortical sulci using an active contour model' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The cortical sulci are brain structures resembling thin convoluted ribbons embedded in three dimensions. The importance of the sulci lies primarily in their relation to the cytoarchitectonic and functional organization of the underlying cortex and in their utilization as features in non-rigid registration methods. This paper presents a methodology for extracting parametric representations of the cerebral sulci from magnetic resonance images. The proposed methodology is based on deformable models utilizing characteristics of the cortical shape. Specifically, a parametric representation of a sulcus is determined by the motion of an active contour along the medial surface of the corresponding cortical fold. The active contour is initialized along the outer boundary of the brain and deforms toward the deep root of a sulcus under the influence of an external force field, restricting it to lie along the medial surface of the particular cortical fold. A parametric representation of the medial surface of the sulcus is obtained as the active contour traverses the sulcus. Based on the first fundamental form of this representation, the location and degree of an interruption of a sulcus can be readily quantified; based on its second fundamental form, shape properties of the sulcus can be determined. This methodology is tested on magnetic resonance images and it is applied to three medical imaging problems: quantitative morphological analysis of the central sulcus; mapping of functional activation along the primary motor cortex and non-rigid registration of brain images."
      }
    ]
  },
  {
    "tid":"68",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Optimal Combination of Base Station Densities for Energy-Efficient Two-Tier Heterogeneous Cellular Networks\nContent: In this paper, the optimal BS (Base Station) density for both homogeneous and heterogeneous cellular networks to minimize network energy cost is analyzed with stochastic geometry theory. For homogeneous cellular networks, both upper and lower bounds of the optimal BS density are derived. For heterogeneous cellular networks, our analysis reveals the best type of BSs to be deployed for capacity extension, or to be switched off for energy saving. Specifically, if the ratio between the micro BS cost and the macro BS cost is lower than a threshold, which is a function of path loss and their transmit power, then the optimal strategy is to deploy micro BSs for capacity extension or to switch off macro BSs (if possible) for energy saving with higher priority. Otherwise, the optimal strategy is the opposite. The optimal combination of macro and micro BS densities can be calculated numerically through our analysis, or alternatively be conservatively approximated with a closed-form solution. Based on the parameters from EARTH, numerical results show that in the dense urban scenario, compared to the traditional macro-only homogeneous cellular network with no BS sleeping, deploying micro BSs can reduce about 40% of the total energy cost, and further reduce up to 35% with BS sleeping capability.\n\nTitle: On Optimal Relay Placement and Sleep Control to Improve Energy Efficiency in Cellular Networks\nContent: We consider the joint optimization of relay station (RS) placement and RS sleep/active probability to enhance the energy efficiency of a one-dimensional cellular network. When the RSs are always active, conditions for optimal RS placement that minimizes transmission power are derived, based on which closed-form solution is obtained with path-loss exponent being two, and a simple numerical method for general values of path-loss exponent is proposed. When the circuit power consumption of active RSs is considered, RSs should enter sleep mode appropriately to save power. An algorithm based on projected Newton method is proposed to jointly optimize the RS placement and sleep/active probability. It is shown via numerical examples that the benefit of implementing RSs and optimizing RS placement is substantial and increases with the path-loss exponent. We also justify the interaction between RS placement and RS sleep control, which is effectively tackled by the proposed algorithm to minimize the total power consumption.\n\nTitle: Optimal base station density for energy-efficient heterogeneous cellular networks\nContent: In this paper, we adopt stochastic geometry theory to analyze the optimal macro/micro BS (base station) density for energy-efficient heterogeneous cellular networks with QoS constraints. We first derive the upper and lower bounds of the optimal BS density for homogeneous scenarios and, based on these, we analyze the optimal BS density for heterogeneous networks. The optimal macro/micro BS density can be calculated numerically through our analysis, and the closed-form approximation is also derived. Our results reveal the best type of BSs to be deployed for capacity extension, or to be switched off for energy saving. Specifically, if the ratio between the micro BS cost and the macro BS cost is lower than a threshold, which is a function of path loss and their transmit power, the micro BSs are preferred, i.e., deploy more micro BSs for capacity extension or switch off certain macro BSs for energy saving. Otherwise, the optimal choice is the opposite. Our work provides guidance for energy efficient cellular network planning and dynamic operation control.\n\nTitle: Toward dynamic energy-efficient operation of cellular network infrastructure\nContent: The operation of cellular network infrastructure incurs significant electrical energy consumption. From the perspective of cellular network operators, reducing this consumption is not only a matter of showing environmental responsibility, but also of substantially reducing their operational expenditure. We discuss how dynamic operation of cellular base stations, in which redundant base stations are switched off during periods of low traffic such as at night, can provide significant energy savings. We quantitatively estimate these potential savings through a first-order analysis based on real cellular traffic traces and information regarding base station locations in a part of Manchester, United Kingdom. We also discuss a number of open issues pertinent to implementing such energy-efficient dynamic base station operation schemes, such as various approaches to ensure coverage, and interoperator coordination.\n\nTitle: Energy-Efficient Cellular Network Planning under Insufficient Cell Zooming\nContent: With rapid growth of cellular systems, energy consumption has become a critical issue. The existing cellular network planning is performance-oriented, whose objective is to satisfy peak traffic requirements, without too much considerations on energy efficiency. Unfortunately, real-world traffic profiles have indicated that in most time, most of the cells are in low utilization. Switching off certain cells in low traffic period for some time is proved energy-efficient. To switch off cells, remaining operating cells need to extend their coverage by cell zooming to guarantee service. However, such zooming might be insufficient, depending greatly on cell configurations. In this paper, we consider energy efficiency in cellular network planning. We introduce a new parameter for traffic estimation, which is low traffic time ratio \u03c4 . In order to switch off more cells for insufficient cell zooming, two solutions are feasible: to deploy smaller but more cells or to implement coverage extension technologies. We focus on former solution to determine cell configurations and propose an evaluation method to determine whether certain cell deployment is energy-efficient and how much energy it could save, compared with traditional planning. It is shown that when cell zooming ratio is reaching sufficient for certain switching-off scheme, deploying more cells could be more energy-efficient. Also, after exceeding the threshold, the larger the parameter \u03c4 is, the more energy-efficient our solution is.\n\n## Task\nCompose an abstract for the title 'Improving the Energy Efficiency of Two-Tier Heterogeneous Cellular Networks through Partial Spectrum Reuse' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Partial Spectrum Reuse (PSR) in the second tier of two-tier heterogeneous cellular networks has a potential to improve spectrum efficiency by reducing inter-cell interference, and thus energy efficiency as well by deploying less or switching off more Base Stations (BSs). In this paper, we analyze the optimal PSR factor, defined as the portion of spectrum reused by micro cells in two-tier heterogeneous networks, which is not in an explicit form generally. Then, a closed-form limit of the optimal PSR factor is derived as the ratio of the user rate requirement over the whole system spectrum bandwidth is approaching zero, based on which a threshold of the micro-BS energy cost is also derived to determine which type of BSs is preferable. Specifically, one should deploy more micro BSs or switch off more macro BSs if the micro-BS energy cost is lower than the threshold. Otherwise, the optimal choice is the opposite. This threshold with the PSR scheme is higher than that without PSR scheme, i.e., PSR can improve both spectrum efficiency and energy efficiency. Numerical results show that adopting PSR can reduce the network energy consumption by up to 50% when the transmit power of macro BSs is 10dB higher than that of micro BSs."
      }
    ]
  },
  {
    "tid":"69",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: RT-ROS: A real-time ROS architecture on multi-core processors\nContent: ROS, an open-source robot operating system, is widely used and rapidly developed in the robotics community. However, running on Linux, ROS does not provide real-time guarantees, while real-time tasks are required in many robot applications such as robot motion control. This paper for the first time presents a real-time ROS architecture called RT-RTOS on multi-core processors. RT-ROS provides an integrated real-time/non-real-time task execution environment so real-time and non-real-time ROS nodes can be separately run on a real-time OS and Linux, respectively, with different processor cores. In such a way, real-time tasks can be supported by real-time ROS nodes on a real-time OS, while non-real-time ROS nodes on Linux can provide other functions of ROS. Furthermore, high performance is achieved by executing real-time ROS nodes and non-real-time ROS nodes on different processor cores. We have implemented RT-ROS on a dual-core processor and conducted various experiments with real robot applications. The experimental results show that RT-ROS can effectively provide real-time support for the ROS platform with high performance by exploring the multi-core architecture.\n\nTitle: Formal Analysis and Verification of DDS in ROS2.\nContent: Robot Operating System(ROS) is widely used in the development of various service robots, which have high requirements in safety and reliability. Data distribution service(DDS) is deployed in ROS2 to implement the control of nodes and data distribution, which is very important for the operation of application on ROS2. In this paper, we focus on model abstraction, formal modeling and automatic verification for DDS in ROS2. A distributed model with interface parameters was established in PRISM. Properties such as security, liveness and priority of DDS in ROS2 were verified. The results show that the design satisfies these properties and high-priority messages can be sent and transmitted preferentially by the system. Due to the experiment result, we come up with a suggestion that the system will be better if its buffer can store six messages or more.\n\n\n\nTitle: Runtime Verification of Robots Collision Avoidance Case Study\nContent: The robot has attracted much attention to anticipate improved quality of human life. Real-time obstacle avoidance is one of hot spots of the research. Runtime verification is a real-time and lightweight verification technology to verify the properties in many fields. In this case study, we use the JavaMOP, a runtime verification tool to verify the implementations' correctness of the safety strategies for avoiding collision as a complement to the design. The design of the safety strategies can be classified as the pre-contact safety strategy and the post-contact safety strategy. The former can avoid obstacles and the latter can reduce the physical damage after a collision. Additionally, this case study also proposes a new method of dynamic parameter selection. It can automatically update the parameters during the operation of the robot without having to get familiar with and to modify robot programs. Because some special parameters may change as mutative factors or be updated by engineers' experience in the uncertain environment, they cannot be fixed. We follow the JavaMOP specification to describe informal requirements using the FSM and ptLTL languages. Finally, the experimental results verify the correctness of the safety strategies and the effectiveness of dynamic parameter selection.\n\nTitle: Timed automata based motion planning for a self-assembly robot system\nContent: Sambot is a module robot system, with the advantages of self-assembly. A target robotic configuration can be organized by a group of Sambots. A novel motion planning method for Sambot configuration using model checking is presented in this paper. This hierarchical method contains two layers. The abstract logic layer is responsible for the discrete planning of Sambots configuration. The robot and the environment are all modeled as timed automata. System requirements are formalized as Computational Tree Logic (CTL) formulas. Model checking is applied on the system model. The verification result gives the optimal discrete plans for the configuration of Sambot. In physical layer, a sample-based planner generates the trajectory trace considering the dynamics of Sambot and the suggested high level plans. The experiment results illustrate the effectiveness of our approach.\n\nTitle: Dependable Model-driven Development of CPS: From Stateflow Simulation to Verified Implementation.\nContent: Simulink is widely used for model-driven development (MDD) of cyber-physical systems. Typically, the Simulink-based development starts with Stateflow modeling, followed by simulation, validation, and code generation mapped to physical execution platforms. However, recent trends have raised the demands of rigorous verification on safety-critical applications to prevent intrinsic development faults and improve the system dependability, which is unfortunately challenging. Even though the constructed Stateflow model and the generated code pass the validation of Simulink Design Verifier and Simulink Polyspace, respectively, the system may still fail due to some implicit defects contained in the design model (design defect) and the generated code (implementation defects).\n\nIn this article, we bridge the Stateflow-based MDD and a well-defined rigorous verification to reduce development faults. First, we develop a self-contained toolkit to translate a Stateflow model into timed automata, where major advanced modeling features in Stateflow are supported. Taking advantage of the strong verification capability of Uppaal, we can not only find bugs in Stateflow models that are missed by Simulink Design Verifier but also check more important temporal properties. Next, we customize a runtime verifier for the generated non-intrusive VHDL and C code of a Stateflow model for monitoring. The major strength of the customization is the flexibility to collect and analyze runtime properties with a pure software monitor, which offers more opportunities for engineers to achieve high reliability of the target system compared with the traditional act that only relies on Simulink Polyspace. In this way, safety-critical properties are both verified at the model level and at the consistent system implementation level with physical execution environment in consideration. We apply our approach to the development of a typical cyber-physical system-train communication controller based on the IEC standard 61375. Experiments show that more ambiguousness in the standard are detected and confirmed and more development faults and those corresponding errors that would lead to system failure have been removed. Furthermore, the verified implementation has been deployed on real trains.\n\n\n\n## Task\nCompose an abstract for the title 'RGMP-ROS: A real-time ROS architecture of hybrid RTOS and GPOS on multi-core processor' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Recently, the open-source robot operating system (ROS) has been growing rapidly in the robotics community. However, the ROS runs on Linux, which does not provide timing guarantees for robot motion. This paper present a hybrid real-time ROS architecture on multi-core processor \u201cRGMP-ROS\u201d, which consists of two parts including the non-real-time subsystem \u201cGPOS (General Operating system)\u201d and the real-time one \u201cRTOS (Real-time Operating system)\u201d. The GPOS is comprised of non-real-time ROS nodes running in Linux, while the RTOS only contains real-time ROS nodes running in Nuttx. To get higher operational efficiency, the RGMP-ROS system is executed by a dual-core processor, one CPU for GPOS and the other for RTOS. The RGMP-ROS has used in the controller of a 6-DOF modular manipulator, and its effectiveness and efficiency are demonstrated by software testing and experiments. The main contributions of the present work lie in the realization of real-time ROS architecture and the application of multi-core processor in the hybrid control of an industrial robot."
      }
    ]
  },
  {
    "tid":"70",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Improving music auto-tagging by intra-song instance bagging\nContent: Bagging is one the most classic ensemble learning techniques in the machine learning literature. The idea is to generate multiple subsets of the training data via bootstrapping (random sampling with replacement), and then aggregate the output of the models trained from each subset via voting or averaging. As music is a temporal signal, we propose and study two bagging methods in this paper: the inter-song instance bagging that bootstraps song-level features, and the intra-song instance bagging that draws bootstrapping samples directly from short-time features for each training song. In particular, we focus on the latter method, as it better exploits the temporal information of music signals. The bagging methods result in surprisingly effective models for music auto-tagging: incorporating the idea to a simple linear support vector machine (SVM) based system yields accuracies that are comparable or even superior to state-of-the-art, possibly more sophisticated methods for three different datasets. As the bagging method is a meta algorithm, it holds the promise of improving other MIR systems.\n\nTitle: Towards time-varying music auto-tagging based on CAL500 expansion\nContent: Music auto-tagging refers to automatically assigning semantic labels (tags) such as genre, mood and instrument to music so as to facilitate text-based music retrieval. Although significant progress has been made in recent years, relatively little research has focused on semantic labels that are time-varying within a track. Existing approaches and datasets usually assume that different fragments of a track share the same tag labels, disregarding the tags that are time-varying (e.g., mood) or local in time (e.g., instrument solo). In this paper, we present a new dataset dedicated to time-varying music auto-tagging. The dataset, called CAL500exp, is an enriched version of the well-known CAL500 dataset used for conventional track-level tagging. Given the tag set of CAL500, eleven subjects with strong music background were recruited to annotate the time-varying tag labels. A new user interface for annotation is developed to reduce the subject's annotation effort yet increase the quality of labels. Moreover, we present an empirical evaluation that demonstrates the performance improvement CAL500exp brings about for time-varying music auto-tagging. By providing more accurate and consistent descriptions of music content in a finer granularity, CAL500exp may open new opportunities to understand and to model the temporal context of musical semantics.\n\nTitle: Music retagging using label propagation and robust principal component analysis\nContent: The emergence of social tagging websites such as Last.fm has provided new opportunities for learning computational models that automatically tag music. Researchers typically obtain music tags from the Internet and use them to construct machine learning models. Nevertheless, such tags are usually noisy and sparse. In this paper, we present a preliminary study that aims at refining (retagging) social tags by exploiting the content similarity between tracks and the semantic redundancy of the track-tag matrix. The evaluated algorithms include a graph-based label propagation method that is often used in semi-supervised learning and a robust principal component analysis (PCA) algorithm that has led to state-of-the-art results in matrix completion. The results indicate that robust PCA with content similarity constraint is particularly effective; it improves the robustness of tagging against three types of synthetic errors and boosts the recall rate of music auto-tagging by 7% in a real-world setting.\n\nTitle: Multipitch Estimation of Piano Music by Exemplar-Based Sparse Representation\nContent: Pitch, together with other midlevel music features such as rhythm and timbre, holds the promise of bridging the semantic gap between low-level features and high-level semantics for music understanding. This paper investigates the pitch estimation of a piano music signal by exemplar-based sparse representation. A note exemplar is a segment of a piano note, stored in the dictionary. We first describe how to represent a segment of the piano music signal as a linear combination of a small number of note exemplars from a large note exemplar dictionary and then show how the sparse representation problem can be solved by $l_{1}$-regularized minimization. The proposed approach incorporates tuning factor estimation, note candidate selection, and hidden-Markov-model-based smoothing into the estimation process to improve accuracy. Unlike previous approaches, the proposed approach does not require retraining for a new piano. Instead, only a dozen notes of the new piano are needed. This feature is computationally attractive and avoids intense manual labeling. The system performance is evaluated using 70 classical music recordings of two real pianos under different recording conditions. The results show that the proposed system outperforms four state-of-the-art systems.\n\nTitle: Modeling the Affective Content of Music with a Gaussian Mixture Model\nContent: Modeling the association between music and emotion has been considered important for music information retrieval and affective human computer interaction. This paper presents a novel generative model called acoustic emotion Gaussians (AEG) for computational modeling of emotion. Instead of assigning a music excerpt with a deterministic (hard) emotion label, AEG treats the affective content of music as a (soft) probability distribution in the valence-arousal space and parameterizes it with a Gaussian mixture model (GMM). In this way, the subjective nature of emotion perception is explicitly modeled. Specifically, AEG employs two GMMs to characterize the audio and emotion data. The fitting algorithm of the GMM parameters makes the model learning process transparent and interpretable. Based on AEG, a probabilistic graphical structure for predicting the emotion distribution from music audio data is also developed. A comprehensive performance study over two emotion-labeled datasets demonstrates that AEG offers new insights into the relationship between music and emotion (e.g., to assess the \u201caffective diversity\u201d of a corpus) and represents an effective means of emotion modeling. Readers can easily implement AEG via the publicly available codes. As the AEG model is generic, it holds the promise of analyzing any signal that carries affective or other highly subjective information.\n\n## Task\nCompose an abstract for the title 'IMPROVING MUSICAL CONCEPT DETECTION BY ORDINAL REGRESSION AND CONTEXT FUSION' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Tofacilitateinformationretrievaloflarge-scalemusicdata- bases, the detection of musical concepts, or auto-tagging, has been an active research topic. This paper concerns the use of concept correlations to improve musical concept de- tection. We propose to formulate concept detection as an ordinal regression problem to explicitly take advantage of the ordinal relationship between concepts and avoid the data imbalance problem of conventional multi-label clas- sification methods. To further improve the detection ac- curacy, we propose to leverage the co-occurrence patterns of concepts for context fusion and employ concept selec- tion to remove irrelevant or noisy concepts. Evaluation on the cal500 dataset shows that we are able to improve the detection accuracy of 174 concepts from 0.2513 to 0.2924."
      }
    ]
  },
  {
    "tid":"71",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A distributed coverage optimization and connectivity maintenance strategy based on unknown sensors in WSAN\nContent: Wireless sensor and actor network (WSAN) has received growing interests with the rapid development of Internet of things. Multiple function actors are added into the traditional wireless sensor networks to sense the environment, conduct tasks and give feedback. In this paper, an efficient distributed actor deployment strategy is proposed to address the connectivity maintenance and coverage optimization issues in WSAN with the consideration of unknown sensors' locations. In this proposed strategy, an artificial potential force based algorithm is utilized to detect the unknown sensors, and a fully distributed link deletion algorithm is introduced to delete redundant communication links thus expanding the search area. Then, an actor location optimization algorithm is developed based on the Voronoi diagram method to get a better coverage result. The global connectivity of the actors is guaranteed during the entire process. The performances of proposed strategy are validated analytically and experimentally.\n\nTitle: Connectivity preserving localized coverage algorithm for area monitoring using wireless sensor networks\nContent: Efficient network coverage and connectivity are the requisites for most Wireless Sensor Network (WSN) deployments, particularly those concerned with area monitoring. Due to the resource constraints of the sensor nodes, redundancy of coverage area must be reduced for effective utilization of the available resources. If two nodes have the same coverage area in their active state, and if both the nodes are activated simultaneously, it leads to redundancy in network and wastage of precious sensor resources. In this paper, we address the problem of network coverage and connectivity and propose an efficient solution to maintain coverage, while preserving the connectivity of the network. The proposed solution aims to cover the area of interest (AOI), while minimizing the count of the active sensor nodes. The overlap region of two sensor nodes varies with the distance between the nodes. If the distance between two sensor nodes is maximized, the overall coverage area of these nodes will also be maximized. Also, to preserve the connectivity of the network, each sensor node must be in the communication range of at least one other node. Results of simulation of the proposed solution indicate up to 95% coverage of the area, while consuming very less energy of 9.44J per unit time in the network, simulated in an area of 2500m^2.\n\nTitle: A high efficient node capture attack algorithm in wireless sensor network based on route minimum key set.\nContent: Wireless sensor networks are often deployed in hostile and unattended environment that are very prone to node capture attack. In node capture attack, information such as key, data on captured nodes, can all be extracted by the adversary. Node capture attack in wireless sensor networks suffers from low efficiency and high resource expenditure. To enhance the efficiency of node capture attack, we propose here a high efficiency node capture attack algorithm that is based on route minimum key set, namely greedy node captured based on route minimum key set (GNRMK). To obtain the route minimum key set, the sensor network is mapped as a flow network. The route minimum key set can be calculated by the maximum flow of the flow network. Then, an overlapping value is assigned to each node on the basis of route minimum key set. The node with maximum overlapping value will be captured in every round of attack. Simulation results indicate that, compared with other node capture attack schemes, GNRMK can compromise the network by capturing fewer nodes. Moreover, the fraction of traffic compromised is much higher. Copyright (c) 2012 John Wiley & Sons, Ltd.\n\nTitle: An ant colony optimization approach for reputation and quality-of-service-based security in wireless sensor networks\nContent: In wireless sensor networks (WSN), message security is an important concern. The protection of integrity and confidentiality of information and the protection from unauthorized access are important issues. However, due to factors such as resource limitations, absence of centralized access points, open wireless medium and small size of the sensor nodes, the implementation of security in WSN is a challenging task. In this paper, we propose a protocol, quality-based distance vector routing (QDV), for securing WSN using concepts based on Ant colony optimization ACO [1]. Two fundamental parameters-quality-of-service (QoS) and reputation [2]-are used. The high value of reputation of a node signifies that the node is trusted and is more reliable for data communication purposes. As a node shows signs of misbehavior, its reputation decreases, which, in turn, affects its quality-of-security QSec [2], thereby disabling the malicious nodes from gaining access to the network. By incorporating these two factors, we are able to distinguish the nodes present in the network. We, then, present a method to achieve \"equilibrium\" where the node is able to guarantee that its neighbors are secure. Copyright (C) 2008 John Wiley & Sons, Ltd.\n\nTitle: Energy-Efficient Connectivity Re-Establishment In Wsn In The Presence Of Dumb Nodes\nContent: In this work, we propose a scheme, named LECRAD, for the re-construction of temporarily lost connectivity in the presence of dumb nodes in stationary wireless sensor networks. A sensor node is termed as \"dumb\", when it continues its physical sensing, but fails to communicate due to shrinkage in communication range, typically attributed to adverse environmental effects such as rainfall, fog and high temperature. On the resumption of favorable environmental conditions, the node starts to behave normally. So, dumb behavior is dynamic in nature. Such behavior of a node leads to network partitioning and node isolation, which results in disruption of connected topology. Therefore, the proposed scheme in this paper reconstructs the lost connectivities between nodes by activating intermediate sleep nodes or by adjusting the communication range of the sensor nodes, while there is no neighbor node within the reduced communication range. In the proposed scheme, a learning automata-based approach is used for activation of intermediate sleep nodes or adjustment of communication range of isolated or intermediate nodes to decrease the message overhead and energy consumption of the network. Simulation results show that the proposed scheme, LECRAD, exhibits better energy efficiency and message overhead than that of the recently proposed topology management protocols, LETC and A1, if they are applied in such scenario.\n\n## Task\nCompose an abstract for the title 'A Min-Actor algorithm for connected dominating set formation in WSAN' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"As the virtual backbone of the network, connected dominating set (CDS) plays an important role in supporting data communication, reducing the routing overhead, and enhancing the scalability of the network. Traditional CDS construction algorithms are usually applied to the networks with homogeneous nodes, e.g., a pure wireless sensor network. In the past few years, Wireless Sensor and Actor Networks (WSANs) have attracted growing attentions due to its capabilities of sensing and gathering information, as well as performing certain tasks by actor nodes. One of the most critical issues in WSAN is how to construct an appropriate CDS with the consideration of heterogeneous nodes. Specifically, how to construct the CDS that contains minimum number of actors, so that the movement of actors poses a minimal threat to the integrity of the CDS. In this paper, we take the nature of the actor nodes into consideration and propose a CDS construction algorithm named Min-Actor algorithm (MIA), which can reduce nodes' movement constraints in WSAN. Simulation results confirm that MIA can effectively reduce the number of the actors in CDS, thus reducing the impact of actors' movement on the network."
      }
    ]
  },
  {
    "tid":"72",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Familiarity or conceptual priming: event-related potentials in name recognition.\nContent: Recent interest has been drawn to the separate components of recognition memory, as studied by event-related potentials (ERPs). In ERPs, recollection is usually accompanied by a late, parietal positive deflection. An earlier, frontal component has been suggested to be a counterpart, accompanying recognition by familiarity. However, this component, the FN400, has alternatively been suggested to reflect a form of implicit memory, conceptual priming. The present study examined the ERP components of recognition memory using an episodic memory task with a stimulus material consisting of names, half of which were famous. Along a different dimension, the names varied in how rare or common they were. These dimensions, frequency and fame, exerted powerful effects on memory accuracy, and dissociated the two recognition processes, such that frequency gave rise to familiarity and fame fostered recollection, when the receiver operating characteristics data were analyzed with Yonelinas' dual-process signal detection model. The ERPs corresponded fully to the behavioral data because frequency affected the frontal component exclusively, and fame affected the parietal component exclusively. Moreover, a separate behavioral experiment showed that conceptual priming was sensitive to fame, but not to frequency. Our data therefore indicate that the FN400 varies jointly with familiarity, but independently of conceptual priming.\n\nTitle: Visual quality of the ground in 3D models: using color-coded images to blend aerial photos with tiled detail-textures\nContent: In interactive 3D-environments of urban-planning projects, the ground materials and land features are often represented by aerial photos. These lack of detail and therefore often look blurred and diffuse. This paper presents a technique to enhance the visual quality of the ground by blending the aerial photos with tiled detail-textures in a level-of-detail integrated system that is controlled by color-coded images. The color-coded images are created by extracting land features such as grass, asphalt, rock and water from the aerial photos or 2D CAD-drawings. The effects of different resolutions of the aerial photo and color-coded image are presented and discussed. To evaluate this approach, the technique was tested on a simple rendering performance scene and on two sub-sets of the 3D-city model of G\u00f6teborg, namely a landscape and an urban data set. The visual quality and rendering speed of the 3D model were positive and are presented in several images and table.\n\nTitle: A multi-agent projected dual gradient method with primal convergence guarantees.\nContent: We consider a class of multi-agent optimization problems, where each agent is endowed with a strongly convex (but not necessarily differentiable) loss function and is subject to individual constraints composed of linear equalities, convex inequalities, and convex set constraints. We derive a novel algorithm that allows the agents to collaboratively reach a decision that minimizes the sum of the loss functions over the intersection of the individual constraints. The algorithm is based on a projected dual gradient technique and exploits the structure of the individual constraint sets to avoid costly projections. A convergence rate analysis shows that the primal iterates produced by individual agents under our algorithm converge to the primal optimal solution at a rate that is superior to alternatives in the literature. Finally, we provide a thorough comparison of our algorithm with alternatives in terms of both theoretical and algorithmic aspects.\n\nTitle: Electrophysiological correlates of retrieval orientation in reality monitoring.\nContent: Retrieval orientation describes the modulation in the processing of retrieval cues by the nature of the targeted material in memory. Retrieval orientation is usually investigated by analyzing the cortical responses to new (unstudied) material when different memory contents are targeted. This approach avoids confounding effects of retrieval success. We investigated the neural correlates of retrieval orientation in reality monitoring with event-related potentials (ERPs) and assessed the impact of retrieval accuracy on obtained ERP measures. Thirty-two subjects studied visually presented object names that were followed either by a picture of that object (perceived condition) or by the instruction to mentally generate such a picture (imagine condition). Subsequently, subjects had to identify object names of one study condition and reject object names of the second study condition together with newly presented object names. The data analysis showed that object names were more accurately identified when they had been presented in the perceived condition. Two topographically distinct ERP effects of retrieval orientation were revealed: From 600 to 1100ms after stimulus representation, ERPs were more positive at frontal electrode sites when object names from the imagine condition were targeted. The analysis of response-locked ERP data revealed an additional effect at posterior electrode sites, with more negative ERPs shortly after response onset when items from the imagine condition were targeted. The ERP effect at frontal electrode sites, but not at posterior electrode sites was modulated by relative memory accuracy, with stronger effects in subjects who had lower memory accuracy for items of the imagine condition. The findings are suggestive for a contribution of frontal brain areas to retrieval orientation processes in reality monitoring and indicate that neural correlates of retrieval orientation can be modulated by retrieval effort, with stronger activation of these correlates with increasing task demands.\n\nTitle: Emergent Behaviors over Signed Random Networks in Dynamical Environments.\nContent:   We study asymptotic dynamical patterns that emerge among a set of nodes that interact in a dynamically evolving signed random network. Node interactions take place at random on a sequence of deterministic signed graphs. Each node receives positive or negative recommendations from its neighbors depending on the sign of the interaction arcs, and updates its state accordingly. Positive recommendations follow the standard consensus update while two types of negative recommendations, each modeling a different type of antagonistic or malicious interaction, are considered. Nodes may weigh positive and negative recommendations differently, and random processes are introduced to model the time-varying attention that nodes pay to the positive and negative recommendations. Various conditions for almost sure convergence, divergence, and clustering of the node states are established. Some fundamental similarities and differences are established for the two notions of negative recommendations. \n\n## Task\nCompose an abstract for the title '\"do you see yonder cloud?\"-on priming concepts, a new test, and a familiar outcome. reply to lucas et al.: \"familiarity or conceptual priming? good question! comment on stenberg, hellman, johansson, and ros\u00e9n (2009)\"' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Lucas, Voss, and Paller sympathize with our intentions but disagree with our findings. They argue that a relation between frequency and conceptual priming may have been obscured by methodological details in our second experiment, therefore failing to complete a bridge between conceptual priming and FN400 with name frequency as the mediator. However, renewed inspections of our experiment and a new additional experiment, designed to meet the objections, fail to find any role for name frequency in conceptual priming and therefore re-establish the dissociation of priming and the FN400. On closer inspection, our differing views seem to derive from different interpretations of the term \u201cconcept.\u201d"
      }
    ]
  },
  {
    "tid":"73",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Complete Algorithm For Designing Planar Fixtures Using Modular Components\nContent: Commercially available modular fixturing systems typically include a lattice of holes with precise spacing and an assortment of precision locating and clamping modules that can be rigidly attached to the lattice. Currently, machinists manually design a suitable arrangement of these modules to hold a given part. This requires expertise and can delay production. Futhermore, a machinist may in many cases settle upon an arrangement that is not optimal for a given machining operation.In this paper we present an implemented algorithm that accepts a polygonal description of the part silhouette, and efficiently constructs the set of all feasible fixture designs that kinematically constrain the part in the plane. Each fixture is comprised of three locators rigidly attached to the lattice and one sliding clamp, and constrains the part without relying on friction.The algorithm is based on an efficient enumeration of admissible designs that exploits part geometry and a graphical force analysis. The algorithm run time is linear in the number of designs found, which is bounded by a polynomial in the number of part edges and the part's maximal diameter in lattice units. Our review of the literature suggests that this is the first fixturing algorithm that is complete in the sense that it is guaranteed to find all admissible fixture designs for an arbitrary polygonal part silhouette and to identify the optimal fixture relative to an arbitrary quality metric. The algorithm does not consider out-of-plane forces or motions; however, we view this planar result as an essential component of a larger algorithm that solves the 3-D fixture design problem by treating the planar and out-of-plane constraint problems separately. This approach is analogous to the widely used 3-2-1 fixture design heuristic, and appears to be applicable to a broad class of man-made parts.\n\nTitle: Complete algorithms for feeding polyhedral parts using pivot grasps\nContent: To rapidly feed industrial parts on an assembly line, Carlisle et al. (1994) proposed a flexible part feeding system that drops parts on a flat conveyor belt, determines position and orientation of each part with a vision system, and then moves them into a desired orientation. When a part is grasped with two hard finger contacts and lifted, it pivots under gravity into a stable configuration. The authors refer to the sequence of picking up the part, allowing it to pivot, and replacing it on the table as a pivot grasp. The authors show that under idealized conditions, a robot arm with four degrees of freedom (DOF) can move (feed) parts arbitrarily in 6 DOF using pivot grasps. This paper considers the following planning problem: Given a polyhedral part shape, coefficient of friction, and a pair of stable configurations as input, find pairs of grasp points that will cause the part to pivot from one stable configuration to the other. For a part with n faces and m stable configurations, the authors give an O(m2n log n) algorithm to generate the m\u00d7m matrix of pivot grasps. When the part is star-shaped, this reduces to O(m2n). Since pivot grasps may not exist for some transitions, multiple steps may be needed. Alternatively, the authors consider the set of grasps where the part pivots to a configuration within a \u201ccapture region\u201d around the stable configuration; when the part is released, it will tumble to the desired configuration. Both algorithms are complete in that they are guaranteed to find pivot grasps when they exist\n\nTitle: A Complete Algorithm For Fixture Loading\nContent: A fixture is a device for locating and holding parts. Since the initial position and orientation of a part may be uncertain, the act of loading the part into the fixture must compensate for this uncertainty. Machinists often refer to the 3-2-1 rule: place the part onto 3-point contact with a horizontal support plane, slide the part along this plane into 2-point contact with the fixture, then translate along this edge until a 1-point contact uniquely locates the part. Finally, apply clamps (Mani and Wilson 1988; Chou, Chandru, and Barash 1989). This rule of thumb implicitly assumes both sensing and compliance: applied forces change as contacts are detected.In this paper; we geometrically formalize robotic fixture loading as a sensor-based compliant assembly problem and give a complete planning algorithm. We consider the class of modular fixtures that use three locators and one clamp (Brost and Goldberg 1996), and discuss a class of robot commands that cause the part to slide and rotate in the support plane. Sensing is achieved with binary contact sensors on each locator; compliance is achieved with a passive spring-loaded mechanism at the robot end effector. We extend the theory of sensor-based compliant motion planning (Lozano-Perez, Mason, and Taylor 1984; Erdmann 1986) to generalized-polygonal C-spaces, and give a complete planning algorithm: it is guaranteed to find a loading plan when one exists and to return a negative report otherwise. We report on experiments using the resulting plans. Finally we use this formalization to prove a sufficient condition for the 3-2-1 rule.\n\nTitle: A Complete Algorithm for Designing Passive Fences to Orient Parts\nContent: Reviews Peshkin and Sanderson (1988) who showed that two dimensional parts can be aligned as they move on a conveyor belt against a passive sequence of fences. Describes the first complete algorithm to design such sequences for a given two dimensional convex polygonal part. The algorithm is complete in the sense that it is guaranteed to find a design if one exists and to terminate with a negative report otherwise. Based on an exact breadth-first search of the design space, the algorithm is also guaranteed to find the design requiring the fewest fences. Describes the algorithm and compares results with those previously reported. Conjectures that a fence design exists to orient any 2D convex polygonal part defined by a sequence of rational vertices.\n\nTitle: Fixturing hinged polygons\nContent: We study the problem of fixturing a chain of hinged objects in a given placement with frictionless point contacts. We define the notions of immobility and robust immobility - which are comparable to the second and first order immobility for a single object - to capture the intuitive requirement for the fixture of a chain of hinged objects. Robust immobility differs from immobility in that it additionally requires insensitivity to small perturbations of contacts. We show that (p+2) frictionless point contacts can immobilize any chain of p\u22603 polygons without parallel edges; six contacts can immobilize any chain of three such polygons. Any chain of p arbitrary polygons can be immobilized with at most (p+4) contacts. We also show that \u250c(6/5)(p+2)\u2510 contacts suffice to robustly immobilize p polygons without parallel edges, and that \u250c(5/4)(p+2)\u2510 contacts can robustly immobilize p\u22603 arbitrary polygons, and eight contacts can robustly immobilize three polygons.\n\n## Task\nCompose an abstract for the title 'A complete algorithm for synthesizing modular fixtures for polygonal parts' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Commercially-available modular fixturing systems typically include a square lattice of tapped and bushed holes with precise spacing and an assortment of precision locating and clamping elements that can be rigidly attached to the lattice using dowel pins or expanding mandrels. Currently, human expertise is required to synthesize a suitable arrangement of these elements to hold a given part. Besides being time consuming, if the set of alternatives is not systematically explored, the designer may fail to find an acceptable fixture or may settle upon a suboptimal fixture. The authors consider a class of modular fixtures that prevent a part from translating or rotating in the plane using four point contacts on the part's boundary. These fixtures are based on three round locators, each centered on a lattice point, and one translating clamp. The authors present an algorithm that accepts a polygonal part shape as input and synthesizes the set of all fixture designs that achieve form closure for the given part. The algorithm also allows the user to specify geometric access constraints on fixtures. If the part has n edges and its maximal diameter is d lattice units, the asymptotic running time of the algorithm is O(n5d5). The authors have implemented the algorithm and present example fixtures that it has synthesized. This implementation includes a metric to rank fixtures based on their ability to resist applied forces. The authors believe this is the first fixture synthesis algorithm that is complete in the sense that it is guaranteed to find an admissible fixture if one exists. Furthermore, the algorithm is guaranteed to find the optimal fixture, relative to any well-defined quality metric"
      }
    ]
  },
  {
    "tid":"74",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: CI-Graph simultaneous localization and mapping for three-dimensional reconstruction of large and complex environments using a multicamera system\nContent: Submapping and graphical methods have been shown to be valuable approaches to simultaneous localization and mapping (SLAM), providing significant advantages over the classical extended Kalman filter (EKF) solution: they are faster and, when using local coordinates, produce more consistent estimates. The main contribution of this paper is CI-Graph SLAM, a novel algorithm that is able to efficiently map large environments by building a graph of submaps and a spanning tree of this graph with the following properties: (1) any pair of neighboring submaps in the spanning tree are conditionally independent and (2) the current submap is always up to date, containing the marginal probabilities of the submap variables given all previous measurements. Thanks to these properties, an old submap can be updated at any time by performing a single propagation from the current map to the old submap along the spanning tree. This operation is required only when a map is revisited, with a cost linear with the number of maps in the loop. At the end of the experiment the method performs a single propagation through the whole tree, recovering exactly the same marginals for all the map variables as the EKF\u2013SLAM algorithm does, without ever needing to compute the whole covariance matrix. To evaluate CI-Graph performance in extremely loopy environments, the method was tested using a synthetic Manhattan world. The behavior of the algorithm in large real environments is shown using the public data sets from the RAWSEEDS project in which a robot equipped with a trinocular camera traversed indoor and outdoor environments with several loops and revisited areas. Loops are robustly closed using a novel technique that detects candidate loop closures using a visual vocabulary tree and filters them using temporal and geometric constraints. Our experiments show that when using frontal cameras, the technique outperforms FAB-MAP. The epipolar geometry of the loop-closing images is used to find feature matches that are imposed on the CI-Graph to correct the submap estimations along the loop. \u00a9 2010 Wiley Periodicals, Inc.\n\nTitle: Large-Scale SLAM Building Conditionally Independent Local Maps: Application to Monocular Vision\nContent: Simultaneous localization and mapping (SLAM) algorithms based on local maps have been demonstrated to be well suited for mapping large environments as they reduce the computational cost and improve the consistency of the final estimation. The main contribution of this paper is a novel submapping technique that does not require independence between maps. The technique is based on the intrinsic structure of the SLAM problem that allows the building of submaps that can share information, remaining conditionally independent. The resulting algorithm obtains local maps in constant time during the exploration of new terrain and recovers the global map in linear time after simple loop closures without introducing any approximations besides the inherent extended Kalman filter linearizations. The memory requirements are also linear with the size of the map. As the algorithm works in a covariance form, well-known data-association techniques can be used in the usual manner. We present experimental results using a handheld monocular camera, building a map along a closed-loop trajectory of 140 m in a public square, with people and other clutter. Our results show that the combination of conditional independence, which enables the system to share the camera and feature states between submaps, and local coordinates, which reduce the effects of linearization errors, allow us to obtain precise maps of large areas with pure monocular SLAM in real time.\n\nTitle: Scalable Slam Building Conditionally Independent Local Maps\nContent: Local maps algorithms have demonstrated to be well suited for mapping large environments as can reduce the computational cost and improve the consistency of the final estimation. In this paper we present a new technique that allows the use of local mapping algorithms in the context of EKF SLAM but without the constrain of probabilistic independence between local maps. By means of this procedure, salient features of the environment or vehicle state components as velocity or global attitude, can be shared between local maps without affecting the posterior joining process or introducing any undesirable approximations in the final global map estimate. The overload cost introduced by the technique is minimum since building up local maps does not require any additional operations apart from the usual EKF steps. As the algorithm works with covariance matrices, well-known data association techniques can be used in the usual manner. To test the technique, experimental results using a monocular camera in an outdoor environment are provided. The initialization of the features is based on the inverse depth algorithm.\n\nTitle: Hierarchical SLAM: Real-Time Accurate Mapping of Large Environments\nContent: In this paper, we present a hierarchical mapping method that allows us to obtain accurate metric maps of large environments in real time. The lower (or local) map level is composed of a set of local maps that are guaranteed to be statistically independent. The upper (or global) level is an adjacency graph whose arcs are labeled with the relative location between local maps. An estimation of these relative locations is maintained at this level in a relative stochastic map. We propose a close to optimal loop closing method that, while maintaining independence at the local level, imposes consistency at the global level at a computational cost that is linear with the size of the loop. Experimental results demonstrate the efficiency and precision of the proposed method by mapping the Ada Byron building at our campus. We also analyze, using simulations, the precision and convergence of our method for larger loops.\n\nTitle: Large-Scale 6-DOF SLAM With Stereo-in-Hand\nContent: In this paper, we describe a system that can carry out simultaneous localization and mapping (SLAM) in large indoor and outdoor environments using a stereo pair moving with 6 DOF as the only sensor. Unlike current visual SLAM systems that use either bearing-only monocular information or 3-D stereo information, our system accommodates both monocular and stereo. Textured point features are extracted from the images and stored as 3-D points if seen in both images with sufficient disparity, or stored as inverse depth points otherwise. This allows the system to map both near and far features: the first provide distance and orientation, and the second provide orientation information. Unlike other vision-only SLAM systems, stereo does not suffer from ldquoscale driftrdquo because of unobservability problems, and thus, no other information such as gyroscopes or accelerometers is required in our system. Our SLAM algorithm generates sequences of conditionally independent local maps that can share information related to the camera motion and common features being tracked. The system computes the full map using the novel conditionally independent divide and conquer algorithm, which allows constant time operation most of the time, with linear time updates to compute the full map. To demonstrate the robustness and scalability of our system, we show experimental results in indoor and outdoor urban environments of 210 m and 140 m loop trajectories, with the stereo camera being carried in hand by a person walking at normal walking speeds of 4--5 km/h.\n\n## Task\nCompose an abstract for the title 'CI-Graph: An efficient approach for large scale SLAM' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"When solving the Simultaneous Localization and Mapping (SLAM) problem, submapping and graphical methods have shown to be valuable approaches that provide significant advantages over the standard EKF solution: they are faster and can produce more consistent estimates when using local coordinates. In this paper we present CI-Graph, a submapping method for SLAM that uses a graph structure to efficiently solve complex trajectories reducing the computational cost. Unlike other submapping SLAM approaches, we are able to transmit and share information through maps in the graph in a consistent manner by using conditionally independent submaps. In addition, the current submap always summarizes, without further computations, all information available making CI-Graph be an intrinsically \u201cup to date\u201d algorithm. Moreover, the technique is also efficient in memory requirements since it does not need to recover the full covariance matrix. To evaluate CI-Graph performance, the method has been tested using a synthetic Manhattan world and Victoria Park data set."
      }
    ]
  },
  {
    "tid":"75",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Framework for Semantic Recovery Strategies in Case of Process Activity Failures\nContent: During automated process execution semantic activity failures may frequently occur, e.g., when a vehicle transporting a container has a breakdown. So far there are no applicable solutions to overcome such exceptional situations. Often the only possibility is to cancel and roll back respective process instances what is not always possible and more often not desired. In this paper we contribute towards the system-assisted support of finding forward recovery solutions. Our framework is based on the facility to (automatically) perform dynamic changes of single process instances in order to deal with the exceptional situation. We identify and formalize factors which influence the kind of applicable recovery solutions. Then we show how to derive possible recovery solutions and how to evaluate their quality with respect to different constraints. All statements are illustrated by well-studied cases from different domains.\n\nTitle: Discovery and Analysis of Activity Pattern Co-occurrences in Business Process Models\nContent: Research on workflow activity patterns recently emerged in order to increase the reuse of recurring business functions (e.g., notification, approval, and decision). One important aspect is to identify pattern co- occurrences and to utilize respective information for creating modeling recommendations regarding the most suited activity patterns to be combined with an already used one. Activity patterns as well as their co- occurrences can be identified through the analysis of process models rather than event logs. Related to this problem, this paper proposes a method for discovering and analyzing activity pattern co-occurrences in business process models. Our results are used for developing a BPM tool which fosters the modeling of business processes based on the reuse of activity patterns. Our tool includes an inference engine which considers the patterns co-occurrences to give design time recommendations for pattern usage.\n\nTitle: Activity patterns in process-aware information systems: basic concepts and empirical evidence\nContent: Recently, a variety of workflow patterns was suggested for capturing different aspects in process-aware information systems (PAISs) including control and data flow, resources, process change, and exception handling. All these patterns are highly relevant for implementing PAISs and for designing process modeling languages. However, current patterns provide only a partial answer to the question which business functions a designer might want to reuse when modeling processes. This paper presents a revised version of a collection of activity patterns to deal with this challenge. Each of them is related to a recurrent business function as it can be frequently found in process models (e.g., task execution request, notification, approval). We describe the identified activity patterns and their variants in detail. The main purpose of our paper is to discuss results from empirical studies, in which we analyzed more than 200 process models in order to evidence the practical relevance of the patterns. This includes a detailed analysis of the context in which activity patterns occur as well the frequency of this occurrence. These empirical findings can be used for the design of more intelligent, pattern-based process modeling tools.\n\nTitle: Object-Aware Business Processes: Fundamental Requirements and their Support in Existing Approaches\nContent: Despite the increasing maturity of process management technology not all business processes are adequately supported by it. Support for unstructured and knowledge-intensive processes is missing, especially since they cannot be straight-jacketed into predefined activities. A common characteristic of these processes is the role of business objects and data as drivers for process modeling and enactment. This paper elicits fundamental requirements for effectively supporting such object-aware processes; i.e., their modeling, execution, and monitoring. Imperative, declarative, and data-driven process support approaches are evaluated and how well they support object-aware processes are investigated. A tight integration of process and data as major steps towards further maturation of process management technology is considered.\n\nTitle: On the Context-aware, Personalized Delivery of Process Information: Viewpoints, Problems, and Requirements\nContent: Enterprises are confronted with an increasing amount of data. This data overload makes it difficult to provide knowledge-workers and decision-makers with the needed information. Particularly challenging in this context is the integrated provision of both structured and unstructured information depending on the current process context and user, i.e., the context-aware, personalized delivery of process information. Examples of unstructured process information include all kinds of office documents or e-mails. Examples of structured process information are business process models or data from enterprise information systems. Picking up the need for a context-aware, personalized delivery of process information, this paper presents results from three empirical studies: two exploratory case studies from the automotive domain and the healthcare sector, and an online survey among 219 participants. In a first step, we identify and describe problems with respect to process-oriented information management in general and the personalized provision of process information in particular. In a second step, we derive requirements on the user-adequate handling of process information.\n\n## Task\nCompose an abstract for the title 'Preserving the Context of Interrupted Business Process Activities' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The capability to safely interrupt business process activities is an important requirement for advanced process-aware information systems. Indeed, exceptions stemming from the application environment often appear while one or more application-related process activities are running. Safely interrupting an activity consists of preserving its context, i.e., saying the data associated with this activity. This is important since possible solutions for an exceptional situation are often based on the current data context of the interrupted activity. In this paper, a data classification scheme based on data relevance and on data update frequency is proposed and discussed with respect to two different real-world applications. Taking into account this classification. a correctness criterion for interrupting running activities while preserving their context is proposed and analyzed."
      }
    ]
  },
  {
    "tid":"76",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Automata Evaluation and Text Search Protocols with Simulation Based Security.\nContent: This paper presents efficient protocols for securely computing the following two problems: (1) The fundamental problem of pattern matching. This problem is defined in the two-party setting, where party holds a pattern and party holds a text. The goal of is to learn where the pattern appears in the text, without revealing it to or learning anything else about 's text. This problem has been widely studied for decades due to its broad applicability. We present several protocols for several notions of security. We further generalize one of our solutions to solve additional pattern matching-related problems of interest. (2) Our construction from above, in the malicious case, is based on a novel protocol for secure oblivious automata evaluation which is of independent interest. In this problem, party holds an automaton and party holds an input string, and they need to decide whether the automaton accepts the input, without learning anything else. Our protocol obtains full security in the face of malicious adversaries.\n\nTitle: Text search protocols with simulation based security\nContent: This paper presents an efficient protocol for securely computing the fundamental problem of pattern matching. This problem is defined in the two-party setting, where party P1 holds a pattern and party P2 holds a text. The goal of P1 is to learn where the pattern appears in the text, without revealing it to P2 or learning anything else about P2\u2019s text. Our protocol is the first to address this problem with full security in the face of malicious adversaries. The construction is based on a novel protocol for secure oblivious automata evaluation which is of independent interest. In this problem party P1 holds an automaton and party P2 holds an input string, and they need to decide if the automaton accepts the input, without learning anything else.\n\nTitle: Algorithmic Tamper-Proof (ATP) Security: Theoretical Foundations for Security against Hardware Tampering\nContent: Traditionally, secure cryptographic algorithms provide security against an adversary who has only black-box access to the secret information of honest parties. However, such models are not always adequate. In particular, the security of these algorithms may completely break under (feasible) attacks that tamper with the secret key. In this paper we propose a theoretical framework to investigate the algorithmic aspects related to tamper-proof security. In particular, we define a model of security against an adversary who is allowed to apply arbitrary feasible functions f to the secret key sk, and obtain the result of the cryptographic algorithms using the new secret key f (sk). We prove that in the most general setting it is impossible to achieve this strong notion of security. We then show minimal additions to the model, which are needed in order to obtain provable security. We prove that these additions are necessary and also sufficient for most common cryptographic primitives, such as encryption and signature schemes. We discuss the applications to portable devices protected by PINs and show how to integrate PIN security into the generic security design. Finally we investigate restrictions of the model in which the tampering powers of the adversary are limited. These restrictions model realistic attacks (like differential fault analysis) that have been demonstrated in practice. In these settings we show security solutions that work even without the additions mentioned above.\n\nTitle: Off-line/on-line signatures revisited: a general unifying paradigm, efficient threshold variants and experimental results\nContent: The notion of off-line/on-line digital signature scheme was introduced by Even, Goldreich and Micali. Informally such signatures schemes are used to reduce the time required to compute a signature using some kind of preprocessing. Even, Goldreich and Micali show how to realize off-line/on-line digital signature schemes by combining regular digital signatures with efficient one-time signatures. Later, Shamir and Tauman presented an alternative construction (which produces shorter signatures) obtained by combining regular signatures with chameleon hash functions. In this paper, we study off-line/on-line digital signature schemes both from a theoretic and a practical perspective. More precisely, our contribution is threefold. First, we unify the Shamir---Tauman and Even et al. approaches by showing that they can be seen as different instantiations of the same paradigm. We do this by showing that the one-time signatures needed in the Even et al. approach only need to satisfy a weak notion of security. We then show that chameleon hashing is basically a one-time signature which satisfies such a weaker security notion. As a by-product of this result, we study the relationship between one-time signatures and chameleon hashing, and we prove that a special type of chameleon hashing (which we call double-trapdoor) is actually a fully secure one-time signature. Next, we consider the task of building, in a generic fashion, threshold variants of known schemes: Crutchfield et al. proposed a generic way to construct a threshold off-line/on-line signature scheme given a threshold regular one. They applied known threshold techniques to the Shamir---Tauman construction using a specific chameleon hash function. Their solution introduces additional computational assumptions which turn out to be implied by the so-called one-more discrete logarithm assumption. Here, we propose two generic constructions that can be based on any threshold signature scheme, combined with a specific (double-trapdoor) chameleon hash function. Our constructions are efficient and can be proven secure in the standard model using only the traditional discrete logarithm assumption. Finally, we ran experimental tests to measure the difference between the real efficiency of the two known constructions for non-threshold off-line/on-line signatures. Interestingly, we show that, using some optimizations, the two approaches are comparable in efficiency and signature length.\n\nTitle: Okamoto-Tanaka revisited: fully authenticated diffie-hellman with minimal overhead\nContent: This paper investigates the question of whether a key agreement protocol with the same communication complexity as the original Diffie-Hellman protocol (DHP) (two messages with a single group element per message), and similar low computational overhead, can achieve forward secrecy against active attackers in a provable way.We answer this question in the affirmative by resorting to an old and elegant key agreement protocol: the Okamoto-Tanaka protocol [22]. We analyze a variant of the protocol (denoted mOT) which achieves the above goal. Moreover, due to the identity-based properties of mOT, even the sending of certificates (typical for authenticated DHPs) can be avoided in the protocol. As additional contributions, we apply our analysis to prove the security of a recent multi-domain extension of the Okamoto-Tanaka protocol by Schridde et al. and show how to adapt mOT to the (non id-based) certificate-based setting.\n\n## Task\nCompose an abstract for the title 'Automata Evaluation and Text Search Protocols with Simulation-Based Security.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper presents efficient protocols for securely computing the following two problems: (1) The fundamental problem of pattern matching. This problem is defined in the two-party setting, where party $$P_1$$P1 holds a pattern and party $$P_2$$P2 holds a text. The goal of $$P_1$$P1 is to learn where the pattern appears in the text, without revealing it to $$P_2$$P2 or learning anything else about $$P_2$$P2's text. This problem has been widely studied for decades due to its broad applicability. We present several protocols for several notions of security. We further generalize one of our solutions to solve additional pattern matching-related problems of interest. (2) Our construction from above, in the malicious case, is based on a novel protocol for secure oblivious automata evaluation which is of independent interest. In this problem, party $$P_1$$P1 holds an automaton and party $$P_2$$P2 holds an input string, and they need to decide whether the automaton accepts the input, without learning anything else. Our protocol obtains full security in the face of malicious adversaries."
      }
    ]
  },
  {
    "tid":"77",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Novel Local Surface Description for Automatic 3D Object Recognition in Low Resolution Cluttered Scenes\nContent: Local surface description is a critical stage for feature matching and recognition tasks. This paper presents a rotation invariant local surface descriptor, called 3D-Div. The proposed descriptor is based on the concept of 3D vector field's divergence, extensively used in electromagnetic theory. To generate a 3D-Div descriptor of a 3D surface, a local surface patch is parameterized around a randomly selected 3D point at a fixed scale. A unique Local Reference Frame (LRF) is then constructed at that 3D point using all the neighboring points forming the patch. A normalized 3D vector field is then computed at each point in the patch and referenced with LRF vectors. The 3D-Div descriptor is finally generated as the divergence of the reoriented 3D vector field. We tested our proposed descriptor on the challenging low resolution Washington RGB-D (Kinect) object dataset, for the task of automatic 3D object recognition. Reported experimental results show that 3D-Div based recognition achieves 93% accuracy as compared to 85% for existing state-of-the-art depth kernel descriptors [2].\n\nTitle: A novel local surface feature for 3D object recognition under clutter and occlusion.\nContent: This paper presents a highly distinctive local surface feature called the TriSI feature for recognizing 3D objects in the presence of clutter and occlusion. For a feature point, we first construct a unique and repeatable Local Reference Frame (LRF) using the implicit geometrical information of neighboring triangular faces. We then generate three signatures from the three orthogonal coordinate axes of the LRF. These signatures are concatenated and then compressed into a TriSI feature. Finally, we propose an effective 3D object recognition algorithm based on hierarchical feature matching. We tested our TriSI feature on two popular datasets. Rigorous experimental results show that the TriSI feature was highly descriptive and outperformed existing algorithms under all levels of Gaussian noise, Laplacian noise, shot noise, varying mesh resolutions, occlusion, and clutter. Moreover, we tested our TriSI-based 3D object recognition algorithm on four standard datasets. The experimental results show that our algorithm achieved the best overall recognition results on these datasets.\n\nTitle: Rotational Projection Statistics for 3D Local Surface Description and Object Recognition.\nContent: Recognizing 3D objects in the presence of noise, varying mesh resolution, occlusion and clutter is a very challenging task. This paper presents a novel method named Rotational Projection Statistics (RoPS). It has three major modules: local reference frame (LRF) definition, RoPS feature description and 3D object recognition. We propose a novel technique to define the LRF by calculating the scatter matrix of all points lying on the local surface. RoPS feature descriptors are obtained by rotationally projecting the neighboring points of a feature point onto 2D planes and calculating a set of statistics (including low-order central moments and entropy) of the distribution of these projected points. Using the proposed LRF and RoPS descriptor, we present a hierarchical 3D object recognition algorithm. The performance of the proposed LRF, RoPS descriptor and object recognition algorithm was rigorously tested on a number of popular and publicly available datasets. Our proposed techniques exhibited superior performance compared to existing techniques. We also showed that our method is robust with respect to noise and varyingmesh resolution. Our RoPS based algorithm achieved recognition rates of 100, 98.9, 95.4 and 96.0% respectively when tested on the Bologna, UWA, Queen's and Ca' Foscari Venezia Datasets. \u00a9 Springer Science+Business Media New York 2013.\n\nTitle: A Comprehensive Performance Evaluation of 3D Local Feature Descriptors\nContent: A number of 3D local feature descriptors have been proposed in the literature. It is however, unclear which descriptors are more appropriate for a particular application. A good descriptor should be descriptive, compact, and robust to a set of nuisances. This paper compares ten popular local feature descriptors in the contexts of 3D object recognition, 3D shape retrieval, and 3D modeling. We first evaluate the descriptiveness of these descriptors on eight popular datasets which were acquired using different techniques. We then analyze their compactness using the recall of feature matching per each float value in the descriptor. We also test the robustness of the selected descriptors with respect to support radius variations, Gaussian noise, shot noise, varying mesh resolution, distance to the mesh boundary, keypoint localization error, occlusion, clutter, and dataset size. Moreover, we present the performance results of these descriptors when combined with different 3D keypoint detection methods. We finally analyze the computational efficiency for generating each descriptor.\n\nTitle: 3D Object Recognition in Cluttered Scenes with Local Surface Features: A Survey\nContent: 3D object recognition in cluttered scenes is a rapidly growing research area. Based on the used types of features, 3D object recognition methods can broadly be divided into two categories-global or local feature based methods. Intensive research has been done on local surface feature based methods as they are more robust to occlusion and clutter which are frequently present in a real-world scene. This paper presents a comprehensive survey of existing local surface feature based 3D object recognition methods. These methods generally comprise three phases: 3D keypoint detection, local surface feature description, and surface matching. This paper covers an extensive literature survey of each phase of the process. It also enlists a number of popular and contemporary databases together with their relevant attributes.\n\n## Task\nCompose an abstract for the title 'TriSI: A Distinctive Local Surface Descriptor for 3D Modeling and Object Recognition.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Local surface description is a critical stage for surface matching. This paper presents a highly distinctive local surface descriptor, namely TriSI. From a keypoint, we first construct a unique and repeatable local reference frame (LRF) using all the points lying on the local surface. We then generate three spin images from the three coordinate axes of the LRF. These spin images are concatenated and further compressed into a TriSI descriptor using the principal component analysis technique. We tested our TriSI descriptor on the Bologna Dataset and compared it to several existing methods. Experimental results show that TriSI outperformed existing methods under all levels of noise and varying mesh resolutions. The TriSI was further tested to demonstrate its effectiveness in 3D modeling. Experimental results show that it can accurately perform pairwise and multiview range image registration. We finally used the TriSI descriptor for 3D object recognition. The results on the UWA Dataset show that TriSI outperformed the state-of-the-art methods including spin image, tensor and exponential map. The TriSI based method achieved a high recognition rate of 98.4%."
      }
    ]
  },
  {
    "tid":"78",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Conformance Testing of Distributed Concurrent Systems with Executable Designs\nContent: This paper presents a unified approach to test case generation and conformance test execution in a distributed setting. A model in the object-oriented, concurrent modeling language Creol is used both for generating test inputs and as a test oracle. For test case generation, we extend Dynamic Symbolic Execution (also called Concolic Execution) to work with multi-threaded models and use this to generate test inputs that maximize model coverage. For test case execution, we establish a conformance relation based on trace inclusion by recording traces of events in the system under test and replaying them in the model. User input is handled by generating a test driver that supplies the needed stimuli to the model. An industrial case study of the Credo project serves to demonstrate the approach.\n\nTitle: Model-based mutation testing of hybrid systems\nContent: This paper presents a novel model-based testing approach developed in the MOGENTES project. The aim is to test embedded systems controlling a continuous environment, i.e., hybrid systems. We present our two key abstractions against which we systematically test for conformance. (1) Classical action systems are used to model the discrete controller behavior. (2) Qualitative differential equations are used to model the evolutions of the environment. The latter is based on a technique from the domain of Artificial Intelligence called qualitative reasoning. Mutation testing on these models is used to generate effective test cases. A test case generator has been developed that searches for all test cases that would kill a mutant. The mutant models represent our fault models. The generated test cases are then executed on the implementation in order to systematically exclude the possibility that a mutant has been implemented.\n\nTitle: Model-based mutation testing of reactive systems: from semantics to automated test-case generation\nContent: In this paper we give an overview of our work on combining model-based testing and mutation testing. Model-based testing is a black-box testing technique that avoids the labour of manually writing hundreds of test cases, but instead advocates the capturing of the expected behaviour in a model of the system-under-test. The test cases are automatically generated from this model. The technique is receiving growing interest in the embedded-systems domain, where models are the rule rather than the exception. Mutation testing is a technique for assessing and improving a test suite. A number of faulty versions of a program-under-test are produced by injecting bugs into its source code. These faulty programs are called mutants. A tester analyses if his test suite can \"kill\" all mutants. We say that a test kills a mutant if it is able to distinguish it from the original. The tester improves his test suite until all faulty mutants get killed. In model-based mutation testing, we combine the central ideas of model-based testing and mutation testing: we inject bugs in a model and generate a test suite that will kill these bugs. In this paper, we discuss its scientific foundations and tools. The foundations include semantics and conformance relations; the supporting tools involve model checkers, constraint solvers and SMT solvers.\n\nTitle: A Systematic Introduction to Mutation Testing in Unifying Theories of Programming\nContent: This chapter presents a theory of testing that integrates into Hoare and He's Unifying Theories of Programming (UTP). We give test cases a denotational semantics by viewing them as specification predicates. This reformulation of test cases allows for relating test cases via refinement to specifications and programs. Having such a refinement order that integrates test cases, we develop a testing theory for fault-based testing. As discussed in Chapter 1, fault-based testing uses test data designed to demonstrate the absence of a set of pre-specified faults. A well-known fault-based technique is mutation testing. In mutation testing, first, faults are injected into a program by altering (mutating) its source code. Then, test cases that can detect these errors are designed. The assumption is that other faults will be caught, too. In this chapter, we apply the mutation technique to both specifications and programs. Using our theory of testing, two test-case generation laws for detecting injected (anticipated) faults are presented: one is based on the semantic level of UTP design predicates, the other on the algebraic properties of a small programming language. The chapter is structured as follows. After a general introduction, Section 2 introduces UTP [173], the mathematical framework used throughout this article. In particular, an overview of the theory of designs is presented. Section 3 briefly discusses the testing terminology and, based on the theory of designs, a series of formal definitions of concepts like test cases, test equivalence classes, faults, and a criterion for finding faults is presented. It is this section that highlights the important role of refinement in our testing theory. The next two sections include the main technical ideas of this chapter. Section 4 contains a construction for test cases that will find anticipated errors in a design. This test-case generation method works on the semantic level of designs. In Section 5 a purely algebraic (syntax-oriented) test-case generation algorithm is presented. It is based on the algebraic properties of a small, but non-trivial, programming language. Next, Section 6 discusses tool building and application domains. Finally, in Section 7 we discuss the results, give a review of related work, and present an overview of further research directions.\n\nTitle: Improving Fault-based Conformance Testing\nContent: Fault-based conformance testing is a conformance testing strategy that relies on specific fault models. Previously, this mutation testing technique has been applied to protocol specifications. Although a practical case study of web-server testing has been conducted, we observed several issues when applying this method in a large industrial project. In this paper, we discuss the foundations, techniques and tools to overcome these shortcomings. More specifically, we show a solution to the problem of state-space explosion in generating mutation tests for industrial scale applications. Furthermore, the previous approach used the counterexamples of a bisimulation check (between the original and the mutant) as test purposes. With respect to input-output conformance (ioco), this is an over-approximation resulting in more tests than are necessary. Hence, we propose to use an ioco-checker in order to generate less test cases. An industrial case study demonstrates these improvements.\n\n## Task\nCompose an abstract for the title 'Conformance Testing of Hybrid Systems with Qualitative Reasoning Models' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Embedded systems are of growing importance in industry. For example, in a today's vehicle a huge number of embedded and communicating systems can be found. Exhaustive testing of such systems is a requirement, because changes after delivery and use are expensive and sometimes even impossible. In this paper we propose the use of qualitative models, which are an abstraction of quantitative physical models, for test case generation and test execution. In particular, we show how Simulink models from which control programs are automatically extracted can be tested with respect to qualitative models. Since Simulink models are heavily used in industry, the approach is of practical interest."
      }
    ]
  },
  {
    "tid":"79",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A parallel circuit-partitioned algorithm for timing-driven standard cell placement\nContent: Simulated annealing based standard cell placement for VLSI designs has long been acknowledged as a computation-intensive process, and as a result, several research efforts have been undertaken to parallelize this algorithm. Parallel placement is most needed for very large circuits. Since these circuits do not fit in memory, the traditional approach has been to partition and place individual modules. This causes a degradation in placement quality in terms of area and wirelength. Our algorithm is circuit-partitioned and can handle arbitrarily large circuits on cluster-of-workstations-type parallel machines, such as the Intel Paragon and IBM SP-2. Most previous work in parallel placement has minimized just area and wirelength, but with current deep submicron designs, minimizing wirelength delay is most important. As a result the algorithm discussed in this paper also supports timing driven placement for partitioned circuits. The algorithm, called mpiPLACE , has been tested on several large industry benchmarks on a variety of parallel architectures.\n\nTitle: Parallel global routing algorithms for standard cells\nContent: In this paper, we propose three different parallel algorithms based on a state-of-the-art global router called TimberWolfSC. The parallel algorithms have been implemented by using the Message Passing Interface (MPI), and have been evaluated on a wide range of parallel platforms such as the Sun SparcCenter~1000 and the Intel Paragon. Our experimental results show good speedups and qualities from two of these parallel algorithms. We have been able to reduce runtimes of some circuits from half an hour to 5 minutes, obtained speedups of about 4.0 to 5.0 on 8 processors, with less than 2-3% degradation of quality of the solutions.\n\nTitle: A parallel circuit-partitioned algorithm for timing driven cell placement\nContent: Simulated annealing based standard cell placement for VLSI designs has long been acknowledged as a compute-intensive process. All previous work in parallel simulated annealing based placement has minimized area, but with deep submicron design, minimizing wirelength delay is also needed. The algorithm discussed in this paper is the first parallel algorithm for timing driven placement. We have used a very accurate Elmore delay model which is more complete intensive and hence the need for parallel placement is more apparent. Parallel placement is also needed for very large circuits that may not fit in the memory of a single processor. Therefore, our algorithm is circuit partitioned and can handle arbitrary large circuits on distributed memory multiprocessors. The algorithm, called mpi PLACE, has been tested on several large benchmarks on a variety of parallel architectures.\n\nTitle: Incremental capacitance extraction and its application to iterative timing-driven detailed routing\nContent: In this paper, we consider delay optimization in multilayer detailed routing. Given a detailed routing by some detailed router, we iteratively improve the delays of critical nets or nets with timing violation in an aggressive way. A rip-up and reroute approach is employed to generate the alter- nate routes. The optimal route which satisfies the timing constraints is chosen. The net delays are calculated us- ing the Elmore delay model. The coupling capacitances are computed using a 3D extractor by applying the process parameters. In such an approach, the interconnect delay can be most accurately modeled. However, this aggressive approach is likely to be computationally unaffordable, be- cause the delay of each alternate route should be evaluated. To overcome this problem, the key is to efficiently handle the detailed 3-D extraction in each iteration step. In our approach, we represent the design changes in an incremen- tal way. Then we compute the coupling capacitance using the incremental extraction algorithm which is specifically tuned to an iterative design environment. This makes the aggressive improvement approach computationally practi- cal. Experimental results are presented to demonstrate the efficiency of the approach.\n\nTitle: A parallel algorithm for zero skew clock tree routing\nContent: In deep sub-micron fabrication technology, clock skew is one of the dominant factors which determine system performance. Previous works in zero skew clock tree routing assume that the wires have uniform size, and previous wire-sizing algorithms for general signal nets do not produce the exact zero skew. In this paper, we first propose an algorithm to get the exact zero skew wire-sizing by using an iterative method to make the wire size improvement. Our experiments on benchmark clock trees show that the algorithm reduces the source sink delay more than 3 times that of the clock trees with uniform wire sizes and keeps the clock skew zero. Motivated by the computation intensive nature of the zero skew clock tree construction and wire-sizing, we propose a parallel algorithm using a cluster-based clock tree construction algorithm and our zero skew wire-sizing algorithm. Without sacrificing the quality of the solution, on the average we obtain speedups of 7.8 from the parallel clustering based clock tree construction algorithm on an 8 processor SUN SPARC Server 1000E shared memory multi-processor.\n\n## Task\nCompose an abstract for the title 'A Parallel Algorithm for Timing-driven Global Routing for Standard Cells' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The timing-driven global routing problem is an extremely important and time consuming phase of any automated layout system. In this paper, by integrating high performance interconnection tree construction, wire-sizing, and switch-able segment channel optimization together, we propose an adaptive timing-driven global routing algorithm which minimizes the timing delay as well as circuit area. Our experiments on MONO benchmarks show that our timing-driven global routing algorithm reduces the maximum path delays significantly from the global router TimberWolfSC. Based on this adaptive timing-driven global routing algorithm, a parallel algorithm on timing-driven global routing for standard cells is given. This algorithm has been implemented on an 8 processor IBM J-40 shared memory multi-processor by using the Message Passing Interface (MPI). Our experimental results show good speedup and circuit delay results for this parallel algorithm using MONO benchmark circuits."
      }
    ]
  },
  {
    "tid":"80",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: RHENE: A Case Retrieval System for Hemodialysis Cases with Dynamically Monitored Parameters\nContent: In this paper, we present a case-based retrieval system called RHENE (Retrieval of HEmodialysis in NEphrological disorders) working in the domain of patients affected by nephropatologies and treated with hemodialysis. Defining a dialysis session as a case, retrieval of past similar cases has to operate both on static and on dynamic (time-dependent) features, since most of the monitoring variables of a dialysis session are time series. In RHENE, retrieval relies upon a multi-step procedure. In particular, a preliminary grouping/classification step, based on static features, reduces the retrieval search space. Intra-class retrieval then takes place by considering time-dependent features, and is articulated as follows: (1) \"locally\" similar cases (considering one feature at a time) are extracted and the intersection of the retrieved sets is computed; (2) \"global\" similarity is computed - as a weighted average of local distances - and the best cases are listed. The main goal of the paper is to present an approach for efficiently implementing step (2), by taking into account specific information regarding the final application. We concentrate on a classical dimensionality reduction technique for time series allowing for efficient indexing, namely Discrete Fourier Transform (DFT). Thanks to specific index structures (i.e. k-d trees) range queries (on local feature similarity) can be efficiently performed on our case base; as mentioned above, results of such local queries are then suitably combined, allowing the physician to examine the most similar stored dialysis sessions with respect to the current one and to assess the quality of the overall hemodialysis service.\n\nTitle: Retrieval in a Prototype-Based Case Library: A Case Study in Diabetes Therapy Revision\nContent: Case retrieval is a complex and important stage of the overall CBR process, involving situation assessment and a flexible\n combination of case memory search and matching. The goal of the paper is to discuss a retrieval approach in a case library\n organized through classes of prototypes. Situation assessment is realized with a Bayesian classification step, aimed at defining\n a uniform framework for feature evaluation and at restricting search only to relevant parts of the case library. Classical\n K-NN methods or pruning based technique like Pivoting-Based Retrieval can then be applied to retrieve and match cases, by considering intra-class or inter-class retrieval. The proposed method\n has been evaluated within a decision support system for the therapy revision of patients affected by Diabetes Mellitus.\n \n\nTitle: Diabetic Patients Management Exploiting Case-Based Reasoning Techniques\nContent: In this paper we propose a case-based decision support tool, designed to help physicians in Ist type diabetes therapy revision through the intelligent retrieval of data related to past situations (or 'cases') similar to the current one. A case is defined as a set of variable values (or features) collected during a visit. We defined taxonomy of prototypical patients' conditions, or classes, to which each case should belong. For each input case, the system allows the physician to find similar past cases, both from the same patient and from different ones. We have implemented a two-steps procedure; (1) it finds the classes to which the input case could belong; (2) it lists the most similar cases from these classes, through a nearest neighbor technique, and provides some statistics useful for decision taking. The performance of the system has been tested on a data-base of 147 real cases, collected at the Policlinico S. Matteo Hospital of Pavia. The tool is fully integrated in the web-based architecture of the EU funded Telematic management of Insulin Dependent Diabetes Mellitus (T-IDDM) project. (C) 2000 Elsevier Science Ireland Ltd. All rights reserved.\n\nTitle: Achieving self-healing in service delivery software systems by means of case-based reasoning\nContent: Self-healing, i.e. the capability of a system to autonomously detect failures and recover from them, is a very attractive property that may enable large-scale software systems, aimed at delivering services on a 24/7 fashion, to meet their goals with little or no human intervention. Achieving self-healing requires the elicitation and maintenance of domain knowledge in the form of \u9a74service failure diagnosis, repair plan\u9a74 patterns, a task which can be overwhelming. Case-Based Reasoning (CBR) is a lazy learning paradigm that largely reduces this kind of knowledge acquisition bottleneck. Moreover, the application of CBR for failure diagnosis and remediation in software systems appears to be very suitable, as in this domain most errors are re-occurrences of known problems. In this paper, we describe a CBR approach for providing large-scale, distributed software systems with self-healing capabilities, and demonstrate the practical applicability of our methodology by means of some experimental results on a real world application.\n\nTitle: Accounting For The Temporal Dimension In Case-Based Retrieval: A Framework For Medical Applications\nContent: Time-varying information embedded in cases has often been neglected and its role oversimplified in case-based reasoning systems. In several real-world problems, and in particular in medical applications, a case should capture the evolution of the observed phenomenon over time. To this end, we propose to represent temporal information at two levels: (1) at the case level, when some features are collected in the form of time series, because they describe parameters varying within a period of time (which corresponds to the case duration), and we aim at analyzing the system behavior within the case duration interval itself; (2) at the history level, when we are interested in reconstructing the evolution of the system by retrieving temporally related cases. In this paper, we describe a framework for case representation and retrieval that is able to take into account the temporal dimension, and is meant to be used in any time dependent domain, which is particularly well suited for medical applications. To support case retrieval, we provide an analysis of similarity-based time series retrieval techniques; to support history retrieval, we introduce possible ways to summarize the case content, together with the corresponding strategies for identifying similar instances in the knowledge base. A concrete application of our framework is represented by RHENE, a system for intelligent retrieval in the hemodialysis domain.\n\n## Task\nCompose an abstract for the title 'Case-based retrieval to support the treatment of end stage renal failure patients.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In the present paper, we describe an application of case-based retrieval to the domain of end stage renal failure patients, treated with hemodialysis.Defining a dialysis session as a case, retrieval of past similar cases has to operate both on static and on dynamic features, since most of the monitoring variables of a dialysis session are time series. Retrieval is then articulated as a two-step procedure: (1) classification, based on static features and (2) intra-class retrieval, in which dynamic features are considered. As regards step (2), we concentrate on a classical dimensionality reduction technique for time series allowing for efficient indexing, namely discrete Fourier transform (DFT). Thanks to specific index structures (i.e. k -d trees), range queries (on local feature similarity) can be efficiently performed on our case base, allowing the physician to examine the most similar stored dialysis sessions with respect to the current one.The retrieval tool has been positively tested on real patients' data, coming from the nephrology and dialysis unit of the Vigevano hospital, in Italy.The overall system can be seen as a means for supporting quality assessment of the hemodialysis service, providing a useful input from the knowledge management perspective."
      }
    ]
  },
  {
    "tid":"81",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: ABC-miner: an ant-based bayesian classification algorithm\nContent: Bayesian networks (BNs) are powerful tools for knowledge representation and inference that encode (in)dependencies among random variables. A Bayesian network classifier is a special kind of these networks that aims to compute the posterior probability of each class given an instance of the attributes and predicts the class with the highest posterior probability. Since learning the optimal BN structure from a dataset is ${\\cal NP}$-hard, heuristic search algorithms need to be applied effectively to build high-quality networks. In this paper, we propose a novel algorithm, called ABC-Miner, for learning the structure of BN classifiers using the Ant Colony Optimization (ACO) meta-heuristic. We describe all the elements necessary to tackle our learning problem using ACO, and experimentally compare the performance of our ant-based Bayesian classification algorithm with other algorithms for learning BN classifiers used in the literature.\n\nTitle: ABC-Miner+: constructing Markov blanket classifiers with ant colony algorithms.\nContent: ABC-Miner is a Bayesian classification algorithm based on the Ant colony optimization (ACO) meta-heuristic. The algorithm learns Bayesian network Augmented Na\u00efve-Bayes (BAN) classifiers, where the class node is the parent of all the nodes representing the input variables. However, this assumes the existence of a dependency relationship between the class variable and all the input variables, and this relationship is always a type of \u201ccausal\u201d (rather than \u201ceffect\u201d) relationship, which restricts the flexibility of the algorithm to learn. In this paper, we extended the ABC-Miner algorithm to be able to learn the Markov blanket of the class variable. Such a produced model has a more flexible Bayesian network classifier structure, where it is not necessary to have a (direct) dependency relationship between the class variable and each of the input variables, and the dependency between the class and the input variables varies from \u201ccausal\u201d to \u201ceffect\u201d relationships. In this context, we propose two algorithms: \\({\\hbox {ABC-Miner}+_1}\\), in which the dependency relationships between the class and the input variables are defined in a separate phase before the dependency relationships among the input variables are defined, and \\({\\hbox {ABC-Miner}+_2}\\), in which the two types of dependency relationships in the Markov blanket classifier are discovered in a single integrated process. Empirical evaluations on 33 UCI benchmark datasets show that our extended algorithms outperform the original version in terms of predictive accuracy, model size and computational time. Moreover, they have shown a very competitive performance against other well-known classification algorithms in the literature.\n\nTitle: A new ant colony algorithm for multi-label classification with applications in bioinfomatics\nContent: The conventional classification task of data mining can be called single-label classification, since there is a single class attribute to be predicted. This paper addresses a more challenging version of the classification task, where there are two or more class attributes to be predicted. We propose a new ant colony algorithm for the multi-label classification task. The new algorithm, called MuLAM (Multi-Label Ant-Miner) is a major extension of Ant-Miner, the first ant colony algorithm for discovering classification rules. We report results comparing the performance of MuLAM with the performance of three other classification techniques, namely the very simple majority classifier, the original Ant-Miner algorithm and C5.0, a very popular rule induction algorithm. The experiments were performed using five bioinformatics datasets, involving the prediction of several kinds of protein function.\n\nTitle: A new classification-rule pruning procedure for an ant colony algorithm\nContent: This work proposes a new rule pruning procedure for Ant-Miner, an Ant Colony algorithm that discovers classification rules in the context of data mining. The performance of Ant-Miner with the new pruning procedure is evaluated and compared with the performance of the original Ant-Miner across several datasets. The results show that the new pruning procedure has a mixed effect on the performance of Ant-Miner. On one hand, overall it tends to decrease the classification accuracy more often than it improves it. On the other hand, the new pruning procedure in general leads to the discovery of classification rules that are considerably shorter, and so simpler (more easily interpretable by the users) than the rules discovered by the original Ant-Miner.\n\nTitle: Classification with cluster-based Bayesian multi-nets using Ant Colony Optimisation.\nContent: Bayesian multi-net (BMN) classifiers consist of several local models, one for each data subset, to model asymmetric, more consistent dependency relationships among variables in each subset. This paper extends an earlier work of ours and proposes several contributions to the field of clustering-based BMN classifiers, using Ant Colony Optimisation (ACO). First, we introduce a new medoid-based method for ACO-based clustering in the Ant-ClustBMB algorithm to learn BMNs. Both this algorithm and our previously introduced Ant-ClustBIB for instance-based clustering have their effectiveness empirically compared in the context of the \u201ccluster-then-learn\u201d approach, in which the ACO clustering step completes before learning the local BN classifiers. Second, we propose a novel \u201ccluster-with-learn\u201d approach, in which the ACO meta-heuristic performs the clustering and the BMN learning in a synergistic fashion. Third, we adopt the latter approach in two new ACO algorithms: ACO-ClustBIB, using the instance-based method, and ACO-ClustBMB, using the medoid-based method. Empirical results are obtained on 30 UCI datasets.\n\n## Task\nCompose an abstract for the title 'cAnt-Miner: An Ant Colony Classification Algorithm to Cope with Continuous Attributes' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper presents an extension to Ant-Miner, named cAnt-Miner (Ant-Miner coping with continuous attributes), which incorporates an entropy-based discretization method in order to cope with continuous attributes during the rule construction process. By having the ability to create discrete intervals for continuous attributes \"on-the-fly\", cAnt-Miner does not requires a discretization method in a preprocessing step, as Ant-Miner requires. cAnt-Miner has been compared against Ant-Miner in eight public domain datasets with respect to predictive accuracy and simplicity of the discovered rules. Empirical results show that creating discrete intervals during the rule construction process facilitates the discovery of more accurate and significantly simpler classification rules."
      }
    ]
  },
  {
    "tid":"82",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Privacy-Enhanced Microaggregation Method\nContent: Microaggregation is a statistical disclosure control technique for protecting microdata (i.e., individual records), which are important products of statistical offices. The basic idea of microaggregation is to cluster individual records in microdata into a number of mutually exclusive groups prior to publication, and then publish the average over each group instead of individual records. Previous methods require fixed or variable group size in clustering in order to reduce information loss. However, the security aspect of microaggregation has not been extensively studied. We argue that the group size requirement is not enough for protecting the privacy of microdata. We propose a new microaggregation method, which we call secure-k-Ward, to enhance the individual's privacy. Our method, which is optimization based, minimizes information loss and overall mean deviation while at the same time guarantees that the security requirement for protecting the microdata is satisfied.\n\nTitle: k-jump strategy for preserving privacy in micro-data disclosure\nContent: In disclosing micro-data with sensitive attributes, the goal is usually two fold. First, the data utility of disclosed data should be maximized for analysis purposes. Second, the private information contained in such data must be limited to an acceptable level. Recent studies show that adversarial inferences using knowledge about a disclosure algorithm can usually render the algorithm unsafe. In this paper, we show that an existing unsafe algorithm can be transformed into a large family of distinct safe algorithms, namely, k-jump algorithms. We prove that the data utility of different k-jump algorithms is generally incomparable. Therefore, a secret choice can be made among all k-jump algorithms to eliminate adversarial inferences while improving the data utility of disclosed micro-data.\n\nTitle: Privacy streamliner: a two-stage approach to improving algorithm efficiency\nContent: In releasing data with sensitive information, a data owner usually has seemingly conflicting goals, including privacy preservation, utility optimization, and algorithm efficiency. In this paper, we observe that a high computational complexity is usually incurred when an algorithm conflates the processes of privacy preservation and utility optimization. We then propose a novel privacy streamliner approach to decouple those two processes for improving algorithm efficiency. More specifically, we first identify a set of potential privacy-preserving solutions satisfying that an adversary's knowledge about this set itself will not help him/her to violate the privacy property; we can then optimize utility within this set without worrying about privacy breaches since such an optimization is now simulatable by adversaries. To make our approach more concrete, we study it in the context of micro-data release with publicly known generalization algorithms. The analysis and experiments both confirm our algorithms to be more efficient than existing solutions.\n\nTitle: Privacy-preserving data publishing for cluster analysis\nContent: Releasing person-specific data could potentially reveal sensitive information about individuals. k-anonymization is a promising privacy protection mechanism in data publishing. Although substantial research has been conducted on k-anonymization and its extensions in recent years, only a few prior works have considered releasing data for some specific purpose of data analysis. This paper presents a practical data publishing framework for generating a masked version of data that preserves both individual privacy and information usefulness for cluster analysis. Experiments on real-life data suggest that by focusing on preserving cluster structure in the masking process, the cluster quality is significantly better than the cluster quality of the masked data without such focus. The major challenge of masking data for cluster analysis is the lack of class labels that could be used to guide the masking process. Our approach converts the problem into the counterpart problem for classification analysis, wherein class labels encode the cluster structure in the data, and presents a framework to evaluate the cluster quality on the masked data.\n\nTitle: A framework for privacy-preserving cluster analysis\nContent: Abstract\u2014Releasing person-specific data could potentially re- veal sensitive information,of individuals. k-anonymization,is a promising,privacy,protection,mechanism,in data,publish- ing. Though,substantial,research,has,been,conducted,on,k- anonymization and its extensions in recent years, few of them consider,releasing data for a specific purpose,of data analysis. This paper,presents,a practical,data,publishing,framework for determining,a generalized,version,of data,that preserves both,individual privacy,and,information,usefulness,for cluster analysis. Experiments on real-life data suggest that, by focusing on preserving cluster structure in the generalization process, the cluster quality is significantly better than the cluster quality on the generalized,data without such focus. The major,challenge of generalizing,data for cluster analysis is the lack of class labels that could,be used,to guide,the generalization,process. Our approach,converts,the problem,into the counterpart,problem for classification analysis where,class labels encode,the cluster structure in the data, and presents a framework to evaluate the cluster quality on the generalized,data.\n\n## Task\nCompose an abstract for the title 'Exclusive Strategy for Generalization Algorithms in Micro-data Disclosure' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"When generalization algorithms are known to the public, an adversary can obtain a more precise estimation of the secret table than what can be deduced from the disclosed generalization result. Therefore, whether a generalization algorithm can satisfy a privacy property should be judged based on such an estimation. In this paper, we show that the computation of the estimation is inherently a recursive process that exhibits a high complexity when generalization algorithms take a straightforward inclusive strategy. To facilitate the design of more efficient generalization algorithms, we suggest an alternative exclusive strategy, which adopts a seemingly drastic approach to eliminate the need for recursion. Surprisingly, the data utility of the two strategies are actually not comparable and the exclusive strategy can provide better data utility in certain cases."
      }
    ]
  },
  {
    "tid":"83",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Measure Locally, Reason Globally: Occlusion-sensitive Articulated Pose Estimation\nContent: Part-based tree-structured models have been widely used for 2D articulated human pose-estimation. These approaches admit efficient inference algorithms while capturing the important kinematic constraints of the human body as a graphical model. These methods often fail however when multiple body parts fit the same image region resulting in global pose estimates that poorly explain the overall image evidence. Attempts to solve this problem have focused on the use of strong prior models that are limited to learned activities such as walking. We argue that the problem actually lies with the image observations and not with the prior. In particular, image evidence for each body part is estimated independently of other parts without regard to self-occlusion. To address this we introduce occlusion-sensitive local likelihoods that approximate the global image likelihood using per-pixel hidden binary variables that encode the occlusion relationships between parts. This occlusion reasoning introduces interactions between non-adjacent body parts creating loops in the underlying graphical model. We deal with this using an extension of an approximate belief propagation algorithm (PAMPAS). The algorithm recovers the real-valued 2D pose of the body in the presence of occlusions, does not require strong priors over body pose and does a quantitatively better job of explaining image evidence than previous methods.\n\nTitle: HumanEva: Synchronized Video and Motion Capture Dataset and Baseline Algorithm for Evaluation of Articulated Human Motion\nContent: While research on articulated human motion and pose estimation has progressed rapidly in the last few years, there has been no systematic quantitative evaluation of competing methods to establish the current state of the art. We present data obtained using a hardware system that is able to capture synchronized video and ground-truth 3D motion. The resulting HumanEva datasets contain multiple subjects performing a set of predefined actions with a number of repetitions. On the order of 40,000 frames of synchronized motion capture and multi-view video (resulting in over one quarter million image frames in total) were collected at 60 Hz with an additional 37,000 time instants of pure motion capture data. A standard set of error measures is defined for evaluating both 2D and 3D pose estimation and tracking algorithms. We also describe a baseline algorithm for 3D articulated tracking that uses a relatively standard Bayesian framework with optimization in the form of Sequential Importance Resampling and Annealed Particle Filtering. In the context of this baseline algorithm we explore a variety of likelihood functions, prior models of human motion and the effects of algorithm parameters. Our experiments suggest that image observation models and motion priors play important roles in performance, and that in a multi-view laboratory environment, where initialization is available, Bayesian filtering tends to perform well. The datasets and the software are made available to the research community. This infrastructure will support the development of new articulated motion and pose estimation algorithms, will provide a baseline for the evaluation and comparison of new methods, and will help establish the current state of the art in human pose estimation and tracking.\n\nTitle: A Database and Evaluation Methodology for Optical Flow\nContent: The quantitative evaluation of optical flow algorithms by Barron et al. (1994) led to significant advances in performance. The challenges for optical flow algorithms today go beyond the datasets and evaluation methods proposed in that paper. Instead, they center on problems associated with complex natural scenes, including nonrigid motion, real sensor noise, and motion discontinuities. We propose a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms. To that end, we contribute four types of data to test different aspects of optical flow algorithms: (1) sequences with nonrigid motion where the ground-truth flow is determined by tracking hidden fluorescent texture, (2) realistic synthetic sequences, (3) high frame-rate video used to study interpolation error, and (4) modified stereo sequences of static scenes. In addition to the average angular error used by Barron et al., we compute the absolute flow endpoint error, measures for frame interpolation error, improved statistics, and results at motion discontinuities and in textureless regions. In October 2007, we published the performance of several well-known methods on a preliminary version of our data to establish the current state of the art. We also made the data freely available on the web at http://vision.middlebury.edu/flow/ . Subsequently a number of researchers have uploaded their results to our website and published papers using the data. A significant improvement in performance has already been achieved. In this paper we analyze the results obtained to date and draw a large number of conclusions from them.\n\nTitle: Patches, Planes And Probabilities: A Non-Local Prior For Volumetric 3d Reconstruction\nContent: In this paper, we propose a non-local structured prior for volumetric multi-view 3D reconstruction. Towards this goal, we present a novel Markov random field model based on ray potentials in which assumptions about large 3D surface patches such as planarity or Manhattan world constraints can be efficiently encoded as probabilistic priors. We further derive an inference algorithm that reasons jointly about voxels, pixels and image segments, and estimates marginal distributions of appearance, occupancy, depth, normals and planarity. Key to tractable inference is a novel hybrid representation that spans both voxel and pixel space and that integrates non-local information from 2D image segmentations in a principled way. We compare our non-local prior to commonly employed local smoothness assumptions and a variety of state-of-the-art volumetric reconstruction baselines on challenging outdoor scenes with textureless and reflective surfaces. Our experiments indicate that regularizing over larger distances has the potential to resolve ambiguities where local regularizers fail.\n\nTitle: 3D Human Limb Detection using Space Carving and Multi-View Eigen Models\nContent: In this paper, we integrate space carving and eigen detection methods to develop a bottom-up 3D human limb detector. We model the body in terms of its constituent body parts; here we focus on the head, lower arms, upper arms and calves. For each body part, we build a multi-view eigen model that combines image views from multiple calibrated cameras. This approach is much more constraining than the conventional multiple single-view eigen models and provides coarse 3D pose information. We use ideas from space carving using multiple silhouette images to constrain the volume of our search for the body part locations. We have applied the method to detect the body parts of a subject in long test sequences. The approach provides bottom-up in-formation that supports the automatic initialization of a full 3D human body model.\n\n## Task\nCompose an abstract for the title 'FAUST: Dataset and Evaluation for 3D Mesh Registration' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"New scanning technologies are increasing the importance of 3D mesh data and the need for algorithms that can reliably align it. Surface registration is important for building full 3D models from partial scans, creating statistical shape models, shape retrieval, and tracking. The problem is particularly challenging for non-rigid and articulated objects like human bodies. While the challenges of real-world data registration are not present in existing synthetic datasets, establishing ground-truth correspondences for real 3D scans is difficult. We address this with a novel mesh registration technique that combines 3D shape and appearance information to produce high-quality alignments. We define a new dataset called FAUST that contains 300 scans of 10 people in a wide range of poses together with an evaluation methodology. To achieve accurate registration, we paint the subjects with high-frequency textures and use an extensive validation process to ensure accurate ground truth. We find that current shape registration methods have trouble with this real-world data. The dataset and evaluation website are available for research purposes at http://faust.is.tue.mpg.de."
      }
    ]
  },
  {
    "tid":"84",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A hierarchical framework for modeling multimodality and emotional evolution in affective dialogs.\nContent: Incorporating multimodal information and temporal context from speakers during an emotional dialog can contribute to improving performance of automatic emotion recognition systems. Motivated by these issues, we propose a hierarchical framework which models emotional evolution within and between emotional utterances, i.e., at the utterance and dialog level respectively. Our approach can incorporate a variety of generative or discriminative classifiers at each level and provides flexibility and extensibility in terms of multimodal fusion; facial, vocal, head and hand movement cues can be included and fused according to the modality and the emotion classification task. Our results using the multimodal, multi-speaker IEMOCAP database indicate that this framework is well-suited for cases where emotions are expressed multimodally and in context, as in many real-life situations.\n\nTitle: A Low-Complexity Dynamic Face-Voice Feature Fusion Approach to Multimodal Person Recognition\nContent: In this paper, we show the importance of face-voice correlation for audio-visual person recognition. We evaluate the performance of a system which uses the correlation between audio-visual features during speech against audio-only, video-only and audio-visual systems which use audio and visual features independently neglecting the interdependency of a person's spoken utterance and the associated facial movements. Experiments performed on the Vid-TIMIT dataset show that the proposed multimodal scheme has lower error rate than all other comparison conditions and is more robust against replay attacks. The simplicity of the fusion technique also allows the use of only one classifier which greatly simplifies system design and allows for a simple real-time DSP implementation.\n\nTitle: Modeling Mutual Influence Of Multimodal Behavior In Affective Dyadic Interactions\nContent: To accomplish effective communication, interaction partners generally adapt their verbal and non-verbal behavior to that of their interlocutors. This behavior adaptation is often modulated by the underlying emotional states of partners. Modeling such mutual behavioral influence is critical for emotion characterization in an interaction. In this paper, we focus on explicitly modeling the mutual influence of multimodal behavior (speech and hand gesture) in affective dyadic interactions. In our framework, the behavior adaptation in each interaction is modeled by an interaction matrix which assembles all behavioral information on the path between the dyad's behavior. Experimental results show that our modeling approach can significantly improve the performance of emotion recognition. We further investigate the properties of the interaction model. Analysis results reveal that the entrainment effect of dyad's behavior can be better embodied by interaction modeling, and that the interaction patterns captured by interaction matrices are dependent on the emotional states of interaction partners. These observations corroborate the validity of our interaction model for capturing emotion-dependent mutual influence of dyad's interaction behavior.\n\nTitle: Audio-Visual Emotion Recognition Using Gaussian Mixture Models for Face and Voice\nContent: Emotion expression associated with human communication is known to be a multimodal process. In this work, we investigate the way that emotional information is conveyed by facial and vocal modalities, and how these modalities can be effectively combined to achieve improved emotion recognition accuracy. In particular, the behaviors of different facial regions are studied in detail. We analyze an emotion database recorded from ten speakers (five female, five male), which contains speech and facial marker data. Each individual modality is modeled by Gaussian Mixture Models (GMMs). Multiple modalities are combined using two different methods: a Bayesian classifier weighting scheme and support vector machines that use post classification accuracies as features. Individual modality recognition performances indicate that anger and sadness have comparable accuracies for facial and vocal modalities, while happiness seems to be more accurately transmitted by facial expressions than voice. The neutral state has the lowest performance, possibly due to the vague definition of neutrality. Cheek regions achieve better emotion recognition accuracy compared to other facial regions. Moreover, classifier combination leads to significantly higher performance, which confirms that training detailed single modality classifiers and combining them at a later stage is an effective approach.\n\nTitle: Combining acoustic and language information for emotion recognition\nContent: This paper reports on emotion recognition using both acoustic and language information in spoken utterances. So far, most previous efforts have focused on emotion recognition using acoustic cor- relates although it is well known that language information also conveys emotions. For capturing emotional information at the language level, we introduce the information-theoretic notion of 'emotional salience'. For acoustic information, linear discrimi- nant classifiers and k-nearest neighborhood classifiers were used in the emotion classification. The combination of acoustic and linguistic information is posed as a data fusion problem to ob- tain the combined decision. Results using spoken dialog data ob- tained from a telephone-based human-machine interaction appli- cation show that combining acoustic and language information im- proves negative emotion classification by 45.7% (linear discrimi- nant classifier used for acoustic information) and 32.9%, respec- tively, over using only acoustic and language information.\n\n## Task\nCompose an abstract for the title 'A Multimodal Approach to Understanding Human Vocal Expressions and Beyond.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Human verbal and nonverbal expressions carry crucial information not only about intent but also emotions, individual identity, and the state of health and wellbeing. From a basic science perspective, understanding how such rich information is encoded in these signals can illuminate underlying production mechanisms including the variability therein, within and across individuals. From a technology perspective, finding ways for automatically processing and decoding this complex information continues to be of interest across a variety of applications.\n\nThe convergence of sensing, communication and computing technologies is allowing access to data, in diverse forms and modalities, in ways that were unimaginable even a few years ago. These include data that afford the multimodal analysis and interpretation of the generation of human expressions.\n\nThe first part of the talk will highlight advances that allow us to perform investigations on the dynamics of vocal production using real-time imaging and audio modeling to offer insights about how we produce speech and song with the vocal instrument. The second part of the talk will focus on the production of vocal expressions in conjunction with other signals from the face and body especially in encoding affect. The talk will draw data from various domains notably in health to illustrate some of the applications.\n\n"
      }
    ]
  },
  {
    "tid":"85",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: An Analysis Of Pca-Based Vocal Entrainment Measures In Married Couples' Affective Spoken Interactions\nContent: Entrainment has played a crucial role in analyzing marital couples interactions. In this work, we introduce a novel technique for quantifying vocal entrainment based on Principal Component Analysis (PCA). The entrainment measure, as we define in this work, is the amount of preserved variability of one interlocutor's speaking characteristic when projected onto representing space of the other's speaking characteristics. Our analysis on real couples interactions shows that when a spouse is rated as having positive emotion, he/she has a higher value of vocal entrainment compared when rated as having negative emotion. We further performed various statistical analyses on the strength and the directionality of vocal entrainment under different affective interaction conditions to bring quantitative insights into the entrainment phenomenon. These analyses along with a baseline prediction model demonstrate the validity and utility of the proposed PCA-based vocal entrainment measure.\n\nTitle: Computing vocal entrainment: A signal-derived PCA-based quantification scheme with application to affect analysis in married couple interactions\nContent: In human-human interactions, entrainment is a naturally occurring phenomenon that happens when interlocutors mutually adapt their behaviors through the course of an interaction. This mutual behavioral dependency has been at the center of psychological studies of human communication for decades. Quantitative descriptors of the degree of entrainment can provide psychologists an objective method to advance studies of human communication including in mental health domains. However, the subtle nature of the entrainment phenomenon makes it challenging for computing such an effect based on just human annotations. In this paper, we propose an unsupervised signal-derived approach within a principal component analysis framework for quantifying one aspect of entrainment in communication, namely, vocal entrainment. The proposed approach to quantify the degree of vocal entrainment involves measuring the similarity of specific vocal characteristics between the interlocutors in a dialog. These quantitative descriptors were analyzed using two psychology-inspired hypothesis tests to not only establish that these signal-derived measures carry meaningful information in interpersonal communication but also offer statistical evidence into aspects of behavioral dependency and associated affective states in marital conflictual interactions. Finally, affect recognition experiments were performed with the proposed vocal entrainment descriptors as features using a large database of real distressed married couples' interactions. An accuracy of 62.56% in differentiating between positive and negative affect was obtained using these entrainment measures with Factorial Hidden Markov Models lending further support that entrainment is an active component underlying affective processes in interactions.\n\nTitle: Affective state recognition in married couples' interactions using PCA-based vocal entrainment measures with multiple instance learning\nContent: Recently there has been an increase in efforts in Behavioral Signal Processing (BSP), that aims to bring quantitative analysis using signal processing techniques in the domain of observational coding. Currently observational coding in fields such as psychology is based on subjective expert coding of abstract human interaction dynamics. In this work, we use a Multiple Instance Learning (MIL) framework, a saliencybased prediction model, with a signal-driven vocal entrainment measure as the feature to predict the affective state of a spouse in problem solving interactions. We generate 18 MIL classifiers to capture the variablelength saliency of vocal entrainment, and a cross-validation scheme with maximum accuracy and mutual information as the metric to select the best performing classifier for each testing couple. This method obtains a recognition accuracy of 53.93%, a 2.14% (4.13% relative) improvement over baseline model using Support Vector Machine. Furthermore, this MIL-based framework has potential for identifying meaningful regions of interest for further detailed analysis of married couples interactions.\n\nTitle: Modeling head motion entrainment for prediction of couples' behavioral characteristics\nContent: Our work examines the link between head motion entrainment of interacting couples and human expert's judgment on certain overall behavioral characteristics (e.g., Blame patterns). We employ a data-driven model that clusters head motion in an unsupervised manner into elementary types called kinemes. We propose three groups of similarity measures based on Kullback-Leibler divergence to model entrainment. We find that the divergence of the (joint) distribution of kinemes yields consistent and significant correlation with target behavior characteristics. The divergence of the conditional distribution of kinemes is shown to predict the polarity of the behavioral characteristics. We partly explain the strong correlations via associating the conditional distributions with the prominent behavioral implications of their respective associated kinemes. These results show the possibility of inferring human behavioral characteristics through the modeling of dyadic head motion entrainment.\n\nTitle: Toward automating a human behavioral coding system for married couples' interactions using speech acoustic features\nContent: Observational methods are fundamental to the study of human behavior in the behavioral sciences. For example, in the context of research on intimate relationships, psychologists' hypotheses are often empirically tested by video recording interactions of couples and manually coding relevant behaviors using standardized coding systems. This coding process can be time-consuming, and the resulting coded data may have a high degree of variability because of a number of factors (e.g., inter-evaluator differences). These challenges provide an opportunity to employ engineering methods to aid in automatically coding human behavioral data. In this work, we analyzed a large corpus of married couples' problem-solving interactions. Each spouse was manually coded with multiple session-level behavioral observations (e.g., level of blame toward other spouse), and we used acoustic speech features to automatically classify extreme instances for six selected codes (e.g., ''low'' vs. ''high'' blame). Specifically, we extracted prosodic, spectral, and voice quality features to capture global acoustic properties for each spouse and trained gender-specific and gender-independent classifiers. The best overall automatic system correctly classified 74.1% of the instances, an improvement of 3.95% absolute (5.63% relative) over our previously reported best results. We compare performance for the various factors: across codes, gender, classifier type, and feature type.\n\n## Task\nCompose an abstract for the title 'Using measures of vocal entrainment to inform outcome-related behaviors in marital conflicts' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Behavioral entrainment is an important, naturally-occurring dynamic phenomenon in human interactions. In this paper, we carry out two quantitative analyses of the vocal entrainment phenomenon in the context of studying conflictual marital interactions. We investigate the role of vocal entrainment in reflecting different dimensions of couple-specific behaviors, such as withdrawal, that are commonly-used in assessing the effectiveness on the outcome of couple therapy. The results indicate a statistically-significant relation between these behaviors and vocal entrainment, as quantified using our proposed unsupervised signal-derived computational framework. We further demonstrate the potential of the signal-based vocal entrainment framework in characterizing influential factors in distressed couples relationship satisfaction outcomes."
      }
    ]
  },
  {
    "tid":"86",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A workload-driven unit of cache replacement for mid-tier database caching\nContent: Making multi-terabyte scientific databases publicly accessible over the Internet is increasingly important in disciplines such as Biology and Astronomy. However, contention at a centralized, backend database is a major performance bottleneck, limiting the scalability of Internet-based, database applications. Midtier caching reduces contention at the backend database by distributing database operations to the cache. To improve the performance of mid-tier caches, we propose the caching of query prototypes, a workload-driven unit of cache replacement in which the cache object is chosen from various classes of queries in the workload. In existingmid-tier caching systems, the storage organization in the cache is statically defined. Our approach adapts cache storage to workload changes, requires no prior knowledge about the workload, and is transparent to the application. Experiments over a one-month, 1.4 million query Astronomy workload demonstrate up to 70% reduction in network traffic and reduce query response time by up to a factor of three when compared with alternative units of cache replacement.\n\nTitle: Data management and query - Estimating query result sizes for proxy caching in scientific database federations\nContent: In a proxy cache for federations of scientific databases it is important to estimate the size of a query before making a caching decision. With accurate estimates, near-optimal cache performance can be obtained. On the other extreme, inaccurate estimates can render the cache totally ineffective.We present classification and regression over templates (CAROT), a general method for estimating query result sizes, which is suited to the resource-limited environment of proxy caches and the distributed nature of database federations. CAROT estimates query result sizes by learning the distribution of query results, not by examining or sampling data, but from observing workload. We have integrated CAROT into the proxy cache of the National Virtual Observatory (NVO) federation of astronomy databases. Experiments conducted in the NVO show that CAROT dramatically outperforms conventional estimation techniques and provides near-optimal cache performance.\n\nTitle: Safe Caching in a Distributed File System for Network Attached Storage\nContent: In a distributed file system built on network attached storage, client computers access data directly from shared storage, rather than submitting I/O requests through a server. Without a server marshaling access to data, if a computer fails or becomes isolated in a network partition while holding locks on cached data objects, those objects become inaccessible to other computers until a locking au- thority can guarantee that the lock holder will not again directly access these data. We describe a server that acts as the locking authority and implements a lease-based proto- col for revoking access to data objects locked by an isolated or failed computer. When a lease expires, the server can be assured that the client no longer acts on locked data, and can safely redistribute locks to other clients. During nor- mal operation, this protocol invokes no message overhead, and uses no memory and performs no computation at the locking authority.\n\nTitle: A parallel page cache: IOPS and caching for multicore systems\nContent: We present a set-associative page cache for scalable parallelism of IOPS in multicore systems. The design eliminates lock contention and hardware cache misses by partitioning the global cache into many independent page sets, each requiring a small amount of metadata that fits in few processor cache lines. We extend this design with message passing among processors in a non-uniform memory architecture (NUMA). We evaluate the set-associative cache on 12-core processors and a 48- core NUMA to show that it realizes the scalable IOPS of direct I/O (no caching) and matches the cache hits rates of Linux's page cache. Set-associative caching maintains IOPS at scale in contrast to Linux for which IOPS crash beyond eight parallel threads.\n\nTitle: Optimize Unsynchronized Garbage Collection in an SSD Array\nContent:   Solid state disks (SSDs) have advanced to outperform traditional hard drives significantly in both random reads and writes. However, heavy random writes trigger fre- quent garbage collection and decrease the performance of SSDs. In an SSD array, garbage collection of individ- ual SSDs is not synchronized, leading to underutilization of some of the SSDs.   We propose a software solution to tackle the unsyn- chronized garbage collection in an SSD array installed in a host bus adaptor (HBA), where individual SSDs are exposed to an operating system. We maintain a long I/O queue for each SSD and flush dirty pages intelligently to fill the long I/O queues so that we hide the performance imbalance among SSDs even when there are few parallel application writes. We further define a policy of select- ing dirty pages to flush and a policy of taking out stale flush requests to reduce the amount of data written to SSDs. We evaluate our solution in a real system. Experi- ments show that our solution fully utilizes all SSDs in an array under random write-heavy workloads. It improves I/O throughput by up to 62% under random workloads of mixed reads and writes when SSDs are under active garbage collection. It causes little extra data writeback and increases the cache hit rate. \n\n## Task\nCompose an abstract for the title 'Automated physical design in database caches' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The ability of a database management system (DBMS) to detect problems or problem trends (which have not yet manifested as problems) and either take corrective actions, and/or provide advice or an automated notification to the database administrator (DBA) ..."
      }
    ]
  },
  {
    "tid":"87",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Multi-Scale Structured Cnn With Label Consistency For Brain Mr Image Segmentation\nContent: In this paper, a novel method for brain MR image segmentation has been proposed, with deep learning techniques to obtain preliminary labelling and graphical models to produce the final result. A specific architecture, namely multi-scale structured convolutional neural networks (MS-CNN), is designed to capture discriminative features for each sub-cortical structure and to generate a label probability map for the target image. Due to complex background in brain images and the lack of spatial constraints among testing samples, the initial result obtained with MS-CNN is not smooth. To deal with this problem, dynamic random walker with decayed region of interest is then proposed to enforce label consistency. Comprehensive evaluations have been carried out on two publicly available data-sets and experimental results indicate that the proposed method can obtain better segmentation quality efficiently.\n\nTitle: A novel framework for segmentation of deep brain structures based on Markov dependence tree.\nContent: The aim of this work is to develop a new framework for multi-object segmentation of deep brain structures (caudate nucleus, putamen and thalamus) in medical brain images. Deep brain segmentation is difficult and challenging because the structures of interest are of relatively small size and have significant shape variations. The structure boundaries may be blurry or even missing, and the surrounding background is full of irrelevant edges. To tackle these problems, we propose a template-based framework to fuse the information of edge features, region statistics and inter-structure constraints for detecting and locating all target brain structures such that initialization by hand is unnecessary. The multi-object template is organized in the form of a hierarchical Markov dependence tree (MDT), and multiple objects are efficiently matched to a target image by a top-to-down optimization strategy. The final segmentation is obtained through refinement by a B-spline based non-rigid registration between the exemplar image and the target image. Our approach needs only one example as training data. We have validated the proposed method on a publicly available T1-weighted magnetic resonance image database with expert-segmented brain structures. In the experiments, the proposed approach has obtained encouraging results with 0.80 Dice score for the caudate nuclei, 0.81 Dice score for the putamina and 0.84 Dice score for the thalami on average.\n\nTitle: Markov dependence tree-based segmentation of deep brain structures.\nContent: We propose a new framework for multi-object segmentation of deep brain structures, which have significant shape variations and relatively small sizes in medical brain images. In the images, the structure boundaries may be blurry or even missing, and the surrounding background is a clutter and full of irrelevant edges. We suggest a template-based framework, which fuses the information of edge features, region statistics and inter-structure constraints to detect and locate all the targeted brain structures such that manual initialization is unnecessary. The multi-object template is organized in the form of a hierarchical Markov dependence tree. It makes the matching of multiple objects efficient. Our approach needs only one example as training data and alleviates the demand of a large training set. The obtained segmentation results on real data are encouraging and the proposed method enjoys several important advantages over existing methods.\n\nTitle: Feature Sensitive Label Fusion with Random Walker for Atlas-based Image Segmentation.\nContent: In this paper, a novel label fusion method is proposed for brain magnetic resonance image segmentation. This label fusion method is formulated on a graph, which embraces both label priors from atlases and anatomical priors from target image. To represent a pixel in a comprehensive way, three kinds of feature vectors are generated, including intensity, gradient, and structural signature. To select candidate atlas nodes for fusion, rather than exact searching, randomized k-d tree with spatial constraint is introduced as an efficient approximation for high-dimensional feature matching. Feature sensitive label prior (FSLP), which takes both the consistency and variety of different features into consideration, is proposed to gather atlas priors. As FSLP is a non-convex problem, one heuristic approach is further designed to solve it efficiently. Moreover, based on the anatomical knowledge, parts of the target pixels are also employed as the graph seeds to assist the label fusion process, and an iterative strategy is utilized to gradually update the label map. The comprehensive experiments carried out on two publicly available databases give results to demonstrate that the proposed method can obtain better segmentation quality.\n\nTitle: Vessel and intracranial aneurysm segmentation using multi-range filters and local variances\nContent: Segmentation of vessels and brain aneurysms on non-invasive and flow-sensitive phase contrast magnetic resonance angiographic (PCMRA) images is essential in the detection of vascular diseases, in particular, intracranial aneurysms. In this paper, we devise a novel method based on multi-range filters and local variances to perform segmentation of vessels and intracranial aneurysms on PCMRA images. The proposed method is validated and compared using a synthetic and numerical image volume and four clinical cases. It is experimentally shown that the proposed method is capable of segmenting vessels and aneurysms with various sizes on PCMRA images.\n\n## Task\nCompose an abstract for the title 'An atlas-based deep brain structure segmentation method: from coarse positioning to fine shaping' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Segmentation of deep brain structures is a challenging task for MRI images due to blurry structure boundaries, small object size and irregular shapes. In this paper, we present a new atlas-based segmentation method. It first uses a prior spatial dependency tree to constrain the relative positions between different deep brain structures and determine an optimal sequence for the structure by-structure segmentation. After positioning the structures, the segmentation result is further fine tuned by a non-rigid registration procedure between the atlas image and the target image using the histogram of the gradient magnitudes lying on the structure boundaries. The pro posed method has been applied on a publicly available MRI brain database and can achieve comparatively high segmentation accuracy."
      }
    ]
  },
  {
    "tid":"88",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Constructions of dynamic and non-dynamic threshold public-key encryption schemes with decryption consistency.\nContent: Dynamic threshold public-key encryption, proposed by Delerabl\u00e9e and Pointcheval (CRYPTO 2008), is an extension of ordinary threshold encryption which enables decryption servers to join the system even after the setup phase, and to choose the authorized set and the threshold of decryption dynamically. Delerabl\u00e9e and Pointcheval proposed the first dynamic threshold public-key encryption scheme, which they proved secure under a non-standard q-type assumption. However, decryption consistency, which is an important security property that guarantees uniqueness of decryption, even when a sender and decryption servers behave maliciously, is only shown to hold in the random oracle model. In this paper, we propose three threshold public-key encryption schemes. The first and second schemes are both dynamic schemes. The former achieves a relatively weaker variant of decryption consistency, while the latter achieves a strong variant thereof. The former is a generic construction from public-key encryption with non-interactive opening (PKENO), while the latter is a specific construction from a standard number-theoretic assumption. These are the first constructions of dynamic public-key encryption, which achieve decryption consistency without relying on the random oracle model. Furthermore, both schemes can be realized based on standard assumptions. The third construction is a generic construction from PKENO achieving the strong variant of decryption consistency. This construction affirmatively answers the question indirectly posed by Galindo et al. (AFRICACRYPT 2010) of whether a generic construction achieving strong decryption consistency is possible.\n\nTitle: Dynamic Threshold Public-Key Encryption With Decryption Consistency From Static Assumptions\nContent: Dynamic threshold public-key encryption (dynamic TPKE) is a natural extension of ordinary TPKE which allows decryption servers to join the system dynamically after the system is set up, and allows the sender to dynamically choose the authorized set and the decryption threshold at the time of encryption. Currently, the only known dynamic TPKE scheme is a scheme proposed by Delerabl'ee and Pointcheval (CRYPTO 2008). This scheme is proven to provide message confidentiality under a q-type assumption, but to achieve decryption consistency, a random oracle extension is required.In this paper we show conceptually simple methods for constructing dynamic TPKE schemes with decryption consistency from only static assumptions (e.g., the decisional linear assumption in bilinear groups) without relying on random oracles. Our first construction is a purely generic construction from public-key encryption with non-interactive opening (PKENO) formalized by Damgard et al. (CT-RSA 2008). However, this construction achieves a slightly weaker notion of decryption consistency compared to the random oracle extension of the Delerablee and Pointcheval scheme, which satisfies the notion defined by Boneh, Boyen and Halevi (CT-RSA 2005). Our second construction uses a specific PKENO scheme based on the decisional linear assumption in combination with the efficient zero-knowledge proofs by Groth and Sahai. In contrast to our first construction, our second construction achieves the stronger notion of decryption consistency defined by Boneh, Boyen and Halevi.\n\nTitle: Efficient variants of the Naor-Yung and Dolev-Dwork-Naor transforms for CCA secure key encapsulation mechanism\nContent: In this paper, we present novel constructions of chosen-ciphertext secure (CCA secure) key encapsulation mechanism (KEM) from chosen-plaintext secure (CPA secure) KEM in the standard model. It is already known that CCA secure public key encryption (PKE) can be generically constructed from CPA secure PKE and ((simulation-sound) non-interactive zero-knowledge proof) via the Naor-Yung or Dolev-Dwork-Naor transforms. Thus, one can also immediately construct CCA secure PKE from CPA secure KEM by converting CPA secure KEM into CPA secure PKE and transforming it to be CCA secure PKE. However, such a construction seems redundant since in general PKE is less efficient than KEM and it would be more efficient if we can directly construct CCA secure KEM from CPA secure KEM without intermediating CPA secure PKE. In this work, we propose new variants of the Naor-Yung and Dolev-Dwork-Naor transforms that directly convert CPA secure KEM into CCA secure KEM, and show that our proposed schemes are more efficient than the above straightforward constructions. For example, when instantiating from the decision linear assumption, ciphertext size of our Naor-Yung variant consists of 34 group elements while that of the straightforward construction consists of 47 group elements. Furthermore, we also propose another variant of the Dolev-Dwork-Naor transform from multiple KEM and show that a KEM which is obtained from Wee's extractable hash proof system can also be considered as an efficient construction of multiple KEM.\n\nTitle: Key-Insulated Public Key Encryption with Auxiliary Helper Key: Model, Constructions and Formal Security Proofs\nContent: To deal with the problem of secret key exposure, Dodis, Katz, Xu and Yung [6] proposed a key-insulated public key encryption (KIE), which uses the secure helper key to update the secret key more often and helps to minimize the damage overall. We take this idea further in improving the helper key security by introducing an auxiliary helper key besides of the main helper key. The auxiliary helper key is used less than the main helper key and can help the system to reduce the damage even in case of helper key exposure. This paper give two different schemes of KIE with auxiliary helper key. There are trade-offs between public key length and ciphertext size, as well as between encryption algorithms and decryption algorithms in these two schemes. With these trade-offs, it is flexible to choose an efficient one to implement in reality. We also show concrete constructions with chosen-ciphertext security. The formal security proofs for all schemes are given in this paper, based on the CBDH assumption [2] in the random oracle model.\n\nTitle: Unconditionally Secure Key Insulated Cryptosystems: Models, Bounds and Constructions\nContent: Computer systems are constantly under attack and illegal access is a constant threat which makes security even more critical. A system can be broken into and secret information, e.g. decryption key, may be exposed. Very recently, a new framework for the protection against such key exposure problem was suggested and was called, key-insulated encryption (KIE). In this paper, we study key insulated cryptosystems without computational assumptions. First, we define the model of KIE in unconditional setting and show lower bounds on required memory sizes. Our bounds are all tight since our concrete construction of KIE achieves the bounds. In other words, this construction is optimal in terms of memory sizes of a user, a trusted device and a sender. We then, extend the concept of it further, and add an extra property so that any pair of users in the system can communicate with each other. We called the version with this added extension, dynamic and mutual key insulated encryption (DMKIE), and concrete implementations of DMKIE are also shown. Finally, we discuss the relationship among KIE, key predistribution schemes (KPS) and broadcast encryption schemes (BES), specifically, showing that DMKIE can be constructed from KPS or BES.\n\n## Task\nCompose an abstract for the title 'More Constructions Of Re-Splittable Threshold Public Key Encryption' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The concept of threshold public key encryption (TPKE) with the special property called key re-splittability (re-splittable TPKE, for short) was introduced by Hanaoka et al. (CT-RSA 2012), and used as one of the building blocks for constructing their proxy re-encryption scheme. In a re-splittable TPKE scheme, a secret key can be split into a set of secret key shares not only once, but also multiple times, and the security of the TPKE scheme is guaranteed as long as the number of corrupted secret key shares under the same splitting is smaller than the threshold. In this paper, we show several new constructions of a re-splittable TPKE scheme by extending the previous (ordinary) TPKE schemes. All of our proposed schemes are based on discrete logarithm (DL)-type assumptions. Therefore, our results suggest that key re-splittability is a very natural property for DL-type TPKE schemes."
      }
    ]
  },
  {
    "tid":"89",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: IT Security in L\u00fcbeck \u2013 The design of a modern and future-proof security curriculum\nContent: With the increasing reliance on digitally connected systems, the need for secure and dependable systems continues to rise in lockstep with the need for well trained security professionals that can address this rising demand. The University of Lubeck established a new degree program in IT Security in the Fall of 2016. The creation of the degree program results from a high demand for specialists both nationally and internationally, along with the already existing strengths in security and dependability related research areas at the institutes of the university. The educational goals of the new degree program are the training of computer scientists with profound practical skills in building and protecting secure and dependable IT systems as well as all the necessary theoretical background to create systems according to the security by design principle on a large scale.\n\nTitle: A Simulation-Based Methodology for Evaluating the DPA-Resistance of Cryptographic Functional Units with Application to CMOS and MCML Technologies\nContent: This paper explores the resistance of MOS Current Mode Logic (MCML) against Differential Power Analysis (DPA) attacks. Circuits implemented in MCML, in fact, have unique characteristics both in terms of power consumption and the dependency of the power profile from the input signal pattern. Therefore, MCML is suitable to protect cryptographic hardware from DPA and similar side-channel attacks. In order to demonstrate the effectiveness of different logic styles against power analysis attacks, the non-linear bijective function of the Kasumi algorithm (known as substitution box S7) was implemented with CMOS and MCML technology, and a set of attacks was performed using power traces derived from SPICE-level simulations. Although all keys were discovered for CMOS, only very few attacks to MCML were successful.\n\nTitle: Extending Glitch-Free Multiparty Protocols to Resist Fault Injection Attacks.\nContent: Side channel analysis and fault attacks are two powerful methods to analyze and break cryptographic implementations. At CHES 2011, Roche and Prouff applied secure multiparty computation to prevent side-channel attacks. While multiparty computation is known to be fault-resistant as well, the particular scheme used for side-channel protection does not currently offer this feature. This work introduces a new secure multiparty circuit to prevent both fault injection attacks and sidechannel analysis. The new scheme extends the Roche and Prouff scheme to make faults detectable. Arithmetic operations have been redesigned to propagate fault information until a new secrecy-preserving fault detection can be performed. A new recombination operation ensures randomization of the output in the case of a fault, ensuring that nothing can be learned from the faulty output. The security of the new scheme is proved in the ISW probing model, using the reformulated t-SNI security notion. Besides the new scheme and its security proof, we also present an extensive performance analysis, including a proof-of-concept, software-based AES implementation featuring the masking technique to resist both fault and side-channel attacks at the same time. The performance analysis for different security levels are given for the ARM-M0+ MCU with its memory requirements. A comprehensive leakage analysis shows that a careful implementation of the scheme achieves the expected security level.\n\nTitle: Evaluating Resistance of MCML Technology to Power Analysis Attacks Using a Simulation-Based Methodology\nContent: This paper explores the resistance of MOS Current Mode Logic (MCML) against attacks based on the observation of the power consumption. Circuits implemented in MCML, in fact, have unique characteristics both in terms of power consumption and the dependency of the power profile from the input signal pattern. Therefore, MCML is suitable to protect cryptographic hardware from Differential Power Analysis and similar side-channel attacks. In order to demonstrate the effectiveness of different logic styles against power analysis attacks, two full cores implementing the AES algorithm were realized and implemented with CMOS and MCML technology, and a set of different types of attack was performed using power traces derived from SPICE-level simulations. Although all keys were discovered for CMOS, MCML traces did not presents characteristic that can lead to a successful attack.\n\nTitle: KeeLoq and Side-Channel Analysis-Evolution of an Attack\nContent: Last year we were able to break KeeLoq, which is a 64 bit block cipher that is popular for remote keyless entry (RKE) systems. KeeLoq RKEs are widely used for access control purposes such as garage openers or car door systems. Even though the attack seems almost straightforward in hindsight, there where many practical and theoretical problems to overcome. In this talk I want to describe the evolution of the attack over about two years. Also, some possible future improvements using fault-injection will be mentioned. During the first phase of breaking KeeLoq, a surprisingly long time was spent on analyzing the target hardware, taking measurements and wondering why we did not succeed. In the second phase, we were able to use differential power analysis attacks successfully on numerous commercially available products employing KeeLoq code hopping. Our techniques allow for efficiently revealing both the secret key of a remote transmitter and the manufacturer key stored in a receiver. As a result, a remote control can be cloned from only ten power traces, allowing for a practical key recovery in a few minutes. With similar techniques but with considerably more measurements (typically on the order of 10,000) we can extract the manufacturer key which is stored in every receiver device, e.g., a garage door opener unit. In the third phase, and most recent phase, we were able to come up with several improvements. Most notably, we found that an SPA (simple power analysis) attack allows to recover the manufacturer key with one measurement. In the talk, we will also speculate about extensions to fault-injection and timing attacks. It is important to note that most of our findings are not specific to KeeLoq but are - in principle - applicable to any symmetric cipher with an implementation that is not sidechannel resistant.\n\n## Task\nCompose an abstract for the title 'Sicherheit moderner Funkt\u00fcr\u00f6ffnersysteme' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Zusammenfassung\u00a0\u00a0Durch Automatisierung gelangt immer mehr Komfort in unseren Alltag. Ein System, das in den vergangenen Jahren rasante Verbreitung\n gefunden hat, sind Funkt\u00fcr\u00f6ffner. Garagentor, Haust\u00fcr und Auto entriegeln sich auf Knopfdruck aus mehreren Metern Entfernung.\n Wie sicher aber sind diese Verfahren? Die Autoren entwickelten einen erfolgreichen Seitenkanalangriff auf Funkt\u00fcr\u00f6ffnersysteme.\n "
      }
    ]
  },
  {
    "tid":"90",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: World-stabilized annotations and virtual scene navigation for remote collaboration\nContent: We present a system that supports an augmented shared visual space for live mobile remote collaboration on physical tasks. The remote user can explore the scene independently of the local user's current camera position and can communicate via spatial annotations that are immediately visible to the local user in augmented reality. Our system operates on off-the-shelf hardware and uses real-time visual tracking and modeling, thus not requiring any preparation or instrumentation of the environment. It creates a synergy between video conferencing and remote scene exploration under a unique coherent interface. To evaluate the collaboration with our system, we conducted an extensive outdoor user study with 60 participants comparing our system with two baseline interfaces. Our results indicate an overwhelming user preference (80%) for our system, a high level of usability, as well as performance benefits compared with one of the two baselines.\n\nTitle: In touch with the remote world: remote collaboration with augmented reality drawings and virtual navigation\nContent: Augmented reality annotations and virtual scene navigation add new dimensions to remote collaboration. In this paper, we present a touchscreen interface for creating freehand drawings as world-stabilized annotations and for virtually navigating a scene reconstructed live in 3D, all in the context of live remote collaboration. Two main focuses of this work are (1) automatically inferring depth for 2D drawings in 3D space, for which we evaluate four possible alternatives, and (2) gesture-based virtual navigation designed specifically to incorporate constraints arising from partially modeled remote scenes. We evaluate these elements via qualitative user studies, which in addition provide insights regarding the design of individual visual feedback elements and the need to visualize the direction of drawings.\n\nTitle: Interpreting 2D gesture annotations in 3D augmented reality\nContent: A 2D gesture annotation provides a simple way to annotate the physical world in augmented reality for a range of applications such as remote collaboration. When rendered from novel viewpoints, these annotations have previously only worked with statically positioned cameras or planar scenes. However, if the camera moves and is observing an arbitrary environment, 2D gesture annotations can easily lose their meaning when shown from novel viewpoints due to perspective effects. In this paper, we present a new approach towards solving this problem by using a gesture enhanced annotation interpretation. By first classifying which type of gesture the user drew, we show that it is possible to render the 2D annotations in 3D in a way that conforms more to the original intention of the user than with traditional methods. We first determined a generic vocabulary of important 2D gestures for an augmented reality enhanced remote collaboration scenario by running an Amazon Mechanical Turk study with 88 participants. Next, we designed a novel real-time method to automatically handle the two most common 2D gesture annotations - arrows and circles - and give a detailed analysis of the ambiguities that must be handled in each case. Arrow gestures are interpreted by identifying their anchor points and using scene surface normals for better perspective rendering. For circle gestures, we designed a novel energy function to help infer the object of interest using both 2D image cues and 3D geometric cues. Results indicate that our method outperforms previous approaches by better conveying the meaning of the original drawing from different viewpoints.\n\nTitle: Anchoring 2D gesture annotations in augmented reality\nContent: Augmented reality enhanced collaboration systems often allow users to draw 2D gesture annotations onto video feeds to help collaborators to complete physical tasks. This works well for static cameras, but for movable cameras, perspective effects cause problems when trying to render 2D annotations from a new viewpoint in 3D. In this paper, we present a new approach towards solving this problem by using gesture enhanced annotations. By first classifying which type of gesture the user drew, we show that it is possible to render annotations in 3D in a way that conforms more to the original intention of the user than with traditional methods. We first determined a generic vocabulary of important 2D gestures for remote collaboration by running an Amazon Mechanical Turk study with 88 participants. Next, we designed a novel system to automatically handle the top two 2D gesture annotations - arrows and circles. Arrows are handled by identifying their anchor points and using surface normals for better perspective rendering. For circles, we designed a novel energy function to help infer the object of interest using both 2D image cues and 3D geometric cues. Results indicate that our approach outperforms previous methods in terms of better conveying the original drawing's meaning from different viewpoints.\n\nTitle: Automatic Head-size Equalization in Panorama Images for Video Conferencing\nContent: In panorama images captured by omni-directional cameras during video conferencing, the image sizes of the people around the conference table are not uniform due to the varying distances to the camera. Spatially-varying-uniform (SVU) scaling functions have been proposed to warp a panorama image smoothly such that the participants have similar sizes on the image. To generate the SVU function, one needs to segment the table boundaries, which was generated manually in the previous work. In this paper, we propose a robust algorithm to automatically segment the table boundaries. To ensure the robustness, we apply a symmetry voting scheme to filter out noisy points on the edge map. Trigonometry and quadratic fitting methods are developed to fit a continuous curve to the remaining edge points. We report experimental results on both synthetic and real images\n\n## Task\nCompose an abstract for the title 'Evaluating snapping-to-photos virtual travel interfaces for 3D reconstructed visual reality' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Navigating through a virtual, 3D reconstructed scene has recently become very important in many applications. A popular approach is to virtually travel to the photos used in reconstructing the scene; such an approach may be generally termed a \"snapping-to-photos\" virtual travel interface. While previous work has either used fully constrained interfaces (always at the photos) or minimally constrained interfaces (free-flight navigation), in this paper we introduce new snapping-to-photos interfaces that lie in between these two extremes. Our snapping-to-photos interfaces snap the view to a photo in 3D based on viewpoint similarity and optionally the user's mouse cursor or finger-tap position. Experimental results, with both indoor and outdoor scene reconstructions, found that our snapping-to-photos interfaces are preferred over the baseline fully constrained-to-photos interface, that there exist differences between indoor and outdoor scenes, and that users preferred and were able to reach target photos better with click-to-snap point-of-interest snapping compared to automatic point-of-view snapping.\n\n"
      }
    ]
  },
  {
    "tid":"91",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Type 1 and 2 mixtures of Kullback-Leibler divergences as cost functions in dimensionality reduction based on similarity preservation\nContent: Stochastic neighbor embedding (SNE) and its variants are methods of dimensionality reduction (DR) that involve normalized softmax similarities derived from pairwise distances. These methods try to reproduce in the low-dimensional embedding space the similarities observed in the high-dimensional data space. Their outstanding experimental results, compared to previous state-of-the-art methods, originate from their capability to foil the curse of dimensionality. Previous work has shown that this immunity stems partly from a property of shift invariance that allows appropriately normalized softmax similarities to mitigate the phenomenon of norm concentration. This paper investigates a complementary aspect, namely, the cost function that quantifies the mismatch between similarities computed in the high- and low-dimensional spaces. Stochastic neighbor embedding and its variant t-SNE rely on a single Kullback-Leibler divergence, whereas a weighted mixture of two dual KL divergences is used in neighborhood retrieval and visualization (NeRV). We propose in this paper a different mixture of KL divergences, which is a scaled version of the generalized Jensen-Shannon divergence. We show experimentally that this divergence produces embeddings that better preserve small K-ary neighborhoods, as compared to both the single KL divergence used in SNE and t-SNE and the mixture used in NeRV. These results allow us to conclude that future improvements in similarity-based DR will likely emerge from better definitions of the cost function.\n\nTitle: Multi-scale similarities in stochastic neighbour embedding: Reducing dimensionality while preserving both local and global structure.\nContent: Stochastic neighbour embedding (SNE) and its variants are methods of nonlinear dimensionality reduction that involve soft Gaussian neighbourhoods to measure similarities for all pairs of data. In order to build a suitable embedding, these methods try to reproduce in a low-dimensional space the neighbourhoods that are observed in the high-dimensional data space. Previous works have investigated the immunity of such similarities to norm concentration, as well as enhanced cost functions, like sums of Jensen\u2013Shannon divergences. This paper proposes an additional refinement, namely multi-scale similarities, which are averages of soft Gaussian neighbourhoods with exponentially growing bandwidths. Such multi-scale similarities can replace the regular, single-scale neighbourhoods in SNE-like methods. Their objective is then to maximise the embedding quality on all scales, with the best preservation of both local and global neighbourhoods, and also to exempt the user from having to fix a scale arbitrarily. Experiments with several data sets show that the proposed multi-scale approach captures better the structure of data and improves significantly the quality of dimensionality reduction.\n\nTitle: Quality assessment of dimensionality reduction: Rank-based criteria\nContent: Dimensionality reduction aims at providing low-dimensional representations of high-dimensional data sets. Many new nonlinear methods have been proposed for the last years, yet the question of their assessment and comparison remains open. This paper first reviews some of the existing quality measures that are based on distance ranking and K-ary neighborhoods. Next, the definition of the co-ranking matrix provides a tool for comparing the ranks in the initial data set and some low-dimensional embedding. Rank errors and concepts such as neighborhood intrusions and extrusions can then be associated with different blocks of the co-ranking matrix. Several quality criteria can be cast within this unifying framework; they are shown to involve one or several of these characteristic blocks. Following this line, simple criteria are proposed, which quantify two aspects of the embedding quality, namely its overall quality and its tendency to favor intrusions or extrusions. They are applied to several recent dimensionality reduction methods in two experiments, with both artificial and real data.\n\nTitle: Scale-independent quality criteria for dimensionality reduction\nContent: Dimensionality reduction aims at representing high-dimensional data in low-dimensional spaces, in order to facilitate their visual interpretation. Many techniques exist, ranging from simple linear projections to more complex nonlinear transformations. The large variety of methods emphasizes the need of quality criteria that allow for fair comparisons between them. This paper extends previous work about rank-based quality criteria and proposes to circumvent their scale dependency. Most dimensionality reduction techniques indeed rely on a scale parameter that distinguish between local and global data properties. Such a scale dependency can be similarly found in usual quality criteria: they assess the embedding quality on a certain scale. Experiments with various dimensionality reduction techniques eventually show the strengths and weaknesses of the proposed scale-independent criteria.\n\nTitle: Quality assessment of nonlinear dimensionality reduction based on K-ary neighborhoods\nContent: Nonlinear dimensionality reduction aims at providing low-dimensional representions of high-dimensional data sets. Many new methods have been recently proposed, but the question of their assessment and comparison remains open. This paper reviews some of the existing quality measures that are based on distance ranking and K-ary neighborhoods. In this context, the comparison of the ranks in the high- and low-dimensional spaces leads to the definition of the co-ranking matrix. Rank errors and concepts such as neighborhood intrusions and extrusions can be associated with different blocks of the co-ranking matrix. The considered quality criteria are then cast within this unifying framework and the blocks they involve are identified. The same framework allows us to propose simpler criteria, which quantify two aspects of the embedding, namely its overall quality and its tendency to favor either intrusions or extrusions. Eventually, a simple experiment illustrates the soundness of the approach.\n\n## Task\nCompose an abstract for the title 'Two key properties of dimensionality reduction methods' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Dimensionality reduction aims at providing faithful low-dimensional representations of high-dimensional data. Its general principle is to attempt to reproduce in a low-dimensional space the salient characteristics of data, such as proximities. A large variety of methods exist in the literature, ranging from principal component analysis to deep neural networks with a bottleneck layer. In this cornucopia, it is rather difficult to find out why a few methods clearly outperform others. This paper identifies two important properties that enable some recent methods like stochastic neighborhood embedding and its variants to produce improved visualizations of high-dimensional data. The first property is a low sensitivity to the phenomenon of distance concentration. The second one is plasticity, that is, the capability to forget about some data characteristics to better reproduce the other ones. In a manifold learning perspective, breaking some proximities typically allow for a better unfolding of data. Theoretical developments as well as experiments support our claim that both properties have a strong impact. In particular, we show that equipping classical methods with the missing properties significantly improves their results."
      }
    ]
  },
  {
    "tid":"92",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Traffic engineering with semiautonomous users: a game-theoretic perspective\nContent: In this paper, we explore the interaction between traffic engineering and the users of a network. Because a traffic engineer may be unaware of the structure of content distribution systems or overlay networks, his management of the network does not fully anticipate how traffic might change as a result of his actions. Content distribution systems that assign servers at the application level can respond very rapidly to changes in the routing of the network. Consequently, the traffic engineer's decisions may not be applied to the intended traffic. We use a game-theoretic framework in which infinitesimal users of a network select the source of content, and the traffic engineer decides how the traffic will route through the network. We formulate a game and prove the existence of equilibria. Additionally, we present a setting in which equilibria are socially optimal, essentially unique, and stable. Conditions under which efficiency loss may be bounded are presented, and the results are extended to the cases of general overlay networks and multiple autonomous systems.\n\nTitle: Traffic Engineering vs. Content Distribution: A Game Theoretic Perspective\nContent: In this paper we explore the interaction between content distribution and traffic engineering. Because a traffic engineer may be unaware of the structure of content distribution systems or overlay networks, this management of the network does not fully anticipate how traffic might change as a result of his actions. Content distribution systems that assign servers at the application level can respond very rapidly to changes in the routing of the network. Consequently, the traffic engineer's decisions may almost never be applied to the intended traffic. We use a game-theoretic framework in which infinitesimal users of a network select the source of content, and the traffic engineer decides how the traffic will route through the network. We formulate a game and prove the existence of equilibria. Additionally, we present a setting in which equilibria are socially optimal, essentially unique, and stable. Conditions under which efficiency loss may be bounded are presented, and the results are extended to the cases of general overlay networks and multiple autonomous systems.\n\nTitle: Routing and peering in a competitive Internet\nContent: Today's Internet is a loose federation of independent network providers, each acting in their own self interest. In this paper, we consider some implications of this economic reality. Specifically, we consider how the incentives of the providers might determine where they choose to interconnect with each other; we show that for any given provider, determining an optimal placement of interconnection links is generally NP-complete. However, we present simple solutions for some special cases of this placement problem. We also consider the phenomenon of nearest-exit, or \"hot-potato,\" routing, where outgoing traffic exits a provider's network as quickly as possible. If each link in a network is assessed a linear cost per unit flow through the link, we show that the total cost of nearest exit routing is no worse than three times the optimal cost. In the general case, with nonlinear cost functions, we consider allowing providers to charge each other for flow on their links. We study the efficiency of such a scheme, and arrive at bounds on the worst possible efficiency loss. Traditional analyses of routing in data networks have assumed that the network is owned by a single operator. Typically, the network operator attempts to achieve some overall performance objective\u2014e.g., low average delay or low packet loss rates. Such analyses persisted throughout the growth of the Internet (see, for example, Bertsekas and Gallager (2)), since the network's development was largely maintained and directed by the government and academia: it could be reasonably assumed that improving performance of the network was a goal in the best interests of all parties. However, since the National Science Foundation ended its involvement in shaping the Internet in 1995, the Internet has taken on a very different shape. Today, it is a network owned by a loosely connected federation of independent network providers. Fundamentally, the objectives of each provider are not necessarily aligned with any global performance objective; rather, each network provider will typically be interested in maximizing their own monetary profits. This profit maxi- mizing, self interested behavior has important ramifications for the performance of the network\u2014 we can no longer expect that engineering a network properly can be independent of the economic realities of the implementation of that network. To understand the economic incentives driving the actions of network providers, we must first understand the structure of the interconnections they form with each other. Most relationships between two providers may be classfied into one of two types: transit, and peer. Provider A provides transit service to provider B if B pays A to carry traffic originating within B and destined\n\nTitle: A Comparison of Bilateral and Multilateral Exchanges for Peer-Assisted Content Distribution\nContent: Peer-assisted content distribution matches user demand for content with available supply at other peers in the network. Inspired by this supply-and-demand interpretation of the nature of content sharing, we employ price theory to study peer-assisted content distribution. In this approach, the market-clearing prices are those which exactly align supply and demand, and the system is studied through the characterization of price equilibria. We rigorously analyze the efficiency and robustness gains that are enabled by price-based multilateral exchange. We show that multilateral exchanges satisfy several desirable efficiency and robustness properties that bilateral exchanges do not, e.g. , equilibria in bilateral exchange may fail to exist, be inefficient if they do exist, and fail to remain robust to collusive deviations even if they are Pareto efficient. Further, we show that an equilibrium in bilateral exchange corresponds to a multilateral exchange equilibrium if and only if it is robust to deviations by coalitions of users.\n\nTitle: A scalable network resource allocation mechanism with bounded efficiency loss\nContent: The design of pricing mechanisms for network resource allocation has two important objectives: 1) a simple and scalable end-to-end implementation and 2) efficiency of the resulting equilibria. Both objectives are met by certain recently proposed mechanisms when users are price taking, but not when users can anticipate the effects of their actions on the resulting prices. In this paper, we partially close this gap, by demonstrating an alternative resource allocation mechanism which is scalable and guarantees a fully efficient allocation when users are price taking. In addition, when links have affine marginal cost, this mechanism has efficiency loss bounded by 1/3 when users are price anticipating. These results are derived by studying Cournot games, and in the process we derive the first nontrivial constant factor bounds on efficiency loss in these well-studied economic models.\n\n## Task\nCompose an abstract for the title 'Implications of autonomy for the expressiveness of policy routing' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Thousands of competing autonomous systems must cooperate with each other to provide global Internet connectivity. Each autonomous system (AS) encodes various economic, business, and performance decisions in its routing policy. The current interdomain routing system enables each AS to express policy using rankings that determine how each router in the AS chooses among different routes to a destination, and filters that determine which routes are hidden from each neighboring AS. Because the Internet is composed of many independent, competing networks, the interdomain routing system should provide autonomy, allowing network operators to set their rankings independently, and to have no constraints on allowed filters. This paper studies routing protocol stability under these conditions. We first demonstrate that \"next-hop rankings,\" commonly used in practice, may not ensure routing stability. We then prove that, when providers can set rankings and filters autonomously, guaranteeing that the routing system will converge to a stable path assignment imposes strong restrictions on the rankings ASes are allowed to choose. We discuss the implications of these results for the future of interdomain routing."
      }
    ]
  },
  {
    "tid":"93",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Spectral clustering based on iterative optimization for large-scale and high-dimensional data.\nContent: Spectral graph theoretic methods have been a fundamental and important topic in the field of manifold learning and it has become a vital tool in data clustering. However, spectral clustering approaches are limited by their computational demands. It would be too expensive to provide an optimal approximation for spectral decomposition in dealing with large-scale and high-dimensional data sets. On the other hand, the rapid development of data on the Web has posed many rising challenges to the traditional single-task clustering, while the multi-task clustering provides many new thoughts for real-world applications such as video segmentation. In this paper, we will study a Spectral Clustering based on Iterative Optimization (SCIO), which solves the spectral decomposition problem of large-scale and high-dimensional data sets and it well performs on multi-task clustering. Extensive experiments on various synthetic data sets and real-world data sets demonstrate that the proposed method provides an efficient solution for spectral clustering.\n\nTitle: Fuzzy weighted c-harmonic regressions clustering algorithm.\nContent: As a well-known regression clustering algorithm, fuzzy c-regressions (FCR) has been widely studied and applied in various areas. However, FCR appears to be rather sensitive to the undesirable initialization and the presence of noise or outliers in data sets. As a modified alternative, possibilistic c-regressions (PCR) can ameliorate the problem of noise and outliers, but it depends more heavily on initial values. Besides, the number of models should be determined a priori in both algorithms. To overcome these issues, this paper proposes a generalized alternative, called fuzzy weighted c-harmonic regressions (FWCHR), in which, a dynamic-like weight term based on the distinguished feature of the harmonic average is first introduced to enhance robustness. Furthermore, FWCHR can encompass FCR and PCR if some conditions are satisfied. And then a generalized mountain method (GMM) is proposed to automatically determine the number of models and estimate the initial values, which makes the proposed FWCHR algorithm totally unsupervised. Some numerical simulations and real applications are conducted to validate the performance of our algorithms.\n\nTitle: Iterative projection reconstruction for fast and efficient image upsampling\nContent: With the development of ultra-high-resolution display devices, the visual perception of fine texture details is becoming increasingly important. Traditional image upsampling methods suffer from either loss of high-frequency texture details or very high time cost. In this paper, we propose an iterative projection reconstruction (IPR) method for fast and efficient image upsampling. The proposed method refines high-frequency texture details with an iterative projection process, and utilizes the pre-computed projection matrix to accelerate the example-based image reconstruction. As a result, the proposed method can reproduce fine texture details with low time cost. Experimental results demonstrate that the proposed method outperforms some state-of-the-art methods. \u00a9 2016 Elsevier B.V.\n\nTitle: An Euclidean Distance based on the Weighted Self-information Related Data Transformation for Nominal Data Clustering.\nContent: Numerical data clustering is a tractable task since well-defined numerical measures like traditional Euclidean distance can be directly used for it, but nominal data clustering is a very difficult problem because there exists no natural relative ordering between nominal attribute values. This paper mainly aims to make the Euclidean distance measure appropriate to nominal data clustering, and the core idea is to transform each nominal attribute value into numerical. This transformation method consists of three steps. In the first step, the weighted self-information, which can quantify the amount of information in attribute values, is calculated for each value in each nominal attribute. In the second step, we find k nearest neighbors for each object because k nearest neighbors of one object have close similarities with it. In the last step, the weighted self-information of each attribute value in each nominal object is modified according to the object's k nearest neighbors. To evaluate the effectiveness of our proposed method, experiments are done on 10 data sets. Experimental results demonstrate that our method not only enables the Euclidean distance to be used for nominal data clustering, but also can acquire the better clustering performance than several existing state-of-the-art approaches.\n\n\n\nTitle: A Two-Phase Gauss-Seidel Algorithm for the Stationary Solution of EVMDD-Encoded CTMCs\nContent: We present a new symbolic approach for the steady-state solution of ergodic continuous-time Markov chains(CTMCs) using iterative methods, based on Edge-Valued Multi-way Decision Diagrams (EVMDDs) to store an indexing function for the structured states and the transition rate matrix. The approach is applicable to any structured CTMC, is memory efficient, and supports both Jacobi and Gauss-Seidel iterations. In particular, our main contribution is a new two-phase algorithm to perform Gauss-Seidel iterations with a reduced decision diagram traversal overhead (a cost also encountered by Kronecker-based approaches). Then, we show how even better speedup can be achieved through a caching scheme. The complexity of our algorithm is linear in the number of nonzero entries in the transition rate matrix, and, even more importantly, it is independent of the number L of submodels in which the CTMC is decomposed under most common conditions. This is an improvement over previous structured methods, which are plagued by this L factor in practice. The advantages of our algorithm are supported by experimental results and a comparison with the tool PRISM.\n\n## Task\nCompose an abstract for the title 'Fast Spectral Clustering for Unsupervised Hyperspectral Image Classification.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Hyperspectral image classification is a challenging and significant domain in the field of remote sensing with numerous applications in agriculture, environmental science, mineralogy, and surveillance. In the past years, a growing number of advanced hyperspectral remote sensing image classification techniques based on manifold learning, sparse representation and deep learning have been proposed and reported a good performance in accuracy and efficiency on state-of-the-art public datasets. However, most existing methods still face challenges in dealing with large-scale hyperspectral image datasets due to their high computational complexity. In this work, we propose an improved spectral clustering method for large-scale hyperspectral image classification without any prior information. The proposed algorithm introduces two efficient approximation techniques based on Nystrom extension and anchor-based graph to construct the affinity matrix. We also propose an effective solution to solve the eigenvalue decomposition problem by multiplicative update optimization. Experiments on both the synthetic datasets and the hyperspectral image datasets were conducted to demonstrate the efficiency and effectiveness of the proposed algorithm."
      }
    ]
  },
  {
    "tid":"94",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Retrieval based interactive cartoon synthesis via unsupervised bi-distance metric learning\nContent: Cartoons play important roles in many areas, but it requires a lot of labor to produce new cartoon clips. In this paper, we propose a gesture recognition method for cartoon character images with two applications, namely content-based cartoon image retrieval and cartoon clip synthesis. We first define Edge Features (EF) and Motion Direction Features (MDF) for cartoon character images. The features are classified into two different groups, namely intra-features and inter-features. An Unsupervised Bi-Distance Metric Learning (UBDML) algorithm is proposed to recognize the gestures of cartoon character images. Different from the previous research efforts on distance metric learning, UBDML learns the optimal distance metric from the heterogeneous distance metrics derived from intra-features and inter-features. Content-based cartoon character image retrieval and cartoon clip synthesis can be carried out based on the distance metric learned by UBDML. Experiments show that the cartoon character image retrieval has a high precision and that the cartoon clip synthesis can be carried out efficiently.\n\nTitle: Cartoon synthesis using constrained spreading activation network\nContent: In this paper, we propose an approach to synthesize cartoons from the existing cartoon data by controlling the character's path which is defined by the cartoonists in a background image. First, detailed pre-experiments are conducted in which different cartoon features are extracted and compared. During the pre-experiments, three features extracted from edge, motion and color are demonstrated effectively for evaluating cartoon similarity according to the quantitative analysis. The three features are then fused and a Cartoon Frame Relationship Network is constructed. Based on the graph, we propose a Constrained Spreading Activation Algorithm to select candidate frames which are visually similar to the current frame to generate the next frame. The cartoons are synthesized by choosing the most appropriate frame from the candidates in accordance with the path designed by the cartoonists. When the new cartoons are applied into the background image, our approach coordinates the cartoon character's size according to the image's perspective as well. The experiment results demonstrate that the combination of the three proposed features are effective in similarity evaluation, and the candidates selected by Constrained Spreading Activation Algorithm, are more similar to the current frame compared with other algorithms. The results also show that our approach can synthesize visually smooth cartoons from the existing cartoon library.\n\nTitle: Recognizing Cartoon Image Gestures for Retrieval and Interactive Cartoon Clip Synthesis\nContent: In this paper, we propose a new method to recognize gestures of cartoon images with two practical applications, i.e., content-based cartoon image retrieval and interactive cartoon clip synthesis. Upon analyzing the unique properties of four types of features including global color histogram, local color histogram (LCH), edge feature (EF), and motion direction feature (MDF), we propose to employ different features for different purposes and in various phases. We use EF to define a graph and then refine its local structure by LCH. Based on this graph, we adopt a transductive learning algorithm to construct local patches for each cartoon image. A spectral method is then proposed to optimize the local structure of each patch and then align these patches globally. MDF is fused with EF and LCH and a cartoon gesture space is constructed for cartoon image gesture recognition. We apply the proposed method to content-based cartoon image retrieval and interactive cartoon clip synthesis. The experiments demonstrate the effectiveness of our method.\n\nTitle: Retrieval-based cartoon gesture recognition and applications via semi-supervised heterogeneous classifiers learning\nContent: 2D cartoon plays an important role in many areas, but it requires effective methods to relieve manual labors. In this paper, we propose a heterogeneous cartoon gesture recognition method with applications. Firstly, heterogeneous features with different dimensions are assigned to express cartoon and human-subject images according to their characteristics. Then for recognition, we simultaneously integrate shared structure learning (SSL) and graph-based transductive learning into a joint framework to learn reliable classifiers on heterogeneous features. Provided with the framework, the similarities between cartoon and human-subject gestures can be quantitatively evaluated in a cross-feature manner. Extensive experiments on self-defined datasets have demonstrated the effectiveness of our method. Finally, applications illustrate the usages in various aspects of 2D cartoon industry.\n\nTitle: Adaptive control in cartoon data reusing\nContent: In this paper, we propose a novel approach, which reuses Traditional Chinese Cartoon to create new animations. In order to extract the cartoon character precisely, a segmentation method based on edge detection is implemented. Before reusing the data, a lower- dimensional space of the cartoon data is constructed by ISOmap. The character's gesture difference calculated by optical flow is combined with character's edge difference through a novel distance function, which is controlled by a weight parameter. The animation is created by reordering the existing data into a sequence, which is the shortest path between two designated data in the space. Our approach utilizes image processing, computer vision, and machine learning in cartoon creation and the experiment results demonstrate that the animation's quality can be effectively improved by the fusion of these techniques. Copyright \u00a9 2007 John Wiley & Sons, Ltd.\n\n## Task\nCompose an abstract for the title 'Synthesizing style-preserving cartoons via non-negative style factorization' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We present a complete framework for synthesizing style-preserving 2D cartoons by learning from traditional Chinese cartoons. In contrast to reusing-based approaches which rely on rearranging or retrieving existing cartoon sequences, we aim to generate stylized cartoons with the idea of style factorization. Specifically, starting with 2D skeleton features of cartoon characters extracted by an improved rotoscoping system, we present a non-negative style factorization (NNSF) algorithm to obtain style basis and weights and simultaneously preserve class separability. Thus, factorized style basis can be combined with heterogeneous weights to re-synthesize style-preserving features, and then these features are used as the driving source in the character reshaping process via our proposed subkey-driving strategy. Extensive experiments and examples demonstrate the effectiveness of the proposed framework."
      }
    ]
  },
  {
    "tid":"95",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Multi-Objective Mobile Agent-Based Sensor Network Routing Using Moea/D\nContent: Mobile agents are often used in wireless sensor networks for distributed target detection with the goal of minimizing the transmission of non-critical data that negatively affects the performance of the network. A challenge is to find optimal mobile agent routes for minimizing the data path loss and the sensors energy consumption as well as maximizing the data accuracy. Existing approaches deal with the objectives individually, or by optimizing one and constraining the others or by combining them into a single objective. This often results in missing \"good\" tradeoff solutions. Only few approaches have tackled the Mobile Agent-based Distributed Sensor Network Routing problem as a Multiobjective Optimization Problem (MOP) using conventional Multi-Objective Evolutionary Algorithms (MOEAs). It is well known that the incorporation of problem specific knowledge in MOEAs is a difficult task. In this paper, we propose a problem-specific MOEA based on Decomposition (MOEA/D) for optimizing the three objectives. Experimental studies have shown that the proposed problem-specific approach performs better than two conventional MOEAs in several WSN test instances.\n\nTitle: An Evolutionary Algorithm to a Multi-Objective Deployment and Power Assignment Problem in Wireless Sensor Networks\nContent: Wireless Sensor Networks design requires high quality location assignment and energy efficient power assignment for maximizing the network coverage and lifetime. Classical deployment and power assignment approaches optimize these two objectives individually or by combining them together in a single objective or by constraining one and optimizing the other. In this article a multi-objective Deployment and Power Assignment Problem (DPAP) is formulated and a Multi-objective Evolutionary Algorithm based on Decomposition (MOEA/D) is specialized. Following the MOEA/D's framework the above Multiobjective Optimization Problem (MOP) is decomposed into many scalar single objective problems. The sub-problems are solved simultaneously by using neighborhood information. Additionally, unique problem-specific, parameter-rising, genetic operators and local search heuristics were designed specifically for the DPAP. In addition, a new encoding scheme is designed to represent a WSN based on the DPAP's design variables. Simulation results show that MOEA/D provides a high quality set of alternative solutions without any prior knowledge on the objectives preference.\n\nTitle: Problem-specific encoding and genetic operation for a multi-objective deployment and power assignment problem in wireless sensor networks\nContent: Wireless Sensor Networks Deployment and Power Assignment Problems (DPAPs) for maximizing the network coverage and lifetime respectively, have received increasing attention recently. Classical approaches optimize these two objectives individually, or by combining them together in a single objective, or by constraining one and optimizing the other. In this paper, the two problems are formulated as a multi-objective DPAP and tackled simultaneously. Problem-specific encoding representation and genetic operators are designed for the DPAP and a Multi-Objective Evolutionary Algorithm based on Decomposition (MOEA/D) is specialized. The multi-objective DPAP is decomposed into many scalar subproblems which are solved simultaneously by using neighborhood information and network knowledge. Simulation results have shown the effectiveness of the proposed evolutionary components by providing a high quality set of alternative solutions without any prior knowledge on the objectives preference, and the superiority of our problem-specific MOEA/D approach against a state of the art MOEA.\n\nTitle: A multi-objective evolutionary algorithm for the deployment and power assignment problem in wireless sensor networks\nContent: A Wireless Sensor Network (WSN) design often requires the decision of optimal locations (deployment) and transmit power levels (power assignment) of the sensors to be deployed in an area of interest. Few attempts have been made on optimizing both decision variables for maximizing the network coverage and lifetime objectives, even though, most of the latter studies consider the two objectives individually. This paper defines the multiobjective Deployment and Power Assignment Problem (DPAP). Using the Multi-Objective Evolutionary Algorithm based on Decomposition (MOEA/D), the DPAP is decomposed into a set of scalar subproblems that are classified based on their objective preference and tackled in parallel by using neighborhood information and problem-specific evolutionary operators, in a single run. The proposed operators adapt to the requirements and objective preferences of each subproblem dynamically during the evolution, resulting in significant improvements on the overall performance of MOEA/D. Simulation results have shown the superiority of the problem-specific MOEA/D against the NSGA-II in several network instances, providing a diverse set of high quality network designs to facilitate the decision maker's choice.\n\nTitle: A multiobjective evolutionary algorithm based on decomposition with normal boundary intersection for traffic grooming in optical networks.\nContent: This paper presents a multiobjective optimization algorithm for solving tri-objective optimization problems with objectives of very different scales. As a version of the Multiobjective Evolutionary Algorithm based on Decomposition (MOEA/D), it adopts the Normal Boundary Intersection (NBI) based Tchebycheff approach to decompose a multiobjective optimization problem into a number of single objective subproblems. Particular attention has been paid to set the weight vectors of these subproblems for handling disparately scaled objectives. We have applied our proposed algorithm to a traffic grooming problem in the telecommunication fields. We also propose to use an indirect encoding approach to handle this very complicated problem. Our experimental studies show that our proposed method is able to produce very promising results for this real-world telecommunication problem.\n\n## Task\nCompose an abstract for the title 'Identification of multi-resolution network structures with multi-objective immune algorithm' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Community structure is one of the most important properties in complex networks, and the field of community detection has received an enormous amount of attention in the past several years. Many quality metrics and methods have been proposed for revealing community structures at multiple resolution levels, while most existing methods need a tunable parameter in their quality metrics to determine the resolution level in advance. In this study, a multi-objective evolutionary algorithm (MOEA) for revealing multi-resolution community structures is proposed. The proposed MOEA-based community detection algorithm aims to find a set of tradeoff solutions which represent network partitions at different resolution levels in a single run. It adopts an efficient multi-objective immune algorithm to simultaneously optimize two contradictory objective functions, Modified Ratio Association and Ratio Cut. The optimization of Modified Ratio Association tends to divide a network into small communities, while the optimization of Ratio Cut tends to divide a network into large communities. The simultaneous optimization of these two contradictory objectives returns a set of tradeoff solutions between the two objectives. Each of these solutions corresponds to a network partition at one resolution level. Experiments on artificial and real-world networks show that the proposed method has the ability to reveal community structures of networks at different resolution levels in a single run."
      }
    ]
  },
  {
    "tid":"96",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Efficient image reconstruction for gigapixel quantum image sensors\nContent: Recent advances in materials, devices and fabrication technologies have motivated a strong momentum in developing solid-state sensors that can detect individual photons in space and time. It has been envisioned that such sensors can eventually achieve very high spatial resolutions (e.g., 109 pixels/chip) as well as high frame rates (e.g., 106 frames/sec). In this paper, we present an efficient algorithm to reconstruct images from the massive binary bit-streams generated by these sensors. Based on the concept of alternating direction method of multipliers (ADMM), we transform the computationally intensive optimization problem into a sequence of subproblems, each of which has efficient implementations in the form of polyphase-domain filtering or pixel-wise nonlinear mappings. Moreover, we reformulate the original maximum likelihood estimation as maximum a posterior estimation by introducing a total variation prior. Numerical results demonstrate the strong performance of the proposed method, which achieves several dB's of improvement in PSNR and requires a shorter runtime as compared to standard gradient-based approaches.\n\nTitle: A novel compressive sensing approach to simultaneously acquire color and near-infrared images on a single sensor\nContent: Sensors of most digital cameras are made of silicon that is inherently sensitive to both the visible and near-infrared parts of the electromagnetic spectrum. In this paper, we address the problem of color and NIR joint acquisition. We propose a framework for the joint acquisition that uses only a single silicon sensor and a slightly modified version of the Bayer color-filter array that is already mounted in most color cameras. Implementing such a design for an RGB and NIR joint acquisition system requires minor changes to the hardware of commercial color cameras. One of the important differences between this design and the conventional color camera is the post-processing applied to the captured values to reconstruct full resolution images. By using a CFA similar to Bayer, the sensor records a mixture of NIR and one color channel in each pixel. In this case, separating NIR and color channels in different pixels is equivalent to solving an under-determined system of linear equations. To solve this problem, we propose a novel algorithm that uses the tools developed in the field of compressive sensing. Our method results in high-quality RGB and NIR images (the average PSNR of more than 30 dB for the reconstructed images) and shows a promising path towards RGB and NIR cameras.\n\nTitle: Bits From Photons: Oversampled Image Acquisition Using Binary Poisson Statistics\nContent: We study a new image sensor that is reminiscent of a traditional photographic film. Each pixel in the sensor has a binary response, giving only a 1-bit quantized measurement of the local light intensity. To analyze its performance, we formulate the oversampled binary sensing scheme as a parameter estimation problem based on quantized Poisson statistics. We show that, with a single-photon quantization threshold and large oversampling factors, the Cram\u00e9r\u2013Rao lower bound (CRLB) of the estimation variance approaches that of an ideal unquantized sensor, i.e., as if there were no quantization in the sensor measurements. Furthermore, the CRLB is shown to be asymptotically achievable by the maximum-likelihood estimator (MLE). By showing that the log-likelihood function of our problem is concave, we guarantee the global optimality of iterative algorithms in finding the MLE. Numerical results on both synthetic data and images taken by a prototype sensor verify our theoretical analysis and demonstrate the effectiveness of our image reconstruction algorithm. They also suggest the potential application of the oversampled binary sensing scheme in high dynamic range photography.\n\nTitle: Spatial super-resolution of a diffusion field by temporal oversampling in sensor networks\nContent: We study the spatial-temporal sampling of a linear diffusion field, and show that it is possible to compensate for insufficient spatial sampling densities by oversampling in time. Our work is motivated by the following issue often encountered in sensor network sampling, namely increasing the temporal sampling density is often easier and less expensive than increasing the spatial sampling density of the network. For the case of sampling a diffusion field, we show that, to achieve trade-off between spatial and temporal sampling, the spatial arrangement of the sensors must satisfy certain conditions. We provide in this paper the precise relationships between the achievable reduction of spatial sampling density, the required temporal oversampling rate, the spatial arrangement of the sensors, and the bound for the condition numbers of the resulting sampling and reconstruction procedures.\n\nTitle: A framework for adaptive parameter estimation with finite memory\nContent: We consider the problem of estimating an unknown parameter from a finite collection of different statistical experiments. The measurements are taken sequentially. Based on the observations made so far, we adaptively select the next experiment that provides the most information about the parameter. Summarizing past information with finite memory, we present a general framework for efficient adaptive estimation, with the sensing schemes fully characterized by finite-state parametric Markov chains. We establish an analytic formula linking the asymptotic performance of adaptive estimation schemes to the steady-state distributions of the associated Markov chains. Consequently, finding optimal adaptive strategies can be reformulated as the problem of designing a (continuous) family of Markov chains with prescribed steady-state distributions. We also propose a quantitative design criterion for optimal sensing policies based on minimax ratio regret.\n\n## Task\nCompose an abstract for the title 'Adaptive sensing and inference for single-photon imaging' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In recent years, there have been increasing efforts to develop solid-state sensors with single-photon sensitivity, with applications ranging from bio-imaging to 3D computer vision. In this paper, we present adaptive sensing models, theory and algorithms for these single-photon sensors, aiming to improve their dynamic ranges. Mapping different sensor configurations onto a finite set of states, we represent adaptive sensing schemes as finite-state parametric Markov chains. After deriving an asymptotic expression for the Fisher information rate of these Markovian systems, we propose a design criterion for sensing policies based on minimax ratio regret. We also present a suboptimal yet effective sensing policy based on random walks. Numerical experiments demonstrate the strong performance of the proposed scheme, which expands the sensor dynamic ranges of existing nonadaptive approaches by several orders of magnitude."
      }
    ]
  },
  {
    "tid":"97",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Distributed Memory Partitioning of High-Throughput Sequencing Datasets for Enabling Parallel Genomics Analyses\nContent: State-of-the-art high-throughput sequencing instruments decipher in excess of a billion short genomic fragments per run. The output sequences are referred to as 'reads'. These read datasets facilitate a wide variety of analyses with applications in areas such as genomics, metagenomics, and transcriptomics. Owing to the large size of the read datasets, such analyses are often compute and memory intensive. In this paper, we present a parallel algorithm for partitioning large-scale read datasets in order to facilitate distributed-memory parallel analyses. During the process of partitioning the read datasets, we construct and partition the associated de Bruijn graph in parallel. This allows applications that make use of a variant of the de Bruijn graph, such as de novo assembly, to directly leverage the generated de Bruijn graph partitions. In addition, we propose a mechanism for evaluating the quality of the generated partitions of reads and demonstrate that our algorithm produces high quality partitions. Our implementation is available at github.com/ParBLiSS/read_partitioning.\n\nTitle: Repeat-aware modeling and correction of short read errors.\nContent: High-throughput short read sequencing is revolutionizing genomics and systems biology research by enabling cost-effective deep coverage sequencing of genomes and transcriptomes. Error detection and correction are crucial to many short read sequencing applications including de novo genome sequencing, genome resequencing, and digital gene expression analysis. Short read error detection is typically carried out by counting the observed frequencies of kmers in reads and validating those with frequencies exceeding a threshold. In case of genomes with high repeat content, an erroneous kmer may be frequently observed if it has few nucleotide differences with valid kmers with multiple occurrences in the genome. Error detection and correction were mostly applied to genomes with low repeat content and this remains a challenging problem for genomes with high repeat content.We develop a statistical model and a computational method for error detection and correction in the presence of genomic repeats. We propose a method to infer genomic frequencies of kmers from their observed frequencies by analyzing the misread relationships among observed kmers. We also propose a method to estimate the threshold useful for validating kmers whose estimated genomic frequency exceeds the threshold. We demonstrate that superior error detection is achieved using these methods. Furthermore, we break away from the common assumption of uniformly distributed errors within a read, and provide a framework to model position-dependent error occurrence frequencies common to many short read platforms. Lastly, we achieve better error correction in genomes with high repeat content.The software is implemented in C++ and is freely available under GNU GPL3 license and Boost Software V1.0 license at \"http://aluru-sun.ece.iastate.edu/doku.php?id = redeem\".We introduce a statistical framework to model sequencing errors in next-generation reads, which led to promising results in detecting and correcting errors for genomes with high repeat content.\n\nTitle: A Parallel Algorithm for Spectrum-based Short Read Error Correction\nContent: Correcting sequence errors in high-throughput DNA sequencing by taking advantage of redundant sampling and low error rates is often an important first step in applications of this technology. Consequently, a number of error correction methods have been developed in the recent years. Due to an order of magnitude throughput gain per year, some of these technologies are now generating upwards of a billion reads per run. In this paper, we present an algorithm for parallel zing error correction methods that are based on frequency spectrum of kmers observed in input reads. Based on this, we present a parallelization of Reptile, a recently introduced error correction method that employs frequency spectrum of two different lengths, one for identifying correction possibilities and another for providing contextual information. Our method is well suited for distributed memory parallel computers and clusters. Experimental results indicate the method achieves near linear speedup and provides the ability to scale to larger data sets than previously demonstrated.\n\nTitle: Assembling genomes on large-scale parallel computers\nContent: Assembly of large genomes from tens of millions of short genomic fragments is computationally demanding requiring hundreds of gigabytes of memory and tens of thousands of CPU hours. The advent of high throughput sequencing technologies, new gene-enrichment sequencing strategies, and collective sequencing of environmental samples further exacerbate this situation. In this paper, we present the first massively parallel genome assembly framework. The unique features of our approach include space-efficient and on-demand algorithms that consume only linear space, and strategies to reduce the number of expensive pairwise sequence alignments while maintaining assembly quality. Developed as part of the ongoing efforts in maize genome sequencing, we applied our assembly framework to genomic data containing a mixture of gene enriched and random shotgun sequences. We report the partitioning of more than 1.6 million fragments of over 1.25 billion nucleotides total size into genomic islands in under 2h on 1024 processors of an IBM BlueGene/L supercomputer. We also demonstrate the effectiveness of the proposed approach for traditional whole genome shotgun sequencing and assembly of environmental sequences.\n\nTitle: Parallel Genomic Alignments on the Cell Broadband Engine\nContent: Genomic alignments, as a means to uncover evolutionary relationships among organisms, are a fundamental tool in computational biology. There is considerable recent interest in using the Cell Broadband Engine, a heterogeneous multicore chip that provides high performance, for biological applications. However, work in genomic alignments so far has been limited to computing optimal alignment scores using quadratic space for the basic global/local alignment problem. In this paper, we present a comprehensive study of developing alignment algorithms on the Cell, exploiting its thread and data level parallelism features. First, we develop a parallel implementation on the Cell that computes optimal alignments and adopts Hirschberg's linear space technique. The former is essential, as merely computing optimal alignment scores is not useful, while the latter is needed to permit alignments of longer sequences. We then present Cell implementations of two advanced alignment techniques\u2014spliced alignments and syntenic alignments. Spliced alignments are useful in aligning mRNA sequences with corresponding genomic sequences to uncover the gene structure. Syntenic alignments are used to discover conserved exons and other sequences between long genomic sequences from different organisms. We present experimental results for these three types of alignments on 16 Synergistic Processing Elements of the IBM QS20 dual-Cell blade system.\n\n## Task\nCompose an abstract for the title 'Parallel Read Error Correction for Big Genomic Datasets.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Genome sequencing, using instruments in voguetoday, deciphers in the order of a billion short genomic fragmentsper run. These fragments are a few hundred bases long andare commonly referred to as 'reads'. Reads contain errors dueto limitations of sequencing technology. Read error correctionenhances the quality of results produced by applications inareas such as genomics, metagenomics, and transcriptomics. Useof error corrected reads also improves the runtime and thememory usage of such applications. Sequential error correctiontools cannot cope with the large number of reads producedby modern day sequencing instruments. A distributed-memoryParallel Spectrum-based Error Correction (PSbEC) algorithmwas proposed to overcome this drawback [1].In this work, we propose techniques to address three majorshortcomings of the PSbEC algorithm. Our optimizations enhancethe scope and the speedup of the PSbEC algorithm, therebyenabling error correction of big genomic datasets. More specifically, by combining our optimizations, we are able to achieve a cumulative speedup of up to 11 X. Further, we demonstrate error correction of a human dataset containing nearly 1.55 billion reads. This work stands as the first demonstration of distributed-memorygenomic read error correction for a dataset consistingof more than a billion reads."
      }
    ]
  },
  {
    "tid":"98",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Modified Radial Basis Function Neural Network Integrated With Multiple Regression Analysis And Its Application In The Chemical Industry Processes\nContent: The construct of a radial basis function neural network (RBFNN) plays an important role in predicting performance. However, determining the optimal construct is difficult. A modified RBFNN integrated with correlation pruning algorithm-least squares regression (CPA-LSR) was proposed to optimize the number of hidden neurons as well as the weights and bias. First, an initial RBFNN was built by superposing each center to a training set point. This RBFNN was then trained. Next, CPA-LSR was applied to eliminate the redundant information of the initial network and to improve the predicting performance by optimizing the structure as well as the weights and bias. Finally, the developed naphtha dry point soft sensor and the industrial oxidation of p-xylene to terephthalic acid were employed to illustrate the performances of the modified RBFNNs. The result reveals an improvement in the predicting performances of the RBFNNs integrated with CPA-LSR. Conclusively, RBFNNs integrated with CPA-LSR are recommended because the redundant neurons are effectively eliminated, and the optimal structure of the RBFNN is obtained by CPA-LSR. Moreover, such RBFNNs are intuitive and eliminate the need for parameterization.\n\nTitle: A Comparison Of Mamdani And Sugeno Fuzzy Inference Systems For Traffic Flow Prediction\nContent: Information on the future state of traffic flow provides a solid foundation for the efficient implementation of traffic control and guidance. The prediction approaches based on fuzzy logic theory is of great interests, because the rule-based inference is similar to the way humans process casual relations and fuzzy linguistic variables provide a natural way to deal with uncertainties. This paper presents a comparative study on a set of widely used Mamdani and Sugeno fuzzy inference systems in the application on the short-term prediction for traffic flow based on the historical recordings. To fulfill the comparison, a series of experiments was designed and performed to evaluate prediction performance for each fuzzy inference system in terms of model complexity, execution time, noise resistance, performance consistency, missing data, and multi-step-ahead predictability. Before discussing the primary results, a description on the fuzzy inference systems, evaluation factors and criteria was given. The analyses on the experimental results led to several findings which can be referenced when choosing a FIS for traffic flow prediction based on historical recordings.\n\nTitle: Multi-robot odor-plume tracing in indoor natural airflow environments using an improved ACO algorithm\nContent: We consider odor-plume tracing using a multi-robot system in indoor natural airflow environments. The purpose of odor-plume tracing is to approach the odor source via following the found plume with mobile robot(s). Owing to the chaotic nature of the odor transport in the atmosphere, tracing the resultant patchy meandering plume down to its source is thus not a trivial task. A novel multi-robot based plume tracing method is proposed. When the plume is found, multiple robots are coordinated to trace the time-variant plume by an improved ant colony optimization (ACO) algorithm combined with upwind search. The results of experiments compared with the spiral surge approach using real robots in indoor natural airflow environments validate the feasibility and robustness of the proposed plume tracing method.\n\nTitle: Multi-agent system design and evaluation for collaborative wireless sensor network in large structure health monitoring\nContent: Much attention has been focused on the research of structural health monitoring (SHM), since it could increase the safety and reduce the maintenance costs of engineering structures. In recent years, wireless sensor network (WSN) has been explored for adoption to improve the centralized cable-based SHM system performances. This paper presents a multi-agent design method and system evaluation for wireless sensor network based structural health monitoring to validate the efficiency of the multi-agent technology. Through the cooperation of six different agents for SHM applications, the distributed wireless sensor network can automatically allocate SHM tasks, self-organize the sensor network and aggregate different sensor information. In the evaluation work, the strain gauge and PZT sensors are used to monitor strain distribution change and joint failure of an experimental aluminum plate structure. A dedicated sensor network platform including the wireless strain node, wireless PZT node and wireless USB station is designed for the evaluation system. Based on the hardware platform, the multi-agents software architecture is defined. The multi-agent monitoring principle and implementation in the validation work for two typical kinds of structure states are presented. This paper shows the efficiency of the multi-agent technology for WSN based the SHM applications on the large aircraft structures.\n\nTitle: Adaptive multifeature visual tracking in a probability-hypothesis-density filtering framework\nContent: Probability hypothesis density (PHD) based trackers have enjoyed growing popularity in recent years, particularly in the field of nonlinear non-Gaussian visual tracking scenarios. These visual trackers can be degraded by a variety of factors, including changes of illumination, occlusion, poor image contrast, shape and appearance variation, clutter and other unmodeled changes of tracked objects. In this paper, for enhancing the performance of PHD based trackers, both scale invariant feature and color distribution feature are used as descriptors of targets of interest. To adaptively adjust the weights of each feature in the tracking process, a confidence measure, i.e., a quantitative measure for the spatial uncertainty of each feature is incorporated into the multifeature tracking algorithm. Experimental results show that the proposed multifeature tracker can improve the reliability and robustness of state estimation and the number estimation in tracking a variable number of objects of varying scales even when background region was similar to the object's appearance.\n\n## Task\nCompose an abstract for the title 'Simulating multiple class urban land-use/cover changes by RBFN-based CA model' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Land use systems are complex adaptive systems, and they are characterized by emergence, nonlinearity, feedbacks, self organization, path dependence, adaptation, and multiple-scale characteristics. Land use/cover change has been recognized as one of the major drivers of global environmental change. This paper presents a coupled Cellular Automata (CA) and Radial Basis Function Neural (RBFN) Network model, which combines Geographic Information Systems (GIS) to contribute to the understanding of the complex land use/cover change process. In this model, GIS analysis is used to generate spatial drivers of land use/cover changes, and RBFN is trained to extract model parameters. Through the RBFN-CA model, the conversion probabilities of each cell from its initial land use state to the target type can be generated automatically. Future land use/cover scenarios are projected by using generated parameters in the model training process. This RBFN-CA model is tested based on the comparison of model output and the real data. A BPN-CA model is also built and compared with the RBFN-CA model by using a variety of calibration metrics, including confusion matrix, figure of merit, and landscape metrics. Both the location and landscape metrics based assessment for model simulation indicate that the RBFN-CA model performs better than the BPN-CA model for simulating land use changes in the study area. Therefore the RBFN-CA model is capable of simulating multiple classes of land use/cover changes and can be used as a useful communication environment for stakeholders involved in land use decision-making."
      }
    ]
  },
  {
    "tid":"99",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A multi-level grammar approach to grammar-guided genetic programming: the case of scheduling in heterogeneous networks.\nContent: The scale at which the human race consumes data has increased exponentially in recent years. One key part in this increase has been the usage of smart phones and connected devices by the populous. Multi-level heterogeneous networks are the driving force behind this mobile revolution, but these are constrained with limited bandwidth and over-subscription. Scheduling users on these networks has become a growing issue. In recent years grammar-guided genetic programming (G3P) has shown its capability to evolve beyond human-competitive network schedulers. Despite the performance of the G3P schedulers, a large margin of improvement is demonstrated to still exist. In the pursuit of this goal we recently proposed a multi-level grammar approach to generating schedulers. The complexity of the grammar was increased at various stages during evolution, allowing for individuals to add more complex functions through variation operations. The goal is to evolve good quality solutions before allowing the population to specialise more as the grammar functionality increased in a layered learning way. In this paper the results of this initial study are replicated, and confirmed, and it is seen that this approach improves the quality of the evolved schedulers. However, despite the gain in performance, we notice that the proposed approach comes with an acute sensitivity to the generation at which the grammar complexity is increased. Therefore, we put forward a novel seeding strategy and show that the seeding strategy mitigates the shortcomings of the original approach. The use of the seeding strategy outperforms the original approach in all the studied cases, and thus yields a better overall performance than the state-of-the-art G3P generated schedulers.\n\nTitle: Multi-level Grammar Genetic Programming for Scheduling in Heterogeneous Networks.\nContent: Co-ordination of Inter-Cell Interference through scheduling enables telecommunication companies to better exploit their Heterogeneous Networks. However, it requires from these entities to implement an effective scheduling algorithm. The state-of-the-art for the scheduling in Heterogeneous Networks is a Grammar-Guided Genetic Programming algorithm which evolves, from a given grammar, an expression that maps to the scheduling of transmissions. We evaluate in our work the possibility of improving the results obtained by the state-of-the-art using a layered grammar approach. We show that starting with a small restricted grammar and introducing the full functionality after 10 generations outperforms the state-of-the-art, even when varying the algorithm used to generate the initial population and the maximum initial tree depth.\n\nTitle: Comparing the robustness of grammatical genetic programming solutions for femtocell algorithms\nContent: Methods for evolving robust solutions are necessary when the evolved solutions are algorithms which are deployed in actual consumer products, e.g. Femtocells, low power, low-cost, user-deployed cellular base stations. We compare how multiple and dynamic applications of training scenarios in the evolutionary search produce different solutions and performance on training and test scenarios. For Femtocells, robustness is especially important since each fitness evaluation is a simulation that is computationally expensive. Previous studies in robustness and dynamic environments have not shown differences in the robustness of the solution when a dynamic or multiple setup is used, or if they are negligible. In the dynamic setup the solution gets exposed to a multitude of scenarios during the evolution. Therefore a solution could be evolved which is capable of surviving, and is also more general. The experiments use grammar based Genetic Programming on the Femtocell problem with one grammar for generating real-values and another grammar for generating discrete values for changing the pilot power. The results show that the solutions evolved using multiple scenarios have the best test performance. Moreover, the use of a grammar which produces discrete changes to the pilot power generate better solutions on the training and the test scenarios.\n\nTitle: A comparison of grammatical genetic programming grammars for controlling femtocell network coverage\nContent: We study grammars used in grammatical genetic programming (GP) which create algorithms that control the base station pilot power in a femtocell network. The overall goal of evolving algorithms for femtocells is to create a continuous online evolution of the femtocell pilot power control algorithm in order to optimize their coverage. We compare the performance of different grammars and analyse the femtocell simulation model using the grammatical genetic programming method called grammatical evolution. The grammars consist of conditional statements or mathematical functions as are used in symbolic regression applications of GP, as well as a hybrid containing both kinds of statements. To benchmark and gain further information about our femtocell network simulation model we also perform random sampling and limited enumeration of femtocell pilot power settings. The symbolic regression based grammars require the most configuration of the evolutionary algorithm and more fitness evaluations, whereas the conditional statement grammar requires more domain knowledge to set the parameters. The content of the resulting femtocell algorithms shows that the evolutionary computation (EC) methods are exploiting the assumptions in the model. The ability of EC to exploit bias in both the fitness function and the underlying model is vital for identifying the current system and improves the model and the EC method. Finally, the results show that the best fitness and engineering performances for the grammars are similar over both test and training scenarios. In addition, the evolved solutions' performance is superior to those designed by humans.\n\nTitle: A grammatical genetic programming approach to modularity in genetic algorithms\nContent: The ability of Genetic Programming to scale to problems of increasing difficulty operates on the premise that it is possible to capture regularities that exist in a problem environment by decomposition of the problem into a hierarchy of modules. As computer scientists and more generally as humans we tend to adopt a similar divide-and-conquer strategy in our problem solving. In this paper we consider the adoption of such a strategy for Genetic Algorithms. By adopting a modular representation in a Genetic Algorithm we can make efficiency gains that enable superior scaling characteristics to problems of increasing size. We present a comparison of two modular Genetic Algorithms, one of which is a Grammatical Genetic Programming algorithm, the meta-Grammar Genetic Algorithm (mGGA), which generates binary string sentences instead of traditional GP trees. A number of problems instances are tackled which extend the Checkerboard problem by introducing different kinds of regularity and noise. The results demonstrate some limitations of the modular GA (MGA) representation and how the mGGA can overcome these. The mGGA shows improved scaling when compared the MGA.\n\n## Task\nCompose an abstract for the title 'Multilayer optimization of heterogeneous networks using grammatical genetic programming' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Wireless communications networks are a global trillion dollar industry, where small improvements can scale to provide significant cost savings to networks operators. In a field full of NP-hard optimisation problems, heuristic optimisation techniques such as Evolutionary Computation offer a means to provide bespoke, scalable solutions. Grammatical Genetic Programming is applied to optimise three aspects of an LTE Heterogeneous Network: setting optimal Small Cell powers and biases, Macro Cell ABS patterns, and Small Cell scheduling. The evolved heuristics yield minimum downlink rates three times greater than a baseline technique, and twice that of a state-of-the-art industry standard benchmark. This work appears in full in Fenton et al., \\\"Multilayer Optimization of Heterogeneous Networks using Grammatical Genetic Programming\\\", IEEE Transactions on Cybernetics, 2017. DOI: 10.1109/TCYB.2017.2688280."
      }
    ]
  },
  {
    "tid":"100",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Exploiting User Demand Diversity in Heterogeneous Wireless Networks\nContent: Radio resource management (RRM) is crucial for improving resource utilization in heterogeneous wireless networks. Existing work attempts to exploit the network diversity to gain throughput improvement for users, which, however, neglects the impact of user demand on RRM. Armed with the idea that the ultimate goal of communications is to serve users with personalized demand, we introduce another dimension of potential performance gain, user demand diversity gain. This gain derives from the elaborate matching between user demand and radio resource, which can not be directly attained in existing throughput-centric optimization due to users\u2019 blindness in maximizing throughput. Aiming at obtaining this gain, we propose the user demand-centric optimization, where users seek to maximize quality of experience (QoE), instead of throughput. This shift enables us to propose a novel game formulation, QoE game. We derive the condition on the existence of the QoE equilibrium, validate the user demand diversity gain and propose a distributed QoE equilibrium learning algorithm. Finally, a cloud assisted learning framework is proposed to accommodate the learning algorithm with significantly reduced cost. Simulation results validate the existence of user demand diversity gain and the effectiveness of the proposed learning algorithm in improving the system efficiency and QoE fairness.\n\nTitle: Tunable Probing: Towards Timely Channel Selection Mechanism in Dynamic Multi-channel Wireless Networks\nContent: Optimal spectrum access is a fundamental issue for efficient wireless communications in multi-channel wireless networking systems. Opportunistic spectrum access, making fully use of channels with high quality, could pave a way for efficient transmission. Unfortunately, in heterogeneous wireless networks, these strategies suffer from the assumption of identical wireless channel quality, and the difficulties lie in manipulating between multiple radios and channels. In this paper, we propose a cooperative and cognitive spectrum access algorithm, so as to build a timely and distributed multi-channel probing and access scheme in heterogeneous wireless network. In modeling the probing problem into a time-constrained knapsack problem, we propose an optimal access scheme considering both channel bandwidth and the probing time. Numerical results and simulation results show that, our proposed timely probing paradigm effectively improves network resource utilization with deadlines.\n\nTitle: Dynamic User Demand Driven Online Network Selection.\nContent: Network selection plays a key role in reaping the potential benefit of heterogeneous wireless networks. Aiming at improving the user's quality of experience, we study the network selection problem with time-varying user demand and non-uniform network handoff costs in a dynamic environment. One appealing solution in converging the time-vary user demand and the diverse network performance is dynamic network selection, which, however, poses the dilemma between satisfying user demand and controlling the network handoff cost. To get around this problem, we propose an online network selection algorithm to learn the optimal network selection policy with network handoff cost consideration. In addition, we exploit the inherent dependency in the problem and derive another two algorithms with much faster convergence speed. Simulations reveal that the proposed algorithms can achieve 10%~ performance gain over existing methods.\n\nTitle: Optimal Frequency-Temporal Opportunity Exploitation for Multichannel Ad Hoc Networks\nContent: In multichannel system, user could keep transmitting over an instantaneous \u201con peak\u201d channel by opportunistically accessing and switching among channels. Previous studies rely on constant transmission duration, which would fail to leverage more opportunities in time and frequency domain. In this paper, we consider opportunistic channel accessing/releasing scheme in multichannel system with Rayleigh fading channels. Our main goal is to derive a throughput-optimal strategy for determining when and which channel to access and when to release it. We formulate this real-time decision-making process as a two-dimensional optimal stopping problem. We prove that the two-dimensional optimal stopping rule can be reduced to a simple threshold-based policy. Leveraging the absorbing Markov chain theory, we obtain the optimal threshold as well as the maximum achievable throughput with computational efficiency. Numerical and simulation results show that our proposed channel utilization scheme achieves up to 140 percent throughput gain over opportunistic transmission with a single channel and up to 60 percent throughput gain over opportunistic channel access with constant transmission duration.\n\nTitle: Finding Optimal Action Point for Multi-Stage Spectrum Access in Cognitive Radio Networks\nContent: A critical challenge in Cognitive Radio Networks (CRN) is to make decision in real-time on accessing and releasing available channels that maximize the spectrum utilization and the overall system throughput. In this work, we make investigations on optimal action point to explore and exploit the frequency-temporal diversity in addition to spectrum availability. By modeling the Rayleigh fading channel under primary user (PU) activity to be a finite state Markov channel (FSMC) with an absorbing state, we formulate this difficulty into a 2-Dimension optimal stopping problem. Further, we've proved that, the complexity of the 2D optimal stopping rule can be reduced to one threshold policy, where the optimal character still holds. After properly constructing multi-absorbing-states Markov chain for dynamic analysis, we get the throughput of our strategy accurately. Numerical and simulations results have verified that, our threshold based access/switch strategy gains much more throughput than conventional idle/busy based access/switch strategy at the cost of access delay in most cases.\n\n## Task\nCompose an abstract for the title 'Traffic-Aware Online Network Selection in Heterogeneous Wireless Networks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We focus on the network selection problem in heterogeneous wireless networks. Many traditional approaches select the best network according to quality of service (QoS) related criteria, which neglects diverse user demands. We aim to select networks maximizing the quality of experience (QoE) of users. When the availability and dynamics of network state information are considered, most of the existing approaches can not make effective selection decisions since they are vulnerable to the uncertainty in network state information. To address this issue, we introduce the idea of online learning for network selection. In this paper, we formulate the network selection problem as a continuous time multi-armed bandit (CT-MAB) problem. A traffic-aware online network selection algorithm (ONES) is designed to match typical traffic types of users with respective optimal networks in terms of QoE. Moreover, we found that the correlation among multiple traffic network selections can be exploited to improve the learning capability. This motivates us to propose another two more efficient algorithms: decoupled online network selection algorithm (D-ONES) and virtual multiplexing ONES (VM-ONES). Simulation results demonstrate that our online network selection algorithms attain around 10% gain in QoE reward rate over non learning-based algorithms and learning based algorithms without QoE considerations."
      }
    ]
  },
  {
    "tid":"101",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Query and Keyframe Representations for Ad-hoc Video Search.\nContent: This paper presents a fully-automatic method that combines video concept detection and textual query analysis in order to solve the problem of ad-hoc video search. We present a set of NLP steps that cleverly analyse different parts of the query in order to convert it to related semantic concepts, we propose a new method for transforming concept-based keyframe and query representations into a common semantic embedding space, and we show that our proposed combination of concept-based representations with their corresponding semantic embeddings results to improved video search accuracy. Our experiments in the TRECVID AVS 2016 and the Video Search 2008 datasets show the effectiveness of the proposed method compared to other similar approaches.\n\nTitle: Multimodal Video Annotation for Retrieval and Discovery of Newsworthy Video in a News Verification Scenario.\nContent: This paper describes the combination of advanced technologies for social-media-based story detection, story-based video retrieval and concept-based video (fragment) labeling under a novel approach for multimodal video annotation. This approach involves textual metadata, structural information and visual concepts - and a multimodal analytics dashboard that enables journalists to discover videos of news events, posted to social networks, in order to verify the details of the events shown. It outlines the characteristics of each individual method and describes how these techniques are blended to facilitate the content-based retrieval, discovery and summarization of (parts of) news videos. A set of case-driven experiments conducted with the help of journalists, indicate that the proposed multimodal video annotation mechanism - combined with a professional analytics dashboard which presents the collected and generated metadata about the news stories and their visual summaries - can support journalists in their content discovery and verification work.\n\nTitle: DnS: Distill-and-Select for Efficient and Accurate Video Indexing and Retrieval\nContent: In this paper, we address the problem of high performance and computationally efficient content-based video retrieval in large-scale datasets. Current methods typically propose either: (i) fine-grained approaches employing spatio-temporal representations and similarity calculations, achieving high performance at a high computational cost or (ii) coarse-grained approaches representing/indexing videos as global vectors, where the spatio-temporal structure is lost, providing low performance but also having low computational cost. In this work, we propose a Knowledge Distillation framework, called Distill-and-Select (DnS), that starting from a well-performing fine-grained Teacher Network learns: (a) Student Networks at different retrieval performance and computational efficiency trade-offs and (b) a Selector Network that at test time rapidly directs samples to the appropriate student to maintain both high retrieval performance and high computational efficiency. We train several students with different architectures and arrive at different trade-offs of performance and efficiency, i.e., speed and storage requirements, including fine-grained students that store/index videos using binary representations. Importantly, the proposed scheme allows Knowledge Distillation in large, unlabelled datasets\u2014this leads to good students. We evaluate DnS on five public datasets on three different video retrieval tasks and demonstrate (a) that our students achieve state-of-the-art performance in several cases and (b) that the DnS framework provides an excellent trade-off between retrieval performance, computational speed, and storage space. In specific configurations, the proposed method achieves similar mAP with the teacher but is 20 times faster and requires 240 times less storage space. The collected dataset and implementation are publicly available: \n                https://github.com/mever-team/distill-and-select\n                \n              .\n\nTitle: Utilizing Implicit User Feedback To Improve Interactive Video Retrieval\nContent: This paper describes an approach to exploit the implicit user feedback gathered during interactive video retrieval tasks. We propose a framework, where the video is first indexed according to temporal, textual, and visual features and then implicit user feedback analysis is realized using a graph- based methodology. The generated graph encodes the semantic relations between video segments based on past user interaction and is subsequently used to generate recommendations. Moreover, we combine the visual features and implicit feedback information by training a support vector machine classifier with examples generated from the aforementioned graph in order to optimize the query by visual example search. The proposed framework is evaluated by conducting real-user experiments. The results demonstrate that significant improvement in terms of precision and recall is reported after the exploitation of implicit user feedback, while an improved ranking is presented in most of the evaluated queries by visual example.\n\nTitle: Optimizing visual search with implicit user feedback in interactive video retrieval\nContent: This paper describes an approach to optimize query by visual example results, by combining visual features and implicit user feedback in interactive video retrieval. To this end, we propose a framework, in which video processing is performed by employing well established techniques, while implicit user feedback analysis is realized with a graph based approach that processes the user actions and navigation patterns during a search session, in order to initiate semantic relations between the video segments. To combine the visual and implicit feedback information, we train a support vector machine classifier with positive and negative examples generated from the graph structured past user interaction data. Then, the classifier reranks the results of visual search that were initially based on visual features. This framework is embedded in an interactive video search engine and evaluated by conducting a user experiment in two phases: first, we record the user actions during typical retrieval sessions and then, we evaluate the reranking of the results of visual query by example. The evaluation and the results demonstrate that the proposed approach provides an improved ranking in most of the evaluated queries.\n\n## Task\nCompose an abstract for the title 'FIVR: Fine-grained Incident Video Retrieval.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper introduces the problem of Fine-grained Incident Video Retrieval (FIVR). Given a query video, the objective is to retrieve all associated videos, considering several types of associations that range from duplicate videos to videos from the same incident. FIVR offers a single framework that contains several retrieval tasks as special cases. To address the benchmarking needs of all such ta..."
      }
    ]
  },
  {
    "tid":"102",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Sensor placement for optimal Kalman filtering: Fundamental limits, submodularity, and algorithms\nContent: In this paper, we focus on sensor placement in linear dynamic estimation, where the objective is to place a small number of sensors in a system of interdependent states so to design an estimator with a desired estimation performance. In particular, we consider a linear time-variant system that is corrupted with process and measurement noise, and study how the selection of its sensors affects the estimation error of the corresponding Kalman filter over a finite observation interval. Our contributions are threefold: First, we prove that the minimum mean square error of the Kalman filter decreases only linearly as the number of sensors increases. That is, adding extra sensors so to reduce this estimation error is ineffective, a fundamental design limit. Similarly, we prove that the number of sensors grows linearly with the system's size for fixed minimum mean square error and number of output measurements over an observation interval; this is another fundamental limit, especially for systems where the system's size is large. Second, we prove that the log det of the error covariance of the Kalman filter, which captures the volume of the corresponding confidence ellipsoid, with respect to the system's initial condition and process noise is a supermodular and non-increasing set function in the choice of the sensor set. Therefore, it exhibits the diminishing returns property. Third, we provide an efficient approximation algorithm that selects a small number sensors so to optimize the Kalman filter with respect to this estimation error -the worst-case performance guarantees of this algorithm are provided as well.\n\nTitle: Near-Optimal Sensor Scheduling For Batch State Estimation: Complexity, Algorithms, And Limits\nContent: In this paper, we focus on batch state estimation for linear systems. This problem is important in applications such as environmental field estimation, robotic navigation, and target tracking. Its difficulty lies on that limited operational resources among the sensors, e.g., shared communication bandwidth or battery power, constrain the number of sensors that can be active at each measurement step. As a result, sensor scheduling algorithms must be employed. Notwithstanding, current sensor scheduling algorithms for batch state estimation scale poorly with the system size and the time horizon. In addition, current sensor scheduling algorithms for Kalman filtering, although they scale better, provide no performance guarantees or approximation bounds for the minimization of the batch state estimation error. In this paper, one of our main contributions is to provide an algorithm that enjoys both the estimation accuracy of the batch state scheduling algorithms and the low time complexity of the Kalman filtering scheduling algorithms. In particular: 1) our algorithm is near-optimal: it achieves a solution up to a multiplicative factor 1/2 from the optimal solution, and this factor is close to the best approximation factor 1/e one can achieve in polynomial time for this problem; 2) our algorithm has (polynomial) time complexity that is not only lower than that of the current algorithms for batch state estimation; it is also lower than, or similar to, that of the current algorithms for Kalman filtering. We achieve these results by proving two properties for our batch state estimation error metric, which quantifies the square error of the minimum variance linear estimator of the batch state vector: a) it is supermodular in the choice of the sensors; b) it has a sparsity pattern (it involves matrices that are block tri-diagonal) that facilitates its evaluation at each sensor set.\n\nTitle: Sensor-Based Dynamic Assignment in Distributed Motion Planning\nContent: Distributed motion planning of multiple agents raises fundamental and novel problems in control theory and robotics. Recently, one such great challenge has been the development of motion planning algorithms that dynamically assign targets or destinations to multiple homogeneous agents, not relying on any a priori assignment of agents to destinations. In this paper, we address this challenge using two novel ideas. First, we develop distributed multi-destination potential fields able to drive every agent to any available destination for almost all initial conditions. Second, we propose sensor- based coordination protocols that ensure that distinct agents are assigned to distinct destinations. Integration of the overall system results in a distributed, multi-agent, hybrid system for which we show that the mutual exclusion property of the final assignment is guaranteed for almost all initial conditions. Moreover, we show that our dynamic assignment algorithm converges after exploring at most a polynomial number of assignments, dramatically reducing the combinatorial nature of purely discrete assignment problems. Our scalable approach is illustrated with nontrivial computer simulations.\n\nTitle: Deep Reinforcement Learning for Active Target Tracking\nContent: We solve active target tracking, one of the essential tasks in autonomous systems, using a deep reinforcement learning (RL) approach. In this problem, an autonomous agent is tasked with acquiring information about targets of interests using its on-board sensors. The classical challenges in this problem are system model dependence and the difficulty of computing information-theoretic cost functions for a long planning horizon. RL provides solutions for these challenges as the length of its effective planning horizon does not affect the computational complexity, and it drops the strong dependency of an algorithm on system models. In particular, we introduce Active Tracking Target Network (ATTN), a unified deep RL policy that is capable of solving major sub-tasks of active target tracking - in-sight tracking, navigation, and exploration. The policy shows robust behavior for tracking agile and anomalous targets with a partially known target model. Additionally, the same policy is able to navigate in obstacle environments to reach distant targets as well as explore the environment when targets are positioned in unexpected locations.\n\nTitle: Information acquisition with sensing robots: Algorithms and error bounds\nContent: Utilizing the capabilities of configurable sensing systems requires addressing difficult information gathering problems. Near-optimal approaches exist for sensing systems without internal states. However, when it comes to optimizing the trajectories of mobile sensors the solutions are often greedy and rarely provide performance guarantees. Notably, under linear Gaussian assumptions, the problem becomes deterministic and can be solved off-line. Approaches based on submodularity have been applied by ignoring the sensor dynamics and greedily selecting informative locations in the environment. This paper presents a non-greedy algorithm with suboptimality guarantees, which relies on concavity instead of submodularity and takes the sensor dynamics into account. Coupled with linearization and model predictive control, the algorithm can be used to generate adaptive policies for mobile sensors with non-linear sensing models. Applications in gas concentration mapping and target tracking are presented.\n\n## Task\nCompose an abstract for the title 'On trajectory optimization for active sensing in Gaussian process models.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We consider the problem of optimizing the trajec- tory of a mobile sensor with perfect localization whose task is to estimate a stochastic, perhaps multidimensional field modeling the environment. When the estimator is the Kalman filter, and for certain classes of objective functions capturing the informativeness of the sensor paths, the sensor trajectory op- timization problem is a deterministic optimal control problem. This estimation problem arises in many applications besides the field estimation problem, such as active mapping with mobile robots. The main difficulties in solving this problem are computational, since the Gaussian process of interest is usually high dimensional. We review some recent work on this problem and propose a suboptimal non-greedy trajectory optimization scheme with a manageable computational cost, at least in static field models based on sparse graphical models."
      }
    ]
  },
  {
    "tid":"103",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: MLPNN adaptive controller based on a reference model to drive an actuated lower limb orthosis\nContent: In this paper we propose to drive an actuated orthosis using an adaptive controller based on a reference model. It is not necessary to know all the functions of the dynamic model. Needing only the global structure of the dynamic model, we use a specific adaptive controller to obtain good performance in terms of trajectory tracking both in position and in velocity. A Multi-Layer Perceptron Neural Network (MLPNN) is used to estimate dynamics related to inertia, gravitational and frictional forces along with other unmodeled dynamics. The Lyapunov formalism is used for stability study of the system (shank+orthosis) in closed loop and to determine adaptation laws of the neural parameters. To treat the non-linearties related to the MLPNN, we have used first order Taylor series expansion. Experimental results have been obtained using a real orthosis worn by an appropriate dummy. Several tests have been realized to verify the effectiveness and the robustness of the proposed controller. For instance, our proposed orthosis model has given robust tracking performance under assistive as well as resistive forces.\n\nTitle: Bounded Control of an Actuated Lower-Limb Exoskeleton\nContent: AbstractA bounded control strategy is employed for the rehabilitation and assistance of a patient with lower-limb disorder. Complete and partial lower-limb motor function disorders are considered. This application is centered on the knee and the ankle joint level, thereby considering a user in a sitting position. A high gain observer is used in the estimation of the angular position and angular velocities which is then applied to the estimation of the joint torques. The level of human contribution is feedback of a fraction of the estimated joint torque. This is utilised in order to meet the demands for a bounded human torque; that is, \u03c4h\u2264N2,n\u2264N1,n. The asymptotic stability of the bounded control law without human contribution and the convergence analysis of the high gain observer is verified using Lyapunov-based analysis. Simulations are performed to verify the proposed control law. Results obtained guarantee a fair trajectory tracking of the physiotherapist trajectory.\n\nTitle: Shank-foot trajectory control: A forward dynamics approach using computed-torque control\nContent: This paper presents an approach to shank-foot trajectory control using computed-torque control method. The aim is to provide rehabilitative measures to help patients with lower limb disorders improve their mobility performance. This measure is achieved by designing a desired trajectory to be followed by the shank-foot model for the rehabilitative purpose. The shank-foot model is designed to depict the knee-ankle orthosis having joints at the knee and ankle with respect to the subject being in a sitting position. The trajectories were defined using inverse kinematic transformations and Matsuoka oscillators. Inverse kinematic transformation is employed to convert the specified circular trajectory into joint space trajectories for the purpose of control; while the Matsuoka oscillator has no explicit desired trajectory since it is being generated internally by its dynamics. The nonlinear forward dynamics of the Shank-Foot model is derived using Lagrange's equation method to describe the two degree of freedom (DoF) system. Computed-Torque Proportional-Integral-Derivative (PID) control method was used for tracking the circular designed trajectory, whereas a simple Proportional-Derivative (PD) control law incorporated into the Computed-Torque was used for the tracking of the internally generated trajectory of the oscillator. Tracking capabilities of both control methods based on each trajectory in the presence of external disturbances (uncertainties) and joint friction were analysed using SCILAB. The results obtained show satisfactory performances of the proposed approach.\n\nTitle: Analysis of Knee-Ankle Orthosis Modelling: An Inverse Dynamics Approach Using Adaptive Coupled Oscillator\nContent: In this paper, an inverse dynamics approach by means of adaptive coupled oscillators is used in the modelling and control of a lower limb orthosis applied at the knee and ankle joint level. This design is aimed at providing assistance and rehabilitative measures to humans with lower limb disorders and as such presents a platform for which their mobility performance can be improved. Adaptive oscillators are known to have the capability of learning high level parameters of sinusoidal, quasi-sinusoidal or non-sinusoidal signals (amplitude, frequency and offset). However, the later signal (non-sinusoidal) considered in this paper requires a number of oscillators in parallel to replicate the moving joint regarding filtering via adaptive oscillator. The dynamic model for the knee and ankle are considered to take the form of a damped pendulum model connected by two revolute joints. This maps the input torque of both joints (knee and ankle) to their output trajectories, hence integrating the different forces at the joint level of the different joints. The coupling effect is achieved by the use of coupled Adaptive Frequency Oscillator (AFO) for the estimation of the joint trajectories. Tracking performance for the knee-ankle orthosis is studied for non-sinusoidal reference trajectories, having a global coupling between the joints. The results obtained using SCILAB show a good performance of the controller trajectory tracking capabilities even in the presence of external disturbances.\n\nTitle: Bounded Control Of A Full-Exoskeleton Device With Four (4) Degree Of Freedom\nContent: This study deals with the bounded control of a lower limb exoskeleton having four (4) Degree of Freedom (DoF), which are located at hips and knees joint level. It is intended to be used as a rehabilitative device for individuals with hip or knee joint impairments. The rehabilitation protocol entails using Central Pattern Generators (CPGs) to design rhythmic movement close to that of the human gait. In order to remedy the drawback associated with CPGs which is characterised by the inability to choose specified initial conditions for the generated trajectories, the use of time polynomial functions is employed to approximate the joint angle profile generated. This gait is meant to be tracked by the user under the influence of a controller. The control torque is envisaged to ensure a proper trajectory following of the exoskeleton for the flexion/extension movements of the hip and knee within the range of motion associated with walking. With this approach, the wearer is completely passive. Using a bounded controller further protect the user and the device by ensuring the actuator's physical limits are not exceeded. Simulation results with the aid of Matlab/Simulink does help validate the proposed assertion.\n\n## Task\nCompose an abstract for the title 'A radial basis function neural network adaptive controller to drive a powered lower limb knee joint orthosis' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Graphical abstractDisplay Omitted HighlightsHuman safety is ensured for the wearer of exoskeleton.Adaptive controller without prior knowledge.Stability proved theoretically using Lyapunov approach.No chattering phenomena in the input control.External disturbances are rejected and the wearer efforts are taken into account. This paper deals with the rehabilitation purposes using an active orthosis driven by an adaptive neural controller based on a radial basis function neural network (RBFNN). Two essential conditions are required in our study: ensuring the wearer safety and the good trajectory tracking. We consider for our experiments the same movements often recommended by the doctor during therapy sessions. In this context, it is possible to add some trivial prior knowledge as the dynamic model structure and all dynamical identified parts. The unknown or the uncertainty part of the inertia term of the knee-shank-orthosis system is identified online using an adaptive term. All other uncertainties or unknown dynamics are identified online by the RBFNN. The Lyapunov approach has been used to derive adaptation laws of the neural parameters and the inertia term. These adaptation laws ensure the stability of the system composed of the exoskeleton and its wearer. The wearer can be completely inactive or applying either a resistive or an assistive effort. Experimental results have been conducted on a real exoskeleton that is used for rehabilitation reasons. Based on these results we conclude with the effectiveness of the proposed approach."
      }
    ]
  },
  {
    "tid":"104",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Experiential Sampling on Multiple Data Streams\nContent: Multimedia systems must deal with multiple data streams. Each data stream usually contains significant volume of redundant noisy data. In many real-time applications, it is essential to focus the computing resources on a relevant subset of data streams at any given time instant and use it to build the model of the environment. We formulate this problem as an experiential sampling problem and propose an approach to utilize computing resources efficiently on the most informative subset of data streams. In this paper, we generalize our experiential sampling framework to multiple data streams and provide an evaluation measure for this technique. We have successfully applied this framework to the problems of traffic monitoring, face detection and monologue detection\n\nTitle: Experience based sampling technique for multimedia analysis\nContent: We present a novel experience based sampling or experiential sampling technique which has the ability to focus on the analysis's task by making use of the contextual information from the environment. In this technique, sensor samples are used to gather information about the current environment and attention samples are used to represent the current state of attention. The task-attended samples are inferred from experience and maintained by a sampling based dynamical system. The multimedia analysis task can then focus on the attention samples only. Moreover, past experiences and the current environment can be used to adaptively correct and tune the attention. Experimental results have been presented to demonstrate the efficacy of our technique.\n\nTitle: Experiential Sampling Based Foreground/Background Segmentation For Video Surveillance\nContent: Segmentation of foreground and background has been an important research problem arising out of many applications including video surveillance. A method commonly used for segmentation is \"background subtraction\" or thresholding the difference between the estimated background image and current image. Adaptive Gaussian mixture based background modelling has been proposed by many researchers for increasing the robustness against environmental changes. However, all these methods, being computationally intensive, need to be optimized for efficient and real-time performance especially at a higher image resolution. In this paper, we propose an improved foreground/background segmentation method which uses Experiential Sampling technique to restrict the computational efforts in the region of interest. We exploit the fact that the region of interest in general is present only in a small part of the image, therefore, the attention should only be focused in those regions. The proposed method shows a significant gain in processing speed at the expense of minor loss in accuracy. We provide experimental results and detailed analysis to show the utility of our method.\n\nTitle: Timeline-based information assimilation in multimedia surveillance and monitoring systems\nContent: Most surveillance and monitoring systems nowadays utilize multiple types of sensors. However, due to the asynchrony among and diversity of sensors, information assimilation - how to combine the information obtained from asynchronous and multifarious sources is an important and challenging research problem. In this paper, we propose a hierarchical probabilistic method for information assimilation in order to detect events of interest in a surveillance and monitoring environment. The proposed method adopts a bottom-up approach and performs assimilation of information at three different levels - media-stream level, atomic-event level and compound-event level.To detect an event, our method uses not only the current media streams but it also utilizes their two important properties - first, accumulated past history of whether they have been providing the concurring or contradictory evidences, and - second, the system designer's confidence in them. A compound event, which comprises of two or more atomic-events, is detected by first estimating probabilistic decisions for the atomic-events based on individual streams, and then by aligning these decisions along a timeline and hierarchically assimilating them. The experimental results show the utility of our method.\n\nTitle: A design methodology for selection and placement of sensors in multimedia surveillance systems\nContent: This paper addresses the problem of how to select the optimal number of sensors and how to determine their placement in a given monitored area for multimedia surveillance systems. We propose to solve this problem by obtaining a novel performance metric in terms of a probability measure for accomplishing the task as a function of set of sensors and their placement. This measure is then used to find the optimal set. The same measure can be used to analyze the degradation in system 's performance with respect to the failure of various sensors. We also build a surveillance system using the optimal set of sensors obtained based on the proposed design methodology. Experimental results show the effectiveness of the proposed design methodology in selecting the optimal set of sensors and their placement.\n\n## Task\nCompose an abstract for the title 'Experiential Sampling in Multimedia Systems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Multimedia systems must deal with multiple data streams. Each data stream usually contains significant volume of redundant noisy data. In many real-time applications, it is essential to focus the computing resources on a relevant subset of data streams at any given time instant and use it to build the model of the environment. We formulate this problem as an experiential sampling problem and propose an approach to utilize computing resources efficiently on the most informative subset of data streams. First, in this paper, we focus on theoretical background and develop a theoretical framework for a single data stream. We generalize the notion of static visual attention in a dynamical systems setting and propose a dynamical attention-orientated analysis method. This is achieved by a sampling representation that utilizes the current context and past experience for attention evolution. Hence, the multimedia analysis task at hand can select its data of interest while immediately discarding the irrelevant data to achieve efficiency and adaptability"
      }
    ]
  },
  {
    "tid":"105",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Near-optimal policies for energy-aware task assignment in server farms.\nContent: Rising energy costs and the push for green computing have inspired a lot of research effort towards energy efficient computing. Incorporating low energy sleep states in server farms is one of the proposed solutions. This paper studies the trade-off between energy and performance that is inherent in such solutions using the popular cost metric Energy-Response-time-Weighted-Sum (ERWS). We apply the Markov Decision Process (MDP) theory to the task assignment problem, and derive a near-optimal dynamic task assignment policy for minimizing the ERWS cost metric. Furthermore, we consider a performance constrained energy minimization problem, and provide an algorithm that builds a dynamic task assignment policy by choosing the right energy weight value for the ERWS cost metric. We also show that the resulting task assignment policy behaves like a modified version of the Join the Shortest Queue (JSQ), having a near-optimal performance by minimizing energy consumption while still obeying response time constraint.\n\nTitle: Energy-Aware Server With Srpt Scheduling: Analysis And Optimization\nContent: We consider the optimal energy-aware control of a single server in a server farm. The server is modeled as an M/G/1 queue with a particular control policy that allows to put the server to a sleep mode to save energy with an additional delay cost, the setup delay, after the server is turned on again. Our main result is the derivation of mean response time for such a system under SRPT scheduling. In particular, we show that the mean response time can be decomposed into two parts: the mean response time of an ordinary M/G/1-SRPT, and an additional penalty term for switching the server to a sleep state. Furthermore, we study the energy-performance optimization of the system and prove that, for the Energy Response time Weighted Sum (ERWS) and Energy Response time Product (ERP) cost metrics, the optimal control either puts the server into a sleep state immediately when it becomes idle or keeps it idling until the next job arrives.\n\nTitle: Energy-aware SRPT server with batch arrivals: Analysis and optimization.\nContent: We consider the optimal energy-aware control of a single server with batch arrivals and applying the SRPT scheduling rule. The server is modeled as an MX\u2215G\u22151 queue with a particular control policy that puts the server to a sleep mode to save energy with an additional delay cost, the setup delay, after the server is turned on again. We first consider an ordinary MX\u2215G\u22151-SRPT system that does not go to sleep, and derive the mean response time equation. We then consider the more elaborate energy-aware MX\u2215G\u22151-SRPT system, and show that the mean response time can be decomposed into two parts: the mean response time of an ordinary MX\u2215G\u22151-SRPT, and an additional penalty term for switching the server to a sleep state. Furthermore, we study the energy-performance optimization of the system and prove that, for the Energy Response time Weighted Sum (ERWS) and Energy Response time Product (ERP) cost metrics, the optimal control either puts the server into a sleep state immediately when it becomes idle or keeps it idling until the next job arrives.\n\nTitle: Optimal sleep-state control of energy-aware M/G/1 queues.\nContent: We study the problem of optimally controlling the use of sleep states in an energy-aware M/G/1 queue. In our model, we consider a family of policies where the server upon becoming idle can wait for a random period before entering, potentially randomly, any of a finite number of possible sleep states to save energy. The server becomes busy again after a possibly random number of jobs have arrived. However, jobs are served only after a random setup time. This kind of an energy-aware queuing system has been analyzed in recent papers under specific assumptions regarding the cost metrics and the distributions of the random variables. In this paper, we consider an essentially more general model. Notably we show that the optimal control of the idle time and sleep states is deterministic and does not benefit from randomization: either the system only uses the idle state and no sleep states, or the idle state is not used at all and the server immediately goes to some fixed sleep state and waits until a fixed number of jobs have arrived before starting the setup. We prove this result for two popular cost metrics, namely weighted sum of energy and response time (ERWS) and their product ERP.\n\nTitle: Energy-aware dispatching in parallel queues with on-off energy consumption\nContent: We consider a dynamic dispatching problem where jobs are assigned upon arrival into parallel queues. Jobs have an arbitrary size distribution and each queue has its own service rate, queueing discipline, and operating power while serving jobs. Our goal is to minimize a weighted sum of delay and energy consumption under the assumption that the dispatcher is aware of the remaining service time of each job in the system, including that of the arriving job. We devise efficient dispatching heuristics based on the first policy iteration procedure of Markov Decision Processes. The resulting policies are illustrated by numerical examples. Direct control over the trade-off between performance and energy consumption will be increasingly important in future ICT equipment designs wherever dynamic queue assignment is needed.\n\n## Task\nCompose an abstract for the title 'Optimal energy-aware control policies for FIFO servers.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We study the optimal control of sleep states in an energy-aware M/G/1-FIFO queue. Two families of policies are considered, referred to as randomized and sequential policies. Under a randomized policy, the server upon becoming idle can wait for a random period before entering, potentially randomly, any of the possible sleep states. The server enters setup after a possibly random number of jobs have arrived. A server controlled by a sequential policy waits for a random period upon becoming idle and starts sequentially transiting the sleep states in a decreasing order of power consumption until it reaches the deepest sleep state, where it waits for a threshold number of jobs to arrive before setup. We show for Poisson arrivals, generally distributed service times, setup delays and idle times that the optimal control of the idle time and sleep states neither randomizes nor transits sleep states sequentially. Either the system only uses the idle state and no sleep states, or the server immediately goes to some fixed sleep state and waits until a fixed number of jobs have arrived before setup. We prove this result for two cost metrics, namely weighted sum of energy and response time (ERWS) and their product (ERP)."
      }
    ]
  },
  {
    "tid":"106",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A comparison of position estimation techniques using occupancy grids\nContent: A mobile robot requires a perception of its local environment for both sensor-based locomotion and for position estimation. Occupancy grids, based on ultrasonic range data, provide a robust description of the local environment for locomotion. Unfortunately, current techniques for position estimation based on occupancy grids are both unreliable and computationally expensive. This paper reports on experiments with four techniques for position estimation using occupancy grids. A world modelling technique based on combining global and local occupancy grids is described. Techniques are described for extracting line segments from an occupancy grid based on a Hough transform. The use of an extended Kalman filter for position estimation is then adapted to this framework. Four matching techniques are presented for obtaining the innovation vector required by the Kalman filter equations. Experimental results show that matching of segments extracted from both the local and global occupancy grids gives results which are superior to a direct matching of grids, or to a mixed matching of segments to grids.\n\nTitle: Revisiting 3D geometric models for accurate object shape and pose\nContent: Geometric 3D reasoning has received renewed attention recently, in the context of visual scene understanding. The level of geometric detail, however, is typically limited to qualitative or coarse-grained quantitative representations. This is linked to the fact that today's object class detectors are tuned towards robust 2D matching rather than accurate 3D pose estimation, encouraged by 2D bounding box-based benchmarks such as Pascal VOC. In this paper, we therefore revisit ideas from the early days of computer vision, namely, 3D geometric object class representations for recognition. These representations can recover geometrically far more accurate object hypotheses than just 2D bounding boxes, including relative 3D positions of object parts. In combination with recent robust techniques for shape description and inference, our approach outperforms state-of-the-art results in 3D pose estimation, while at the same time improving 2D localization. In a series of experiments, we analyze our approach in detail, and demonstrate novel applications enabled by our geometric object class representation, such as fine-grained categorization of cars according to their 3D geometry and ultra-wide baseline matching.\n\nTitle: 3D Pictorial Structures for Multiple Human Pose Estimation\nContent: In this work, we address the problem of 3D pose estimation of multiple humans from multiple views. This is a more challenging problem than single human 3D pose estimation due to the much larger state space, partial occlusions as well as across view ambiguities when not knowing the identity of the humans in advance. To address these problems, we first create a reduced state space by triangulation of corresponding body joints obtained from part detectors in pairs of camera views. In order to resolve the ambiguities of wrong and mixed body parts of multiple humans after triangulation and also those coming from false positive body part detections, we introduce a novel 3D pictorial structures (3DPS) model. Our model infers 3D human body configurations from our reduced state space. The 3DPS model is generic and applicable to both single and multiple human pose estimation. In order to compare to the state-of-the art, we first evaluate our method on single human 3D pose estimation on HumanEva-I [22] and KTH Multiview Football Dataset II [8] datasets. Then, we introduce and evaluate our method on two datasets for multiple human 3D pose estimation.\n\nTitle: 3D Pictorial Structures Revisited: Multiple Human Pose Estimation.\nContent: We address the problem of 3D pose estimation of multiple humans from multiple views. The transition from single to multiple human pose estimation and from the 2D to 3D space is challenging due to a much larger state space, occlusions and across-view ambiguities when not knowing the identity of the humans in advance. To address these problems, we first create a reduced state space by triangulation of corresponding pairs of body parts obtained by part detectors for each camera view. In order to resolve ambiguities of wrong and mixed parts of multiple humans after triangulation and also those coming from false positive detections, we introduce a 3D pictorial structures (3DPS) model. Our model builds on multi-view unary potentials, while a prior model is integrated into pairwise and ternary potential functions. To balance the potentials\u2019 influence, the model parameters are learnt using a Structured SVM (SSVM). The model is generic and applicable to both single and multiple human pose estimation. To evaluate our model on single and multiple human pose estimation, we rely on four different datasets. We first analyse the contribution of the potentials and then compare our results with related work where we demonstrate superior performance.\n\nTitle: Multiple Human Pose Estimation With Temporally Consistent 3d Pictorial Structures\nContent: Multiple human 3D pose estimation from multiple camera views is a challenging task in unconstrained environments. Each individual has to be matched across each view and then the body pose has to be estimated. Additionally, the body pose of every individual changes in a consistent manner over time. To address these challenges, we propose a temporally consistent 3D Pictorial Structures model (3DPS) for multiple human pose estimation from multiple camera views. Our model builds on the 3D Pictorial Structures to introduce the notion of temporal consistency between the inferred body poses. We derive this property by relying on multi-view human tracking. Identifying each individual before inference significantly reduces the size of the state space and positively influences the performance as well. To evaluate our method, we use two challenging multiple human datasets in unconstrained environments. We compare our method with the state-of-the-art approaches and achieve better results.\n\n## Task\nCompose an abstract for the title 'Position estimation using principal components of range data' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper describes a new approach to mobile robot position estimation, based on principal component analysis of laser range data. An eigenspace is constructed from the principal components of a large number of range data sets. The structure of an environment, as seen by a range sensor, is represented as a family of surfaces in this space. Subsequent range data sets from the environment project as a point in this space. Associating this point to the family of surfaces gives a set of candidate positions and orientations (poses) for the sensor. These candidate poses correspond to positions and orientations in the environment which have similar range profiles. A Kalman filter can be used to select the most likely candidate pose based on coherence with small movements. The first part of this paper describes how a relatively small number of depth profiles of an environment can be used to generate a complete eigenspace. This space is used to build a representation of the range scan profiles obtained from a regular grid of positions and orientations (poses). This representation has the form of a family of surfaces (a manifold). This representation converts the problem of associating a range profile to possible positions and orientations into a table lookup. As a side benefit, the method provides a simple means to detect obstacles in a range profile. The final section of the paper reviews the use of estimation theory to determine the correct pose hypothesis by tracking. (C) 1998 Elsevier Science B.V. All rights reserved."
      }
    ]
  },
  {
    "tid":"107",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Probabilistic Principal Surfaces for Yeast Gene Microarray Data Mining\nContent: The recent technological advances are producing huge data sets in almost all fields of scientific research, from astronomy to genetics. Although each research field often requires ad-hoc, fine tuned, procedures to properly exploit all the available information inherently present in the data, there is an urgent need for a new generation of general computational theories and tools capable to boost most human activities of data analysis. Here we propose Probabilistic Principal Surfaces (PPS) as an effective high-D data visualization and clustering tool for data mining applications, emphasizing its flexibility and generality of use in data-rich field. In order to better illustrate the potentialities of the method, we also provide a real world case-study by discussing the use of PPS for the analysis of yeast gene expression levels from microarray chips.\n\nTitle: A multi-step approach to time series analysis and gene expression clustering\nContent: Motivation: The huge growth in gene expression data calls for the implementation of automatic tools for data processing and interpretation. Results: We present a new and comprehensive machine learning data mining framework consisting in a non-linear PCA neural network for feature extraction, and probabilistic principal surfaces combined with an agglomerative approach based on Negentropy aimed at clustering gene microarray data. The method, which provides a user-friendly visualization interface, can work on noisy data with missing points and represents an automatic procedure to get, with no a priori assumptions, the number of clusters present in the data. Cell-cycle dataset and a detailed analysis confirm the biological nature of the most significant clusters. Availability: The software described here is a subpackage part of the ASTRONEURAL package and is available upon request from the corresponding author. Contact: robtag@unisa.it Supplementary information: Supplementary data are available at Bioinformatics online.\n\nTitle: Building Maps of Drugs Mode-of-Action from Gene Expression Data\nContent: We developed a data-mining and visualization approach to analyze the mode-of-action (MOA) of a set of drugs. Starting from wide-genome expression data following perturbations with different compounds in a reference data-set, our method realizes an euclidean embedding providing a map of MOAs in which drugs sharing the therapeutic application or a subset of molecular targets lies in close positions. First we build a low-dimensional, visualizable space combining a rank-aggregation method and a recent tool for the analysis of the enrichment of a set of genes in ranked lists (based on the Kolmogorov-Smirnov statistic). This space is obtained using prior knowledge about the data-set composition but with no assumptions about the similarities between different drugs. Then we assess that, despite the complexity and the variety of the experimental conditions, our aim is reached with good performance without across-condition normalization procedures.\n\nTitle: Robust and sparse correlation matrix estimation for the analysis of high-dimensional genomics data.\nContent: Motivation: Microarray technology can be used to study the expression of thousands of genes across a number of different experimental conditions, usually hundreds. The underlying principle is that genes sharing similar expression patterns, across different samples, can be part of the same co-expression system, or they may share the same biological functions. Groups of genes are usually identified based on cluster analysis. Clustering methods rely on the similarity matrix between genes. A common choice to measure similarity is to compute the sample correlation matrix. Dimensionality reduction is another popular data analysis task which is also based on covariance/correlation matrix estimates. Unfortunately, covariance/correlation matrix estimation suffers from the intrinsic noise present in high-dimensional data. Sources of noise are: sampling variations, presents of outlying sample units, and the fact that in most cases the number of units is much larger than the number of genes. Results: In this paper, we propose a robust correlation matrix estimator that is regularized based on adaptive thresholding. The resulting method jointly tames the effects of the high-dimensionality, and data contamination. Computations are easy to implement and do not require hand tunings. Both simulated and real data are analyzed. A Monte Carlo experiment shows that the proposed method is capable of remarkable performances. Our correlation metric is more robust to outliers compared with the existing alternatives in two gene expression datasets. It is also shown how the regularization allows to automatically detect and filter spurious correlations. The same regularization is also extended to other less robust correlation measures. Finally, we apply the ARACNE algorithm on the SyNTreN gene expression data. Sensitivity and specificity of the reconstructed network is compared with the gold standard. We show that ARACNE performs better when it takes the proposed correlation matrix estimator as input.\n\nTitle: Robust clustering of noisy high-dimensional gene expression data for patients subtyping.\nContent: Motivation: One of the most important research areas in personalized medicine is the discovery of disease sub-types with relevance in clinical applications. This is usually accomplished by exploring gene expression data with unsupervised clustering methodologies. Then, with the advent of multiple omics technologies, data integration methodologies have been further developed to obtain better performances in patient separability. However, these methods do not guarantee the survival separability of the patients in different clusters. Results: We propose a new methodology that first computes a robust and sparse correlation matrix of the genes, then decomposes it and projects the patient data onto the first m spectral components of the correlation matrix. After that, a robust and adaptive to noise clustering algorithm is applied. The clustering is set up to optimize the separation between survival curves estimated cluster-wise. The method is able to identify clusters that have different omics signatures and also statistically significant differences in survival time. The proposed methodology is tested on five cancer datasets downloaded from The Cancer Genome Atlas repository. The proposed method is compared with the Similarity Network Fusion (SNF) approach, and model based clustering based on Student's t-distribution (TMIX). Our method obtains a better performance in terms of survival separability, even if it uses a single gene expression view compared to the multi-view approach of the SNF method. Finally, a pathway based analysis is accomplished to highlight the biological processes that differentiate the obtained patient groups.\n\n## Task\nCompose an abstract for the title 'Mining Yeast Gene Microarray Data with Latent Variable Models.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Gene-expression microarrays make it possible to simultaneously measure the rate at which a cell or tissue is expressing each of its thousands of genes. One can use these comprehensive snapshots of biological activity to infer regulatory pathways in cells, identify novel targets for drug design, and improve diagnosis, prognosis, and treatment planning for those suffering from disease. However, the amount of data this new technology produces is more than one can manually analyze. Hence, the need for automated analysis of microarray data offers an opportunity for machine learning to have a significant impact on biology and medicine. Probabilistic Principal Surfaces defines a unified theoretical framework for nonlinear latent variable models embracing the Generative Topographic Mapping as a special case. This article describes the use of PPS for the analysis of yeast gene expression levels from microarray chips showing its effectiveness for high-D data visualization and clustering."
      }
    ]
  },
  {
    "tid":"108",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Rectification of Optical Characters as Transform Invariant Low-Rank Textures\nContent: Character rectification is very important for character recognition. Front view standard character images are much easier to recognize since most character recognition algorithms were trained with such data. However, the existing text rectification methods only work for a paragraph or a page. We discover that the modified TILT algorithm can be applied to rectify many single Chinese, English, and digit characters robustly. By changing the character image into a low-rank texture image via binarization and gray level inversion, the modified TILT method applies a rank minimization technique to recover the deformation and the proposed algorithm can work for almost all characters. To further enhance the robustness of the proposed algorithm, the modified TILT algorithm is extended for short phrases that consist of multiple characters. Extensive experiments testify to the effectiveness of the proposed method in rectifying texts with significant affine or perspective deformation in real images, such as street signs taken by mobile phones.\n\nTitle: Joint target tracking and recognition using view and identity manifolds\nContent: We propose a new concept of identity manifold for automated target tracking and recognition (ATR) that captures both inter-class (e.g., between tanks and armored cars) and intra-class (e.g., between different tanks) variability of target appearances (e.g., shapes). A hemisphere-shaped view manifold is also involved for mutli-view target modeling. Combining the two continuous-valued manifolds via nonlinear tensor decomposition gives rise to a new generative model that can be learned from a small training set. This model can not only deal with arbitrary view/pose variations by tracking along the view manifold, but also interpolate the appearance of an unknown target along the identity manifold. The proposed model is examined based on the recently released SENSIAC ATR database, and the experimental results confirm the usefulness of this generative model. \u00a9 2011 IEEE.\n\nTitle: Structure-guided manifold learning for video-based motion estimation\nContent: We present a new structure-guided joint gait pose manifold (JGPM) that represents gait kinematics by two variables. One is the pose to denote a series of stages in a walking cycle and the other is the gait to reflect the individual walking styles. Coupling pose and gait variables in the same latent space, such as a torus-like JGPM, was shown promising and effective for video-based motion estimation. However, the two-step learning used in torus-like JGPM is computationally expensive and it separates the optimization of pose and gait variables. This work overcomes the limitations of the previous method by developing a new structure-guided JGPM that is able to jointly optimize four variables in the same latent space, leading to a much compact parameter set while sustaining a comparable performance on video-based motion estimation, as well as a great potential for large-scale learning.\n\nTitle: Non-blind deblurring of structured images with geometric deformation\nContent: Non-blind deconvolution, which is to restore a sharp version of a given blurred image when the blur kernel is known, is a fundamental step in image deblurring. While the problem has been extensively studied, existing methods have conveniently ignored an important fact that deformation can significantly affect the statistical characteristics of an image and introduce additional blurring effect. In this paper, we show how to enhance non-blind deconvolution by recovering and undoing the deformation while deconvolving a given blurred image. We show that this is the case for almost all popular regularizers that have been proposed for image deblurring such as total variation and its variants. We conduct extensive simulations and experiments on real images and verify that the incorporation of geometric deformation in deconvolution can significantly improve the final deblurring results. Combined with existing blur kernel estimation techniques, our method can also be used to enhance blind image deblurring.\n\nTitle: Two-layer dual gait generative models for human motion estimation from a single camera\nContent: This paper presents a two-layer gait representation framework for video-based human motion estimation that extends our recent dual gait generative models, visual gait generative model (VGGM) and kinematic gait generative model (KGGM), with a new capability of part-whole gait modeling. Specifically, the idea of gait manifold learning is revisited to capture the gait variability among different individuals at both whole and part levels. A key issue is the selection of an appropriate distance metric to evaluate the dissimilarity between two gaits (either at whole or part levels) that determines an optimal manifold topology. Several metrics are studied and compared in terms of their effectiveness for gait manifold learning at both whole and part levels. This work involves one whole-based and two part-level gait manifolds by which three pairs of KGGM and VGGM can be learned and integrated for part-whole gait modeling. Moreover, a two-stage Monte Carlo Markov Chain (MCMC) inference algorithm is developed for video-based part-whole motion estimation. The proposed algorithm is tested on the HumanEva data and reaches state-of-art results.\n\n## Task\nCompose an abstract for the title 'Depth perception of the surfaces in occluded scenic images' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The adaptive disparity filter is refined, along with the monocular feature filter designed based on neuron dynamics, and a new stereopsis model is set up in this study. The disparities of the matched binocular features in stereo image pair are detected by the adaptive disparity filter, and removed by the monocular feature filter, only leaving those unmatched monocular features to be added to all depth planes determined by the disparities of the matched binocular features. Finally, visible surface perception is generated by the closed boundaries in the corresponding depth plane during the filling-in processing. By the above mechanism, the depth perception of surfaces in the occluded scenic images is realized, also, the figure-ground segmentation."
      }
    ]
  },
  {
    "tid":"109",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Topic-Sensitive Influencer Mining in Interest-Based Social Media Networks via Hypergraph Learning\nContent: Social media is emerging as a new mainstream means of interacting around online media. Social influence mining in social networks is therefore of critical importance in real-world applications such as friend suggestion and photo recommendation. Social media is inherently multimodal, including rich types of user contributed content and social link information. Most of the existing research suffers from two limitations: 1) only utilizing the textual information, and/or 2) only analyzing the generic influence but ignoring the more important topic-level influence. To address these limitations, in this paper we develop a novel Topic-Sensitive Influencer Mining (TSIM) framework in interest-based social media networks. Specifically, we take Flickr as the study platform. People in Flickr interact with each other through images. TSIM aims to find topical influential users and images. The influence estimation is determined with a hypergraph learning approach. In the hypergraph, the vertices represent users and images, and the hyperedges are utilized to capture multi-type relations including visual-textual content relations among images, and social links between users and images. Algorithmwise, TSIM first learns the topic distribution by leveraging user-contributed images, and then infers the influence strength under different topics for each node in the hypergraph. Extensive experiments on a real-world dataset of more than 50 K images and 70 K comment/favorite links from Flickr have demonstrated the effectiveness of our proposed framework. In addition, we also report promising results of friend suggestion and photo recommendation via TSIM on the same dataset.\n\nTitle: Bilateral Correspondence Model for Words-and-Pictures Association in Multimedia-Rich Microblogs\nContent: Nowadays, the amount of multimedia contents in microblogs is growing significantly. More than 20&percnt; of microblogs link to a picture or video in certain large systems. The rich semantics in microblogs provides an opportunity to endow images with higher-level semantics beyond object labels. However, this raises new challenges for understanding the association between multimodal multimedia contents in multimedia-rich microblogs. Disobeying the fundamental assumptions of traditional annotation, tagging, and retrieval systems, pictures and words in multimedia-rich microblogs are loosely associated and a correspondence between pictures and words cannot be established. To address the aforementioned challenges, we present the first study analyzing and modeling the associations between multimodal contents in microblog streams, aiming to discover multimodal topics from microblogs by establishing correspondences between pictures and words in microblogs. We first use a data-driven approach to analyze the new characteristics of the words, pictures, and their association types in microblogs. We then propose a novel generative model called the Bilateral Correspondence Latent Dirichlet Allocation (BC-LDA) model. Our BC-LDA model can assign flexible associations between pictures and words and is able to not only allow picture-word co-occurrence with bilateral directions, but also single modal association. This flexible association can best fit the data distribution, so that the model can discover various types of joint topics and generate pictures and words with the topics accordingly. We evaluate this model extensively on a large-scale real multimedia-rich microblogs dataset. We demonstrate the advantages of the proposed model in several application scenarios, including image tagging, text illustration, and topic discovery. The experimental results demonstrate that our proposed model can significantly and consistently outperform traditional approaches.\n\nTitle: Regularity and Conformity: Location Prediction Using Heterogeneous Mobility Data\nContent: Mobility prediction enables appealing proactive experiences for location-aware services and offers essential intelligence to business and governments. Recent studies suggest that human mobility is highly regular and predictable. Additionally, social conformity theory indicates that people's movements are influenced by others. However, existing approaches for location prediction fail to organically combine both the regularity and conformity of human mobility in a unified model, and lack the capacity to incorporate heterogeneous mobility datasets to boost prediction performance. To address these challenges, in this paper we propose a hybrid predictive model integrating both the regularity and conformity of human mobility as well as their mutual reinforcement. In addition, we further elevate the predictive power of our model by learning location profiles from heterogeneous mobility datasets based on a gravity model. We evaluate the proposed model using several city-scale mobility datasets including location check-ins, GPS trajectories of taxis, and public transit data. The experimental results validate that our model significantly outperforms state-of-the-art approaches for mobility prediction in terms of multiple metrics such as accuracy and percentile rank. The results also suggest that the predictability of human mobility is time-varying, e.g., the overall predictability is higher on workdays than holidays while predicting users' unvisited locations is more challenging for workdays than holidays.\n\nTitle: Mining Latent Attributes from Click-through Logs for Image Recognition\nContent: Attribute-based image representation, which represents an image by projecting it into a space spanned by attributes, has attracted increasing attention from both computer vision and multimedia communities for its compactness and potential to bridge the semantic gap. While many works focus on learning attribute models and utilizing them in image recognition and retrieval, few touch the problem of how to effectively construct a vocabulary of attributes, which is an essential part of effective attribute-based representation. Most existing approaches define the attribute vocabulary by human experts or through existing ontology, which is often limited in coverage of general concept space. In this paper, we propose automatically constructing the attribute vocabulary by mining latent topics from the click-through log of a commercial image search engine. These attributes are referred to as Latent Topic Attributes (LTA), which take advantage of tens of millions of interactions between user submitted queries and images, thereby providing better coverage for the concept space than existing approaches. The mining of latent topics from the click log is formulated as a matrix factorization problem, and further improved by weighted terms based matrix factorization to address the extreme sparsity of the click-through matrix. Both qualitative results of the mined LTA and quantitative results on the standard image recognition benchmark demonstrate the mined LTA\u2019s effectiveness.\n\nTitle: Cross-media semantic representation via bi-directional learning to rank\nContent: In multimedia information retrieval, most classic approaches tend to represent different modalities of media in the same feature space. Existing approaches take either one-to-one paired data or uni-directional ranking examples (i.e., utilizing only text-query-image ranking examples or image-query-text ranking examples) as training examples, which do not make full use of bi-directional ranking examples (bi-directional ranking means that both text-query-image and image-query-text ranking examples are utilized in the training period) to achieve a better performance. In this paper, we consider learning a cross-media representation model from the perspective of optimizing a listwise ranking problem while taking advantage of bi-directional ranking examples. We propose a general cross-media ranking algorithm to optimize the bi-directional listwise ranking loss with a latent space embedding, which we call Bi-directional Cross-Media Semantic Representation Model (Bi-CMSRM). The latent space embedding is discriminatively learned by the structural large margin learning for optimization with certain ranking criteria (mean average precision in this paper) directly. We evaluate Bi-CMSRM on the Wikipedia and NUS-WIDE datasets and show that the utilization of the bi-directional ranking examples achieves a much better performance than only using the uni-directional ranking examples.\n\n## Task\nCompose an abstract for the title 'Beyond the Words: Predicting User Personality from Heterogeneous Information.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"An incisive understanding of user personality is not only essential to many scientific disciplines, but also has a profound business impact on practical applications such as digital marketing, personalized recommendation, mental diagnosis, and human resources management. Previous studies have demonstrated that language usage in social media is effective in personality prediction. However, except for single language features, a less researched direction is how to leverage the heterogeneous information on social media to have a better understanding of user personality. In this paper, we propose a Heterogeneous Information Ensemble framework, called HIE, to predict users' personality traits by integrating heterogeneous information including self-language usage, avatar, emoticon, and responsive patterns. In our framework, to improve the performance of personality prediction, we have designed different strategies extracting semantic representations to fully leverage heterogeneous information on social media. We evaluate our methods with extensive experiments based on a real-world data covering both personality survey results and social media usage from thousands of volunteers. The results reveal that our approaches significantly outperform several widely adopted state-of-the-art baseline methods. To figure out the utility of HIE in a real-world interactive setting, we also present DiPsy, a personalized chatbot to predict user personality through heterogeneous information in digital traces and conversation logs."
      }
    ]
  },
  {
    "tid":"110",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Optimized cluster-based filtering algorithm for graph metadata\nContent: With the increasing amount of information on the Web and the proliferation of RSS offerings, efficient graph-based metadata filtering algorithm for large scale information dissemination is very important today. Matching graph-based documents is expensive due to the expressiveness of the language. The centralized architecture does not work well for the large scale information dissemination service. To address these problems, in this paper we develop a cluster-based publish/subscribe system for filtering graph-based RSS documents. Essentially, we develop two indexing algorithms to enable workload distribution and cluster-based filtering. Furthermore, we proposed an optimized graph matching algorithm which speeds up the constraint evaluation for subscriptions. The experimental results show that we can support one million subscriptions on a compute cluster with 5-20 nodes and the throughput scales linearly with the number of cluster nodes.\n\nTitle: G-ToPSS: fast filtering of graph-based metadata\nContent: RDF is increasingly being used to represent metadata. RDF Site Summary (RSS) is an application of RDF on the Web that has considerably grown in popularity. However, the way RSS systems operate today does not scale well. In this paper we introduce G-ToPSS, a scalable publish/subscribe system for selective information dissemination. G-ToPSS is particularly well suited for applications that deal with large-volume content distribution from diverse sources. RSS is an instance of the content distribution problem. G-ToPSS allows use of ontology as a way to provide additional information about the data. Furthermore, in this paper we show how G-ToPSS can support RDFS class taxonomies. We have implemented and experimentally evaluated G-ToPSS and we provide results in the paper demonstrating its scalability compared to alternatives.\n\nTitle: Scaling Construction of Low Fan-out Overlays for Topic-Based Publish/Subscribe Systems\nContent: It is a key challenge and fundamental problem in the design of distributed publish/subscribe systems to construct the underlying dissemination overlay. In this paper, we focus on effective practical solution for the Min Max-TCO problem: Create a topic-connected pub/sub overlay in which all nodes interested in the same topic are organized in a directly connected dissemination sub-overlay while keeping the maximum node degree to the minimum. Previously known solutions provided an extensive analysis of the problem and an algorithm that achieves a logarithmic approximation for Min Max-TCO. Yet, they did not focus on efficiency of the solution or feasibility of decentralized operation that would not require full knowledge of the system. Compared to these solutions, our proposed algorithm produces an overlay with marginally higher degrees. At the same time, it has drastically reduced runtime cost, which is corroborated by both theoretical analysis and empirical evaluation. The latter shows a speedup by a factor of more than 25 on average for typical pub/sub workloads.\n\nTitle: Safe Distribution and Parallel Execution of Data-centric Workflows over the Publish/Subscribe Abstraction\nContent: In this work, we develop an approach for the safe distribution and parallel execution of data-centric workflows over the publish/subscribe abstraction. In essence, we design a unique representation of data-centric workflows, specifically designed to exploit the loosely coupled and distributed nature of publish/subscribe systems. Furthermore, we argue for the practicality and expressiveness of our approach by mapping a standard and industry-strength data-centric workflow model, namely, IBM Business Artifacts with Guard-Stage-Milestone (GSM), into the publish/subscribe abstraction. In short, the contributions of this work are three-fold: (1) mapping of data-centric workflows into publish/subscribe to achieve distributed and parallel execution; (2) detailed theoretical analysis of the mapping; and (3) formulation of the complexity of the optimal workflow distribution over the publish/subscribe abstraction as an NP-hard problem.\n\nTitle: Efficient matching for state-persistent publish/subscribe systems\nContent: Content-based publish/subscribe systems allow information dissemination and fine-grained information filtering in loosely coupled distributed systems. Stateless publish/subscribe systems send notifications to all subscribers whose subscriptions match an incoming publication. State-persistent publish/subscribe systems, a recently proposed model that stores the states of both publications and subscriptions, only send notifications upon state transitions. The information filtering process requires an efficient matching algorithm with high throughput and scalability. Although there have been studies on matching algorithms for stateless publish/subscribe systems, the matching problem for state-persistent publish/subscribe systems is still an open research problem. This paper presents a novel content-based matching algorithm and its data structures for state-persistent publish/subscribe systems. We will also present the complexity analysis and results of simulations that validates the analytical predictions.\n\n## Task\nCompose an abstract for the title 'Efficient and scalable filtering of graph-based metadata' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"RDF Site Summaries constitute an application of RDF on the Web that has considerably grown in popularity. However, the way RSS systems operate today limits their scalability. Current RSS feed arregators follow a pull-based architecture model, which is not going to scale with the increasing number of RSS feeds becoming available on the Web. In this paper, we introduce G-ToPSS, a scalable publish/subscribe system for selective information dissemination. G-ToPSS only sends newly updated information to the interested user and follows a push-based architecture model. G-ToPSS is particularly well suited for applications that deal with large-volume content distribution from diverse sources. G-ToPSS allows use of an ontology as a way to provide additional information about the data disseminated. We have implemented and experimentally evaluated G-ToPSS and we provide results demonstrating its scalability compared to alternative approaches. In addition, we describe an application of G-ToPSS and RSS to a Web-based content management system that provides an expressive, efficient, and convenient update notification dissemination system."
      }
    ]
  },
  {
    "tid":"111",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Locally Adaptive Bayesian Multivariate Time Series.\nContent: In modeling multivariate time series, it is important to allow time-varying smoothness in the mean and covariance process. In particular, there may be certain time intervals exhibiting rapid changes and others in which changes are slow. If such locally adaptive smoothness is not accounted for, one can obtain misleading inferences and predictions, with over-smoothing across erratic time intervals and under-smoothing across times exhibiting slow variation. This can lead to miscalibration of predictive intervals, which can be substantially too narrow or wide depending on the time. We propose a continuous multivariate stochastic process for time series having locally varying smoothness in both the mean and covariance matrix. This process is constructed utilizing latent dictionary functions in time, which are given nested Gaussian process priors and linearly related to the observed data through a sparse mapping. Using a differential equation representation, we bypass usual computational bottlenecks in obtaining MCMC and online algorithms for approximate Bayesian inference. The performance is assessed in simulations and illustrated in a financial application.\n\nTitle: Bayesian Graphical Models for Multivariate Functional Data\nContent: Graphical models express conditional independence relationships among variables. Although methods for vector-valued data are well established, functional data graphical models remain underdeveloped. By functional data, we refer to data that are realizations of random functions varying over a continuum (e.g., images, signals). We introduce a notion of conditional independence between random functions, and construct a framework for Bayesian inference of undirected, decomposable graphs in the multivariate functional data context. This framework is based on extending Markov distributions and hyper Markov laws from random variables to random processes, providing a principled alternative to naive application of multivariate methods to discretized functional data. Markov properties facilitate the composition of likelihoods and priors according to the decomposition of a graph. Our focus is on Gaussian process graphical models using orthogonal basis expansions. We propose a hyper-inverse-Wishart-process prior for the covariance kernels of the infinite co-efficient sequences of the basis expansion, and establish its existence and uniqueness. We also prove the strong hyper Markov property and the conjugacy of this prior under a finite rank condition of the prior kernel parameter. Stochastic search Markov chain Monte Carlo algorithms are developed for posterior inference, assessed through simulations, and applied to a study of brain activity and alcoholism.\n\nTitle: Bayesian Local Contamination Models for Multivariate Outliers.\nContent: In studies where data are generated from multiple locations or sources it is common for there to exist observations that are quite unlike the majority. Motivated by the application of establishing a reference value in an inter-laboratory setting when outlying labs are present, we propose a local contamination model that is able to accommodate unusual multivariate realizations in a flexible way. The proposed method models the process level of a hierarchical model using a mixture with a parametric component and a possibly nonparametric contamination. Much of the flexibility in the methodology is achieved by allowing varying random subsets of the elements in the lab-specific mean vectors to be allocated to the contamination component. Computational methods are developed and the methodology is compared to three other possible approaches using a simulation study. We apply the proposed method to a NIST/NOAA sponsored inter-laboratory study which motivated the methodological development.\n\nTitle: Locally adaptive factor processes for multivariate time series\nContent: In modeling multivariate time series, it is important to allow time-varying smoothness in the mean and covariance process. In particular, there may be certain time intervals exhibiting rapid changes and others in which changes are slow. If such time-varying smoothness is not accounted for, one can obtain misleading inferences and predictions, with oversmoothing across erratic time intervals and under-smoothing across times exhibiting slow variation. This can lead to mis-calibration of predictive intervals, which can be substantially too narrow or wide depending on the time. We propose a locally adaptive factor process for characterizing multivariate mean-covariance changes in continuous time, allowing locally varying smoothness in both the mean and covariance matrix. This process is constructed utilizing latent dictionary functions evolving in time through nested Gaussian processes and linearly related to the observed data with a sparse mapping. Using a differential equation representation, we bypass usual computational bottlenecks in obtaining MCMC and online algorithms for approximate Bayesian inference. The performance is assessed in simulations and illustrated in a financial application.\n\nTitle: Multivariate convex regression with adaptive partitioning\nContent: We propose a new, nonparametric method for multivariate regression subject to convexity or concavity constraints on the response function. Convexity constraints are common in economics, statistics, operations research, financial engineering and optimization, but there is currently no multivariate method that is stable and computationally feasible for more than a few thousand observations. We introduce convex adaptive partitioning (CAP), which creates a globally convex regression model from locally linear estimates fit on adaptively selected covariate partitions. CAP is a computationally efficient, consistent method for convex regression. We demonstrate empirical performance by comparing the performance of CAP to other shape-constrained and unconstrained regression methods for predicting weekly wages and value function approximation for pricing American basket options.\n\n## Task\nCompose an abstract for the title 'Adaptive sampling for Bayesian geospatial models' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Bayesian hierarchical modeling with Gaussian process random effects provides a popular approach for analyzing point-referenced spatial data. For large spatial data sets, however, generic posterior sampling is infeasible due to the extremely high computational burden in decomposing the spatial correlation matrix. In this paper, we propose an efficient algorithm--the adaptive griddy Gibbs (AGG) algorithm--to address the computational issues with large spatial data sets. The proposed algorithm dramatically reduces the computational complexity. We show theoretically that the proposed method can approximate the real posterior distribution accurately. The sufficient number of grid points for a required accuracy has also been derived. We compare the performance of AGG with that of the state-of-the-art methods in simulation studies. Finally, we apply AGG to spatially indexed data concerning building energy consumption."
      }
    ]
  },
  {
    "tid":"112",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: STR3: A path-optimal filtering algorithm for table constraints.\nContent: Constraint propagation is a key to the success of Constraint Programming (CP). The principle is that filtering algorithms associated with constraints are executed in sequence until quiescence is reached. Many such algorithms have been proposed over the years to enforce the property called Generalized Arc Consistency (GAC) on many types of constraints, including table constraints that are defined extensionally. Recent advances in GAC algorithms for extensional constraints rely on directly manipulating tables during search. This is the case with a simple approach called Simple Tabular Reduction (STR), which systematically maintains tables of constraints to their relevant lists of tuples. In particular, STR2, a refined STR variant is among the most efficient GAC algorithms for positive table constraints. In this paper, we revisit this approach by proposing a new GAC algorithm called STR3 that is specifically designed to enforce GAC during backtrack search. By indexing tables and reasoning from deleted values, we show that STR3 can avoid systematically iterating over the full set of current tuples, contrary to STR2. An important property of STR3 is that it can completely avoid unnecessary traversal of tables, making it optimal along any path of the search tree. We also study a variant of STR3, based on an optimal circular way for traversing tables, and discuss the relationship of STR3 with two other optimal GAC algorithms introduced in the literature, namely, GAC4 and AC5TC-Tr. Finally, we demonstrate experimentally how STR3 is competitive with the state-of-the-art. In particular, our extensive experiments show that STR3 is generally faster than STR2 when the average size of tables is not reduced too drastically during search, making STR3 complementary to STR2.\n\nTitle: An MDD-based generalized arc consistency algorithm for positive and negative table constraints and some global constraints\nContent: A table constraint is explicitly represented as its set of solutions or non-solutions. This ad hoc (or extensional) representation may require space exponential to the arity of the constraint, making enforcing GAC expensive. In this paper, we address the space and time inefficiencies simultaneously by presenting the mddc constraint. mddc is a global constraint that represents its (non-)solutions with a multi-valued decision diagram (MDD). The MDD-based representation has the advantage that it can be exponentially smaller than a table. The associated GAC algorithm (called mddc) has time complexity linear to the size of the MDD, and achieves full incrementality in constant time. In addition, we show how to convert a positive or negative table constraint into an mddc constraint in time linear to the size of the table. Our experiments on structured problems, car sequencing and still-life, show that mddc is also a fast GAC algorithm for some global constraints such as sequence and regular. We also show that mddc is faster than the state-of-the-art generic GAC algorithms in Gent et al. (2007), Lecoutre and Szymanek (2006), Lhomme and R\u00e9gin (2005) for table constraint.\n\nTitle: An optimal coarse-grained arc consistency algorithm\nContent: The use of constraint propagation is the main feature of any constraint solver. It is thus of prime importance to manage the propagation in an efficient and effec- tive fashion. There are two classes of propagation algorithms for general constraints: fine-grained algorithms where the removal of a value for a variable will be propagated to the corresponding values for other variables, and coarse-grained algorithms where the removal of a value will be propagated to the related variables. One big advantage of coarse-grained algorithms, like AC-3, over fine-grained algorithms, like AC-4, is the ease of integration when implementing an algorithm in a constraint solver. How- ever, fine-grained algorithms usually have optimal worst case time complexity while coarse-grained algorithms don't. For example, AC-3 is an algorithm with non-optimal worst case complexity although it is simple, efficient in practice, and widely used. In this paper we propose a coarse-grained algorithm, AC2001/3.1, that is worst case op- timal and preserves as much as possible the ease of its integration into a solver (no heavy data structure to be maintained during search). Experimental results show that AC2001/3.1 is competitive with the best fine-grained algorithms such as AC-6. The idea behind the new algorithm can immediately be applied to obtain a path consis- tency algorithm that has the best-known time and space complexity. The same idea is then extended to non-binary constraints.\n\nTitle: Maintaining Generalized Arc Consistency on Ad Hoc r-Ary Constraints\nContent: In many real-life problems, constraints are explicitly defined as a set of solutions. This ad hoc (table) representation uses exponential memory and makes support checking (for enforcing GAC) difficult. In this paper, we address both problems simultaneously by representing an ad hoc constraint with a multi-valued decision diagram (MDD), a memory efficient data structure that supports fast support search. We explain how to convert a table constraint into an MDD constraint and how to maintain GAC on the MDD constraint. Thanks to a sparse set data structure, our MDD-based GAC algorithm, mddc, achieves full incrementality in constant time. Our experiments on structured problems, car sequencing and still-life, show that mddcis a fast GAC algorithm for ad hoc constraints. It can replace a Boolean sequence constraint [1], and scales up well for structural MDD constraints with 208 variables and 340984 nodes. We also show why it is possible for mddcto be faster than the state-of-the-art generic GAC algorithms in [2,3,4]. Its efficiency on non-structural ad hoc constraints is justified empirically.\n\nTitle: Maintaining Generalized Arc Consistency on Ad-hoc n-ary Boolean Constraints\nContent: Binary decision diagrams (BDDs) can compactly rep-resent ad-hoc n-ary Boolean constraints. However, there is no gen-eralized arc consistency (GAC) algorithm which exploit BDDs. For example, the global case constraint by SICStus Prolog for ad-hoc constraints is designed for non-Boolean domains. In this paper, we introduce a new GAC algorithm, bddc, for BDD constraints. Our empirical results demonstrate the advantages of a new BDD-based global constraint --bddc is more efficient both in terms of mem-ory and time than the case constraint when dealing with ad-hoc Boolean constraints. This becomes important as the size of the ad-hoc constraints becomes large.\n\n## Task\nCompose an abstract for the title 'A Path-Optimal GAC Algorithm for Table Constraints.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Filtering by Generalized Arc Consistency (GAC) is a fundamental technique in Constraint Programming. Recent advances in GAC algorithms for extensional constraints rely on direct manipulation of tables during search. Simple Tabular Reduction (STR), which systematically removes invalid tuples from tables, has been shown to be a simple yet efficient approach. STR2, a refinement of STR, is considered to be among the best filtering algorithms for positive table constraints. In this paper, we introduce a new GAC algorithm called STR3 that is specifically designed to enforce GAC during search. STR3 can completely avoid unnecessary traversal of tables, making it optimal along any path of the search tree. Our experiments show that STR3 is much faster than STR2 when the average size of the tables is not reduced drastically during search."
      }
    ]
  },
  {
    "tid":"113",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A novel general framework for automatic and cost-effective handling of recoverable temporal violations in scientific workflow systems\nContent: Due to the complex nature of scientific workflow environments, temporal violations often take place and may severely reduce the timeliness of the execution's results. To handle temporal violations in an automatic and cost-effective fashion, two interdependent fundamental issues viz. the definition of fine-grained recoverable temporal violations and the design of light-weight effective exception handling strategies need to be resolved. However, most existing works study them separately without defining a comprehensive framework. To address such a problem, with the probability based temporal consistency model which defines the range of recoverable temporal violations, a novel general automatic and cost-effective exception handling framework is proposed in this paper where fine-grained temporal violations are defined based on the empirical function for the capability lower bounds of the exception handling strategies. To serve as a representative case study, a concrete example exception handling framework which consists of three levels of fine-grained temporal violations and their corresponding exception handling strategies is presented. The effectiveness of the example framework is evaluated by large scale simulation experiments conducted in the SwinDeW-G scientific grid workflow system. The experimental results demonstrate that the example framework can significantly reduce the overall average violation rates of local temporal constraints and global temporal constraints to 0.127% and 0.167% respectively.\n\nTitle: Trust-based robust scheduling and runtime adaptation of scientific workflow\nContent: Robustness and reliability with respect to the successful completion of a schedule are crucial requirements for scheduling in scientific workflow management systems because service providers are becoming autonomous. We introduce a model to incorporate trust, which indicates the probability that a service agent will comply with its commitments to improve the predictability and stability of the schedule. To deal with exceptions during the execution of a schedule, we adapt and evolve the schedule at runtime by interleaving the processes of evaluating, scheduling, executing and monitoring in the life cycle of the workflow management. Experiments show that schedules maximizing participants' trust are more likely to survive and succeed in open and dynamic environments. The results also prove that the proposed approach of workflow evaluation can find the most robust execution flow efficiently, thus avoiding the need of scheduling every possible execution path in the workflow definition. Copyright \u00a9 2009 John Wiley & Sons, Ltd.\n\nTitle: A probabilistic strategy for temporal constraint management in scientific workflow systems\nContent: In scientific workflow systems, it is critical to ensure the timely completion of scientific workflows. Therefore, temporal constraints as a type of QoS (Quality of Service) specification are usually required to be managed in scientific workflow systems. Specifically, temporal constraint management includes two basic tasks: setting temporal constraints at workflow build-time and updating temporal constraints at workflow run-time. For constraint setting, the current work mainly adopts user-specified temporal constraints without considering the system performance. Hence, it may result in frequent temporal violations which deteriorate the overall workflow execution effectiveness. As regards constraint updating, although not well investigated, so far is in fact of great importance to workflow management tasks such as workflow scheduling and exception handling. In this paper, with a systematic analysis of the above issues, we propose a probabilistic strategy for temporal constraint management which utilizes a novel probability-based temporal consistency model. Specifically for constraint setting, a negotiation process between the client and the service provider is designed to support the setting of coarse-grained temporal constraints and then automatically derive the fine-grained temporal constraints; for constraint updating, the probability time deficit/redundancy propagation process is proposed to update run-time fine-grained temporal constraints when workflow execution is either ahead of or behind the schedule. The effectiveness of our strategy is demonstrated through a case study on an example scientific workflow process in our scientific workflow system. Copyright \u00a9 2011 John Wiley & Sons, Ltd. (The initial work was published in the Proceedings of 6th International Conference on Business Process Management (BPM2008), Lecture Notes in Computer Science, vol. 5240, pp. 180\u2013195, September 2008 Milan, Italy.)\n\nTitle: A Reverse Order-Based QoS Constraint Correction Approach for Optimizing Execution Path for Service Composition\nContent: In service oriented computing systems, a business process can be exposed as a composite service which consists of a set of logically connected sub-services. For each service in the composition, many service providers can offer the same function but may different QoS. In the general service composition, when a user submits a request, overall QoS constraints called end-to-end QoS composition's requirements, for example, time should be less than one hour, and cost should be less than 60$, can be transmitted at the same time. As such, how to effectively coordinate individual QoS constraints for single service to achieve the best overall QoS benefits without violating such end-to-end QoS constraint requirements has been a critical issue. With an increasing number of abstract services in a service composition, the possibility of execution path by selecting different service providers for each abstract service blows up exponentially. Therefore, service selection problem for service composition is a computational-hard problem, which can be regarded as a Multiple choice Multiple dimension Knapsack Problem (MMKP) that has been proved np-hard [1, 2, 3]. Recently, a lot of approaches such as graph-based techniques[4], runtime adaptation-based techniques[5], Service Level Agreement(SLA), negotiation and auction based techniques[6], Integer Linear Programming (ILP) based techniques[7] have been proposed to resolve overall QoS constraints for optimizing execution path in a service composition. No matter what the merits and the importance current existing methods have, they rely on directly judging constraint conditions to detect multiple paths for picking out a critical execution path, which easily produces a high-time complexity and even an unsatisfactory result in comparison to the best path. As such, the issue on resolving overall QoS constraints to achieve an optimal execution path has not yet been well addressed.\n\nTitle: A clustering based coscheduling strategy for efficient scientific workflow execution in cloud computing.\nContent: Due to its advantages of cost-effectiveness, on-demand provisioning and easy for sharing, cloud computing has grown in popularity with the research community for deploying scientific applications such as workflows. Although such interests continue growing and scientific workflows are widely deployed in collaborative cloud environments that consist of a number of data centers, there is an urgent need for exploiting strategies which can place application datasets across globally distributed data centers and schedule tasks according to the data layout to reduce both latency and makespan for workflow execution. In this paper, by utilizing dependencies among datasets and tasks, we propose an efficient data and task coscheduling strategy that can place input datasets in a load balance way and meanwhile, group the mostly related datasets and tasks together. Moreover, data staging is used to overlap task execution with data transmission in order to shorten the start time of tasks. We build a simulation environment on Tianhe supercomputer for evaluating the proposed strategy and run simulations by random and realistic workflows. The results demonstrate that the proposed strategy can effectively improve scheduling performance while reducing the total volume of data transfer across data centers. Concurrency and Computation: Practice and Experience, 2013.(c) 2013 Wiley Periodicals, Inc.\n\n## Task\nCompose an abstract for the title 'A QoS-aware exception handling method in scientific workflow execution' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Scientific workflow is gaining ever-increasing attention as it integrates a wide range of heterogeneous services to enable and accelerate the scientific discovery processes. However, the exception occurred in a scientific workflow execution often decreases the robustness of a scientific workflow system. For example, if a service engaged in a scientific workflow schedule becomes unavailable during its execution, an exception might be raised and the whole workflow execution may be interrupted unexpectedly. In this situation, it is a challenge to smooth the interrupted workflow execution. In view of this challenge, an exception handling method, named RelaxingMe (constraints Relaxing Method, RelaxingMe) is proposed in this paper. This method aims at relaxing the original QoS constraint values requested by the interrupted task node, in order to find a near-to-optimal candidate service to replace the unavailable one. At last, the feasibility of our method is evaluated by an experiment. Copyright \u00a9 2011 John Wiley & Sons, Ltd."
      }
    ]
  },
  {
    "tid":"114",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Optimizing Touchscreen Keyboards for Gesture Typing\nContent: Despite its growing popularity, gesture typing suffers from a major problem not present in touch typing: gesture ambiguity on the Qwerty keyboard. By applying rigorous mathematical optimization methods, this paper systematically investigates the optimization space related to the accuracy, speed, and Qwerty similarity of a gesture typing keyboard. Our investigation shows that optimizing the layout for gesture clarity (a metric measuring how unique word gestures are on a keyboard) drastically improves the accuracy of gesture typing. Moreover, if we also accommodate gesture speed, or both gesture speed and Qwerty similarity, we can still reduce error rates by 52% and 37% over Qwerty, respectively. In addition to investigating the optimization space, this work contributes a set of optimized layouts such as GK-D and GK-T that can immediately benefit mobile device users.\n\nTitle: Multilingual Touchscreen Keyboard Design and Optimization.\nContent: A keyboard design, once adopted, tends to have a longlasting and worldwide impact on daily user experience. There is a substantial body of research on touch-screen stylus keyboard optimization. Most of it has focused on English only. Applying rigorous mathematical optimization methods and addressing diacritic character design issues, this article expands this body of work to French, Spanish, German, and Chinese. More important and counter to the intuition that optimization by nature is necessarily specific to each language, this article demonstrates that it is possible to find common layouts that are highly optimized across multiple languages for stylus (or single finger) typing. We first obtained a layout that is highly optimized for both English and French input. We then obtained a layout that is optimized for English, French, Spanish, German, and Chinese pinyin simultaneously, reducing its stylus travel distance to about half of QWERTY's for all of the five languages. In comparison to QWERTY's 3.31, 3.51, 3.7, 3.26, and 3.85 keys of movement for English, French, Spanish, German, and Chinese, respectively, the optimized multilingual layout has an average travel distance of 1.88, 1.86, 1.91, 1.77, and 1.68 keys, correspondingly. Applying Fitts's law with parameters validated by a word tapping experimeQWERTYnt, we show that these multilingual keyboards also significantly reduce text input time for multiple languages over the standard QWERTY for experienced users. In comparison to layouts individually optimized for each language, which are also obtained in this article, simultaneously optimizing for multiple languages caused only a minor performance degradation for each language. This surprising result could help to reduce the burden of multilingual users having to switch and learn new layouts for different languages. In addition, we also present and analyze multiple ways of incorporating diacritic characters on multilingual keyboards. Taken together, the present work provides a quantitative foundation for the understanding and designing of multilingual touch-screen keyboards.\n\nTitle: Optimised Virtual Keyboards With And Without Alphabetical Ordering - A Novice User Study\nContent: Optimised virtual keyboards are potentially efficient and fast for expert users, but they are difficult and slow for novice users due to the visual search requirement. Introducing structure and order to the layout may ease the visual searching process. By means of a Metropolis algorithm with an added alphabetical bias term to the Fitts-digraph energy function previously used (Zhai, Hunter, & Smith, 2000), we designed a keyboard layout with alphabetical ordering tendency with little movement efficiency cost. In contrast to previous efforts that failed to find any alphabetical ordering advantage, our experiment demonstrated that the alphabetical ordering created by our design method indeed improves novice users' performance and was preferred by most participants.\n\nTitle: Making touchscreen keyboards adaptive to keys, hand postures, and individuals: a hierarchical spatial backoff model approach\nContent: We propose a new approach for improving text entry accuracy on touchscreen keyboards by adapting the underlying spatial model to factors such as input hand postures, individuals, and target key positions. To combine these factors together, we introduce a hierarchical spatial backoff model (SBM) that consists of submodels with different levels of complexity. The most general model includes no adaptive factors, whereas the most specific model includes all three. Considering that in practice people may switch hand postures (e.g., from two-thumb to one-finger) to better suit a situation, and that the specific submodels may take time to train for each user, a specific submodel should be applied only if its corresponding input posture can be identified with confidence, and if the submodel has enough training data from the user. We introduce the backoff mechanism to fall back to a simpler model if either of these conditions are not met. We implemented a prototype system capable of reducing the language-model-independent error rate by 13.2% using an online posture classifier with 86.4% accuracy. Further improvements in error rate may be possible with even better posture classification.\n\nTitle: A Cost-Benefit Study of Text Entry Suggestion Interaction.\nContent: Mobile keyboards often present error corrections and word completions (suggestions) as candidates for anticipated user input. However, these suggestions are not cognitively free: they require users to attend, evaluate, and act upon them. To understand this trade-off between suggestion savings and interaction costs, we conducted a text transcription experiment that controlled interface assertiveness: the tendency for an interface to present itself. Suggestions were either always present (extraverted), never present (introverted), or gated by a probability threshold (ambiverted). Results showed that although increasing the assertiveness of suggestions reduced the number of keyboard actions to enter text and was subjectively preferred, the costs of attending to and using the suggestions impaired average time performance.\n\n## Task\nCompose an abstract for the title 'TNT: a numeric keypad based text input method' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"With the evolving functionality in television-based (TV-based) information and entertainment appliances, there is an increased need to enable users input text through remote control devices. We present a novel text input method, The Numpad Typer (TNT), for interactive TV, multimedia home terminals or other similar applications. Embodied in a TV remote control and guided by a visual map on the TV screen, TNT was designed for consistent spatial Stimuli-Response (S-R) compatibility and consistency of use. Five users tested TNT in ten sessions of 45-minutes. This initial investigation showed that users on average could type 9.3 and 17.7 correct words per minute with TNT doing the slowest and the fastest session respectively. The study also showed that the users found the TNT method easy to grasp and fun to use. Subjectively the participants felt they mastered the method rather quickly in comparison to their actual speed improvement."
      }
    ]
  },
  {
    "tid":"115",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Synthesis Of Low-Cost Parity-Based Partially Self-Checking Circuits\nContent: A methodology for the synthesis of partially self-checking multilevel logic circuits with low-cost parity-based concurrent error detection (CED) is described. A subset of the inputs of the circuit is selected to realize a simple characteristic function such that CED is disabled whenever the inputs belong to the OFF-set of the characteristic function. This don't-care space in the operation of the CED circuitry is used to optimize the CED circuitry during synthesis. It is shown that this methodology is very effective at targeting faults with a high sensitization probability. Experimental results show that the proposed approach, which is of special interest in applications where a low-cost CED solution is desired, achieves a significant reduction in the error rate in logic circuits.\n\nTitle: Cost-effective approach for reducing soft error failure rate in logic circuits\nContent: In this paper, a new paradigm for designing logic circuits with concurrent error detection (CED) is described. The key idea is to exploit the asymmetric soft error susceptibility of nodes in a logic circuit. Rather than target all modeled faults, CED is targeted towards the nodes that have the highest soft error susceptibility to achieve cost-effective tradeoffs between overhead and reduction in the soft error failure rate. Under this new paradigm, we present one particular approach that is based on partial duplication and show that it is capable of reducing the soft error failure rate significantly with a fraction of the overhead required for full duplication. A procedure for characterizing the soft error susceptibility of nodes in a logic circuit, and a heuristic procedure for selecting the set of nodes for partial duplication are described. A full set of experimental results demonstrate the cost-effective tradeoffs that can be achieved.\n\nTitle: Dominant critical gate identification for power and yield optimization in logic circuits\nContent: With increasing process variations, low-VT swapping is an effective technique that can be used to improve timing yield without having to modify a design following placement and routing. Gate criticality, defined as the probability that a gate lies on a critical path, forms the basis for existing low-VT swapping techniques. This paper presents a simulation-based study that challenges the effectiveness of low-VT swapping based on the conventional definition of gate criticality, especially as random process variations increase with technology scaling. We introduce dominant gate criticality to address the drawbacks of the conventional definition of gate criticality, and formulate dominant critical gate ranking in the presence of process variations as an optimization problem. Simulation results for 12 benchmark circuits from the ISCAS and OpenSPARC suites to achieve timing yields of 95% and 98% indicate that low-VT swapping based on dominant gate criticality reduces leakage power overhead by 61% and 42% for independent and correlated process variations, respectively, over low-VT swapping based on conventional gate criticality.\n\nTitle: A methodology for automated insertion of concurrent error detection hardware in synthesizable Verilog RTL\nContent: This paper describes a methodology for automated insertion of concurrent error detection (CED) circuitry in synthesizable Verilog RTL that allows easy tradeoff between overhead and coverage. The insertion is done at the front-end of the synthesis process which is highly advantageous. Computer-aided design (CAD) tools that support the proposed methodology have been implemented and are described. The CAD tools allow the designer to select from different CED schemes that provide various cost/coverage tradeoffs. The CED circuitry is then automatically inserted into the Verilog design. Experimental results are shown which demonstrate the broad range of design points that the proposed methodology offers the designer to choose from to satisfy cost/coverage requirements. The methodology can be seamlessly integrated into the typical design flow used in industry.\n\nTitle: Novel library of logic gates with ambipolar CNTFETs: opportunities for multi-level logic synthesis\nContent: This paper exploits the unique in-field controllability of the device polarity of ambipolar carbon nanotube field effect transistors (CNT-FETs) to design a technology library with higher expressive power than conventional CMOS libraries. Based on generalized NOR-NAND-AOI-OAI primitives, the proposed library of static ambipolar CNTFET gates efficiently implements XOR functions, provides full-swing outputs, and is extensible to alternate forms with area-performance tradeoffs. Since the design of the gates can be regularized, the ability to functionalize them in-field opens opportunities for novel regular fabrics based on ambipolar CNTFETs. Technology mapping of several multi-level logic benchmarks --- including multipliers, adders, and linear circuits --- indicates that on average, it is possible to reduce both the number of gates and area by ~ 38% while also improving performance by 6.9x.\n\n## Task\nCompose an abstract for the title 'Approximate logic circuits for low overhead, non-intrusive concurrent error detection' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper describes a scalable, technology-independent algorithm for the synthesis of approximate logic circuits. A low overhead, non-intrusive solution for concurrent error detection (CED) based on such circuits is described in this paper. CED based on approximate logic circuits does not impose any performance penalty on the original design. The proposed synthesis algorithm for approximate logic circuits scales with circuit size, and provides fine-grained trade-offs between area-power overhead and CED coverage."
      }
    ]
  },
  {
    "tid":"116",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Versatile and User-Oriented Versioning File System\nContent: File versioning is a useful technique for recording a history of changes. Applications of versioning include backups and disaster recovery, as well as monitoring intruders' activities. Alas, modern systems do not include an automatic and easy-to-use file versioning system. Existing backup solutions are slow and inflexible for users. Even worse, they often lack backups for the most recent day's activities. Online disk snapshotting systems offer more fine-grained versioning, but still do not record the most recent changes to files. Moreover, existing systems also do not give individual users the flexibility to control versioning policies.We designed a lightweight user-oriented versioning file system called Versionfs. Versionfs works with any file system and provides a host of user-configurable policies: versioning by users, groups, processes, or file names and extensions; version retention policies and version storage policies. Versionfs creates file versions automatically, transparently, and in a file-system portable manner-while maintaining Unix semantics. A set of user-level utilities allow administrators to configure and enforce default policies: users can set policies within configured boundaries, as well as view, control, and recover files and their versions. We have implemented the system on Linux. Our performance evaluation demonstrates overheads that are not noticeable by users under normal workloads.\n\nTitle: Discovery and hot replacement of replicated read-only file systems, with application to mobile computing\nContent: We describe a mechanism for replacing files, including open files, of a read-only file system while the file system remains mounted; the act of replacement is transparent to the user. Such a \"hot replacement\" mechanism can improve fault-tolerance, performance, or both. Our mechanism monitors, from the client side, the latency of operations directed at each file system. When latency degrades, the client automatically seeks a replacement file system that is equivalent to but hopefully faster than the current file system. The files in the replacement file system then take the place of those in the current file system. This work has particular relevance to mobile computers, which in some cases might move over a wide area. Wide area movement can be expected to lead to highly variable response time, and give rise to three sorts of problems: increased latency, increased failures, and decreased scalability. If a mobile client moves through regions having partial replicas of common file systems, then the mobile client can depend on our mechanism to provide increased fault tolerance and more uniform performance.\n\nTitle: FIST: a language for stackable file systems\nContent: Traditional file system development is difficult. Stackable file systems promise to ease the development of file systems by offering a mechanism for incremental development. Unfortunately, existing methods often require writing complex low-level kernel code that is specific to a single operating system platform and also difficult to port. We propose a new language, FiST, to describe stackable file systems. FiST uses operations common to file system interfaces. From a single description, FiST's compiler produces file system modules for multiple platforms. The generated code handles many kernel details, freeing developers to concentrate on the main issues of their file systems. This paper describes the design, implementation, and evaluation of FiST. We extended file system functionality in a portable way without changing existing kernels. We built several file systems using FiST on Solaris, FreeBSD, and Linux. Our experiences with these examples shows the following benefits of FiST: average code size over other stackable file systems is reduced ten \u00d7 average development time is reduced seven \u00d7 performance overhead of stacking is 1-2%.\n\nTitle: Extending ACID semantics to the file system\nContent: An organization's data is often its most valuable asset, but today's file systems provide few facilities to ensure its safety. Databases, on the other hand, have long provided transactions. Transactions are useful because they provide atomicity, consistency, isolation, and durability (ACID). Many applications could make use of these semantics, but databases have a wide variety of nonstandard interfaces. For example, applications like mail servers currently perform elaborate error handling to ensure atomicity and consistency, because it is easier than using a DBMS. A transaction-oriented programming model eliminates complex error-handling code because failed operations can simply be aborted without side effects. We have designed a file system that exports ACID transactions to user-level applications, while preserving the ubiquitous and convenient POSIX interface. In our prototype ACID file system, called Amino, updated applications can protect arbitrary sequences of system calls within a transaction. Unmodified applications operate without any changes, but each system call is transaction protected. We also built a recoverable memory library with support for nested transactions to allow applications to keep their in-memory data structures consistent with the file system. Our performance evaluation shows that ACID semantics can be added to applications with acceptable overheads. When Amino adds atomicity, consistency, and isolation functionality to an application, it performs close to Ext3. Amino achieves durability up to 46&percnt; faster than Ext3, thanks to improved locality.\n\nTitle: Overhauling Amd for the '00s: A Case Study of GNU Autotools\nContent: The GNU automatic software configuration tools, Autoconf, Automake, and Libtool, were designed to help the portability of software to multiple platforms. Such autotools also help improve the readability of code and speed up the development cycle of software packages. In this paper we quantify how helpful such autotools are to the open-source software development process. We study several large packages that use these autotools and measure the complexity of their code. We show that total code size is not an accurate measure of code complexity for portability; two better metrics are the distribution of CPP conditionals in that code and the number of new special-purpose Autoconf macros that are written for the package.We studied one package in detail-Am-utils, the Berkeley Automounter. As maintainers and developers of this package, we tracked its evolution over ten years. This package was ported to dozens of different platforms and in 1997 was converted to use GNU autotools. We show how this conversion (autotooling) resulted in a dramatic reduction in code size by over 33%. In addition, the conversion helped speed code development of the Am-utils package by allowing new features and ports to be integrated easily: for the first year after the conversion to GNU autotools, the Am-utils package grew by over 70% in size, adding many new features, and all without increasing the average code complexity.\n\n## Task\nCompose an abstract for the title 'Versatility and Unix semantics in namespace unification' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Administrators often prefer to keep related sets of files in different locations or media, as it is easier to maintain them separately. Users, however, prefer to see all files in one location for convenience. One solution that accommodates both needs is virtual namespace unification---providing a merged view of several directories without physically merging them. For example, namespace unification can merge the contents of several CD-ROM images without unpacking them, merge binary directories from different packages, merge views from several file servers, and more. Namespace unification can also enable snapshotting by marking some data sources read-only and then utilizing copy-on-write for the read-only sources. For example, an OS image may be contained on a read-only CD-ROM image---and the user's configuration, data, and programs could be stored in a separate read-write directory. With copy-on-write unification, the user need not be concerned about the two disparate file systems.It is difficult to maintain Unix semantics while offering a versatile namespace unification system. Past efforts to provide such unification often compromised on the set of features provided or Unix compatibility---resulting in an incomplete solution that users could not use.We designed and implemented a versatile namespace unification system called Unionfs. Unionfs maintains Unix semantics while offering advanced namespace unification features: dynamic insertion and removal of namespaces at any point in the merged view, mixing read-only and read-write components, efficient in-kernel duplicate elimination, NFS interoperability, and more. Since releasing our Linux implementation, it has been used by thousands of users and over a dozen Linux distributions, which helped us discover and solve many practical problems."
      }
    ]
  },
  {
    "tid":"117",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Storing auxiliary data for efficient maintenance and lineage tracing of complex views\nContent: As views in a data warehouse become more complex, the view maintenance process can become very complicated and potentially very inefficient. Storing auxiliary views in the warehouse can reduce the complexity and improve the efficiency of view maintenance, and the same auxiliary views can help in ef- ficiently answering lineage tracing queries over the warehouse views. In this paper, we study the problem of selecting auxiliary views to materialize in order to minimize the total view maintenance and lineage tracing cost. We consider relational views with arbitrary use of aggregation operators, and we define an initial search space for our optimization problem based on a normal form for such view definitions. We present several auxiliary view selection algorithms, and to study their performance we conduct experiments using the TPC-D benchmark in addition to synthetic view definitions and statistics. The results of our experiments show: (1) the exhaustive al-\n\nTitle: DataGuides: Enabling Query Formulation and Optimization in Semistructured Databases\nContent: In semistructured databases there is no schema fixed in advance. To provide the benefits of a schema in such environments, we introduce DataGuides: concise and accurate structural summaries of semistructured databases. DataGuides serve as dynamic schemas, generated from the database; they are useful for browsing database structure, formulating queries, storing information such as statistics and sample values, and enabling query optimization. This paper presents the theoretical foundations of DataGuides along with algorithms for their creation and incremental maintenance. We provide performance results based on our implementation of DataGuides in the Lore DBMS for semistructured data. We also describe the use of DataGuides in Lore, both in the user interface to enable structure browsing and query formulation, and as a means of guiding the query processor and optimizing query execution.\n\nTitle: Deriving Production Rules for Incremental View Maintenance\nContent: It is widely recognized that production rules in database systems can be used to automatically maintain de- rived data such as views. However, writing a correct set of rules for efficiently maintaining a given view can be a diffi- cult and ad-hoc process. We provide a facility whereby a user delines a view as an SQL select expression, from which the system automatically derives set-oriented production rules that maintain a materialization of that view. The maintenance rules are triggered by operations on the view's base tables. Generally, the rules perform incremental maintenance: the materialized view is modified according to the sets of changes made to the base tables, which are ac- cessible through logical tables provided by the rule language. However, for some operations substantial recomputation may be required. We give algorithms that, based on key infor- mation, perform syntactic analysis on a view definit,ion to determine when efficient maintenance is possible.\n\nTitle: Representing and Querying Changes in Semistructured Data\nContent: Semistructured data may be irregular and incomplete and does not necessarily conform to a fixed schema. As with structured data, it is often desirable to maintain a history of changes to data, and to query over both the data and the changes. Representing and querying changes in semistructured data is more difficult than in structured data due to the irregularity and lack of schema. We present a model for representing changes in semistructured data and a language for querying over these changes. An important feature of our approach is that we represent and query changes directly as annotations on the affected data, instead of indirectly as the difference between database states. We describe the implementation of our model and query language. We also describe the design and implementation of a query subscription service that permits users to subscribe to changes in semistructured information sources.\n\nTitle: A Syntax and Semantics for Set-Oriented Production Rules in Relational Database Systems (Extended Abstract)\nContent: We propose incorporating a production rules facility into a relational database system. Such a facility allows definition of database operations that are automatically executed whenever certain conditions are met. In keeping with the set-oriented approach of relational data manipulation languages, our production rules are also set-oriented\u2014they are triggered by sets of changes to the database and may consequently perform sets of changes. The condition and action parts of our production rules may refer to the current state of the database as well as to the sets of changes triggering the rules. We define a syntax for production rule definition as an extension to SQL. A model of system behavior is used to give an exact semantics for production rule execution, taking into account externally-generated operations, self-triggering rules, and simultaneous triggering of multiple rules.Due to space constraints, some details and discussion are omitted, and only a few examples are included. See [19] for a more extensive description.\n\n## Task\nCompose an abstract for the title 'Synthesizing view definitions from data' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Given a database instance and a corresponding view instance, we address the view definitions problem (VDP): Find the most succinct and accurate view definition, when the view query is restricted to a specific family of queries. We study the tradeoffs among succintness, level of approximation, and the family of queries through algorithms and complexity results. For each family of queries, we address three variants of the VDP: (1) Does there exist an exact view definition, and if so find it. (2) Find the best view definition, i.e., one as close to the input view instance as possible, and as succinct as possible. (3) Find an approximate view definition that satisfies an input approximation threshold, and is as succinct as possible."
      }
    ]
  },
  {
    "tid":"118",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: The Agent Reputation and Trust (ART) Testbed Architecture\nContent: The Agent Reputation and Trust (ART) Testbed initiative has been launched with the goal of establishing a testbed for agent reputation-and trust-related technologies. This testbed serves in two roles: (1) as a competition forum in which researchers can compare their technologies against objective metrics, and (2) as a suite of tools with flexible parameters, allowing researchers to perform customizable, easily-repeatable experiments. In the testbed's art appraisal domain, agents, who valuate paintings for clients, may gather opinions from other agents to produce accurate appraisals. This paper first gives a brief overview of the testbed domain problem to orient the reader to the game rules. A discussion of the ART Testbed implementation architecture is presented, explaining the functionality of the testbed's Game Server, Simulation Engine, Database, User Interfaces, and Agent Skeleton.\n\nTitle: The agent reputation and trust (ART) testbed\nContent: The Agent Reputation and Trust (ART) Testbed initiative has been launched with the goal of establishing a testbed for agent reputation- and trust-related technologies. The art Testbed serves in two roles: (1) as a competition forum in which researchers can compare their technologies against objective metrics, and (2) as a suite of tools with flexible parameters, allowing researchers to perform customizable, easily-repeatable experiments. In the Testbed's artwork appraisal domain, agents, who valuate paintings for clients, may purchase opinions and reputation information from other agents to produce accurate appraisals. The art Testbed features useful data collection tools for storing, downloading, and replaying game data for experimental analysis.\n\nTitle: A demonstration of the Agent Reputation and Trust (ART): testbed for experimentation and competition\nContent: The Agent Reputation and Trust (ART) Testbed [1] serves two roles: (1) as a competition forum for comparing technologies against objective metrics, and (2) as an environment for performing customizable, easily-repeatable experiments. In the testbed's art appraisal domain, agents valuate paintings for clients and gather opinions from other agents to produce accurate appraisals. In recent years, researchers [2--4] have recognized objective standards are necessary to justify successful trust modeling systems and provide a baseline for future work. For trust technologies to crossover into application [5,6], the public must obtain system evaluations based on transparent, recognizable standards. As a versatile, universal experimentation site, the ART Testbed scopes relevant trust research problems and unites researchers toward solutions via unified experimentation methods. Through objective, well-defined metrics, the testbed provides researchers with tools for comparing and validating their approaches. The testbed also serves as an objective means of presenting technology features---both advantages and disadvantages---to the community. In addition, the ART Testbed places trust research in the public spotlight, improving confidence in the technology and highlighting relevant applications.\n\nTitle: Belief Revision Process Based on Trust: Agents Evaluating Reputation of Information Sources\nContent: In this paper, we propose a multi-agent belief revision algorithm that utilizes knowledge about the reliability or trustworthiness (reputation) of information sources. Incorporating reliability information into belief revision mechanisms is essential for agents in real world multi-agent systems. This research assumes the global truth is not available to individual agents and agents only maintain a local subjective perspective, which often is different from the perspective of others. This assumption is true for many domains where the global truth is not available (or infeasible to acquire and maintain) and the cost of collecting and maintaining a centralized global perspective is prohibitive. As an agent builds its local perspective, the variance on the quality of the incoming information depends on the originating information sources. Modeling the quality of incoming information is useful regardless of the level and type of security in a given system. This paper introduces the definition of the trust as the agent's confidence in the ability and intention of an information source to deliver correct information and reputation as the amount of trust an information source has created for itself through interactions with other agents. This economical (or monetary) perspective of reputation, viewing reputation as an asset, serves as social law that mandates staying trustworthy to other agents. Algorithms (direct and indirect) maintaining the model of the reputations of other information sources are also introduced.\n\nTitle: A design foundation for a trust-modeling experimental testbed\nContent: Mechanisms for modeling trust and reputation to improve robustness and performance in multi-agent societies make up a growing field of research that has yet to establish unified direction or benchmarks. The trust research community will benefit significantly from the development of a competition testbed; such development is currently in progress under the direction of the Agent Reputation and Trust (art) Testbed initiative. A testbed can serve in two roles: 1) as a competition forum in which researchers can compare their technologies against objective metrics, and 2) as a suite of tools with flexible parameters, allowing researchers to perform easily-repeatable experiments. As a versatile, universal experimentation site, a competition testbed challenges researchers to solve the most prominent problems in the field, fosters a cohesive scoping of trust research problems, identifies successful technologies, and provides researchers with a tool for comparing and validating their approaches. In addition, a competition testbed places trust research in the public spotlight, improving confidence in the technology and highlighting relevant applications. This paper lays the foundation for testbed development by enumerating the important problems in trust and reputation research, describing important requirements for a competition testbed, and addressing necessary parameters for testbed modularity and flexibility. Finally, the art Testbed initiative is highlighted, and future progress toward testbed development is described.\n\n## Task\nCompose an abstract for the title 'A specification of the Agent Reputation and Trust (ART) testbed: experimentation and competition for trust in agent societies' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"A diverse collection of trust-modeling algorithms for multi-agent systems has been developed in recent years, resulting in significant breadth-wise growth without unified direction or benchmarks. Based on enthusiastic response from the agent trust community, the Agent Reputation and Trust (ART) Testbed initiative has been launched, charged with the task of establishing a testbed for agent trust- and reputation-related technologies. This testbed serves in two roles: (1) as a competition forum in which researchers can compare their technologies against objective metrics, and (2) as a suite of tools with flexible parameters, allowing researchers to perform customizable, easily-repeatable experiments. This paper first enumerates trust research objectives to be addressed in the testbed and desirable testbed characteristics, then presents a competition testbed specification that is justified according to these requirements. In the testbed's artwork appraisal domain, agents, who valuate paintings for clients, may gather opinions from other agents to produce accurate appraisals. The testbed's implementation architecture is discussed briefly, as well."
      }
    ]
  },
  {
    "tid":"119",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: The DataPath system: a data-centric analytic processing engine for large data warehouses\nContent: Since the 1970's, database systems have been \"compute-centric\". When a computation needs the data, it requests the data, and the data are pulled through the system. We believe that this is problematic for two reasons. First, requests for data naturally incur high latency as the data are pulled through the memory hierarchy, and second, it makes it difficult or impossible for multiple queries or operations that are interested in the same data to amortize the bandwidth and latency costs associated with their data access. In this paper, we describe a purely-push based, research prototype database system called DataPath. DataPath is \"data-centric\". In DataPath, queries do not request data. Instead, data are automatically pushed onto processors, where they are then processed by any interested computation. We show experimentally on a multi-terabyte benchmark that this basic design principle makes for a very lean and fast database system.\n\nTitle: New Sampling-Based Estimators for OLAP Queries\nContent: One important way in which sampling for approximate query processing in a database environment differs from traditional applications of sampling is that in a database, it is feasible to collect accurate summary statistics from the data in addition to the sample. This paper describes a set of sampling-based estimators for approximate query processing that make use of simple summary statistics to to greatly increase the accuracy of sampling-based estimators. Our estimators are able to give tight probabilistic guarantees on estimation accuracy. They are suitable for low or high dimensional data, and work with categorical or numerical attributes. Furthermore, the information used by our estimators can easily be gathered in a single pass, making them suitable for use in a streaming environment.\n\nTitle: Scalable approximate query processing with the DBO engine\nContent: This paper describes query processing in the DBO database sys- tem. Like other database systems designed for ad-hoc, analytic processing, DBO is able to compute the exact answer to queries over a large relational database in a scalable fashion. Unlike any other system designed for analytic processing, DBO can constantly maintain a guess as to the final answer to an aggregate query throughout execution, along with statistically meaningful bounds for the guess's accuracy. As DBO gathers more and more informa- tion, the guess gets more and more accurate, until it is 100% accu- rate as the query is completed. This allows users to stop the execution at any time that they are happy with the query accuracy, and encourages exploratory data analysis.\n\nTitle: The partitioned exponential file for database storage management\nContent: The rate of increase in hard disk storage capacity continues to outpace the rate of decrease in hard disk seek time. This trend implies that the value of a seek is increasing exponentially relative to the value of storage.With this trend in mind, we introduce the partitioned exponential file (PE file) which is a generic storage manager that can be customized for many different types of data (e.g., numerical, spatial, or temporal). The PE file is intended for use in environments with intense update loads and concurrent, analytic queries. Such an environment may be found, for example, in long-running scientific applications which can produce petabytes of data. For example, the proposed Large Synoptic Survey Telescope [36] will produce 50---100 petabytes of observational, scientific data over its multi-year lifetime. This database will never be taken off-line, so bursty update loads of tens of terabytes per day must be handled concurrently with data analysis. In the PE file, data are organized as a series of on-disk sorts with a careful, global organization. Because the PE file relies heavily on sequential I/O, only a fraction of a disk seek is required for a typical record insertion or retrieval.In addition to describing the PE file, we also detail a set of benchmarking experiments for T1SM, which is a PE file customized for use with multi-attribute data records ordered on a single numerical attribute. In our benchmarking, we implement and test many competing data organizations that can be used to index and store such data, such as the B+-Tree, the LSM-Tree, the Buffer Tree, the Stepped Merge Method, and the Y-Tree. As expected, no organization is the best over all benchmarks, but our experiments show that T1SM is the best choice in many situations, suggesting that it is the best overall. Specifically, T1SM performs exceptionally well in the case of a heavy query workload that must be handled concurrently with an intense insertion stream. Our experiments show that T1SM (and its close cousin, the T2SM storage manager for spatial data) can handle very heavy mixed workloads of this type, and still maintain acceptably small query latencies.\n\nTitle: Randomized algorithms for data reconciliation in wide area aggregate query processing\nContent: Many aspects of the data integration problem have been considered in the literature: how to match schemas across different data sources, how to decide when different records refer to the same entity, how to efficiently perform the required entity resolution in a batch fashion, and so on. However, what has largely been ignored is a way to efficiently deploy these existing methods in a realistic, distributed enterprise integration environment. The straightforward use of existing methods often requires that all data be shipped to a coordinator for cleaning, which is often unacceptable. We develop a set of randomized algorithms that allow efficient application of existing entity resolution methods to the answering of aggregate queries over data that have been distributed across multiple sites. Using our methods, it is possible to efficiently generate aggregate query results that account for duplicate and inconsistent values scattered across a federated system.\n\n## Task\nCompose an abstract for the title 'A Novel Index Supporting High Volume Data Warehouse Insertion' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"While the desire to support fast, ad hoc query processing for large data warehouses has motivated the recent introduction of many new indexing structures, with a few notable exceptions (namely, the LSM-Tree [4] and the Stepped Merge Method ill) little attention has been given to developing new indexing schemes that allow fast insertions. Since additions to a large warehouse may number in the millions per day, indices that require a disk seek (or even a significant fraction of a seek) per insertion are not acceptable.In this paper, we offer an alternative to the B+-tree called the Y-tree for indexing huge warehouses having frequent insertions. The Y-tree is a new indexing structure supporting both point and range queries over a single attribute, with retrieval performance comparable to the B+-tree. For processing insertions, however, the Y-tree may exhibit a speedup of 100 times over hatched insertions into a B+-tree."
      }
    ]
  },
  {
    "tid":"120",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Deliberation in Equilibrium: Bargaining in Computationally Complex Problems\nContent: We develop a normative theory of interaction\u2014 negotiation in particular\u2014among self-interested com- putationally limited agents where computational ac- tions are game-theoretically treated as part of an agent's strategy. We focus on a 2-agent setting where each agent has an intractable individual problem, and there is a potential gain from pooling the problems, giving rise to an intractable joint problem. At any time, an agent can compute to improve its solution to its prob- lem, its opponent's problem, or the joint problem. At a deadline the agents then decide whether to implement the joint solution, and if so, how to divide its value (or cost). We present a fully normative model for control- ling anytime algorithms where each agent has statis- tical performance profiles which are optimally condi- tioned on the problem instance as well as on the path of results of the algorithm run so far. Using this model, we analyze the perfect Bayesian equilibria of the games which differ based on whether the performance profiles are deterministic or stochastic, whether the deadline is known or not, and whether the proposer is known in advance. Finally, we present algorithms for finding the equilibria.\n\nTitle: An alternating offers bargaining model for computationally limited agents\nContent: An alternating offers bargaining model for computationally limited agents is presented. The gents compute to determine plans, but deadlines restrict them from determining an optimal solution. As the agents compute, they also negotiate over whether to perform a joint plan or whether to act independently and how, if implemented, the value of the joint plan would be divided. Their computing actions and bargaining actions are interrelated and both incorporated into each agent's strategy. We analyze the model for equilibrium strategies for agents under different conditions. It is shown that the equilibrium strategies for the alternating offers model where agents take turns making offers and counter-offers, even with its extremely large action space, are equivalent to those of a much simpler single shot, take--it--or--leave--it bargaining model. In particular, agents will compute and make no offers until the first agent's deadline.\n\nTitle: Experiments on Deliberation Equilibria in Auctions\nContent: Auctions are useful mechanisms for allocating items (goods, tasks, resources, etc.) in multiagent systems. The bulk of auction theory assumes that the bidders know their own valuations for items a priori. However, in many applications the bidders need to expend significant effort (computational or information gathering) to determine their valuations. This leads to the possibility of complex strategic behavior as agents may have incentive to not only use resources to determine their own valuations, but may also attempt to determine the valuations of competing bidders. It has been shown that given any commonly used auction protocol, it is theoretically possible to construct special instances such that this strategic deliberation occurs.We study the prevalence of strategic deliberating in order to determine whether it is merely of theoretical interest or if it is an issue which arises in practice. Using anytime algorithms and performance-profile-tree-based deliberation control in different real-world problem domains, and the deliberation equilibrium solution concept, we show that strategic deliberation does occur in practice whenever there is a certain amount of asymmetry between the agents.\n\nTitle: Learning When to Take Advice: A Statistical Test for Achieving A Correlated Equilibrium\nContent: We study a multiagent learning problem where agents can either learn via repeated interactions, or can follow the advice of a mediator who sug- gests possible actions to take. We present an al- gorithm that each agent can use so that, with high probability, they can verify whether or not the mediator's advice is useful. In particular, if the mediator's advice is useful then agents will reach a correlated equilibrium, but if the mediator's ad- vice is not useful, then agents are not harmed by using our test, and can fall back to their original learning algorithm. We then generalize our al- gorithm and show that in the limit it always cor- rectly verifies the mediator's advice.\n\nTitle: Network bargaining: using approximate blocking sets to stabilize unstable instances\nContent: We study a network extension to the Nash bargaining game, as introduced by Kleinberg and Tardos [6], where the set of players corresponds to vertices in a graph G=(V,E) and each edge ij\u2208E represents a possible deal between players i and j. We reformulate the problem as a cooperative game and study the following question: Given a game with an empty core (i.e. an unstable game) is it possible, through minimal changes in the underlying network, to stabilize the game? We show that by removing edges in the network that belong to a blocking set we can find a stable solution in polynomial time. This motivates the problem of finding small blocking sets. While it has been previously shown that finding the smallest blocking set is NP-hard [2], we show that it is possible to efficiently find approximate blocking sets in sparse graphs.\n\n## Task\nCompose an abstract for the title 'Bargaining with limited computation: deliberation equilibrium' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We develop a normative theory of interaction-negotiation in particular-among self-interested computationally limited agents where computational actions are game theoretically treated as part of an agent's strategy. We focus on a 2-agent setting where each agent has an intractable individual problem, and there is a potential gain from pooling the problems, giving rise to an intractable joint problem. At any time, an agent can compute to improve its solution to its own problem, its opponent's problem, or the joint problem. At a deadline the agents then decide whether to implement the joint solution, and if so, how to divide its value (or cost). We present a fully normative model for controlling anytime algorithms where each agent has statistical performance profiles which are optimally conditioned on the problem instance as well as on the path of results of the algorithm run so far. Using this model, we introduce a solution concept, which we call deliberation equilibrium. It is the perfect Bayesian equilibrium of the game where deliberation actions are part of each agent's strategy. The equilibria differ based on whether the performance profiles are deterministic or stochastic, whether the deadline is known or not, and whether the proposer is known in advance or not. We present algorithms for finding the equilibria. Finally, we show that there exist instances of the deliberation-bargaining problem where no pure strategy equilibria exist and also instances where the unique equilibrium outcome is not Pareto efficient. Copyright 2001 Elsevier Science B.V."
      }
    ]
  },
  {
    "tid":"121",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Boosting Saliency in Color Image Features\nContent: The aim of salient point detection is to find distinctive events in images. Salient features are generally determined from the local differential structure of images. They focus on the shape saliency of the local neighborhood. The majority of these detectors is luminance based which has the disadvantage that the distinctiveness of the local color information is completely ignored. To fully exploit the possibilities of color image salient point detection, color distinctiveness should be taken into account next to shape distinctiveness. In this paper color distinctiveness is explicitly incorporated into the design of saliency detection. The algorithm, called color saliency boosting, is based on an analysis of the statistics of color image derivatives. Isosalient color derivatives can be closely approximated by ellipsoidal surfaces in color derivative space. Based on this remarkable statistical finding, isosalient derivatives are transformed by color boosting to have equal impact on the saliency. Color saliency boosting is designed as a generic method easily adaptable to existing feature detectors. Results show that substantial improvements in information content are acquired by targeting color salient features. Further, the generality of the method is illustrated by applying color boosting to multiple existing saliency methods.\n\nTitle: Evaluation of Intensity and Color Corner Detectors for Affine Invariant Salient Regions\nContent: Global features are commonly used to describe the image content. The problem with this approach is that these features cannot capture all parts of the image having different characteristics. Therefore, local computation of image information is necessary. By using salient points to represent local information, more discriminative features can be computed. This research is based on an existing affine invariant local feature detector, in which the features are assumed to be intensity corners. First, the existing algorithm is extended with the intensity based SUSAN corner detector which fundamentally differs from the original Harris corner detector. Second, the algorithm is extended to incorporate color information into the detection process. This results in a comparison between three different detection algorithms: the intensity based algorithm using the Harris or SUSAN detector and a color based algorithm that uses two color extended Harris detectors. The different algorithms are compared in terms of invariance and distinctiveness of the regions and computational complexity.\n\nTitle: Selection and Fusion of Color Models for Feature Detection\nContent: The choice of a color space is of great importance for many computer vision algorithms (e.g. edge detection and object recognition). It induces the equivalence classes to the actual algorithms. However, the problem is how to automatically select the color space that produces the best result for a particular task. The subsequent difficulty then is how to obtain a proper weighting scheme for the algorithms so that the results are combined in an optimal setting. To achieve proper color space selection and fusion of feature detectors, in this paper, we propose a method that exploits non-perfect correlation between the color models derived from the principles of diversification. As a consequence, the weighting scheme yields maximal color discrimination. The method is verified experimentally for two different feature detectors. The experimental results show that the model provides feature detection results having a discriminative power of 30 percent higher than the standard weighting scheme.\n\nTitle: Sparse Color Interest Points for Image Retrieval and Object Categorization\nContent: Interest point detection is an important research area in the field of image processing and computer vision. In particular, image retrieval and object categorization heavily rely on interest point detection from which local image descriptors are computed for image matching. In general, interest points are based on luminance, and color has been largely ignored. However, the use of color increases the distinctiveness of interest points. The use of color may therefore provide selective search reducing the total number of interest points used for image matching. This paper proposes color interest points for sparse image representation. To reduce the sensitivity to varying imaging conditions, light-invariant interest points are introduced. Color statistics based on occurrence probability lead to color boosted points, which are obtained through saliency-based feature selection. Furthermore, a principal component analysis-based scale selection method is proposed, which gives a robust scale estimation per interest point. From large-scale experiments, it is shown that the proposed color interest point detector has higher repeatability than a luminance-based one. Furthermore, in the context of image retrieval, a reduced and predictable number of color features show an increase in performance compared to state-of-the-art interest points. Finally, in the context of object recognition, for the Pascal VOC 2007 challenge, our method gives comparable performance to state-of-the-art methods using only a small fraction of the features, reducing the computing time considerably.\n\nTitle: Corner detectors for affine invariant salient regions: is color important?\nContent: Recently, a lot of research has been done on the matching of images and their structures. Although the approaches are very different, most methods use some kind of point selection from which descriptors or a hierarchy are derived. We focus here on the methods that are related to the detection of points and regions that can be detected in an affine invariant way. Most of the previous research concentrated on intensity based methods. However, we show in this work that color information can make a significant contribution to feature detection and matching. Our color based detection algorithms detect the most distinctive features and the experiments suggest that to obtain optimal performance, a tradeoff should be made between invariance and distinctiveness by an appropriate weighting of the intensity and color information.\n\n## Task\nCompose an abstract for the title 'Boosting Color Saliency in Image Feature Detection' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The aim of salient feature detection is to find distinctive local events in images. Salient features are generally determined from the local differential structure of images. They focus on the shape-saliency of the local neighborhood. The majority of these detectors are luminance-based, which has the disadvantage that the distinctiveness of the local color information is completely ignored in determining salient image features. To fully exploit the possibilities of salient point detection in color images, color distinctiveness should be taken into account in addition to shape distinctiveness. In this paper, color distinctiveness is explicitly incorporated into the design of saliency detection. The algorithm, called color saliency boosting, is based on an analysis of the statistics of color image derivatives. Color saliency boosting is designed as a generic method easily adaptable to existing feature detectors. Results show that substantial improvements in information content are acquired by targeting color salient features."
      }
    ]
  },
  {
    "tid":"122",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Complexity Adaptive H.264 Encoding For Light Weight Streams\nContent: Emerging video coding standard H.264 achieves significant efficiency improvements, at the expense of greatly increased comnputational complexity at both the encoder and the decoder. Most prior works focus on reducing the encoder complexity only. In this paper, we develop a novel approach to reduce the decoder complexity without changing any implementation of standard-compliant decoders. Our solution, called complexity adaptive motion estimation and mode decision (CAMED), involves several core components, including a rigorous rate-distortion-complexity optimization framework, complexity cost modeling, and a complexity control algorithm. Such components are incorporated in the encoder side. Experiments over diverse videos and bit rates have shown that our method can achieve significant decoding complexity reduction (up to 60% of interpolation computation) with little degradation in video quality (<= 0.2dB) and very minor impact on the encoder complexity. Results also show that our method can be readily combined with other methods to reduce the complexity at both encoder and decoder.\n\nTitle: Video-server retrieval scheduling and resource reservation for variable bit rate scalable video\nContent: State-of-the-art digital video compression produces bursty, variable bit rate video. The bursty nature of compressed video raises challenges in the design of video servers. In this paper, we first present a method for the efficient retrieval of bursty video data from the disk system to the memory of a digital video server. For a single video stream, the proposed retrieval schedule minimizes the buffer requirement for continuous retrieval, given that a fixed disk bandwidth is reserved for the entire duration of retrieval. Secondly, we present an optimal resource-reservation algorithm for multiple video streams based on the proposed retrieval schedule. The resource-reservation algorithm maximizes the number of bursty video streams that ran be supported by a video server given any disk bandwidth and memory resource. Thirdly, we present a progressive display scheme for scalable video that is based on the retrieval schedule and resource-reservation algorithm. Performance evaluations based on simulations using MPEG-2 trace data are presented. For a personal computer with four disks and a memory resource of 120 MB, our approach can support 50%-275% more video streams than previously proposed approaches, depending on the pre-fetch delay that users are willing to tolerate in interactive viewing of videos\n\nTitle: An integrated approach for content-based video object segmentation and retrieval\nContent: Object-based video data representations enable unprecedented functionalities of content access and manipulation. We present an integrated approach using region-based analysis for semantic video object segmentation and retrieval. We first present an active system that combines low-level region segmentation with user inputs for defining and tracking semantic video objects. The proposed technique is novel in using an integrated feature fusion framework for tracking and segmentation at both region and object levels. Experimental results and extensive performance evaluation show excellent results compared to existing systems. Building upon the segmentation framework, we then present a unique region-based query system for semantic video object. The model facilitates powerful object search, such as spatio-temporal similarity searching at multiple levels\n\nTitle: Video Server Retrieval Scheduling for Variable Bit Rate Scalable Video\nContent: In advanced multimedia networks, video servers with real time operating systems and high performance storage architectures will deliver multiple concurrent video streams to clients. The video server has a limited disk I/O band width. This research presents a new framework for optimally utilizing the on-board memory to alleviate the disk bandwidth bottleneck. The optimal utilization of the memory resource is achieved by using an optimal retrieval schedule for video data across the disk-memory interface. The optimal retrieval schedule is directly applicable to video servers that are based on general storage architectures (e.g. RAID). Performance evaluation using real MPEG2 trace data verifies that this approach can provide large increases in the number of supported video streams.\n\nTitle: A new approach to decoding and compositing motion-compensated DCT-based images\nContent: A novel decoding algorithm for the MC-DCT (motion-compensation discrete-cosine-transform)-based video, which performs inverse MC before inverse DCT, is designed. This algorithm can be applied in compositing compressed video within the network, which may take multiple compressed video sources and combine them into a single compressed output stream. The proposed algorithm convers all MC-DCT compressed video into the DCT domain and performs compositing in the DCT domain. This DCT-domain approach can reduce the required computations with a speedup factor depending on the compression ratio and the nonzero motion vector percentage. However, dropping some least-significant DCT coefficients may be necessary for the worst case of high-motion video in real-time implementations. Some issues of networked video compositing are also discussed. Another direct application of the proposed decoding algorithm is converting MC-DCT compressed video to the DCT compressed format directly in the DCT domain.<>\n\n## Task\nCompose an abstract for the title 'Hybrid object-based/block-based coding in video compression at very low bit-rate' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, an algorithmic architecture of video coding at very low bit-rate is presented. This architecture incorporates block- and object-based coding algorithms into the same coder and manages them in a cooperative way: I and P frames are coded by block-based algorithm and B frames are coded by bi-directional object-based algorithm. An extra degree of freedom in video coding is introduced to achieve higher compression ratios while maintaining good visual quality. We give details of the object-based coding algorithm which is based on a source model of rigid 2-D objects with 3-D motions. Each moving object in B frame is coded with its contour and a pair of motion parameter sets (forward and backward motions). The bi-directional object-based motion parameters are obtained by a hierarchical estimation. The object image is interpolated using two neighboring pictures, which are block-based coded. ITU-T H.263 is chosen as the block-based coding scheme and the simulations are performed on MPEG-4 test sequences, which contain low-to-medium spatial detail and low amount of movement, but do not restrict to head_and_shoulder images. Experiment results show that the coder provides higher image quality with smooth and satisfactory motion. (C) 1997 Elsevier Science B.V."
      }
    ]
  },
  {
    "tid":"123",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Secure electronic payments in heterogeneous networking: new authentication protocols approach\nContent: Recent research efforts have been addressed towards maintain the heterogeneous networking transparent under the powerful all-IP concept. As example, current global standarization initiative specifies the 3G cellular system to wireless LAN inter-working. On the other hand, smart cards are presented as enough powerful devices capable to perform a strong authentication at lower layers of the protocol stack. Our work proposes a novel model reference and a scenario of applicability for secure electronic payment in this environment. Impact on the trust relations are assesed and a set of authentication requirements are provided. Finally, a new approach based on end-to-end layer 2 authentication protocols is adjusted to this proposal, considering the most interesting improvements in the authentication mechanisms applicable to this context.\n\nTitle: An efficient authentication protocol for RFID systems resistant to active attacks\nContent: RFID technology is a ubiquitous technology, and seems destined to become more a more ubiquitous. Traditional cryptographic primitives are not supported on low-cost RFID tags since, at most, 4K gates can be devoted to security-related tasks. Despite this, there are a vast number of proposals based on the use of classical hash functions, an assumption that is not realistic (at least at the present time). Furthermore, none of the published authentication protocols are resistant to active attacks. We try to address these two issues in this work by designing a new authentication protocol, secure against passive and active attacks, inspired by Shieh et al.'s protocol for smart-cards, but adapted to RFID systems. The original Shieh et al.'s scheme is considered one of the most secure an efficient protocols in the smart-card field. Because in this protocol tags should support a hash-function on-board, a new lightweight hash function, named Tav-128, is also proposed. A preliminary security analysis is shown, as well as a study on its hardware complexity, which concludes that its implementation is possible with around 2.6K gates.\n\nTitle: An Ultra Light Authentication Protocol Resistant to Passive Attacks under the Gen-2 Specification\nContent: Low-cost Radio Frequency Identification (RFID) tags are devices with very limited computational capability, in which only 250-4K logic gates can be devoted to security-related tasks. Classical cryptographic primitives such as block ciphers or hash functions are well beyond the computational capabilities of low-cost RFID tags, as ratified by the EPCglobal Class-1 Gen-2 RFID specification. Moreover, the Gen-2 RFID specification does not pay due attention to security. For this reason, an efficient Ultra Light Authentication Protocol (ULAP) is proposed in this paper. This new scheme offers an adequate security level against passive attacks, and is compliant with Gen-2 RFID specification.\n\nTitle: Towards Automated Design of Multi-party Rational Exchange Security Protocols\nContent: It is only recently that rational exchange schemes have been considered as an alternative solution to the exchange problem. A rational exchange protocol cannot provide fairness but it ensures that rational (i.e. self-interested) parties would have no reason to deviate from the protocol, as misbehaving is not beneficial. The common understanding of rationality as a security property has encouraged researchers to look for methods to formally analyze and categorize rational protocols. By contrast, in this paper, we adopt a completely new approach as we present an automated tool to design multi-party rational exchange security protocols. Given a specific set of initial parameters the tool performs an heuristic search in the space of exchanging protocols, producing a rational exchange scheme as a solution. As this is work in progress, we will present the first results obtained executing the application in a three entity environment and a total set of six exchanging items.\n\nTitle: EMAP: an efficient mutual-authentication protocol for low-cost RFID tags\nContent: RFID tags are devices of very limited computational capabil- ities, which only have 250-3K logic gates that can be devoted to security- related tasks. Many proposals have recently appeared, but all of them are based on RFID tags using classical cryptographic primitives such as PRNGs, hash functions, block ciphers, etc. We believe this assumption to be fairly unrealistic, as classical cryptographic constructions lie well beyond the computational reach of very low-cost RFID tags. A new ap- proach is necessary to tackle this problem, so we propose an extremely e-cient lightweight mutual-authentication protocol that ofiers an ade- quate security level for certain applications and can be implemented even in the most limited low-cost RFID tags, as it only needs around 150 gates.\n\n## Task\nCompose an abstract for the title 'Towards self-authenticable smart cards' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Traditionally, the smart cards have been seen as security devices, but as soon as they could be integrated into distributed and networked environments their vulnerabilities could be attempted and countermeasures against new security threats in an open-access internet were required. In this work, our target could be represented by an end-to-end mutual authentication scenario where the smart card could authenticate by itself to a Network Access Server by means of link layer protocols and therefore in absence of IP connectivity. Some previous related models based on the Extensible Authentication Protocol are presented. However, in these works the smart card and terminal implement jointly the supplicant functionality (split supplicant). We consider the native EAP multiplexing model specified by the IETF to propose a new approach in order to avoid this split and to achieve an autonomous and highly independent smart card in the authentication scheme: a self-authenticable smart card."
      }
    ]
  },
  {
    "tid":"124",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Towards Training-Free Refinement for Semantic Indexing of Visual Media.\nContent: Indexing of visual media based on content analysis has now moved beyond using individual concept detectors and there is now a focus on combining concepts or post-processing the outputs of individual concept detection. Due to the limitations and availability of training corpora which are usually sparsely and imprecisely labeled, training-based refinement methods for semantic indexing of visual media suffer in correctly capturing relationships between concepts, including co-occurrence and ontological relationships. In contrast to training-dependent methods which dominate this field, this paper presents a training-free refinement TFR algorithm for enhancing semantic indexing of visual media based purely on concept detection results, making the refinement of initial concept detections based on semantic enhancement, practical and flexible. This is achieved using global and temporal neighbourhood information inferred from the original concept detections in terms of weighted non-negative matrix factorization and neighbourhood-based graph propagation, respectively. Any available ontological concept relationships can also be integrated into this model as an additional source of external a priori knowledge. Experiments on two datasets demonstrate the efficacy of the proposed TFR solution.\n\nTitle: Semantically Smoothed Refinement for Everyday Concept Indexing.\nContent: Instead of occurring independently, semantic concepts pairs tend to co-occur within a single image and it is intuitive that concept detection accuracy for visual concepts can be enhanced if concept correlation can be leveraged in some way. In everyday concept detection for visual lifelogging using wearable cameras to automatically record everyday activities, the captured images usually have a diversity of concepts which challenges the performance of concept detection. In this paper a semantically smoothed refinement algorithm is proposed using concept correlations which exploit topic-related concept relationships, modeled externally in a user experiment rather than extracted from training data. Results for initial concept detection are factorized based on semantic smoothness and adjusted in compliance with the extracted concept correlations. Refinement performance is demonstrated in experiments to show the effectiveness of our algorithm and the extracted correlations.\n\nTitle: HFAG: Hierarchical Frame Affinity Group for video retrieval on very large video dataset\nContent: Content-based video retrieval systems are desired to fast and accurately find the nearest-neighbors of user input examples from very large video datasets. This poses a great challenge since exhaustive and redundant computation of similarities is required. Cluster based index approaches can be used to address this problem, but the similarity computation and clustering methods for videos are very time-consuming, thus preventing it from indexing very large video datasets. In this paper, we propose the Hierarchical Frame Affinity Group (HFAG), which is a hierarchy of frame clusters built using affinity propagation (AP) method, to represent video clusters. Our proposed video similarity metric and AP method guarantee the high performance of forming HFAG. We then build the cluster-based index structure to support retrieval of the nearest-neighbors of video sequences. The experiments on real large video datasets prove the effectiveness and efficiency of our approach.\n\nTitle: A Joint Matrix Factorization Approach to Unsupervised Action Categorization\nContent: In this paper, a novel unsupervised approach to mining categories from action video sequences is presented. This approach consists of two modules: action representation and learning model. Videos are regarded as spatially distributed dynamic pixel time series, which are quantized into pixel prototypes. After replacing the pixel time eries with their corresponding prototype labels, the video sequences are compressed into 2D action matrices. We put these matrices together to form an multi-action tensor, and propose the joint matrix factorization method to simultaneously cluster the pixel prototypes into pixel signatures, and matrices into action classes. The approach is tested on public and popular Weizmann data set, and promising results are achieved.\n\nTitle: Web video topic discovery and tracking via bipartite graph reinforcement model\nContent: Automatic topic discovery and tracking on web-shared videos can greatly benefit both web service providers and end users. Most of current solutions of topic detection and tracking were done on news and cannot be directly applied on web videos, because the semantic information of web videos is much less than that of news videos. In this paper, we propose a bipartite graph model to address this issue. The bipartite graph represents the correlation between web videos and their keywords, and automatic topic discovery is achieved through two steps - coarse topic filtering and fine topic re-ranking. First, a weight-updating co-clustering algorithm is employed to filter out topic candidates at a coarse level. Then the videos on each topic are re-ranked by analyzing the link structures of the corresponding bipartite graph. After the topics are discovered, the interesting ones can also be tracked over a period of time using the same bipartite graph model. The key is to propagate the relevant scores and keywords from the videos of interests to other relevant ones through the bipartite graph links. Experimental results on real web videos from YouKu, a YouTube counterpart in China, demonstrate the effectiveness of the proposed methods. We report very promising results.\n\n## Task\nCompose an abstract for the title 'Training-free indexing refinement for visual media via multi-semantics.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Indexing of visual media based on content analysis has now moved beyond using individual concept detectors and there is now a focus on combining concepts by post-processing the outputs of individual concept detection. Due to the limitations and availability of training corpora which are usually sparsely and imprecisely labeled with concept groundtruth, training-based refinement methods for semantic indexing of visual media suffer in correctly capturing relationships between concepts, including co-occurrence and ontological relationships. In contrast to training-dependent methods which dominate this field, this paper presents a training-free refinement (TFR) algorithm for enhancing semantic indexing of visual media based purely on concept detection results, making the refinement of initial concept detections based on semantic enhancement, practical and flexible. This is achieved using what can be called multi-semantics, factoring in semantics from multiple sources. In the case of this paper, global and temporal neighbourhood information inferred from the original concept detections in terms of weighted non-negative matrix factorization and neighbourhood-based graph propagation are both used in the refinement of semantics. Furthermore, any available ontological concept relationships among concepts can also be integrated into this model as an additional source of external a priori knowledge. Extended experiments on two heterogeneous datasets, images from wearable cameras and videos from TRECVid, demonstrate the efficacy of the proposed TFR solution."
      }
    ]
  },
  {
    "tid":"125",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Random or Genetic Algorithm Search for Object-Oriented Test Suite Generation?\nContent: Achieving high structural coverage is an important aim in software testing. Several search-based techniques have proved successful at automatically generating tests that achieve high coverage. However, despite the well- established arguments behind using evolutionary search algorithms (e.g., genetic algorithms) in preference to random search, it remains an open question whether the benefits can actually be observed in practice when generating unit test suites for object-oriented classes. In this paper, we report an empirical study on the effects of using a genetic algorithm (GA) to generate test suites over generating test suites incrementally with random search, by applying the EvoSuite unit test suite generator to 1,000 classes randomly selected from the SF110 corpus of open source projects. Surprisingly, the results show little difference between the coverage achieved by test suites generated with evolutionary search compared to those generated using random search. A detailed analysis reveals that the genetic algorithm covers more branches of the type where standard fitness functions provide guidance. In practice, however, we observed that the vast majority of branches in the analyzed projects provide no such guidance.\n\nTitle: Seeding strategies in search-based unit test generation.\nContent: Search-based techniques have been applied successfully to the task of generating unit tests for object-oriented software. However, as for any meta-heuristic search, the efficiency heavily depends on many factors; seeding, which refers to the use of previous related knowledge to help solve the testing problem at hand, is one such factor that may strongly influence this efficiency. This paper investigates different seeding strategies for unit test generation, in particular seeding of numerical and string constants derived statically and dynamically, seeding of type information and seeding of previously generated tests. To understand the effects of these seeding strategies, the results of a large empirical analysis carried out on a large collection of open-source projects from the SF110 corpus and the Apache Commons repository are reported. These experiments show with strong statistical confidence that, even for a testing tool already able to achieve high coverage, the use of appropriate seeding strategies can further improve performance. \u00a9 2016 The Authors. Software Testing, Verification and Reliability Published by John Wiley & Sons Ltd.\n\nTitle: Using coverage to automate and improve test purpose based testing\nContent: Test purposes have been presented as a solution to avoid the state space explosion when selecting test cases from formal models. Although such techniques work very well with regard to the speed of the test derivation, they leave the tester with one important task that influences the quality of the overall testing process: test purposes have to be formulated manually. In this paper, we present an approach that assists a test engineer with test purpose design in two ways: it allows automatic generation of coverage based test suites and can be used to automatically exercise those aspects of the system that are missed by hand-crafted test purposes. We consider coverage of Lotos specifications, and show how labeled transition systems derived from such specifications have to be extended in order to allow the application of logical coverage criteria to Lotos specifications. We then show how existing tools can be used to efficiently derive test cases and suggest how to use the coverage information to minimize test suites while generating them.\n\nTitle: Automated unit test generation during software development: a controlled experiment and think-aloud observations\nContent: Automated unit test generation tools can produce tests that are superior to manually written ones in terms of code coverage, but are these tests helpful to developers while they are writing code? A developer would first need to know when and how to apply such a tool, and would then need to understand the resulting tests in order to provide test oracles and to diagnose and fix any faults that the tests reveal. Considering all this, does automatically generating unit tests provide any benefit over simply writing unit tests manually? We empirically investigated the effects of using an automated unit test generation tool (EvoSuite) during development. A controlled experiment with 41 students shows that using EvoSuite leads to an average branch coverage increase of +13%, and 36% less time is spent on testing compared to writing unit tests manually. However, there is no clear effect on the quality of the implementations, as it depends on how the test generation tool and the generated tests are used. In-depth analysis, using five think-aloud observations with professional programmers, confirms the necessity to increase the usability of automated unit test generation tools, to integrate them better during software development, and to educate software developers on how to best use those tools.\n\nTitle: Do Automatically Generated Unit Tests Find Real Faults? An Empirical Study of Effectiveness and Challenges (T)\nContent: Rather than tediously writing unit tests manually, tools can be used to generate them automatically -- sometimes even resulting in higher code coverage than manual testing. But how good are these tests at actually finding faults? To answer this question, we applied three state-of-the-art unit test generation tools for Java (Randoop, EvoSuite, and Agitar) to the 357 real faults in the Defects4J dataset and investigated how well the generated test suites perform at detecting these faults. Although the automatically generated test suites detected 55.7% of the faults overall, only 19.9% of all the individual test suites detected a fault. By studying the effectiveness and problems of the individual tools and the tests they generate, we derive insights to support the development of automated unit test generators that achieve a higher fault detection rate. These insights include 1) improving the obtained code coverage so that faulty statements are executed in the first instance, 2) improving the propagation of faulty program states to an observable output, coupled with the generation of more sensitive assertions, and 3) improving the simulation of the execution environment to detect faults that are dependent on external factors such as date and time.\n\n## Task\nCompose an abstract for the title 'Testing container classes: random or systematic?' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Container classes such as lists, sets, or maps are elementary data structures common to many programming languages. Since they are a part of standard libraries, they are important to test, which led to research on advanced testing techniques targeting such containers and research on comparing testing techniques using such containers. However, these techniques have not been thoroughly compared to simpler techniques such as random testing. We present the results of a larger case study in which we compare random testing with shape abstraction, a systematic technique that showed the best results in a previous study. Our experiments show that random testing is about as effective as shape abstraction for testing these containers, which raises the question whether containers are well suited as a benchmark for comparing advanced testing techniques."
      }
    ]
  },
  {
    "tid":"126",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Nearly Tight Bounds for the Continuum-Armed Bandit Problem\nContent: In the multi-armed bandit problem, an online algorithm must choose from a set of strategies in a sequence of n trials so as to minimize the total cost of the chosen strategies. While nearly tight upper and lower bounds are known in the case when the strategy set is flnite, much less is known when there is an inflnite strategy set. Here we consider the case when the set of strategies is a subset ofRd, and the cost functions are continuous. In the d = 1 case, we improve on the best-known upper and lower bounds, closing the gap to a sublogarithmic factor. We also consider the case where d > 1 and the cost functions are convex, adapting a recent online convex optimization algorithm of Zinkevich to the sparser feedback model of the multi-armed bandit problem.\n\nTitle: Approximating low-dimensional coverage problems\nContent: We study the complexity of the maximum coverage problem, restricted to set systems of bounded VC-dimension. Our main result is a fixed-parameter tractable approximation scheme: an algorithm that outputs a (1-\u03b5)-approximation to the maximum-cardinality union of k sets, in running time $O(f(\u03b5,k,d)\u22c5 poly(n)) where n is the problem size, d is the VC-dimension of the set system, and f(\u03b5,k,d) is exponential in (kd/\u03b5)c for some constant c. We complement this positive result by showing that the function f(\u03b5,k,d) in the running-time bound cannot be replaced by a function depending only on (\u03b5,d) or on (k,d), under standard complexity assumptions. We also present an improved upper bound on the approximation ratio of the greedy algorithm in special cases of the problem, including when the sets have bounded cardinality and when they are two-dimensional halfspaces. Complementing these positive results, we show that when the sets are four-dimensional halfspaces neither the greedy algorithm nor local search is capable of improving the worst-case approximation ratio of 1-1/e that the greedy algorithm achieves on arbitrary instances of maximum coverage.\n\nTitle: Lexicographic Products and the Power of Non-linear Network Coding\nContent: We introduce a technique for establishing and amplifying gaps between parameters of network coding and index coding problems. The technique uses linear programs to establish separations between combinatorial and coding-theoretic parameters and applies hyper graph lexicographic products to amplify these separations. This entails combining the dual solutions of the lexicographic multiplicands and proving that this is a valid dual solution of the product. Our result is general enough to apply to a large family of linear programs. This blend of linear programs and lexicographic products gives a recipe for constructing hard instances in which the gap between combinatorial or coding-theoretic parameters is polynomially large. We find polynomial gaps in cases in which the largest previously known gaps were only small constant factors or entirely unknown. Most notably, we show a polynomial separation between linear and non-linear network coding rates. This involves exploiting a connection between matroids and index coding to establish a previously unknown separation between linear and non-linear index coding rates. We also construct index coding problems with a polynomial gap between the broadcast rate and the trivial lower bound for which no gap was previously known.\n\nTitle: (Almost) Tight bounds and existence theorems for single-commodity confluent flows\nContent: A flow of a commodity is said to be confluent if at any node all the flow of the commodity leaves along a single edge. In this article, we study single-commodity confluent flow problems, where we need to route given node demands to a single destination using a confluent flow. Single- and multi-commodity confluent flows arise in a variety of application areas, most notably in networking; in fact, most flows in the Internet are (multi-commodity) confluent flows since Internet routing is destination based. We present near-tight approximation algorithms, hardness results, and existence theorems for minimizing congestion in single-commodity confluent flows. The maximum edge congestion of a single-commodity confluent flow occurs at one of the incoming edges of the destination. Therefore, finding a minimum-congestion confluent flow is equivalent to the following problem: given a directed graph G with k sinks and non-negative demands on all the nodes of G, determine a confluent flow that routes every node demand to some sink such that the maximum congestion at a sink is minimized. The main result of this article is a polynomial-time algorithm for determining a confluent flow with congestion at most 1 &plus; ln(k) in G, if G admits a splittable flow with congestion at most 1. We complement this result in two directions. First, we present a graph G that admits a splittable flow with congestion at most 1, yet no confluent flow with congestion smaller than Hk, the kth harmonic number, thus establishing tight upper and lower bounds to within an additive constant less than 1. Second, we show that it is NP-hard to approximate the congestion of an optimal confluent flow to within a factor of (log2k)/2, thus resolving the polynomial-time approximability to within a multiplicative constant. We also consider a demand maximization version of the problem. We show that if G admits a splittable flow of congestion at most 1, then a variant of the congestion minimization algorithm yields a confluent flow in G with congestion at most 1 that satisfies 1/3 fraction of total demand. We show that the gap between confluent flows and splittable flows is much smaller, if the underlying graph is k-connected. In particular, we prove that k-connected graphs with k sinks admit confluent flows of congestion less than C &plus; dmax, where C is the congestion of the best splittable flow, and dmax is the maximum demand of any node in G. The proof of this existence theorem is non-constructive and relies on topological techniques introduced by Lov\u00e1sz.\n\nTitle: Approximation algorithms for the bottleneck asymmetric traveling salesman problem\nContent: We present the first nontrivial approximation algorithm for the bottleneck asymmetric traveling salesman problem. Given an asymmetric metric cost between n vertices, the problem is to find a Hamiltonian cycle that minimizes its bottleneck (or maximum-length edge) cost. We achieve an O(log n/ log log n) approximation performance guarantee by giving a novel algorithmic technique to shortcut Eulerian circuits while bounding the lengths of the shortcuts needed. This allows us to build on the recent result of Asadpour, Goemans, Madry, Oveis Gharan, and Saberi to obtain this guarantee. Furthermore, we show how our technique yields stronger approximation bounds in some cases, such as the bounded orientable genus case studied by Oveis Gharan and Saberi.\n\n## Task\nCompose an abstract for the title 'Broadcasting With Side Information: Bounding and Approximating the Broadcast Rate' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Index coding has received considerable attention recently motivated in part by applications such as fast video-on-demand and efficient communication in wireless networks and in part by its connection to network coding. Optimal encoding schemes and efficient heuristics were studied in various settings, while also leading to new results for network coding such as improved gaps between linear and non-linear capacity as well as hardness of approximation. The problem of broadcasting with side information, a generalization of the index coding problem, begins with a sender and sets of users and messages. Each user possesses a subset of the messages and desires an additional message from the set. The sender wishes to broadcast a message so that on receipt of the broadcast each user can compute her desired message. The fundamental parameter of interest is the broadcast rate, $\\\\beta $, the average communication cost for sufficiently long broadcasts. Though there have been many new nontrivial bounds on $\\\\beta $ by Bar-Yossef (2006), Lubetzky and Stav (2007), Alon (2008), and Blasiak (2011) there was no known polynomial-time algorithm for approximating $\\\\beta $ within a nontrivial factor, and the exact value of $\\\\beta $ remained unknown for all nontrivial instances. Using the information theoretic linear program introduced in Blasiak (2011), we give a polynomial-time algorithm for recognizing instances with $\\\\beta = 2$ and pinpoint $\\\\beta $ precisely for various classes of graphs (e.g., various Cayley graphs of cyclic groups). Further, extending ideas from Ramsey theory, we give a polynomial-time algorithm with a nontrivial approximation ratio for computing $\\\\beta $. Finally, we provide insight into the quality of previous bounds by giving constructions showing separations between $\\\\beta $ and the respective bounds. In particular, we construct graphs where $\\\\beta $ is uniformly bounded while its upper bound derived from the na\u00efve encoding scheme is polynomially worse."
      }
    ]
  },
  {
    "tid":"127",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Imitation learning with hierarchical actions\nContent: Imitation is a powerful mechanism for rapidly learning new skills through observation of a mentor. Developmental studies indicate that children often perform goal-based imitation rather than mimicking a mentor's actual action trajectories. Further, imitation, and human behavior in general, appear to be based on a hierarchy of actions, with higher-level actions composed of sequences of lower-level actions. In this paper, we propose a new model for goal-based imitation that exploits action hierarchies for fast learning of new skills. As in human imitation, learning relies only on sample trajectories of mentor states. Unlike apprenticeship or inverse reinforcement learning, the model does not require that mentor actions be given. We present results from a large-scale grid world task that is modeled after a puzzle box task used in developmental studies for investigating hierarchical imitation in children. We show that the proposed model rapidly learns to combine a given set of hierarchical actions to achieve the subgoals necessary to reach a desired goal state. Our results demonstrate that hierarchical imitation can yield significant speed-up in learning, especially in large state spaces, compared to learning without a mentor or without an action hierarchy.\n\nTitle: Accelerating imitation learning through crowdsourcing\nContent: Although imitation learning is a powerful technique for robot learning and knowledge acquisition from na\u0308\u0131ve human users, it often suffers from the need for expensive human demonstrations. In some cases the robot has an insufficient number of useful demonstrations, while in others its learning ability is limited by the number of users it directly interacts with. We propose an approach that overcomes these shortcomings by using crowdsourcing to collect a wider variety of examples from a large pool of human demonstrators online. We present a new goal-based imitation learning framework which utilizes crowdsourcing as a major source of human demonstration data. We demonstrate the effectiveness of our approach experimentally on a scenario where the robot learns to build 2D object models on a table from basic building blocks using knowledge gained from locals and online crowd workers. In addition, we show how the robot can use this knowledge to support human-robot collaboration tasks such as goal inference through object-part classification and missing-part prediction. We report results from a user study involving fourteen local demonstrators and hundreds of crowd workers on 16 different model building tasks.\n\nTitle: Dynamic Imitation in a Humanoid Robot through Nonparametric Probabilistic Inference\nContent: We tackle the problem of learning imitative whole- body motions in a humanoid robot using probabilistic inference in Bayesian networks. Our inference-based approach affords a straightforward method to exploit rich yet uncertain prior information obtained from human motion capture data. Dynamic imitation implies that the robot must interact with its environ- ment and account for forces such as gravity and inertia during imitation. Rather than explicitly modeling these forces and the body of the humanoid as in traditional approaches, we show that stable imitative motion can be achieved by learning a sensor- based representation of dynamic balance. Bayesian networks provide a sound theoretical framework for combining prior kinematic information (from observing a human demonstrator) with prior dynamic information (based on previous experience) to model and subsequently infer motions which, with high probability, will be dynamically stable. By posing the problem as one of inference in a Bayesian network, we show that methods developed for approximate inference can be leveraged to efficiently perform inference of actions. Additionally, by using nonparametric inference and a nonparametric (Gaussian process) forward model, our approach does not make any strong assump- tions about the physical environment or the mass and inertial properties of the humanoid robot. We propose an iterative, probabilistically constrained algorithm for exploring the space of motor commands and show that the algorithm can quickly discover dynamically stable actions for whole-body imitation of human motion. Experimental results based on simulation and subsequent execution by a HOAP-2 humanoid robot demonstrate that our algorithm is able to imitate a human performing actions such as squatting and a one-legged balance. I. INTRODUCTION Imitation learning presents a promising approach to the problem of enabling complex behavior learning in humanoid robots. Learning through imitation provides the robot with strong prior information by observing a skilled instructor (of- ten assumed to be a human demonstrator). This paper presents a model for exploiting this prior information about whole-body motions gathered from observing a human performance of the motion. Although the observation of the teacher is informative, there is a high degree of uncertainty in how the robot can and should imitate. Our model accounts for some of these sources of uncertainty including: noisy and missing kinematic estimates of the teacher, mapping ambiguities between the human and robot kinematic spaces, and lastly, the large m0 a1\n\nTitle: Learning Shared Latent Structure for Image Synthesis and Robotic Imitation\nContent: We propose an algorithm that uses Gaussian process regression to learn common hidden structure shared between corresponding sets of het- erogenous observations. The observation spaces are linked via a single, reduced-dimensionality latent variable space. We present results from two datasets demonstrating the algorithms's ability to synthesize novel data from learned correspondences. We first show that the method can be used to learn the nonlinear mapping between corresponding views of objects, filling in missing data as needed to synthesize novel views. We then show that the method can be used to acquire a mapping between hu- man degrees of freedom and robotic degrees of freedom for a humanoid robot, allowing robotic imitation of human poses from motion capture data.\n\nTitle: Imitation Learning Using Graphical Models\nContent: Imitation-based learning is a general mechanism for rapid acquisition of new behaviors in autonomous agents and robots. In this paper, we propose a new approach to learning by imitation based on parameter learning in probabilistic graphical models. Graphical models are used not only to model an agent's own dynamics but also the dynamics of an observed teacher. Parameter tying between the agent-teacher models ensures consistency and facilitates learning. Given only observations of the teacher's states, we use the expectation-maximization (EM) algorithm to learn both dynamics and policies within graphical models. We present results demonstrating that EM-based imitation learning outperforms pure exploration-based learning on a benchmark problem (the FlagWorld domain). We additionally show that the graphical model representation can be leveraged to incorporate domain knowledge (e.g., state space factoring) to achieve significant speed-up in learning.\n\n## Task\nCompose an abstract for the title 'Active Imitation Learning' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Imitation learning, also called learning by watching or programming by demonstration, has emerged as a means of accelerating many reinforcement learning tasks. Previous work has shown the value of imitation in domains where a single mentor demonstrates execution of a known optimal policy for the benefit of a learning agent. We consider the more general scenario of learning from mentors who are themselves agents seeking to maximize their own rewards. We propose a new algorithm based on the concept of transferable utility for ensuring that an observer agent can learn efficiently in the context of a selfish, not necessarily helpful, mentor. We also address the questions of when an imitative agent should request help from a mentor, and when the mentor can be expected to acknowledge a request for help. In analogy with other types of active learning, we call the proposed approach active imitation learning."
      }
    ]
  },
  {
    "tid":"128",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Multi-View and Multi-Objective Semi-Supervised Learning for HMM-Based Automatic Speech Recognition\nContent: Current hidden Markov acoustic modeling for large-vocabulary continuous speech recognition (LVCSR) heavily relies on the availability of abundant labeled transcriptions. Given that speech labeling is both expensive and time-consuming while there is a huge amount of unlabeled data easily available nowadays, the semi-supervised learning (SSL) from both labeled and unlabeled data aiming to reduce the development cost for LVCSR becomes more important than ever. In this paper, a new SSL approach is proposed which exploits the cross-view transfer learning for LVCSR through a committee machine consisting of multiple views learned from different acoustic features and randomized decision trees. In addition, a multi-objective learning scheme is developed in each view by maximizing a hybrid information-theoretic criterion which is established by the relative entropy between labeled data and their labels and the entropy of unlabeled data. The multi-objective scheme is then generalized to a unified SSL framework which can be interpreted into a variety of learning strategies under different weighting schemes. Experiments conducted on English Broadcast News using 50 hours of transcribed speech with 50 hours and 150 hours of untranscribed speech show the benefits of proposed approaches.\n\nTitle: Combined Discriminative Training For Multi-Stream Hmm-Based Audio-Visual Speech Recognition\nContent: In this paper we investigate discriminative training of models and feature space for a multi-stream hidden Markov model (HMM) based audio-visual speech recognizer (AVSR). Since the two streams are used together in decoding, we propose to train the parameters of the two streams jointly. This is in contrast to prior work which has considered discriminative training of parameters in each stream independent of the other. In experiments on a 20-speaker one-hour speaker independent test set, we obtain 22% relative gain on AVSR performance over A/V models whose parameters are trained separately. and 50% relative gain on AVSR over the baseline maximum-likelihood models. On a noisy (mismatched to training) test set, we obtain 21% relative gain over AN models whose parameters are trained separately. This represents 30% relative improvement over the maximum-likelihood baseline.\n\nTitle: Multi-view and multi-objective semi-supervised learning for large vocabulary continuous speech recognition.\nContent: Current hidden Markov acoustic modeling for large vocabulary continuous speech recognition (LVCSR) relies on the availability of abundant labeled transcriptions. Given that speech labeling is both expensive and time-consuming while there is a huge amount of unlabeled data easily available nowadays, semi-supervised learning (SSL) from both labeled and unlabeled data which aims to reduce the development cost for LVCSR becomes more important than ever. In this paper, we propose SSL for LVCSR by using the multiple views learned from different acoustic features and randomized decision trees. In addition, we develop the multi-objective learning of HMM-based acoustic models by optimizing a hybrid criterion which is established by the combination of the discriminative mutual information from labeled data and the entropy from unlabeled data. Experiments conducted on Broadcast News show the benefits of proposed methods.\n\nTitle: Deep Speaker Embedding Learning With Multi-Level Pooling For Text-Independent Speaker Verification\nContent: This paper aims to improve the widely used deep speaker embedding x-vector model. We propose the following improvements: ( 1) a hybrid neural network structure using both time delay neural network ( TDNN) and long short-term memory neural networks ( LSTM) to generate complementary speaker information at different levels; ( 2) a multi-level pooling strategy to collect speaker information from both TDNN and LSTM layers; ( 3) a regularization scheme on the speaker embedding extraction layer to make the extracted embeddings suitable for the following fusion step. The synergy of these improvements are shown on the NIST SRE 2016 eval test ( with a 19% EER reduction) and SRE 2018 dev test ( with a 9% EER reduction), as well as more than 10% DCF scores reduction on these two test sets over the x-vector baseline.\n\nTitle: Front-end feature transforms with context filtering for speaker adaptation\nContent: Feature-space transforms such as feature-space maximum likelihood linear regression (FMLLR) are very effective speaker adaptation technique, especially on mismatched test data. In this study, we extend the full-rank square matrix of FMLLR to a non-square matrix that uses neighboring feature vectors in estimating the adapted central feature vector. Through optimizing an appropriate objective function we aim to filter out and transform features through the correlation of the feature context. We compare to FMLLR that just con sider the current feature vector only. Our experiments are conducted on the automobile data with different speed conditions. Results show that context filtering improves 23% on word error rate over conventional FMLLR on noisy 60mph data with adapted ML model, and 7%/9% improvement over the discriminatively trained FMMI/BMMI models.\n\n## Task\nCompose an abstract for the title 'Rapid Feature Space Speaker Adaptation for Multi-Stream HMM-Based Audio-Visual Speech Recognition' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Multi-stream hidden Markov models (HMMs) have recently been very successful in audio-visual speech recognition, where the audio and visual streams are fused at the final decision level. In this paper we investigate fast feature space speaker adaptation using multi-stream HMMs for audio-visual speech recognition. In particular, we focus on studying the performance of feature-space maximum likelihood linear regression (fMLLR), a fast and effective method for estimating feature space transforms. Unlike the common speaker adaptation techniques of MAP or MLLR, fMLLR does not change the audio or visual HMM parameters, but simply applies a single transform to the testing features. We also address the problem of fast and robust on-line fMLLR adaptation using feature space maximum a posterior linear regression (fMAPLR). Adaptation experiments are reported on the IBM infrared headset audio-visual database. On average for a 20-speaker 1 hour independent test set, the multi-stream fMLLR achieves 31% relative gain on the clean audio condition, and 59% relative gain on the noisy audio condition (approximately 7 dB) as compared to the baseline multi-stream system"
      }
    ]
  },
  {
    "tid":"129",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Logic Programming Approach to the Integration, Repairing and Querying of Inconsistent Databases\nContent: In this paper we present a logic programming based framework for the integration of possibly inconsistent databases. In particular we consider the problemof 'merging' databases and, since the resulting 'merged' database may be inconsistent (with respect to the constraints defined on the input databases or with respect to additional constraints), we address the problemof managing inconsistent databases. We propose a general logic framework for computing repairs and consistent answers over inconsistent databases. The logic framework and the techniques for computing repairs and consistent answers proposed here are more general than previously proposed techniques. Indeed, our technique is sound and complete for universally quantified constraints whereas previous defined techniques only consider restricted cases.\n\nTitle: A Logic Programming Approach to Querying and Integrating P2P Deductive Databases\nContent: The paper proposes a logic framework for modeling the interaction among deductive databases and computing consistent answers to logic queries in a P2P environ- ment. As usual, data are exchanged among peers by using logical rules, called mapping rules. The novelty of our approach is that only data not violating integrity constraints are exchanged. The (declarative) semantics of a P2P system is deflned in terms of weak models. Under this semantics only facts not making the local databases inconsistent are imported, and the preferred weak models are those in which peers import maxi- mal sets of facts not violating integrity constraints. An equivalent and alternative characterization of preferred weak model semantics, in terms of prioritized logic pro- grams, is also introduced and the computational com- plexity of P2P logic queries is investigated.\n\nTitle: Approximate Probabilistic Query Answering over Inconsistent Databases\nContent: The problem of managing and querying inconsistent databases has been deeply investigated in the last few years. Most of the approaches proposed so far rely on the notion of repair(a minimal set of delete/insert operations making the database consistent) and consistent query answer(the answer to a query is given by considering the set of `repaired' databases). Since the problem of consistent query answering is hard in the general case, most of the proposed techniques have an exponential complexity, although for special classes of constraints and queries the problem becomes polynomial. A second problem with most of the proposed approaches is that repairs do not take into account update operations (they consider delete and insert operations only).This paper presents a general framework where constraints consist of functional dependencies and queries may be expressed by positive relational algebra. The framework allows us to compute certain (i.e. tuples derivable from all or from none of the repaired databases) and uncertain query answers (i.e. tuples derivable from a proper not empty subset of the repaired databases). Each tuple in the answer is associated with a probability, which depends on the number of repaired databases from which the tuple can be derived. In the proposed framework, databases are repaired by means of update operations and repaired databases are stored by means of a \"condensed\" database, so that all the repaired databases can be derived by \"expanding\" the unique condensed database. A condensed database can be rewritten into a probabilistic database where each tuple is associated with an event (i.e. a boolean formula) and, thus, a probability value. The probabilistic query answer can be computed by querying the so obtained probabilistic database. As the complexity of querying probabilistic databases is #P-complete, approximate probabilistic answers which are computable in polynomial time are considered.\n\nTitle: Probabilistic query answering over inconsistent databases\nContent: This paper presents a framework for querying inconsistent databases in the presence of functional dependencies. Most of the works dealing with the problem of extracting reliable information from inconsistent databases are based on the notion of repair, a minimal set of tuple insertions and deletions which leads the database to a consistent state (called repaired database), and the notion of consistent query answer, a query answer that can be obtained from every repaired database. In this work, both the notion of repair and query answer differ from the original ones. In the presence of functional dependencies, tuple deletions are the only operations that are performed in order to restore the consistency of an inconsistent database. However, deleting a tuple to remove an integrity violation potentially eliminates useful information in that tuple. In order to cope with this problem, we adopt a notion of repair, based on tuple updates, which allows us to better preserve information in the source database. A drawback of the notion of consistent query answer is that it does not allow us to discriminate among non-consistent answers, namely answers which can be obtained from a non-empty proper subset of the repaired databases. To obtain more informative query answers, we propose the notion of probabilistic query answer, that is query answers are tuples associated with probabilities. This new semantics of query answering over inconsistent databases allows us to give a measure of uncertainty to query answers. We show that the problem of computing probabilistic query answers is FP #P -complete. We also propose a technique for computing probabilistic answers to arbitrary relational algebra queries.\n\nTitle: Minimal founded semantics for disjunctive logic programs and deductive databases\nContent: In this paper, we propose a variant of stable model semantics for disjunctive logic programming and deductive databases. The semantics, called minimal founded, generalizes stable model semantics for normal (i.e. non-disjunctive) programs, but differs from disjunctive stable model semantics (the extension of stable model semantics for disjunctive programs). Compared with disjunctive stable model semantics, minimal founded semantics seems to be more intuitive, it gives meaning to programs which are meaningless under stable model semantics and is no harder to compute. More specifically, minimal founded semantics differs from stable model semantics only for disjunctive programs having constraint rules or rules working as constraints. We study the expressive power of the semantics, and show that for general disjunctive datalog programs it has the same power as disjunctive stable model semantics.\n\n## Task\nCompose an abstract for the title 'Programming with non-determinism in deductive databases' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"While non-determinism has long been established as a key concept in logic pro-gramming, its importance in the context of deductive databases was recognized only recently. This paper provides an overview of recent results on this topic with the aim of providing an introduction to the theory and practice of non-determinism in deductive databases. In particular we (i) recall the main results linking non-deterministic constructs in database languages to the theory of data complexity and the expressibility hierarchy of query languages; (ii) provide a reasoned introduction to effective programming with non-deterministic constructs; (iii) compare the usage of non-deterministic constructs in languages such as LDL++ to that of traditional logic programming languages; (iv) discuss the link between the semantics of logic programs with non-deterministic constructs and the stable-model semantics of logic programs with negation."
      }
    ]
  },
  {
    "tid":"130",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Automatic speech data clustering with human perception based weighted distance\nContent: Speech data from internet contain different speaking styles relating to information genre, emotions, sentiments, speaker characters, etc. Automatic classification of such data remains a challenging problem due to the difficulty in defining the categories to characterize different speaking styles clearly. To address the problem, this paper proposes a method based on x-means clustering, an extended version of k-means without fixed number of classes, for the task. Moreover, x-means method clusters the data according to a pre-defined distance measurement considering different features. Current methods on distance measuring only focus on features themselves while ignoring the impact of these features on human perception. To derive a more reasonable distance measurement, this paper also proposes a human perception based weighted distance to capture the contribution of different acoustic features on human perception. In this way, the automatic classification of internet speech data will make use of the prior knowledge of human perception as well as capture the speaking style characteristics in different datasets with varying categories. Experiments on listening test demonstrate that it is useful to introduce the human perception prior knowledge in distance measurement and our proposed method outperforms the baseline with conventional Euclidian distance with 10% improvement in classification accuracy.\n\nTitle: A new approach to fake finger detection based on skin elasticity analysis\nContent: This work introduces a new approach to fake finger detection, based on the analysis of human skin elasticity. When a user puts a finger on the scanner surface, a sequence of fingerprint images which describes the finger deformation process is captured. Then two features which represent the skin elasticity are extracted from the image sequence: 1) the correlation coefficient of the fingerprint area and the signal intensity; 2) the standard deviation of the fingerprint area extension in x and y axes. Finally the Fisher Linear Discriminant is used to discriminate the finger skin from other materials such as gelatin. The experiments carried out on a dataset of real and fake fingers show that the proposed approach and features are effective in fake finger detection.\n\nTitle: Acoustics, content and geo-information based sentiment prediction from large-scale networked voice data\nContent: Sentiment analysis from large-scale networked data attracts increasing attention in recent years. Most previous works on sentiment prediction mainly focus on text or image data. However, voice is the most natural and direct way to express people's sentiments in real-time. With the rapid development of smart phone voice dialogue applications (e.g., Siri and Sogou Voice Assistant), the large-scale networked voice data can help us better quantitatively understand the sentimental world we live in. In this paper, we study the problem of sentiment prediction from large-scale networked voice data. In particular, we first investigate the data observations and underlying sentiment patterns in human-mobile voice communication. Then we propose a deep sparse neural network (DSNN) model to incorporate acoustic features, content information and geo-information to automatically predict sentiments. The effectiveness of the proposed model is verified by the experiments on a real dataset from Sogou Voice Assistant application.\n\nTitle: Snr Estimation For Clipped Audio Based On Amplitude Distribution\nContent: Waveform clipping is a common problem in digital audio signal processing. Clipping changes the audio amplitude distribution and creates noise, which results in the decrease of audio signal-to-noise ratio (SNR). To estimate the degree of the clipping effect, we propose a method for SNR estimation of clipped signals base on the audio amplitude distribution. We first locate the clipping intervals of audio signals with high precision based on the differences between the amplitude probability density functions (PDFs) of normal and clipped audio signals. Then, by using the Gamma distribution, the clipped amplitude PDF is restored to estimate the non-clipped amplitude distribution. Finally, based on the restored PDF, the estimated SNR of distorted audio signals are calculated, which can indicate the degree of the clipping effect. Experiments on 3200 audios show that our approach can achieve a low error of 0.017 in the clipped values locating, and an error less than 0.5dB in the SNR estimation.\n\nTitle: Comparing feature dimension reduction algorithms for GMM-SVM based speech emotion recognition\nContent: How to select effective emotional features are important for improving the performance of automatic speech emotion recognition. Although various feature dimension reduction algorithms were put forward that could help gain the accuracy rate of emotion distinction, but most of them exist various defects, such as high negative impact of the recognition rate, high computational complexity. Regarding this, two dimension reduction algorithms based on PCA (principal component analysis) and KPCA (Kernel-PCA) were comparatively discussed in this paper. The original features extracted from databases were transformed by PCA or KPCA. The weights of these new features over the transforming matrix were calculated and ranked, based on which features were chosen. Experimental results show that feature dimension reduction can make principal contribution to the accuracy of speech emotion recognition, and KPCA slightly outperforms PCA on the hit rate and the remaining dimensions.\n\n## Task\nCompose an abstract for the title 'Distance measurement for tower crane obstacle based on multi-ultrasonic sensors' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Aiming at the anti-collision monitoring of tower crane, the models of state equation and observation equation for obstacle are established, and distance measurement method based on ultrasonic sensor for tower crane obstacle is proposed. According to the multi-sensor information fusion theory and obstacle kinematic model, Kalman filter principle is used to respectively deal with single ultrasonic sensor information and multi-ultrasonic sensor information fusion, then those simulation and experiments results are analyzed and contrasted. The results show that multi-sensor distance measurement is more accurate and stable than single sensor distance measurement, and verify the feasibility and superiority of multi-sensor information fusion theory in tower crane anti-collision applications."
      }
    ]
  },
  {
    "tid":"131",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Demarcation of bacterial ecotypes from DNA sequence data: A comparative analysis of four algorithms\nContent: Identification of closely related, ecologically distinct populations of bacteria would benefit microbiologists working in many fields including systematics, epidemiology, and biotechnology. Several laboratories have recently developed algorithms aimed at demarcating such \"ecotypes.\" In this paper we examine the ability of four of these algorithms to correctly identify ecotypes from sequence data (along with, in the case of one algorithm, information on the habitats where organisms were isolated). We test the algorithms on synthetic sequences, with known history and habitat associations, generated under the Stable Ecotype model [1], and on data from Bacillus strains isolated from Death Valley where previous work [2] has confirmed the existence of multiple ecotypes. We find that one of the algorithms (Ecotype Simulation) performs significantly better than the others (AdaptML, GMYC, BAPS) in both instances. Unfortunately, it is also shown to be the least efficient of the four.\n\nTitle: Estimating bacterial diversity from environmental DNA: a maximum likelihood approach\nContent: The ability to measure bacterial diversity is a prerequisite for the systematic study of bacterial biogeography and ecology. In this paper we describe a method of estimating diversity from an environmental sample of DNA and apply it to data taken from samples from the Sargasso Sea. Our approach combines the coverage depth method of Venter et al. [2] and the contig spectrum approach of Angly et al. [4], but uses maximum likelihood to recover the diversity rather than using hand-fit models as in [2]. We assume four species abundance distributions, then maximize the likelihood of fitting the coverage depth at different positions of the consensus sequence provided in the Sargasso Sea sample. The resulting estimates match well with those obtained using less mathematically rigorous approaches.\n\nTitle: Optimal Online and Offline Algorithms for Robot-Assisted Restoration of Barrier Coverage.\nContent: Cooperation between mobile robots and wireless sensor networks is a line of research that is currently attracting a lot of attention. In this context, we study the following problem of barrier coverage by stationary wireless sensors that are assisted by a mobile robot with the capacity to move sensors. Assume that n sensors are initially arbitrarily distributed on a line segment barrier. Each sensor is said to cover the portion of the barrier that intersects with its sensing area. Owing to incorrect initial position, or the death of some of the sensors, the barrier is not completely covered by the sensors. We employ a mobile robot to move the sensors to final positions on the barrier such that barrier coverage is guaranteed. We seek algorithms that minimize the length of the robot's trajectory, since this allows the restoration of barrier coverage as soon as possible. We give an optimal linear-time offline algorithm that gives a minimum-length trajectory for a robot that starts at one end of the barrier and achieves the restoration of barrier coverage. We also study two different online models: one in which the online robot does not know the length of the barrier in advance, and the other in which the online robot knows it in advance. For the case when the online robot does not know the length of the barrier, we prove a tight bound of 3/2 on the competitive ratio, and we give a tight lower bound of 5/4 on the competitive ratio in the other case. Thus for each case we give an optimal online algorithm.\n\nTitle: Efficient Update Strategies for Geometric Computing with Uncertainty\nContent: We consider the problems of computing maximal points and the convex hull of a set of points in two dimensions, when the points are \u201cin motion.\u201d We assume that the point locations (or trajectories) are not known precisely and determining these values exactly is feasible, but expensive. In our model the algorithm only knows areas within which each of the input points lie, and is required to identify the maximal points or points on the convex hull correctly by updating some points (i.e., determining their location exactly). We compare the number of points updated by the algorithm on a given instance to the minimum number of points that must be updated by a nondeterministic strategy in order to compute the answer provably correctly. We give algorithms for both of the above problems that always update at most three times as many points as the nondeterministic strategy, and show that this is the best possible. Our model is similar to that in [3] and [5].\n\nTitle: New graph decompositions and fast emulations in hypercubes and butterflies\nContent: In this paper, we present a new type of graph decomposition called a cut-cover that combines the notions of graph separators and t-neighborhood covers. We show that graphs with good cut-covers can be emulated in hypercubes and butterflies and we show that planar and certain minor-excluded graphs have good cut-covers. In particular, we show how to emulate any N-node bounded degree planar network or any N-node bounded degree graph that excludes KIOgOOl ~ as a minor with constant slowdown on hypercube networks. We also show how to emulate any N-node bounded degree planar network or any IV-node bounded degree graph that excludes Ko(l) as a minor with O(log* N) slowdown on butterfly networks.\n\n## Task\nCompose an abstract for the title 'Accuracy and efficiency of algorithms for the demarcation of bacterial ecotypes from DNA sequence data' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Identification of closely related, ecologically distinct populations of bacteria would benefit microbiologists working in many fields including systematics, epidemiology and biotechnology. Several laboratories have recently developed algorithms aimed at demarcating such 'ecotypes'. We examine the ability of four of these algorithms to correctly identify ecotypes from sequence data. We tested the algorithms on synthetic sequences, with known history and habitat associations, generated under the stable ecotype model and on data from Bacillus strains isolated from Death Valley where previous work has confirmed the existence of multiple ecotypes. We found that one of the algorithms ecotype simulation performs significantly better than the others AdaptML, GMYC, BAPS in both instances. Unfortunately, it was also shown to be the least efficient of the four. While ecotype simulation is the most accurate, it is by a large margin the slowest of the algorithms tested. Attempts at improving its efficiency are underway."
      }
    ]
  },
  {
    "tid":"132",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Architecting fault tolerance with exception handling: verification and validation\nContent: When building dependable systems by integrating untrusted software components that were not originally designed to interact with each other, it is likely the occurrence of architectural mismatches related to assumptions in their failure behaviour. These mismatches, if not prevented during system design, have to be tolerated during runtime. This paper presents an architectural abstraction based on exception handling for structuring fault-tolerant software systems. This abstraction comprises several components and connectors that promote an existing untrusted software element into an idealised fault-tolerant architectural element. Moreover, it is considered in the context of a rigorous software development approach based on formal methods for representing the structure and behaviour of the software architecture. The proposed approach relies on a formal specification and verification for analysing exception propagation, and verifying important dependability properties, such as deadlock freedom, and scenarios of architectural reconfiguration. The formal models are automatically generated using model transformation from UML diagrams: component diagram representing the system structure, and sequence diagrams representing the system behaviour. Finally, the formal models are also used for generating unit and integration test cases that are used for assessing the correctness of the source code. The feasibility of the proposed architectural approach was evaluated on an embedded critical case study.\n\nTitle: Development of Fault-Tolerant Software Systems Based on Architectural Abstractions\nContent: The incorporation of fault tolerance into systems normally increases their complexity, which consequently makes their analysis more difficult. This paper discusses how architectural abstractions can be effective in developing fault-tolerant software systems. Depending on the fault model and the resources available, different abstractions can be employed for representing issues that are related to fault tolerance, such as error detection, and error and fault handling. These architectural abstractions, and their internal views, can be instantiated into concrete components and connectors for designing fault-tolerant software architectures. Since structural and behavioural properties associated with these abstractions are formally specified, the process of verifying and validating software architectures can be automated. In this paper, we show how appropriate architectural abstractions and a recursive process can facilitate the architectural modelling and analysis of fault-tolerant software systems. The feasibility of the proposed approach is demonstrated in the context of a critical real-time application.\n\nTitle: A fault-tolerant software architecture for component-based systems\nContent: Component-based software built from reusable software components is being used in a wide range of applications that have high dependability requirements. In order to achieve the required levels of dependability, it is necessary to incorporate into these complex systems means for coping with software faults. However, the problem is exacerbated if we consider the current trend of integrating off-the-shelf software components, from independent sources, which allow neither code inspection nor changes. To leverage the dependability properties of these systems, we need solutions at the architectural level that are able to guide the structuring of unreliable components into a fault-tolerant architecture. In this paper, we present an approach for structuring fault-tolerant component-based systems based on the C2 architectural style.\n\nTitle: Verification of Exception Control Flows and Handlers Based on Architectural Scenarios\nContent: The use of exception handling mechanisms to develop robust software systems in a non-systematic manner can be a source of many design faults. This paper presents a rigorous development approach based on formal methods, which allows to systematise the verification of the system's abnormal behaviour at the architectural level. Our solution is based on the specification and verification of architectural scenarios, which describe both exception control flows and exception handlers involving architectural elements (components and connectors). We also adopt an architectural abstraction for guiding the internal structure of the architectural elements. The verification process is conducted using the ProB model checker, which combines the use of set-theory (B-Method) and a process algebra (CSP). The feasibility of our approach was evaluated by a case study from the financial domain.\n\nTitle: Software architectures for dependable systems: a software engineering perspective\nContent: Although there is a large body of research in dependability, architectural level reasoning about dependability is only just emerging as an important theme in software development. This is due to the fact that dependability concerns are often left until too late in the process of development. In addition, the complexity of emerging applications and the trend of building trustworthy systems from existing untrustworthy components are urging dependability concerns to be considered at the architectural level. This tutorial will present the current challenges and promising solutions for structuring dependable systems at the architectural level. In addition of providing basic concepts related to dependability and software architectures, the rest of the tutorial is presented in the context of the dependability technologies. Throughout the tutorial, case studies will be used to exemplify the key concepts.\n\n## Task\nCompose an abstract for the title 'Verifying Architectural Variabilities In Software Fault Tolerance Techniques' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper considers the representation of different software fault tolerance techniques as a product line architecture (PLA) for promoting the muse of software artifact. The proposed PLA enables to specify a series of closely related architectural applications, which is obtained by identifying variation points associated with design decisions regarding software fault tolerance. These decisions are used to choose the appropriate technique depending on the features selected, e.g, the number of redundant resources, or the type of adjudicator. The proposed approach also comprises the formalisation of the PLA, using B-Method and CSP, for systematising the verification of fault-tolerant software systems at the architectural level. The properties verified cover two complementary contexts: the selection of the correct architectural variabilities for instantiating the PLA, and also the properties of the chosen fault tolerance techniques."
      }
    ]
  },
  {
    "tid":"133",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Scalable fault tolerant protocol for parallel runtime environments\nContent: The number of processors embedded on high performance computing platforms is growing daily to satisfy users desire for solving larger and more complex problems. Parallel runtime environments have to support and adapt to the underlying libraries and hardware which require a high degree of scalability in dynamic environments. This paper presents the design of a scalable and fault tolerant protocol for supporting parallel runtime environment communications. The protocol is designed to support transmission of messages across multiple nodes with in a self-healing topology to protect against recursive node and process failures. A formal protocol verification has validated the protocol for both the normal and failure cases. We have implemented multiple routing algorithms for the protocol and concluded that the variant rule-based routing algorithm yields the best overall results for damaged and incomplete topologies .\n\nTitle: Reliability Analysis of Self-Healing Network using Discrete-Event Simulation\nContent: The number of processors embedded on high performance computing platforms is continuously increasing to accommodate user desire to solve larger and more complex problems. However, as the number of components increases, so does the probability of failure. Thus, both scalable and fault-tolerance of software are important issues in this field. To ensure reliability of the software especially under the failure circumstance, the reliability analysis is needed. The discrete-event simulation technique offers an attractive a ternative to traditional Markovian-based analytical models, which often have an intractably large state space. In this paper, we analyze reliability of a self-healing network developed for parallel runtime environments using discrete-event simulation. The network is designed to support transmission of messages across multiple nodes and at the same time, to protect against node and process failures. Results demonstrate the flexibility of a discrete-event simulation approach for studying the network behavior under failure conditions and various protocol parameters, message types, and routing algorithms.\n\nTitle: Constructing Resiliant Communication Infrastructure for Runtime Environments.\nContent: High performance computing platforms are becoming larger, leading to scalability and fault-tolerance issues for both applications and runtime environments (RTE) dedicated to run on such machines. After being deployed, usually following a spanning tree, a RTE needs to build its own communication infrastructure to manage and monitor the tasks of parallel applications. Previous works have demonstrated that the Binomial Graph topology (BMG) is a good candidate as a communication infrastructure for supporting scalable and fault-tolerant RTE. In this paper, we present and analyze a self-stabilizing algorithm to transform the underlying communication infrastructure provided by the launching service into a BMG, and maintain it in spite of failures. We demonstrate that this algorithm is scalable, tolerates transient failures, and adapts itself to topology changes.\n\nTitle: Scalable fault tolerant MPI: extending the recovery algorithm\nContent: Fault Tolerant MPI (FT-MPI) [6] was designed as a solution to allow applications different methods to handle process failures beyond simple check-point restart schemes. The initial implementation of FT-MPI included a robust heavy weight system state recovery algorithm that was designed to manage the membership of MPI communicators during multiple failures. The algorithm and its implementation although robust, was very conservative and this effected its scalability on both very large clusters as well as on distributed systems. This paper details the FT-MPI recovery algorithm and our initial experiments with new recovery algorithms that are aimed at being both scalable and latency tolerant. Our conclusions shows that the use of both topology aware collective communication and distributed consensus algorithms together produce the best results.\n\nTitle: Design for a Soft Error Resilient Dynamic Task-Based Runtime\nContent: As the scale of modern computing systems grows, failures will happen more frequently. On the way to Exactable a generic, low-overhead, resilient extension becomes a desired aptitude of any programming paradigm. In this paper we explore three additions to a dynamic task-based runtime to build a generic framework providing soft error resilience to task-based programming paradigms. The first recovers the application by re-executing the minimum required sub-DAG, the second takes critical checkpoints of the data flowing between tasks to minimize the necessary re-execution, while the last one takes advantage of algorithmic properties to recover the data without re-execution. These mechanisms have been implemented in the PaRSEC task-based runtime framework. Experimental results validate our approach and quantify the overhead introduced by such mechanisms.\n\n## Task\nCompose an abstract for the title 'Self-healing network for scalable fault-tolerant runtime environments' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The number of processors embedded on high performance computing platforms is growing daily to satisfy the user desire for solving larger and more complex problems. Scalable and fault-tolerant runtime environments are needed to support and adapt to the underlying libraries and hardware which require a high degree of scalability in dynamic large-scale environments. This paper presents a self-healing network (SHN) for supporting scalable and fault-tolerant runtime environments. The SHN is designed to support transmission of messages across multiple nodes while also protecting against recursive node and process failures. It will automatically recover itself after a failure occurs. SHN is implemented on top of a scalable fault-tolerant protocol (SFTP). The experimental results show that both the latest multicast and broadcast routing algorithms used in SHN are faster and more reliable than the original SFTP routing algorithms."
      }
    ]
  },
  {
    "tid":"134",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Design considerations for collaborative visual analytics\nContent: Information visualization leverages the human visual system to support the process of sensemaking, in which information is collected, organized, and analyzed to generate knowledge and inform action. Though most research to date assumes a single-user focus on perceptual and cognitive processes, in practice, sensemaking is often a social process involving parallelization of effort, discussion, and consensus building. This suggests that to fully support sensemaking, interactive visualization should also support social interaction. However, the most appropriate collaboration mechanisms for supporting this interaction are not immediately clear. In this article, we present design considerations for asynchronous collaboration in visual analysis environments, highlighting issues of work parallelization, communication, and social organization. These considerations provide a guide for the design and evaluation of collaborative visualization systems.\n\nTitle: The two-user Responsive Workbench: support for collaboration through individual views of a shared space\nContent: We present the two-user Responsive Workbench: a projection- based virtual reality system that allows two people to simulta- neously view individual stereoscopic image pairs from their own viewpoints. The system tracks the head positions of both users and computes four images - one for each eye of each person. To display the four images as two stereo pairs, we must ensure each image is correctly presented to the appropriate eye. We describe a hardware solution to this display problem as well as registration and calibra- tion procedures. These procedures ensure that when two users point to the same location on a virtual object, their fingers will physically touch. Since the stereo pairs are independent, we have the option of displaying specialized views of the shared virtual environment to each user. We present several scenarios in which specialized views might be useful.\n\nTitle: Structuring, Aggregating, and Evaluating Crowdsourced Design Critique\nContent: Feedback is an important component of the design process, but gaining access to high-quality critique outside a classroom or firm is challenging. We present CrowdCrit, a web-based system that allows designers to receive design critiques from non-expert crowd workers. We evaluated CrowdCrit in three studies focusing on the designer's experience and benefits of the critiques. In the first study, we compared crowd and expert critiques and found evidence that aggregated crowd critique approaches expert critique. In a second study, we found that designers who got crowd feedback perceived that it improved their design process. The third study showed that designers were enthusiastic about crowd critiques and used them to change their designs. We conclude with implications for the design of crowd feedback services.\n\nTitle: Graphical histories for visualization: supporting analysis, communication, and evaluation.\nContent: Interactive history tools, ranging from basic undo and redo to branching timelines of user actions, facilitate iterative forms of interaction. In this paper, we investigate the design of history mechanisms for information visualization. We present a design space analysis of both architectural and interface issues, identifying design decisions and associated trade-offs. Based on this analysis, we contribute a design study of graphical history tools for Tableau, a database visualization system. These tools record and visualize interaction histories, support data analysis and communication of findings, and contribute novel mechanisms for presenting, managing, and exporting histories. Furthermore, we have analyzed aggregated collections of history sessions to evaluate Tableau usage. We describe additional tools for analyzing users' history logs and how they have been applied to study usage patterns in Tableau.\n\nTitle: Visualizing Competitive Behaviors in Multi-User Virtual Environments\nContent: We present a system for enhancing observation of user interactions in virtual environments. In particular, we focus on analyzing behavior patterns in the popular team-based first-person perspective game Return to Castle Wolfenstein: Enemy Territory. This game belongs to a genre characterized by two moderate-sized teams (usually 6 to 12 players each) competing over a set of objectives. Our system allows spectators to visualize global features such as large-scale behaviors and team strategies, as opposed to the limited, local view that traditional spectating modes provide. We also add overlay visualizations of semantic information related to the action that might be important to a spectator in order to reduce the information overload that plagues traditional overview visualizations. These overlays can visualize information about abstract concepts such as player distribution over time and areas of intense combat activity, and also highlight important features like player paths, fire coverage, etc. This added information allows spectators to identify important game events more easily and reveals large-scale player behaviors that might otherwise be overlooked.\n\n## Task\nCompose an abstract for the title 'CommentSpace: structured support for collaborative visual analysis' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Collaborative visual analysis tools can enhance sensemaking by facilitating social interpretation and parallelization of effort. These systems enable distributed exploration and evidence gathering, allowing many users to pool their effort as they discuss and analyze the data. We explore how adding lightweight tag and link structure to comments can aid this analysis process. We present CommentSpace, a collaborative system in which analysts comment on visualizations and websites and then use tags and links to organize findings and identify others'\" contributions. In a pair of studies comparing CommentSpace to a system without support for tags and links, we find that a small, fixed vocabulary of tags (question, hypothesis, to-do) and links (evidence-for, evidence-against) helps analysts more consistently and accurately classify evidence and establish common ground. We also find that managing and incentivizing participation is important for analysts to progress from exploratory analysis to deeper analytical tasks. Finally, we demonstrate that tags and links can help teams complete evidence gathering and synthesis tasks and that organizing comments using tags and links improves analytic results."
      }
    ]
  },
  {
    "tid":"135",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: On profiling blogs with representative entries\nContent: With an explosive growth of blogs, information seeking in blogosphere becomes more and more challenging. One example task is to find the most relevant topical blogs against a given query or an existing blog. Such a task requires concise representation of blogs for effective and efficient searching and matching. In this paper, we investigate a new problem of profiling a blog by choosing a set of m most representative entries from the blog, where m is a predefined number that is application-dependent. With the set of selected representative entries, applications on blogs avoid handling hundreds or even thousands of entries (or posts) associated with each blog, which are updated frequently and often noisy in nature. To guide the process of selecting the most representative entries, we propose three principles, i.e., anomaly, representativeness, and diversity. Based on these principles, a greedy yet very efficient entry selection algorithm is proposed. To evaluate the entry selection algorithms, an extrinsic evaluation methodology from document summarization research is adapted. Specifically, we evaluate the proposed entry selection algorithms by examining their blog classification accuracies. By evaluating on a number of different classification methods, our empirical results showed that comparable classification accuracy could be achieved by using fewer than 20 representative entries for each blog compared to that of engaging all entries.\n\nTitle: Efficient Multi-Class Selective Sampling on Graphs.\nContent: A graph-based multi-class classification problem is typically converted into a collection of binary classification tasks via the one-vs.-all strategy, and then tackled by applying proper binary classification algorithms. Unlike the one-vs.-all strategy, we suggest a unified framework which operates directly on the multi-class problem without reducing it to a collection of binary tasks. Moreover, this framework makes active learning practically feasible for multi-class problems, while the one-vs.-all strategy cannot. Specifically, we employ a novel randomized query technique to prioritize the informative instances. This query technique based on the hybrid criterion of \\\"margin\\\" and \\\"uncertainty\\\" can achieve a comparable mistake bound with its fully supervised counterpart. To take full advantage of correctly predicted labels discarded in traditional conservative algorithms, we propose an aggressive selective sampling algorithm that can update the model even if no error occurs. Thanks to the aggressive updating strategy, the aggressive algorithm attains a lower mistake bound than its conservative competitors in expectation. Encouraging experimental results on real-world graph databases show that the proposed technique by querying an extremely small ratio of labels is able to accomplish better classification accuracy.\n\nTitle: On Effective Personalized Music Retrieval by Exploring Online User Behaviors.\nContent: In this paper, we study the problem of personalized text based music retrieval which takes users' music preferences on songs into account via the analysis of online listening behaviours and social tags. Towards the goal, a novel Dual-Layer Music Preference Topic Model (DL-MPTM) is proposed to construct latent music interest space and characterize the correlations among (user, song, term). Based on the DL-MPTM, we further develop an effective personalized music retrieval system. To evaluate the system's performance, extensive experimental studies have been conducted over two test collections to compare the proposed method with the state-of-the-art music retrieval methods. The results demonstrate that our proposed method significantly outperforms those approaches in terms of personalized search accuracy.\n\nTitle: Online Feature Selection and Its Applications\nContent: Feature selection is an important technique for data mining. Despite its importance, most studies of feature selection are restricted to batch learning. Unlike traditional batch learning methods, online learning represents a promising family of efficient and scalable machine learning algorithms for large-scale applications. Most existing studies of online learning require accessing all the attributes/features of training instances. Such a classical setting is not always appropriate for real-world applications when data instances are of high dimensionality or it is expensive to acquire the full set of attributes/features. To address this limitation, we investigate the problem of online feature selection (OFS) in which an online learner is only allowed to maintain a classifier involved only a small and fixed number of features. The key challenge of online feature selection is how to make accurate prediction for an instance using a small number of active features. This is in contrast to the classical setup of online learning where all the features can be used for prediction. We attempt to tackle this challenge by studying sparsity regularization and truncation techniques. Specifically, this article addresses two different tasks of online feature selection: 1) learning with full input, where an learner is allowed to access all the features to decide the subset of active features, and 2) learning with partial input, where only a limited number of features is allowed to be accessed for each instance by the learner. We present novel algorithms to solve each of the two problems and give their performance analysis. We evaluate the performance of the proposed algorithms for online feature selection on several public data sets, and demonstrate their applications to real-world problems including image classification in computer vision and microarray gene expression analysis in bioinformatics. The encouraging results of our experiments validate the efficacy and efficiency of th- proposed techniques.\n\nTitle: OLPS: A Toolbox for On-Line Portfolio Selection\nContent: On-line portfolio selection is a practical financial engineering problem, which aims to sequentially allocate capital among a set of assets in order to maximize long-term return. In recent years, a variety of machine learning algorithms have been proposed to address this challenging problem, but no comprehensive open-source toolbox has been released for various reasons. This article presents the first open-source toolbox for \"On-Line Portfolio Selection\" (OLPS), which implements a collection of classical and state-of-the-art strategies powered by machine learning algorithms. We hope that OLPS can facilitate the development of new learning methods and enable the performance benchmarking and comparisons of different strategies. OLPS is an open-source project released under Apache License (version 2.0), which is available at https://github.com/OLPS/ or http://OLPS.stevenhoi.org/.\n\n## Task\nCompose an abstract for the title 'Representative entry selection for profiling blogs' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Many applications on blog search and mining often meet the challenge of handling huge volume of blog data, in which one single blog could contain hundreds or even thousands of entries. We investigate novel techniques for profiling blogs by selecting a subset of representative entries for each blog. We propose two principles for guiding the entry selection task: representativeness and diversity. Further, we formulate the entry selection task into a combinatorial optimization problem and propose a greedy yet effective algorithm for finding a good approximate solution by exploiting the theory of submodular functions. We suggest blog classification for judging the performance of the proposed entry selection techniques and evaluate their performance on a real blog dataset, in which encouraging results were obtained."
      }
    ]
  },
  {
    "tid":"136",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: The Asymptotics of Ranking Algorithms\nContent: We consider the predictive problem of supervised ranking, where the task is to rank sets of candidate items returned in response to queries. Although there exist statistical procedures that come with guarantees of consistency in this setting, these procedures require that individuals provide a complete ranking of all items, which is rarely feasible in practice. Instead, individuals routinely provide partial preference information, such as pairwise comparisons of items, and more practical approaches to ranking have aimed at modeling this partial preference data directly. As we show, however, such an approach raises serious theoretical challenges. Indeed, we demonstrate that many commonly used surrogate losses for pairwise comparison data do not yield consistency; surprisingly, we show inconsistency even in low-noise settings. With these negative results as motivation, we present a new approach to supervised ranking based on aggregation of partial preferences, and we develop U-statistic-based empirical risk minimization procedures. We present an asymptotic analysis of these new procedures, showing that they yield consistency results that parallel those available for classification. We complement our theoretical results with an experiment studying the new procedures in a large-scale web-ranking task.\n\nTitle: Distributed Estimation of Generalized Matrix Rank: Efficient Algorithms and Lower Bounds.\nContent: We study the following generalized matrix rank estimation problem: given an n \u00d7 n matrix and a constant c \u2265 0, estimate the number of eigenvalues that are greater than c. In the distributed setting, the matrix of interest is the sum of m matrices held by separate machines. We show that any deterministic algorithm solving this problem must communicate \u03a9(n2) bits, which is order equivalent to transmitting the whole matrix. In contrast, we propose a randomized algorithm that communicates only e \u00d5(n) bits. The upper bound is matched by an \u03a9(n) lower bound on the randomized communication complexity. We demonstrate the practical effectiveness of the proposed algorithm with some numerical experiments.\n\nTitle: Universality of Mallows' and degeneracy of Kendall's kernels for rankings.\nContent: Kernel methods provide an attractive framework for aggregating and learning from ranking data, and so understanding the fundamental properties of kernels over permutations is a question of broad interest. We provide a detailed analysis of the Fourier spectra of the standard Kendall and Mallows kernels, and a new class of polynomial-type kernels. We prove that the Kendall kernel has exactly two irreducible representations at which the Fourier transform is non-zero, and moreover, the associated matrices are rank one. This implies that the Kendall kernel is nearly degenerate, with limited expressive and discriminative power. In sharp contrast, we prove that the Fourier transform of the Mallows kernel is a strictly positive definite matrix at all irreducible representations. This property guarantees that the Mallows kernel is both characteristic and universal. We introduce a family of normalized polynomial kernels of degree p that interpolates between the Kendall (degree one) and Mallows (infinite degree) kernels, and show that for d-dimensional permutations, the p-th degree kernel is characteristic when p is greater or equal than d - 1, unlike the Euclidean case in which no finite-degree polynomial kernel is characteristic.\n\nTitle: The Constrained Laplacian Rank Algorithm for Graph-Based Clustering.\nContent: Graph-based clustering methods perform clustering on a fixed input data graph. If this initial construction is of low quality then the resulting clustering may also be of low quality. Moreover, existing graph-based clustering methods require post-processing on the data graph to extract the clustering indicators. We address both of these drawbacks by allowing the data graph itself to be adjusted as part of the clustering procedure. In particular, our Constrained Laplacian Rank (CLR) method learns a graph with exactly k connected components (where k is the number of clusters). We develop two versions of this method, based upon the L1-norm and the L2-norm, which yield two new graph-based clustering objectives. We derive optimization algorithms to solve these objectives. Experimental results on synthetic datasets and real-world benchmark datasets exhibit the effectiveness of this new graph-based clustering method.\n\nTitle: A Robust Minimax Approach to Classification\nContent: When constructing a classifier, the probability of correct classification of future data points should be maximized. We consider a binary classification problem where the mean and covariance matrix of each class are assumed to be known. No further assumptions are made with respect to the class-conditional distributions. Misclassification probabilities are then controlled in a worst-case setting: that is, under all possible choices of class-conditional densities with given mean and covariance matrix, we minimize the worst-case (maximum) probability of misclassification of future data points. For a linear decision boundary, this desideratum is translated in a very direct way into a (convex) second order cone optimization problem, with complexity similar to a support vector machine problem. The minimax problem can be interpreted geometrically as minimizing the maximum of the Mahalanobis distances to the two classes. We address the issue of robustness with respect to estimation errors (in the means and covariances of the classes) via a simple modification of the input data. We also show how to exploit Mercer kernels in this setting to obtain nonlinear decision boundaries, yielding a classifier which proves to be competitive with current methods, including support vector machines. An important feature of this method is that a worst-case bound on the probability of misclassification of future data is always obtained explicitly.\n\n## Task\nCompose an abstract for the title 'On the Consistency of Ranking Algorithms' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We present a theoretical analysis of super- vised ranking, providing necessary and suf- ficient conditions for the asymptotic consis- tency of algorithms based on minimizing a surrogate loss function. We show that many commonly used surrogate losses are incon- sistent; surprisingly, we show inconsistency even in low-noise settings. We present a new value-regularized linear loss, establish its consistency under reasonable assumptions on noise, and show that it outperforms conven- tional ranking losses in a collaborative filter- ing experiment."
      }
    ]
  },
  {
    "tid":"137",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: An Optimal Nonorthogonal Separation of the Anisotropic Gaussian Convolution Filter\nContent: We give an analytical and geometrical treatment of what it means to separate a Gaussian kernel along arbitrary axes in Ropfn, and we present a separation scheme that allows us to efficiently implement anisotropic Gaussian convolution filters for data of arbitrary dimensionality. Based on our previous analysis we show that this scheme is optimal with regard to the number of memory accesses and interpolation operations needed. The proposed method relies on nonorthogonal convolution axes and works completely in image space. Thus, it avoids the need for a fast Fourier transform (FFT)-subroutine. Depending on the accuracy and speed requirements, different interpolation schemes and methods to implement the one-dimensional Gaussian (finite impulse response and infinite impulse response) can be integrated. Special emphasis is put on analyzing the performance and accuracy of the new method. In particular, we show that without any special optimization of the source code, it can perform anisotropic Gaussian filtering faster than methods relying on the FFT\n\nTitle: Learning Equations for Extrapolation and Control.\nContent: We present an approach to identify concise equations from data using a shallow neural network approach. In contrast to ordinary black-box regression, this approach allows understanding functional relations and generalizing them from observed data to unseen parts of the parameter space. We show how to extend the class of learnable equations for a recently proposed equation learning network to include divisions, and we improve the learning and model selection strategy to be useful for challenging real-world data. For systems governed by analytical expressions, our method can in many cases identify the true underlying equation and extrapolate to unseen domains. We demonstrate its effectiveness by experiments on a cart-pendulum system, where only 2 random rollouts are required to learn the forward dynamics and successfully achieve the swing-up task.\n\nTitle: Mind the Nuisance: Gaussian Process Classification using Privileged Noise.\nContent: The learning with privileged information setting has recently attracted a lot of attention within the machine learning community, as it allows the integration of additional knowledge into the training process of a classifier, even when this comes in the form of a data modality that is not available at test time. Here, we show that privileged information can naturally be treated as noise in the latent function of a Gaussian process classifier (GPC). That is, in contrast to the standard GPC setting, the latent function is not just a nuisance but a feature: it becomes a natural measure of confidence about the training data by modulating the slope of the GPC probit likelihood function. Extensive experiments on public datasets show that the proposed GPC method using privileged noise, called GPC+, improves over a standard GPC without privileged knowledge, and also over the current state-of-the-art SVM-based method, SVM+. Moreover, we show that advanced neural networks and deep learning methods can be compressed as privileged information.\n\nTitle: Efficient Subwindow Search: A Branch and Bound Framework for Object Localization\nContent: Most successful object recognition systems rely on binary classification, deciding only if an object is present or not, but not providing information on the actual object location. To estimate the object's location, one can take a sliding window approach, but this strongly increases the computational cost because the classifier or similarity function has to be evaluated over a large set of candidate subwindows. In this paper, we propose a simple yet powerful branch and bound scheme that allows efficient maximization of a large class of quality functions over all possible subimages. It converges to a globally optimal solution typically in linear or even sublinear time, in contrast to the quadratic scaling of exhaustive or sliding window search. We show how our method is applicable to different object detection and image retrieval scenarios. The achieved speedup allows the use of classifiers for localization that formerly were considered too slow for this task, such as SVMs with a spatial pyramid kernel or nearest-neighbor classifiers based on the \\chi^2 distance. We demonstrate state-of-the-art localization performance of the resulting systems on the UIUC Cars data set, the PASCAL VOC 2006 data set, and in the PASCAL VOC 2007 competition.\n\nTitle: An efficient divide-and-conquer cascade for nonlinear object detection.\nContent: We introduce a method to accelerate the evaluation of object detection cascades with the help of a divide-and-conquer procedure in the space of candidate regions. Compared to the exhaustive procedure that thus far is the state-of-the-art for cascade evaluation, the proposed method requires fewer evaluations of the classifier functions, thereby speeding up the search. Furthermore, we show how the recently developed efficient subwindow search (ESS) procedure [11] can be integrated into the last stage of our method. This allows us to use our method to act not only as a faster procedure for cascade evaluation, but also as a tool to perform efficient branch-and-bound object detection with nonlinear quality functions, in particular kernelized support vector machines. Experiments on the PASCAL VOC 2006 dataset show an acceleration of more than 50% by our method compared to standard cascade evaluation.\n\n## Task\nCompose an abstract for the title 'Seed, Expand And Constrain: Three Principles For Weakly-Supervised Image Segmentation' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We introduce a new loss function for the weakly-supervised training of semantic image segmentation models based on three guiding principles: to seed with weak localization cues, to expand objects based on the information about which classes can occur in an image, and to constrain the segmentations to coincide with object boundaries. We show experimentally that training a deep convolutional neural network using the proposed loss function leads to substantially better segmentations than previous state-of-the-art methods on the challenging PASCAL VOC 2012 dataset. We furthermore give insight into the working mechanism of our method by a detailed experimental study that illustrates how the segmentation quality is affected by each term of the proposed loss function as well as their combinations."
      }
    ]
  },
  {
    "tid":"138",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: The software model checker Blast\nContent: Blast is an automatic verification tool for checking temporal safety properties of C programs. Given a C program and a temporal\n safety property, Blast either statically proves that the program satisfies the safety property, or provides an execution path that exhibits a violation\n of the property (or, since the problem is undecidable, does not terminate). Blast constructs, explores, and refines abstractions of the program state space based on lazy predicate abstraction and interpolation-based\n predicate discovery. This paper gives an introduction to Blast and demonstrates, through two case studies, how it can be applied to program verification and test-case generation. In the\n first case study, we use Blast to statically prove memory safety for C programs. We use CCured, a type-based memory-safety analyzer, to annotate a program with run-time assertions that check for safe memory operations.\n Then, we use Blast to remove as many of the run-time checks as possible (by proving that these checks never fail), and to generate execution\n scenarios that violate the assertions for the remaining run-time checks. In our second case study, we use Blast to automatically generate test suites that guarantee full coverage with respect to a given predicate. Given a C program and\n a target predicate p, Blast determines the program locations q for which there exists a program execution that reaches q with p true, and automatically generates a set of test vectors that cause such executions. Our experiments show that Blast can provide automated, precise, and scalable analysis for C programs.\n\nTitle: The Blast Query Language for Software Verification\nContent: BLAST is an automatic verification tool for checking temporal safety properties of C programs. BLAST is based on lazy predicate abstraction driven by interpolation-based predicate discovery. In this paper, we present the BLAST specification language. The language specifies program properties at two levels of precision. At the lower level, monitor automata are used to specify temporal safety properties of program executions (traces). At the higher level, relational reachability queries over program locations are used to combine lower-level trace properties. The two-level specification language can be used to break down a verification task into several independent calls of the model-checking engine. In this way, each call to the model checker may have to analyze only part of the program, or part of the specification, and may thus succeed in a reduction of the number of predicates needed for the analysis. In addition, the two-level specification language provides a means for structuring and maintaining specifications.\n\nTitle: Invited talk: the blast query language for software verification\nContent: Blast is an automatic verification tool for checking temporal safety properties of C programs. Blast is based on lazy predicate abstraction driven by interpolation-based predicate discovery. The Blast specification language specifies program properties at two levels of precision. At the lower level, monitor automata are used to specify temporal safety properties of program executions (traces). At the higher level, relational reachability queries over program locations are used to combine lower-level trace properties. The two-level specification language can be used to break down a verification task into several independent calls of the model-checking engine. In this way, each call to the model checker may have to analyze only part of the program, or part of the specification, and may thus succeed in a reduction of the number of predicates needed for the analysis. In addition, the two-level specification language provides a means for structuring and maintaining specifications.\n\nTitle: The BLAST software verification system\nContent: Blast is a verification system for checking safety properties of C programs. Blast implements a lazy-abstraction algorithm, which integrates automatic abstraction refinement and model checking [8]. The input to Blast is a C program and a safety monitor written in a specification language with C like syntax [1]. The lazy-abstraction algorithm returns either an error trace of the program together with a corresponding test case [2], or a proof that the program satisfies the safety property [6] (or, since the problem is undecidable, the algorithm may fail to terminate). Blast automatically constructs and refines a parsimonious predicate abstraction of the input program, using an interpolation-based decision procedure to find, based on counterexample analysis, the relevant predicates for each individual control location [5]. Blast has successfully verified and found violations of interface safety properties of large device driver programs [6,5], memory safety properties [3], race conditions in nesC programs (using an extension for concurrent programs) [4], and file handling properties of large open-source programs [9]. Extensions to Blast support program testing [2] and incremental programming [7]. Blast is available from http://www.eecs.berkeley.edu/ blast.\n\nTitle: Bounds checking with taint-based analysis\nContent: We analyze the performance of different bounds checking implementations. Specifically, we examine using the x86 bound instruction to reduce the run-time overhead. We also propose a compiler optimization that prunes the bounds checks that are not necessary to guarantee security. The optimization is based on the observation that buffer overflow attacks are launched through external inputs. Therefore, it is sufficient to bounds check only the accesses to those data structures that can possibly hold the external inputs. Also, it is sufficient to bounds check only the memory writes. The proposed optimizations reduce the number of required bounds checks as well as the amount of meta-data that need to be maintained to perform those checks.\n\n## Task\nCompose an abstract for the title 'Checking memory safety with blast' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Blast is an automatic verification tool for checking temporal safety properties of C programs. Given a C program and a temporal safety property, Blast statically proves that either the program satisfies the safety property or the program has an execution trace that exhibits a violation of the property. Blast constructs, explores, and refines abstractions of the program state space based on lazy predicate abstraction and interpolation-based predicate discovery. We show how Blast can be used to statically prove memory safety for C programs. We take a two-step approach. First, we use Ccured, a type-based memory safety analyzer, to annotate with run-time checks all program points that cannot be proved memory safe by the type system. Second, we use Blast to remove as many of the run-time checks as possible (by proving that these checks never fail), and to generate for the remaining run-time checks execution traces that witness them fail. Our experience shows that Blast can remove many of the run-time checks added by Ccured and provide useful information to the programmer about many of the remaining checks."
      }
    ]
  },
  {
    "tid":"139",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Automatic Verification of Heap-Manipulating Programs Using Separation Logic\nContent: The incorrect use of pointers, such as null pointer dereference and memory leak, is one of the most common sources of program errors. In this talk, I will describe our techniques for automatically verifying the absence of such pointer errors, which we have been developing for the past three years, based on a new program logic called separation logic. This talk has two goals. The first is to show, by demo, the current status of techniques for automatically verifying pointer safety. The second is to present interesting instances of the interplay between automatic verification and program logic. In order to reduce the complexity of formal (manual) verification of programs, separation logic has unusual proof rules that exploit programming disciplines used by skilled software developers. I will explain how such rules have been used to improve the performance of our automatic verification techniques. Regarding the influence of automatic verification on program logic, I will describe new types of theorem proving questions on separation logic that were motivated by automatic verification.\n\nTitle: Beyond reachability: shape abstraction in the presence of pointer arithmetic\nContent: Previous shape analysis algorithms use a memory model where the heap is composed of discrete nodes that can be accessed only via access paths built from variables and field names, an assumption that is violated by pointer arithmetic. In this paper we show how this assumption can be removed, and pointer arithmetic embraced, by using an analysis based on separation logic. We describe an abstract domain whose elements are certain separation logic formulae, and an abstraction mechanism that automatically transits between a low-level RAM view of memory and a higher, fictional, view that abstracts from the representation of nodes and multiword linked-lists as certain configurations of the RAM. A widening operator is used to accelerate the analysis. We report experimental results obtained from running our analysis on a number of classic algorithms for dynamic memory management.\n\nTitle: Automatic Construction of Hoare Proofs from Abstract Interpretation Results\nContent: By combining program logic and static analysis, we present an automatic approach to construct program proofs to be used in Proof-Carrying Code. We use Hoare logic in representing the proofs of program properties, and the abstract interpretation in computing the program properties. This combination automatizes proof construction; an abstract interpretation automatically estimates program properties (approximate invariants) of our interest, and our proof-construction method constructs a Hoare-proof for those approximate invariants. The proof-checking side (code consumer's side) is insensitive to a specific static analysis; the assertions in the Hoare proofs are always first-order logic formulas for integers, into which we first compile the abstract interpreters' results. Both the property-compilation and the proof construction refer to the standard safety conditions that are prescribed in the abstract interpretation framework. We demonstrate this approach for a simple imperative language with an example property being the integer ranges of program variables. We prove the correctness of our approach, and analyze the size complexity of the generated proofs.\n\nTitle: Verifying concurrent memory reclamation algorithms with grace\nContent: Memory management is one of the most complex aspects of modern concurrent algorithms, and various techniques proposed for it--such as hazard pointers, read-copy-update and epoch-based reclamation--have proved very challenging for formal reasoning. In this paper, we show that different memory reclamation techniques actually rely on the same implicit synchronisation pattern, not clearly reflected in the code, but only in the form of assertions used to argue its correctness. The pattern is based on the key concept of a grace period, during which a thread can access certain shared memory cells without fear that they get deallocated. We propose a modular reasoning method, motivated by the pattern, that handles all three of the above memory reclamation techniques in a uniform way. By explicating their fundamental core, our method achieves clean and simple proofs, scaling even to realistic implementations of the algorithms without a significant increase in proof complexity. We formalise the method using a combination of separation logic and temporal logic and use it to verify example instantiations of the three approaches to memory reclamation.\n\nTitle: Scalable Shape Analysis for Systems Code\nContent: Pointer safety faults in device drivers are one of the leading causes of crashes in operating systems code. In principle, shape analysis tools can be used to prove the absence of this type of error. In practice, however, shape analysis is not used due to the unacceptable mixture of scalability and precision provided by existing tools. In this paper we report on a new join operation ${\\sqcup\\dagger}$ for the separation domain which aggressively abstracts information for scalability yet does not lead to false error reports. ${\\sqcup\\dagger}$ is a critical piece of a new shape analysis tool that provides an acceptable mixture of scalability and precision for industrial application. Experiments on whole Windows and Linux device drivers (firewire, pci-driver, cdrom, md, etc.) represent the first working application of shape analysis to verification of whole industrial programs.\n\n## Task\nCompose an abstract for the title 'Automatic verification of pointer programs using grammar-based shape analysis' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We present a program analysis that can automatically discover the shape of complex pointer data structures. The discovered invariants are, then, used to verify the absence of safety errors in the program, or to check whether the program preserves the data consistency. Our analysis extends the shape analysis of Sagiv et al. with grammar annotations, which can precisely express the shape of complex data structures. We demonstrate the usefulness of our analysis with binomial heap construction and the Schorr-Waite tree traversal. For a binomial heap construction algorithm, our analysis returns a grammar that precisely describes the shape of a binomial heap; for the Schorr-Waite tree traversal, our analysis shows that at the end of the execution, the result is a tree and there are no memory leaks."
      }
    ]
  },
  {
    "tid":"140",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: HoIP: Haptics over Internet Protocol\nContent: Applications such as telesurgery need low latency communication of haptic data between remote nodes. For the stability of the control loop, a typical round trip delay target is 5 ms or lower. In this paper, we specify an application layer protocol - Haptics over Internet Protocol (HoIP) - which is designed to allow such low latency haptic data transmission without sacrificing on the quality of haptic perception. The key ingredients of HoIP include adaptive sampling strategies for the digitization of the haptic signal, use of a multithreaded architecture at the transmitter and receiver to minimize processing delay, and use of an existing UDP implementation. In this paper, we describe in detail the frame structure used in HoIP and the rationale behind our design choices, the implementation details of HoIP at the transmitter and receiver, and report processing delay measurements done using real haptic devices and HAPI (an open source software used to interface with the haptic device). The HoIP software is entirely written in C++ and our results show that in the worst case transmitter and receiver side processing delays are less than 0.6 ms.\n\nTitle: Congestion Control for Network-Aware Telehaptic Communication.\nContent: Telehaptic applications involve delay-sensitive multimedia communication between remote locations with distinct Quality of Service (QoS) requirements for different media components. These QoS constraints pose a variety of challenges, especially when the communication occurs over a shared network, with unknown and time-varying cross-traffic. In this work, we propose a transport layer congestion control protocol for telehaptic applications operating over shared networks, termed as Dynamic Packetization Module (DPM). DPM is a lossless, network-aware protocol that tunes the telehaptic packetization rate based on the level of congestion in the network. To monitor the network congestion, we devise a novel network feedback module, which communicates the end-to-end delays encountered by the telehaptic packets to the respective transmitters with negligible overhead. Via extensive simulations, we show that DPM meets the QoS requirements of telehaptic applications over a wide range of network cross-traffic conditions. We also report qualitative results of a real-time telepottery experiment with several human subjects, which reveal that DPM preserves the quality of telehaptic activity even under heavily congested network scenarios. Finally, we compare the performance of DPM with several previously proposed telehaptic communication protocols and demonstrate that DPM outperforms these protocols.\n\nTitle: Perception-Based Data Reduction and Transmission of Haptic Data in Telepresence and Teleaction Systems\nContent: We present a novel approach for the transmission of haptic data in telepresence and teleaction systems. The goal of this work is to reduce the packet rate between an operator and a teleoperator without impairing the immersiveness of the system. Our approach exploits the properties of human haptic perception and is, more specifically, based on the concept of just noticeable differences. In our scheme, updates of the haptic amplitude values are signaled across the network only if the change of a haptic stimulus is detectable by the human operator. We investigate haptic data communication for a 1 degree-of-freedom (DoF) and a 3 DoF teleaction system. Our experimental results show that the presented approach is able to reduce the packet rate between the operator and teleoperator by up to 90% of the original rate without affecting the performance of the system.\n\nTitle: Opportunistic adaptive haptic sampling on forward channel in telehaptic communication\nContent: We propose a network-based opportunistic improvisation to adaptive sampling for the forward channel telehaptic data stream on a time-varying network. The algorithm explores real-time tuning of the perceptual deadband parameter to minimize network underutilization, and consequently improves the quality of telehaptic communication. We describe in detail the rationale behind the design choices of the proposed sampling scheme. We perform both real-time telehaptic experiments and simulations to test the proof of concept. The reconstructed haptic signals reveal a substantial improvement in average SNR of 3.57 dB, suggesting that the proposed method outperforms the conventional adaptive sampling technique to a large extent. In addition to satisfying the telehaptic Quality of Service (QoS) requirements, we also demonstrate that our method does not overwhelm the network or penalize the concurrent traffic streams.\n\nTitle: Application Layer Congestion Control for Network-Aware Telehaptic Communication.\nContent: In a real-world network, shared by several users, telehaptic applications involving delay-sensitive multimedia communication between remote locations demand distinct Quality of Service (QoS) guarantees for different media components. These QoS constraints pose a variety of challenges, especially when the network cross-traffic is unknown and time-varying. In this work, we propose an application layer congestion control protocol for telehaptic applications operating over shared networks, termed as dynamic packetization module (DPM). DPM is a lossless, network-aware protocol which tunes the telehaptic packetization rate based on the level of congestion in the network. To monitor the network congestion, we devise a novel network feedback module, which communicates the end-to-end delays encountered by the telehaptic packets to the respective transmitters, with negligible overhead. Via extensive simulations, we show that DPM meets the QoS requirements of telehaptic applications over a wide range of network cross-traffic conditions. We also report the qualitative results of a real-time telepottery experiment with several human subjects, which reveal that DPM preserves the quality of telehaptic activity even under heavily congested network scenarios. Finally, we compare the performance of DPM with existing telehaptic communication protocols, and conclude that DPM outperforms other techniques.\n\n## Task\nCompose an abstract for the title 'HoIP: A point-to-point haptic data communication protocol and its evaluation' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Telehaptics applications are usually characterized by a strict imposition of a round trip haptic data latency of less than 30 ms. In this paper, we present Haptics over Internet Protocol (HoIP) - a low latency application layer protocol that enables haptic, audio and video data transmission over a network between two remotely connected nodes. The evaluation of the protocol is carried out through a set of three experiments, each with distinct objectives. First, a haptic-audio-visual (HAV) interactive application, involving two remotely located human personnel communicating via haptic, auditory and visual media, to evaluate the Quality of Service (QoS) violation due to the protocol. Second, a haptic sawing experiment with the goal of assessing the impact of HoIP and network delays in telehaptics applications, by taking the example of a typical telesurgical activity. Third, a telepottery system to determine the protocol's ability in reproducing a real-time interactive user experience with a remote virtual object, in presence of perceptual data compression and reconstruction techniques. Our experiments reveal that the transmission scheduling of multimedia packets performs well in terms of maintaining the latencies well under the QoS thresholds."
      }
    ]
  },
  {
    "tid":"141",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Dynamic Planar Convex Hull with Optimal Query Time\nContent: The dynamic maintenance of the convex hull of a set of points in the plane is one of the most important problems in computational geometry. We present a data structure supporting point insertions in amortized O(log n \u010b log log log n) time, point deletions in amortized O(log n \u010b log log n) time, and various queries about the convex hull in optimal O(log n) worst-case time. The data structure requires O(n) space. Applications of the new dynamic convex hull data structure are improved deterministic algorithms for the k-level problem and the redblue segment intersection problem where all red and all blue segments are connected.\n\nTitle: Dynamic planar range maxima queries\nContent: We consider the dynamic two-dimensional maxima query problem. Let P be a set of n points in the plane. A point is maximal if it is not dominated by any other point in P. We describe two data structures that support the reporting of the t maximal points that dominate a given query point, and allow for insertions and deletions of points in P. In the pointer machine model we present a linear space data structure with O(log n + t) worst case query time and O(log n) worst case update time. This is the first dynamic data structure for the planar maxima dominance query problem that achieves these bounds in the worst case. The data structure also supports the more general query of reporting the maximal points among the points that lie in a given 3-sided orthogonal range unbounded from above in the same complexity. We can support 4-sided queries in O(log2 n+t) worst case time, and O(log2 n) worst case update time, using O(n log n) space, where t is the size of the output. This improves the worst case deletion time of the dynamic rectangular visibility query problem from O(log3 n) to O(log2 n). We adapt the data structure to the RAM model with word size w, where the coordinates of the points are integers in the range U={0,..., 2w-1}. We present a linear space data structure that supports 3-sided range maxima queries in O(log n/log log n + t) worst case time and updates in O(logn/log log n) worst case time. These are the first sublogarithmic worst case bounds for all operations in the RAM model.\n\nTitle: I/O-efficient dynamic point location in monotone planar subdivisions\nContent: We present an efficient external-memory dynamic data structure for point location in monotone planar subdivisions. Our data. structure uses O(NIB) disk blocks to store a monotone subdivision of size N, where B is the size of a disk block. It supports queries in O(log(B)(2) N) I/Os (worst-case) and up dates in O(log(B)(2) N) I/Os (amortized).We also propose a new variant of B-trees, called level-balanced B-trees, which allow insert, delete, merge, and split operations in O((1 + b/B log(M/B) N/B) log(b) N) I/Os (amortized), 2 less than or equal to b less than or equal to B/2, even if each node stores a pointer to its parent. Here M is the size of main memory. Besides being essential to our point-location data structure, we believe that level-balanced B-trees are of significant independent interest. They can, for example, be used to dynamically maintain a planar st-graph using O((1 + b/B log(M/B) N/B) log(b) N) = O(log(B)(2) N) I/Os (amortized) per update, so that reachability queries can be answered in O(log(B) N) I/Os (worst case).\n\nTitle: Cache-oblivious planar orthogonal range searching and counting\nContent: We present the first cache-oblivious data structure for planar orthogonal range counting, and improve on previous results for cache-oblivious planar orthogonal range searching.Our range counting structure uses O(N log2 N) space and answers queries using O(logB N) memory transfers, where B is the block size of any memory level in a multilevel memory hierarchy. Using bit manipulation techniques, the space can be further reduced to O(N). The structure can also be modified to support more general semigroup range sum queries in O(logB N) memory transfers, using O(N log2 N) space for three-sided queries and O(N log22 N/log2 log2 N) space for four-sided queries.Based on the O(N log N) space range counting structure, we develop a data structure that uses O(N log2 N) space and answers three-sided range queries in O(logB N+T/B) memory transfers, where T is the number of reported points. Based on this structure, we present a general four-sided range searching structure that uses O(N log22 N/log2 log2 N) space and answers queries in O(logB N + T/B) memory transfers.\n\nTitle: Optimal Planar Orthogonal Skyline Counting Queries.\nContent: The skyline of a set of points in the plane is the subset of maximal points, where a point (x, y) is maximal if no other point (x', y') satisfies x' >= x and y' >= y. We consider the problem of preprocessing a set P of n points into a space efficient static data structure supporting orthogonal skyline counting queries, i.e. given a query rectangle R to report the size of the skyline of P boolean AND R. We present a data structure for storing n points with integer coordinates having query time O(lg n/ lg lg n) and space usage O(n) words. The model of computation is a unit cost RAM with logarithmic word size. We prove that these bounds are the best possible by presenting a matching lower bound in the cell probe model with logarithmic word size: Space usage n lg(O(1)) n implies worst case query time Omega(lg n/ lg lg n).\n\n## Task\nCompose an abstract for the title 'Dynamic Planar Convex Hull' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper we determine the computational complexity of the dynamic convex hull problem in the planar case. We present a data structure that maintains a finite set of n points in the plane under insertion and deletion of points in amortized O(log n) time per operation. Thespace usage of the data structure is O(n). The data structure supports extreme point queries in a given direction, tangent queries through a given point, and queries for the neighboring points on the convex hull in O(log n) time. The extreme point queries can be used to decide whether or not a given line intersects the convex hull, and the tangent queries to determine whether a given point is inside the convex hull. We give a lower bound on the amortized asymptotic time complexity that matches the performance of this data structure."
      }
    ]
  },
  {
    "tid":"142",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Integration of affective computing techniques and soft computing for developing a human affective recognition system for U-learning systems\nContent: In this study, a human affective norm emotion and attention recognition system for U-learning systems is developed. Fifth graders in an elementary school were recruited as participants firstly to see some emotional pictures from the International Affective Picture System IAPS, and to do the attention test to obtain the affective information - electroencephalography EEG and electrocardiogram ECG for developing the affective norm recognition system of the study. These bio-physiology signals extract important features by using four types of linear Principal Component Analysis PCA to serve as the input variables for Support Vector Machine SVM model. The results of feature selection showed that factor analysis with covariance extraction method has higher accumulative variances than correlation extraction method. This study suggested that future researchers may try to adopt more non-linear feature selection methods in order to develop a high accuracy SVM-based emotion recognition system.\n\nTitle: Evaluating vehicle telematics system by using a novel MCDM techniques with dependence and feedback\nContent: As the advanced integration of communications, information and vehicle technologies, Vehicle Telematics Systems (VTS), have been initiated for satisfying consumers' needs with respect to automobile movement. Importantly, VTS enables a vehicle to become a multifunction mobile-services platform. Cars are now designed not only for transportation, but also to provide value-added services covering navigation, safety, security, information, communications and entertainment. Drivers or passengers can contact a call center via VTS to access aspired/desired services and information online. Therefore, VTS increases both the utility/functions and the safety of driving. Developing the optimal VTS that satisfies with consumers' needs has become the foremost concern of automobile producers. This study will attempt to identify the required VTS utilities between distinguishing characteristics/features of consumers and propose the ideal service combinations for the next e-era generation VTS. An evaluating model by six aspects to be considered/constructed, which encompasses 25 criteria, is built to identify consumers' needs for the next e-era generation VTS. In the paper, a Decision-Making Trial and Evaluation Laboratory (DEMATEL) technique is used to construct the network relation-map (NRM) among the criteria of each aspect. These criteria are not mutually independent; instead, they have feedback mechanisms. The Analytical Network Process (ANP) based on NRM is used to determine the relative weightings among those criteria. Moreover, the Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) is used to determine and improve the gaps among the distinguishing characteristics/features of consumers' desired utilities with respect to services/provide the decision-maker of Telemetics Service Provider (TSP) for improving existing functions or planning further utilities in the next-generation VTS. Based on above NRM those results can be served as a suggestion for the TSP to improve the existing functions or plan further utilities/functions for reducing the gaps and satisfying the users' needs in next e-era generation VTS.\n\nTitle: Cloud E-Learning Service Strategies For Improving E-Learning Innovation Performance In A Fuzzy Environment By Using A New Hybrid Fuzzy Multiple Attribute Decision-Making Model\nContent: The purpose of this study was to address this problem by applying a new hybrid fuzzy multiple criteria decision-making model including (a) using the fuzzy decision-making trial and evaluation laboratory (DEMATEL) technique to construct the fuzzy scope influential network relationship map (FSINRM) and determine the fuzzy influential weights of the fuzzy DEMATEL-based analytic network process (DANP) as well as (b) the modified VIKOR method combining fuzzy influential weights of fuzzy DANP to integrate each criterion of fuzzy performance gaps into dimensions and, overall, to examine the interdependent and feedback problems among various dimensions/criteria of cloud e-learning services for creating the excellent improvement plan based on the FSINRM to implement the strategic orientations. An empirical case of the network communication service industry was used as an example to show the proposed methods and ultimately present the most effective improvement plan for decision-makers to achieve the aspiration level.\n\nTitle: Creating the aspired intelligent assessment systems for teaching materials\nContent: The core objective of the Nine-Year Integrated Curriculum for primary schools is to ''enable students to demonstrate their talents instead of just scoring high on exams'' around the world. The determining factor in education reform is to declare the competence indicators of necessary educational behavior in primary and junior high school. In the reform process, for all domains, the enriched rate of competence indicators for educational materials and methods is very meaningful. Because educational materials and methods in different domains have their own style, we should evaluate these teaching materials separately. Thus, in this research we propose a novel MCDM (Multiple Criteria Decision Making) framework for evaluating, comparing, and improving the effectiveness of competence indicators in the various publications for teaching materials in primary school based on different viewpoints. The ANP (Analysis Network Process) weights are based on the DEMATEL technique with the MCDM method for resolving the problems of dependence and feedback among criteria. Then, a VIKOR technique with ANP weights is proposed for addressing and reducing the performance gaps for each criterion, thus hopefully improving, re-configuring and selecting the aspired Intelligent Assessment Systems (IAS) for teaching materials. An empirical study of Mandarin Chinese based on this system design of three publishers is illustrated to verify the effectiveness of the proposed method. The results can improve the efficiency and quality of the authored Mandarin Chinese teaching materials and may extend to other Learning Areas.\n\nTitle: System on chip design service e-business value maximization through a novel MCDM framework\nContent: As the IC (integrated circuit) industry migrates to the System-on-Chip (SOC) era, the SOC design service industry is emerging. Meanwhile, in the past decade, the emergence of Internet has changed the high technology marketing approaches while e-commerce has already become one of the most efficient marketing channels. Thus, most leading SOC design service firms tried to leverage novel e-commerce business models to provide better services including online silicon intellectual property (SIP) sourcing, transactions, integration, etc. to assist customers in enhancing their innovation competences to shorten their time to market and thus, time to money. However, defining appropriate e-business models for commercializing new SIPs or SOC design services is not easy for both aspects of technology as well as business development. On one hand, from the aspect of technology, the technical site R&D or production managers are familiar with SOC technologies while do not really understand the needs of customers' over the Internet. On the other hand, from the aspect of business development, the sales or marketing managers may be familiar with online customers' needs, wants as well as demands while are unfamiliar to SOC technology developments. To overcome the above mentioned problems, an appropriate e-business model definition framework can overcome this cognitive gap and maximize the value-added of online SOC design services. In this paper, a novel analytic framework based on the concept of design service customers' competence set expansions by leveraging high technology service firms' capabilities and resources as well as novel multiple criteria decision making (MCDM) techniques, will be proposed. The framework being proposed can be leveraged by the design service firms to define an appropriate e-business model for possible SIP or design service businesses. Based on the proposed MCDM framework, an empirical study of an SIP commercialization e-business model definition inside an SIP Mall, an SIP e-commerce mechanism being operated by a SOC design service firm, will be provided for verifying the effectiveness of this novel analytic framework. The feasibility of the proposed framework in the real world can be verified by the empirical study. In the future, the novel MCDM framework can be applied to novel e-business model definitions in the SOC design service or other high technology industries.\n\n## Task\nCompose an abstract for the title 'Evaluating intertwined effects in e-learning programs: A novel hybrid MCDM model based on factor analysis and DEMATEL' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Internet evolution has affected all industrial and commercial activity and accelerated e-learning growth. Due to cost, time, or flexibility for designer courses and learners, e-learning has been adopted by corporations as an alternative training method. E-learning effectiveness evaluation is vital, and evaluation criteria are diverse. A large effort has been made regarding e-learning effectiveness evaluation; however, a generalized quantitative evaluation model, which considers both the interaffected relation between criteria and the fuzziness of subjective perception concurrently, is lacking. In this paper, the proposed new novel hybrid MCDM model addresses the independent relations of evaluation criteria with the aid of factor analysis and the dependent relations of evaluation criteria with the aid of DEMATEL. The AHP and the fuzzy integral methods are used for synthetic utility in accordance with subjective perception environment. Empirical experimental results show the proposed model is capable of producing effective evaluation of e-learning programs with adequate criteria that fit with respondent's perception patterns, especially when the evaluation criteria are numerous and intertwined."
      }
    ]
  },
  {
    "tid":"143",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Families of Alpha- Beta- and Gamma- Divergences: Flexible and Robust Measures of Similarities\nContent: In this paper, we extend and overview wide families of Alpha-, Beta- and Gamma- divergences and discuss their fundamental properties. In literature usually only one single asymmetric (Alpha, Beta or Gamma) divergence is considered. We show in this paper that there exist families of such divergences with the same consistent properties. Moreover, we establish links and correspondences among these divergences by applying suitable nonlinear transformations. For example, we can generate the Beta- divergences directly from Alpha- divergences and vice versa. Furthermore, we show that a new wide class of Gamma- divergences can be generated not only from the family of Beta- divergences but also from a family of Alpha- divergences. The paper bridges these divergences and shows also their links to Tsallis and Renyi entropies. Most of these divergences have a natural information theoretic interpretation.\n\nTitle: Generalized Alpha-Beta Divergences and Their Application to Robust Nonnegative Matrix Factorization.\nContent: We propose a class of multiplicative algorithms for Nonnegative Matrix Factorization (NMF) which are robust with respect to noise and outliers. To achieve this, we formulate a new family generalized divergences referred to as the Alpha-Beta-divergences (AB-divergences), which are parameterized by the two tuning parameters, alpha and beta, and smoothly connect the fundamental Alpha-, Beta- and Gamma-divergences. By adjusting these tuning parameters, we show that a wide range of standard and new divergences can be obtained. The corresponding learning algorithms for NMF are shown to integrate and generalize many existing ones, including the Lee-Seung, ISRA (Image Space Reconstruction Algorithm), EMML (Expectation Maximization Maximum Likelihood), Alpha-NMF, and Beta-NMF. Owing to more degrees of freedom in tuning the parameters, the proposed family of AB-multiplicative NMF algorithms is shown to improve robustness with respect to noise and outliers. The analysis illuminates the links of between AB-divergence and other divergences, especially Gamma-and Itakura-Saito divergences.\n\nTitle: Non-Linear Canonical Correlation Analysis Using Alpha-Beta Divergence.\nContent: We propose a generalized method of the canonical correlation analysis using Alpha-Beta divergence, called AB-canonical analysis (ABCA). From observations of two random variables, x is an element of R-P and y is an element of R-Q, ABCA finds directions, w(x) is an element of R-P and w(y) is an element of R-Q, such that the AB-divergence between the joint distribution of (w(x)(T)x, w(y)(T)y) and the product of their marginal distributions is maximized. The number of significant non-zero canonical coefficients are determined by using a sequential permutation test. The advantage of our method over the standard canonical correlation analysis (CCA) is that it can reconstruct the hidden non-linear relationship between w(x)(T)x and w(y)(T)y, and it is robust against outliers. We extend ABCA when data are observed in terms of tensors. We further generalize this method by imposing sparseness constraints. Extensive simulation study is performed to justify our approach.\n\nTitle: Nonnegative Tucker decomposition with alpha-divergence\nContent: Nonnegative tucker decomposition (NTD) is a recent multiway extension of nonnegative matrix factorization (NMF), where nonnega- tivity constraints are incorporated into Tucker model. In this paper we consider alpha-divergence as a discrepancy measure and derive multiplicative updating algorithms for NTD. The proposed multiplicative algorithm includes some existing NMF and NTD algorithms as its special cases, since alpha-divergence is a one-parameter family of divergences which accommodates KL-divergence, Hellinger divergence, X2 divergence, and so on. Numerical experiments on face images show how different values of alpha affect the factorization results under different types of noise.\n\nTitle: Csisz\u00e1r\u2019s divergences for non-negative matrix factorization: family of new algorithms\nContent: In this paper we discus a wide class of loss (cost) functions for non-negative matrix factorization (NMF) and derive several novel algorithms with improved efficiency and robustness to noise and outliers. We review several approaches which allow us to obtain generalized forms of multiplicative NMF algorithms and unify some existing algorithms. We give also the flexible and relaxed form of the NMF algorithms to increase convergence speed and impose some desired constraints such as sparsity and smoothness of components. Moreover, the effects of various regularization terms and constraints are clearly shown. The scope of these results is vast since the proposed generalized divergence functions include quite large number of useful loss functions such as the squared Euclidean distance,Kulback-Leibler divergence, Itakura-Saito, Hellinger, Pearson\u2019s chi-square, and Neyman\u2019s chi-square distances, etc. We have applied successfully the developed algorithms to blind (or semi blind) source separation (BSS) where sources can be generally statistically dependent, however they satisfy some other conditions or additional constraints such as nonnegativity, sparsity and/or smoothness.\n\n## Task\nCompose an abstract for the title 'Log-Determinant Divergences Revisited: Alpha-Beta and Gamma Log-Det Divergences.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This work reviews and extends a family of log-determinant (log-det) divergences for symmetric positive definite (SPD) matrices and discusses their fundamental properties. We show how to use parameterized Alpha-Beta (AB) and Gamma log-det divergences to generate many well-known divergences; in particular, we consider the Stein's loss, the S-divergence, also called Jensen-Bregman LogDet (JBLD) divergence, Logdet Zero (Bhattacharyya) divergence, Affine Invariant Riemannian Metric (AIRM), and other divergences. Moreover, we establish links and correspondences between log-det divergences and visualise them on an alpha-beta plane for various sets of parameters. We use this unifying framework to interpret and extend existing similarity measures for semidefinite covariance matrices in finite-dimensional Reproducing Kernel Hilbert Spaces (RKHS). This paper also shows how the Alpha-Beta family of log-det divergences relates to the divergences of multivariate and multilinear normal distributions. Closed form formulas are derived for Gamma divergences of two multivariate Gaussian densities; the special cases of the Kullback-Leibler, Bhattacharyya, Renyi, and Cauchy-Schwartz divergences are discussed. Symmetrized versions of log-det divergences are also considered and briefly reviewed. Finally, a class of divergences is extended to multiway divergences for separable covariance (or precision) matrices."
      }
    ]
  },
  {
    "tid":"144",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Fast 4D Sheared Filtering for Interactive Rendering of Distribution Effects\nContent: Soft shadows, depth of field, and diffuse global illumination are common distribution effects, usually rendered by Monte Carlo ray tracing. Physically correct, noise-free images can require hundreds or thousands of ray samples per pixel, and take a long time to compute. Recent approaches have exploited sparse sampling and filtering; the filtering is either fast (axis-aligned), but requires more input samples, or needs fewer input samples but is very slow (sheared). We present a new approach for fast sheared filtering on the GPU. Our algorithm factors the 4D sheared filter into four 1D filters. We derive complexity bounds for our method, showing that the per-pixel complexity is reduced from O(n2 l2) to O(nl), where n is the linear filter width (filter size is O(n2)) and l is the (usually very small) number of samples for each dimension of the light or lens per pixel (spp is l2). We thus reduce sheared filtering overhead dramatically. We demonstrate rendering of depth of field, soft shadows and diffuse global illumination at interactive speeds. We reduce the number of samples needed by 5-8\u00d7, compared to axis-aligned filtering, and framerates are 4\u00d7 faster for equal quality.\n\nTitle: Frequency analysis and sheared filtering for shadow light fields of complex occluders\nContent: Monte Carlo ray tracing of soft shadows produced by area lighting and intricate geometries, such as the shadows through plant leaves or arrays of blockers, is a critical challenge. The final image often has relatively smooth shadow patterns, since it integrates over the light source. However, Monte Carlo rendering exhibits considerable noise even at high sample counts because of the large variance of the integrand due to the intricate shadow function. This article develops an efficient diffuse soft shadow technique for mid to far occluders that relies on a new 4D cache and sheared reconstruction filter. For this, we first derive a frequency analysis of shadows for planar area lights and complex occluders. Our analysis subsumes convolution soft shadows for parallel planes as a special case. It allows us to derive 4D sheared filters that enable lower sampling rates for soft shadows. While previous sheared-reconstruction techniques were able primarily to index samples according to screen position, we need to perform reconstruction at surface receiver points that integrate over vastly different shapes in the reconstruction domain. This is why we develop a new light-field-like 4D data structure to store shadowing values and depth information. Any ray tracing system that shoots shadow rays can easily incorporate our method to greatly reduce sampling rates for diffuse soft shadows.\n\nTitle: Factored axis-aligned filtering for rendering multiple distribution effects\nContent: Monte Carlo (MC) ray-tracing for photo-realistic rendering often requires hours to render a single image due to the large sampling rates needed for convergence. Previous methods have attempted to filter sparsely sampled MC renders but these methods have high reconstruction overheads. Recent work has shown fast performance for individual effects, like soft shadows and indirect illumination, using axis-aligned filtering. While some components of light transport such as indirect or area illumination are smooth, they are often multiplied by high-frequency components such as texture, which prevents their sparse sampling and reconstruction. We propose an approach to adaptively sample and filter for simultaneously rendering primary (defocus blur) and secondary (soft shadows and indirect illumination) distribution effects, based on a multi-dimensional frequency analysis of the direct and indirect illumination light fields. We describe a novel approach of factoring texture and irradiance in the presence of defocus blur, which allows for pre-filtering noisy irradiance when the texture is not noisy. Our approach naturally allows for different sampling rates for primary and secondary effects, further reducing the overall ray count. While the theory considers only Lambertian surfaces, we obtain promising results for moderately glossy surfaces. We demonstrate 30x sampling rate reduction compared to equal quality noise-free MC. Combined with a GPU implementation and low filtering over-head, we can render scenes with complex geometry and diffuse and glossy BRDFs in a few seconds.\n\nTitle: Axis-aligned filtering for interactive physically-based diffuse indirect lighting\nContent: We introduce an algorithm for interactive rendering of physically-based global illumination, based on a novel frequency analysis of indirect lighting. Our method combines adaptive sampling by Monte Carlo ray or path tracing, using a standard GPU-accelerated raytracer, with real-time reconstruction of the resulting noisy images. Our theoretical analysis assumes diffuse indirect lighting, with general Lambertian and specular receivers. In practice, we demonstrate accurate interactive global illumination with diffuse and moderately glossy objects, at 1-3 fps. We show mathematically that indirect illumination is a structured signal in the Fourier domain, with inherent band-limiting due to the BRDF and geometry terms. We extend previous work on sheared and axis-aligned filtering for motion blur and shadows, to develop an image-space filtering method for interreflections. Our method enables 5--8X reduced sampling rates and wall clock times, and converges to ground truth as more samples are added. To develop our theory, we overcome important technical challenges---unlike previous work, there is no light source to serve as a band-limit in indirect lighting, and we also consider non-parallel geometry of receiver and reflecting surfaces, without first-order approximations.\n\nTitle: Axis-aligned filtering for interactive sampled soft shadows\nContent: We develop a simple and efficient method for soft shadows from planar area light sources, based on explicit occlusion calculation by raytracing, followed by adaptive image-space filtering. Since the method is based on Monte Carlo sampling, it is accurate. Since the filtering is in image-space, it adds minimal overhead and can be performed at real-time frame rates. We obtain interactive speeds, using the Optix GPU raytracing framework. Our technical approach derives from recent work on frequency analysis and sheared pixel-light filtering for offline soft shadows. While sample counts can be reduced dramatically, the sheared filtering step is slow, adding minutes of overhead. We develop the theoretical analysis to instead consider axis-aligned filtering, deriving the sampling rates and filter sizes. We also show how the filter size can be reduced as the number of samples increases, ensuring a consistent result that converges to ground truth as in standard Monte Carlo rendering.\n\n## Task\nCompose an abstract for the title 'Frequency domain normal map filtering' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Filtering is critical for representing detail, such as color textures or normal maps, across a variety of scales. While MIP-mapping texture maps is commonplace, accurate normal map filtering remains a challenging problem because of nonlinearities in shading---we cannot simply average nearby surface normals. In this paper, we show analytically that normal map filtering can be formalized as a spherical convolution of the normal distribution function (NDF) and the BRDF, for a large class of common BRDFs such as Lambertian, microfacet and factored measurements. This theoretical result explains many previous filtering techniques as special cases, and leads to a generalization to a broader class of measured and analytic BRDFs. Our practical algorithms leverage a significant body of work that has studied lighting-BRDF convolution. We show how spherical harmonics can be used to filter the NDF for Lambertian and low-frequency specular BRDFs, while spherical von Mises-Fisher distributions can be used for high-frequency materials."
      }
    ]
  },
  {
    "tid":"145",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Exploration of Metaheuristics through Automatic Algorithm Configuration Techniques and Algorithmic Frameworks.\nContent: In this paper we argue that flexible algorithm frameworks can be useful to capture the wide variety of algorithmic components for heuristic algorithms and serve as basic experimental frameworks. One of the utilities is that they can implement the wide variety of different algorithm components and their alternative choices for single stochastic local search methods and we are currently extending existing frameworks in that direction. We exemplify this approach considering the example of Simulated Annealing (SA). In fact, a wide variety of design choices of SA algorithms has been proposed in the literature and algorithm frameworks may (i) simply collect potentially all available choices, (ii) provide a tool for the experimental analysis of specific algorithms and component choices, and (iii) allow the generation of new algorithm variants by combining exisiting components in new ways. We show some limited computational experiments that show the benefit of tuning in this context and the way conclusions on the performance of algorithms are altered in this way.\n\nTitle: Automatic algorithm configuration based on local search\nContent: The determination of appropriate values for free algorithm parameters is a challenging and tedious task in the design of effective algorithms for hard problems. Such parameters include categorical choices (e.g., neighborhood structure in local search or variable/value ordering heuristics in tree search), as well as numerical parameters (e.g., noise or restart timing). In practice, tuning of these parameters is largely carried out manually by applying rules of thumb and crude heuristics, while more principled approaches are only rarely used. In this paper, we present a local search approach for algorithm configuration and prove its convergence to the globally optimal parameter configuration. Our approach is very versatile: it can, e.g., be used for minimising run-time in decision problems or for maximising solution quality in optimisation problems. It further applies to arbitrary algorithms, including heuristic tree search and local search algorithms, with no limitation on the number of parameters. Experiments in four algorithm configuration scenarios demonstrate that our automatically determined parameter settings always outperform the algorithm defaults, sometimes by several orders of magnitude. Our approach also shows better performance and greater flexibility than the recent CALIBRA system. Our ParamILS code, along with instructions on how to use it for tuning your own algorithms, is available on-line at http://www.cs.ubc.ca/labs/beta/Projects/ParamILS.\n\nTitle: Automatic configuration of multi-objective ACO algorithms\nContent: In the last few years a significant number of ant colony optimization (ACO) algorithms have been proposed for tackling multi-objective optimization problems. In this paper, we propose a software framework that allows to instantiate the most prominent multi-objective ACO (MOACO) algorithms. More importantly, the flexibility of this MOACO framework allows the application of automatic algorithm configuration techniques. The experimental results presented in this paper show that such an automatic configuration of MOACO algorithms is highly desirable, given that our automatically configured algorithms clearly outperform the best performing MOACO algorithms that have been proposed in the literature. As far as we are aware, this paper is also the first to apply automatic algorithm configuration techniques to multi-objective stochastic local search algorithms.\n\nTitle: A Configurable Generalized Artificial Bee Colony Algorithm With Local Search Strategies\nContent: In this paper, we apply a generalized artificial bee colony (ABC-X) algorithm to the learning-based real-parameter optimization competition at the 2015 Congress on Evolutionary Computation. The main idea underlying the ABC-X algorithm is to provide a flexible, freely configurable framework for artificial bee colony (ABC) algorithms. From this framework, one can not only instantiate known ABC algorithms but also configure new, previously unseen ABC algorithms that may perform even better than known ABC algorithms. One key advantage of a configurable algorithm framework is that it is adaptable to many different specific problems without requiring necessarily an algorithm re-design. This is relevant if in the application problem repeatedly instances of the problem need to be solved regularly. This situation arises in many practical settings e.g. in power control or other application areas: Routinely a sequence of specific instances of a more general continuous optimization problem arise and these instances have to be solved repeatedly (possibly for an infinite horizon) in the future: in this case the instances of the problem in the sequence will share similarities as they arise from a same source. This is also the situation that is targeted by the learning-based real-parameter optimization competition and which we have also described in our own earlier research.\n\nTitle: Grammar-based generation of stochastic local search heuristics through automatic algorithm configuration tools\nContent: Several grammar-based genetic programming algorithms have been proposed in the literature to automatically generate heuristics for hard optimization problems. These approaches specify the algorithmic building blocks and the way in which they can be combined in a grammar; the best heuristic for the problem being tackled is found by an evolutionary algorithm that searches in the algorithm design space defined by the grammar. In this work, we propose a novel representation of the grammar by a sequence of categorical, integer, and real-valued parameters. We then use a tool for automatic algorithm configuration to search for the best algorithm for the problem at hand. Our experimental evaluation on the one-dimensional bin packing problem and the permutation flowshop problem with weighted tardiness objective shows that the proposed approach produces better algorithms than grammatical evolution, a well-established variant of grammar-based genetic programming. The reasons behind such improvement lie both in the representation proposed and in the method used to search the algorithm design space.\n\n## Task\nCompose an abstract for the title 'Exploring variable neighborhood search for automatic algorithm configuration.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Methods for automatic algorithm configuration integrate some search mechanism for generating candidate algorithm configurations and mechanisms for handling the stochasticity of the algorithm configuration problem. One popular algorithm configurator is ParamILS, which searches the configuration space using an iterated local search algorithm. In our research, we explore variable neighborhood search mechanisms as an alternative for the one-exchange neighborhood that is searched in the local search phase of ParamILS. In this article, we explore a reduced variable neighborhood search for automatic configuration. Our experimental results are promising and indicate directions for extending our work."
      }
    ]
  },
  {
    "tid":"146",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Inference with Logic Programs with Annotated Disjunctions under the Well Founded Semantics\nContent: Logic Programs with Annotated Disjunctions (LPADs) allow to express probabilistic information in logic programming. The semantics of an LPAD is given in terms of well founded models of the normal logic programs obtained by selecting one disjunct from each ground LPAD clause. The paper presents SLGAD resolution that computes the (conditional) probability of a ground query from an LPAD and is based on SLG resolution for normal logic programs. SLGAD is evaluated on classical benchmarks for well founded semantics inference algorithms, namely the stalemate game and the ancestor relation. SLGAD is compared with Cilog2 and SLDNFAD, an algorithm based on SLDNF, on the programs that are modularly acyclic. The results show that SLGAD deals correctly with cyclic programs and, even if it is more expensive than SLDNFAD on problems where SLDNFAD succeeds, is faster than Cilog2 when the query is true in an exponential number of instances.\n\nTitle: Expectation maximization over binary decision diagrams for probabilistic logic programs\nContent: Recently much work in Machine Learning has concentrated on using expressive representation languages that combine aspects of logic and probability. A whole field has emerged, called Statistical Relational Learning, rich of successful applications in a variety of domains. In this paper we present a Machine Learning technique targeted to Probabilistic Logic Programs, a family of formalisms where uncertainty is represented using Logic Programming tools. Among various proposals for Probabilistic Logic Programming, the one based on the distribution semantics is gaining popularity and is the basis for languages such as ICL, PRISM, ProbLog and Logic Programs with Annotated Disjunctions. This paper proposes a technique for learning parameters of these languages. Since their equivalent Bayesian networks contain hidden variables, an Expectation Maximization EM algorithm is adopted. In order to speed the computation up, expectations are computed directly on the Binary Decision Diagrams that are built for inference. The resulting system, called EMBLEM for \"EM over Bdds for probabilistic Logic programs Efficient Mining\", has been applied to a number of datasets and showed good performances both in terms of speed and memory usage. In particular its speed allows the execution of a high number of restarts, resulting in good quality of the solutions.\n\nTitle: Tabling and Answer Subsumption for Reasoning on Logic Programs with Annotated Disjunctions\nContent: The paper presents the algorithm \"Probabilistic Inference with Tabling and Answer subsumption\" (PITA) for computing the probability of queries from Logic Programs with Annotated Disjunctions. PITA is based on a program transformation techniques that adds an extra argument to every atom. PITA uses tabling for saving intermediate results and answer subsumption for combining different answers for the same subgoal. PITA has been implemented in XSB and compared with the ProbLog, cplint and CVE systems. The results show that in almost all cases, PITA is able to solve larger problems and is faster than competing algorithms.\n\nTitle: A Top Down Interpreter for LPAD and CP-Logic\nContent: Logic Programs with Annotated Disjunctions and CP-logic are two different but related languages for expressing probabilistic information in logic programming. The paper presents a top down interpreter for computing the probability of a query from a program in one of these two languages. The algorithm is based on the one available for ProbLog. The performances of the algorithm are compared with those of a Bayesian reasoner and with those of the ProbLog interpreter. On programs that have a small grounding, the Bayesian reasoner is more scalable, but programs with a large grounding require the top down interpreter. The comparison with ProbLog shows that the added expressiveness effectively requires more computation resources.\n\nTitle: Approximate inference for logic programs with annotated disjunctions\nContent: Logic Programs with Annotated Disjunctions (LPADs) are a promising language for Probabilistic Inductive Logic Programming. In order to develop efficient learning systems for LPADs, it is fundamental to have high-performing inference algorithms. The existing approaches take too long or fail for large problems. In this paper we adapt to LPAD the approaches for approximate inference that have been developed for ProbLog, namely k-best and Monte Carlo. k-Best finds a lower bound of the probability of a query by identifying the k most probable explanations while Monte Carlo estimates the probability by smartly sampling the space of programs. The two techniques have been implemented in the cplint suite and have been tested on real and artificial datasets representing graphs. The results show that both algorithms are able to solve larger problems often in less time than the exact algorithm.\n\n## Task\nCompose an abstract for the title 'Terminating Evaluation of Logic Programs with Finite Three-Valued Models' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"As evaluation methods for logic programs have become more sophisticated, the classes of programs for which termination can be guaranteed have expanded. From the perspective of ar set programs that include function symbols, recent work has identified classes for which grounding routines can terminate either on the entire program [Calimeri et al. 2008] or on suitable queries [Baselice et al. 2009]. From the perspective of tabling, it has long been known that a tabling technique called subgoal abstraction provides good termination properties for definite programs [Tamaki and Sato 1986], and this result was recently extended to stratified programs via the class of bounded term-size programs [Riguzzi and Swift 2013]. In this article, we provide a formal definition of tabling with subgoal abstraction resulting in the SLGSA algorithm. Moreover, we discuss a declarative characterization of the queries and programs for which SLGSA terminates. We call this class strongly bounded term-size programs and show its equivalence to programs with finite well-founded models. For normal programs, strongly bounded term-size programs strictly includes the finitely ground programs of Calimeri et al. [2008]. SLGSA has an asymptotic complexity on strongly bounded term-size programs equal to the best known and produces a residual program that can be sent to an answer set programming system. Finally, we describe the implementation of subgoal abstraction within the SLG-WAM of XSB and provide performance results."
      }
    ]
  },
  {
    "tid":"147",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Field-Sensitive Value Analysis by Field-Insensitive Analysis\nContent: Shared and mutable data-structures pose major problems in static analysis and most analyzers are unable to keep track of the values of numeric variables stored in the heap. In this paper, we first identify sufficient conditions under which heap allocated numeric variables in object oriented programs (i.e., numeric fields) can be handled as non-heap allocated variables. Then, we present a static analysis to infer which numeric fields satisfy these conditions at the level of (sequential) bytecode . This allows instrumenting the code with ghost variables which make such numeric fields observable to any field-insensitive value analysis. Our experimental results in termination analysis show that we greatly enlarge the class of analyzable programs with a reasonable overhead.\n\nTitle: Object-sensitive cost analysis for concurrent objects\nContent: AbstractThis article presents a novel cost analysis framework for concurrent objects. Concurrent objects form a well-established model for distributed concurrent systems. In this model, objects are the concurrency units that communicate among them via asynchronous method calls. Cost analysis aims at automatically approximating the resource consumption of executing a program in terms of its input parameters. While cost analysis for sequential programming languages has received considerable attention, concurrency and distribution have been notably less studied. The main challenges of cost analysis in a concurrent setting are as follows. First, inferring precise size abstractions for data in the program in the presence of shared memory. This information is essential for bounding the number of iterations of loops. Second, distribution suggests that analysis must infer the cost of the diverse distributed components separately. We handle this by means of a novel form of object-sensitive recurrence equations that use cost centres in order to keep the resource usage assigned to the different components separate. We have implemented our analysis and evaluated it on several small applications that are classical examples of concurrent and distributed programming. Copyright \u00a9 2015John Wiley & Sons, Ltd.\n\nTitle: Efficient local unfolding with ancestor stacks for full prolog\nContent: The integration of powerful partial evaluation methods into practical compilers for logic programs is still far from reality. This is related both to 1) efficiency issues and to 2) the complications of dealing with practical programs. Regarding efficiency, the most successful unfolding rules used nowadays are based on structural orders applied over (covering) ancestors, i.e., a subsequence of the atoms selected during a derivation. Unfortunately, maintaining the structure of the ancestor relation during unfolding introduces significant overhead. We propose an efficient, practical local unfolding rule based on the notion of covering ancestors which can be used in combination with any structural order and allows a stack-based implementation without losing any opportunities for specialization. Regarding the second issue, we propose assertion-based techniques which allow our approach to deal with real programs that include (Prolog) built-ins and external predicates in a very extensible manner. Finally, we report on our implementation of these techniques in a practical partial evaluator, embedded in a state of the art compiler which uses global analysis extensively (the Ciao compiler and, specifically, its preprocessor CiaoPP). The performance analysis of the resulting system shows that our techniques, in addition to dealing with practical programs, are also significantly more efficient in time and somewhat more efficient in memory than traditional tree-based implementations.\n\nTitle: Context-sensitive multivariant assertion checking in modular programs\nContent: We propose a modular, assertion-based system for verification and debugging of large logic programs, together with several interesting models for checking assertions statically in modular programs, each with different characteristics and representing different trade-offs. Our proposal is a modular and multivariant extension of our previously proposed abstract assertion checking model and we also report on its implementation in the CiaoPP system. In our approach, the specification of the program, given by a set of assertions, may be partial, instead of the complete specification required by traditional verification systems. Also, the system can deal with properties which cannot always be determined at compile-time. As a result, the proposed system needs to work with safe approximations: all assertions proved correct are guaranteed to be valid and all errors actual errors. The use of modular, context-sensitive static analyzers also allows us to introduce a new distinction between assertions checked in a particular context or checked in general.\n\nTitle: A Study on the Practicality of Poly-Controlled Partial Evaluation\nContent: Poly-controlled partial evaluation (PCPE) is a flexible approach for specializing logic programs, which has been recently proposed. It takes into account repertoires of global control and local control rules instead of a single, predetermined, combination. Thus, different global and local control rules can be assigned to different call patterns, obtaining results that are hybrid in the sense that they cannot be obtained using a single combination of control rules, as traditional partial evaluation does. PCPE can be implemented as a search-based algorithm, producing sets of candidate specialized programs (many of them hybrid), instead of a single one. The quality of each of these programs is assessed through the use of different fitness functions, which can be resource aware, taking into account multiple factors such as run-time, memory consumption, and code size of the specialized programs, among others. Although PCPE is an appealing approach, it suffers from an inherent blowup of its search space when implemented as a search-based algorithm. Thus, in order to be used in practice, and to deal with realistic programs, we must be able to prune its search space without losing the interesting solutions. The contribution of this work is two-fold. On one hand we perform an experimental study on the heterogeneity of solutions obtained by search-based PCPE, showing that the solutions provided behave very differently when compared using a fitness function. Note that this is important since otherwise the cost of producing a large number of candidate specializations would not be justified. The second contribution of this work is the introduction of a technique for pruning the search space of this approach. The proposed technique is easy to apply and produces a considerable reduction of the size of the search space, allowing PCPE to deal with a reasonable number of benchmark programs. Although pruning is done in a heuristic way, our experimental results suggest that our heuristic behaves well in practice, since the fitness value of the solutions obtained using pruning coincide with the fitness value of the solution obtained when no pruning is applied.\n\n## Task\nCompose an abstract for the title 'From object fields to local variables: a practical approach to field-sensitive analysis' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Static analysis which takes into account the value of data stored in the heap is typically considered complex and computationally intractable in practice. Thus, most static analyzers do not keep track of object fields (or fields for short), i.e., they are field-insensitive. In this paper, we propose locality conditions for soundly converting fields into local variables. This way, field-insensitive analysis over the transformed program can infer information on the original fields. Our notion of locality is context-sensitive and can be applied both to numeric and reference fields. We propose then a polyvariant transformation which actually converts object fields meeting the locality condition into variables and which is able to generate multiple versions of code when this leads to increasing the amount of fields which satisfy the locality conditions. We have implemented our analysis within a termination analyzer for Java bytecode."
      }
    ]
  },
  {
    "tid":"148",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Towards a general distributed platform for learning and generalization\nContent: Different learning models employ different styles of generalization on novel inputs. This paper proposes the need for multiple styles of generalizatio n to support a broad application base. The Priority ASOCS model (Priority Adaptive Self-Organizing Concurrent System) is overviewed and presented as a potential platform which can support multiple generalization styles. PASOCS is an adaptive network composed of many simple computing elements operating asynchronously and in parallel. The PASOCS can operate in either a data processing mode or a learning mode. During data processing mode, the system acts as a parallel hardware circuit. During learning mode, the PASOCS incorporates rules, with attached priorities, which represent the application being learned. Learning is accomplished in a distributed fashion in time logarithmic in the number of rules. The new model has significant learning time and space complexity improvements over previous models.\n\nTitle: Location-Independent Transformations: a General Strategy for Implementing Neural Networks.\nContent: Most Artificial Neural Networks (ANNs) have a fixed topology during learning, and typically suffer from a number of short - comings as a result. Variations of ANNs that use dynamic topologies have shown ability to overcome many of these problems. This paper introduces Location-Independent Transformations (LITs) as a general strategy for implementing neural networks that use static and dynamic topologies. A LIT creates a set of location-independent nodes, where each node computes its part of the network output independent of other nodes, using local information. This type of transformation allows efficient support for adding and deleting nodes dynamically during learning. Two simple networks, the single-layer competitive learning network, and the counterpropagation network, which combines elements of supervised learning with competitive learning, are used in this paper to illustrate the LIT strategy. These two networks are localist in the sense that ultimately one node is responsible for each output. LITs for other models are presented in other papers. Artificial Neural Networks (ANNs) use a different computational paradigm than conventional von Neumann mechanisms. ANNs are composed of nodes and weighted connections between nodes, where each node computes its output based on a function of its weighted inputs. The overall function that a network computes is typically changed by altering the values of the weights between nodes, until the desired result is achieved. The main features of ANNs are learning ability, generalization, parallelism, self- organization and fault-tolerance. These features allow ANNs to solve various applications not handled well by current conventional computational mechanisms. Application areas include, but are not limited to, problems requiring learning, such as pattern recognition, control and decision systems, speech, and signal analysis (1). Hardware support for ANNs is important for handling large, complex problems in real time. Learning times can exceed tolerable limits for complex applications with conventional computing schemes. Furthermore, hardware is becoming cheaper and easier to design. The Location-Independent Transformation (LIT) is a general implementation strategy for ANNs that overcomes several weaknesses of current hardware implementation methods. Most ANNs use only static topologies\u2014the topology is fixed initially, and remains the same throughout learning. ANNs with static topologies typically suffer from the following short-comings: \u2022 sensitivity to user-supplied parameters: learning rate(s), etc. \u2022 local error minima during learning \u2022 no a priori mechanism for deciding on an effective initial topology (number of nodes, number of layers, etc.) Current research is demonstrating the use of dynamic topologies in overcoming these problems (2-5). A dynamic topology is one which allows adding and deleting entire nodes and individual weighted connections during learning. Early ANN hardware implementations are model-specific, and are intended to support only static topologies (6-8). More recent neurocomputer systems have specialized neural hardware, and seek to support more general classes of ANNs (9-11). Although some neurocomputers could potentially support dynamic topologies more directly in hardware, rather than in software, they currently do not Of course, general parallel machines, like the Connection Machine (12) and the CRAY (13), can simulate the desired dynamics in software, but these machines are not optimized for neural computation. LIT supports general classes of ANNs and dynamic topologies in an efficient parallel hardware implementation.\n\nTitle: An Integrated Instance-Based Learning Algorithm\nContent: The basic nearest-neighbor rule generalizes well in many domains but has several shortcomings, including inappropriate distance functions, large storage requirements, slow execution time, sensitivity to noise, and an inability to adjust its decision boundaries after storing the training data. This paper proposes methods for overcoming each of these weaknesses and combines the methods into a comprehensive learning system called the Integrated Decremental Instance-Based Learning Algorithm (IDIBL) that seeks to reduce storage, improve execution speed, and increase generalization accuracy, when compared to the basic nearest neighbor algorithm and other learning models. IDIBL tunes its own parameters using a new measure of fitness that combines confidence and cross-validation accuracy in order to avoid discretization problems with more traditional leave-one-out cross-validation. Tn our experiments IDIBL achieves higher generalization accuracy than other less comprehensive instance-based learning algorithms, while requiring less than one-fourth the storage of the nearest neighbor algorithm and improving execution speed by a corresponding factor. In experiments on twenty-one data sets, IDIBL also achieves higher generalization accuracy than that reported for sixteen major machine learning and neural network models.\n\nTitle: Using a Neural Network to Approximate an Ensemble of Classifiers\nContent: Several methods (e.g., Bagging, Boosting) of constructing and combining an ensemble of classifiers have recently been shown capable of improving accuracy of a class of commonly used classifiers (e.g., decision trees, neural networks). The accuracy gain achieved, however, is at the expense of a higher requirement for storage and computation. This storage and computation overhead can decrease the utility of these methods when applied to real-world situations. In this Letter, we propose a learning approach which allows a single neural network to approximate a given ensemble of classifiers. Experiments on a large number of real-world data sets show that this approach can substantially save storage and computation while still maintaining accuracy similar to that of the entire ensemble.\n\nTitle: Lia: A Location-Independent Transformation For Asocs Adaptive Algorithm 2\nContent: Most Artificial Neural Networks (ANNs) have a fixed topology during learning, and often suffer from a number of short-comings as a result. ANNs that use dynamic topologies have shown the ability to overcome many of these problems. Adaptive Self-Organizing Concurrent Systems (ASOCS) are a class of learning models with inherently dynamic topologies. This paper introduces Location-Independent Transformations (LITs) as a general strategy for implementing learning models that use dynamic topologies efficiently in parallel hardware. An LIT creates a set of location-independent nodes, where each node computes its part of the network output independent of other nodes, using local information. This type of transformation allows efficient support for adding and deleting nodes dynamically during learning. In particular, this paper presents the Location-Independent ASOCS (LIA) model as an LIT for ASOCS Adaptive Algorithm 2. The description of LIA gives formal definitions for LIA algorithms. Because LIA implements basic ASOCS mechanisms, these definitions provide a formal description of basic ASOCS mechanisms in general, in addition to LIA.\n\n## Task\nCompose an abstract for the title 'Analysis of the Convergence and Generalization of AA1' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"AA1 is an incremental learning algorithm for Adaptive Self-Organizing Concurrent Systems (ASOCS). ASOCS are self-organizing, dynamically growing networks of computing nodes. AA1 learns by discrimination and implements knowledge in a distributed fashion over all the nodes. This paper reviews AA1 from the perspective of convergence and generalization. A formal proof that AA1 converges on any arbitrary Boolean instance set is given. A discussion of generalization and other aspects of AA1, including the problem of handling inconsistency, follows. Results of simulations with real-world data are presented. They show that AA1 gives promising generalization."
      }
    ]
  },
  {
    "tid":"149",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Large-scale factorization of type-constrained multi-relational data\nContent: The statistical modeling of large multi-relational datasets has increasingly gained attention in recent years. Typical applications involve large knowledge bases like DBpedia, Freebase, YAGO and the recently introduced Google Knowledge Graph that contain millions of entities, hundreds and thousands of relations, and billions of relational tuples. Collective factorization methods have been shown to scale up to these large multi-relational datasets, in particular in form of tensor approaches that can exploit the highly scalable alternating least squares (ALS) algorithms for calculating the factors. In this paper we extend the recently proposed state-of-the-art RESCAL tensor factorization to consider relational type-constraints. Relational type-constraints explicitly define the logic of relations by excluding entities from the subject or object role. In addition we will show that in absence of prior knowledge about type-constraints, local closed-world assumptions can be approximated for each relation by ignoring unobserved subject or object entities in a relation. In our experiments on representative large datasets (Cora, DBpedia), that contain up to millions of entities and hundreds of type-constrained relations, we show that the proposed approach is scalable. It further significantly outperforms RESCAL without type-constraints in both, runtime and prediction quality.\n\nTitle: A Three-Way Model for Collective Learning on Multi-Relational Data\nContent: Relational learning is becoming increasingly important in many areas of application. Here, we present a novel approach to relational learning based on the factorization of a three-way tensor. We show that unlike other tensor approaches, our method is able to perform collective learning via the latent components of the model and provide an efficient algorithm to compute the factorization. We substantiate our theoretical considerations regarding the collective learning capabilities of our model by the means of experiments on both a new dataset and a dataset commonly used in entity resolution. Furthermore, we show on common benchmark datasets that our approach achieves better or on-par results, if compared to current state-of-the-art relational learning solutions, while it is significantly faster to compute.\n\nTitle: Factorizing YAGO: scalable machine learning for linked data\nContent: Vast amounts of structured information have been published in the Semantic Web's Linked Open Data (LOD) cloud and their size is still growing rapidly. Yet, access to this information via reasoning and querying is sometimes difficult, due to LOD's size, partial data inconsistencies and inherent noisiness. Machine Learning offers an alternative approach to exploiting LOD's data with the advantages that Machine Learning algorithms are typically robust to both noise and data inconsistencies and are able to efficiently utilize non-deterministic dependencies in the data. From a Machine Learning point of view, LOD is challenging due to its relational nature and its scale. Here, we present an efficient approach to relational learning on LOD data, based on the factorization of a sparse tensor that scales to data consisting of millions of entities, hundreds of relations and billions of known facts. Furthermore, we show how ontological knowledge can be incorporated in the factorization to improve learning results and how computation can be distributed across multiple nodes. We demonstrate that our approach is able to factorize the YAGO~2 core ontology and globally predict statements for this large knowledge base using a single dual-core desktop computer. Furthermore, we show experimentally that our approach achieves good results in several relational learning tasks that are relevant to Linked Data. Once a factorization has been computed, our model is able to predict efficiently, and without any additional training, the likelihood of any of the 4.3 \u22c5 1014 possible triples in the YAGO~2 core ontology.\n\nTitle: A scalable approach for statistical learning in semantic graphs\nContent: Increasingly, data is published in the form of semantic graphs. The most notable example is the Linked Open Data LOD initiative where an increasing number of data sources are published in the Semantic Web's Resource Description Framework and where the various data sources are linked to reference one another. In this paper we apply machine learning to semantic graph data and argue that scalability and robustness can be achieved via an urn-based statistical sampling scheme. We apply the urn model to the SUNS framework which is based on multivariate prediction. We argue that multivariate prediction approaches are most suitable for dealing with the resulting high-dimensional sparse data matrix. Within the statistical framework, the approach scales up to large domains and is able to deal with highly sparse relationship data. We summarize experimental results using a friend-of-a-friend data set and a data set derived from DBpedia. In more detail, we describe novel experiments on disease gene prioritization using LOD data sources. The experiments confirm the ease-of-use, the scalability and the good performance of the approach.\n\nTitle: Combining information extraction, deductive reasoning and machine learning for relation prediction\nContent: Three common approaches for deriving or predicting instantiated relations are information extraction, deductive reasoning and machine learning. Information extraction uses subsymbolic unstructured sensory information, e.g. in form of texts or images, and extracts statements using various methods ranging from simple classifiers to the most sophisticated NLP approaches. Deductive reasoning is based on a symbolic representation and derives new statements from logical axioms. Finally, machine learning can both support information extraction by deriving symbolic representations from sensory data, e.g., via classification, and can support deductive reasoning by exploiting regularities in structured data. In this paper we combine all three methods to exploit the available information in a modular way, by which we mean that each approach, i.e., information extraction, deductive reasoning, machine learning, can be optimized independently to be combined in an overall system. We validate our model using data from the YAGO2 ontology, and from Linked Life Data and Bio2RDF, all of which are part of the Linked Open Data (LOD) cloud.\n\n## Task\nCompose an abstract for the title 'Type-Constrained Representation Learning in Knowledge Graphs' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Large knowledge graphs increasingly add value to various applications that require machines to recognize and understand queries and their semantics, as in search or question answering systems. Latent variable models have increasingly gained attention for the statistical modeling of knowledge graphs, showing promising results in tasks related to knowledge graph completion and cleaning. Besides storing facts about the world, schema-based knowledge graphs are backed by rich semantic descriptions of entities and relation-types that allow machines to understand the notion of things and their semantic relationships. In this work, we study how type-constraints can generally support the statistical modeling with latent variable models. More precisely, we integrated prior knowledge in form of type-constraints in various state of the art latent variable approaches. Our experimental results show that prior knowledge on relation-types significantly improves these models up to 77% in link-prediction tasks. The achieved improvements are especially prominent when a low model complexity is enforced, a crucial requirement when these models are applied to very large datasets. Unfortunately, type-constraints are neither always available nor always complete e.g., they can become fuzzy when entities lack proper typing. We show that in these cases, it can be beneficial to apply a local closed-world assumption that approximates the semantics of relation-types based on observations made in the data."
      }
    ]
  },
  {
    "tid":"150",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Tool Framework for Deriving the Application Architecture for Global Software Development Projects\nContent: In order to meet the communication, coordination and control requirements of distributed Global Software Development (GSD) teams, it is necessary to define a proper software architecture. Designing a GSD architecture, however, involves a multitude of design decisions that are related in different ways. As such, it is not trivial for the architect to design a system that meets the different GSD concerns. To assist the architect in designing a suitable GSD architecture we propose the tool framework Global Architect. The tool framework is based on a common meta-model for GSD and a question framework, which includes a predefined set of questions that are related to abstract design rules for designing GSD systems. Based on the answers provided to the questions of the question framework, the tool automatically selects and instantiates the necessary rules and generates the GSD architecture. Global Architect has been applied to design the GSD architecture for a real industrial project of Cybersoft, a leading GSD company in Turkey.\n\nTitle: Interaction-based feature-driven model-transformations for generating e-forms\nContent: One of the basic pillars in Model-Driven Software Development (MDSD) is defined by model transformations and likewise several useful approaches have been proposed in this context. In parallel, domain modeling plays an essential role in MDSD to support the definition of concepts in the domain, and support the model transformation process. In this paper, we will discuss the results of an e-government project for the generation of e-forms from feature models. Very often existing model transformation practices seem to largely adopt a closed world assumption whereby the transformation definitions of models are defined beforehand and interaction with the user at run-time is largely omitted. Our study shows the need for a more interactive approach in model transformations in which e-forms are generated after interaction with the end-user. To show the case we illustrate three different approaches for generation in increasing complexity: (1) offline model transformation without interaction (2) model transformation with initial interaction (3) model-transformation with run-time interaction.\n\nTitle: Managing aspect orderings to support multiple quality concerns\nContent: When multiple aspects are composed undesired behavior may emerge due to the interference of aspects. Different interference management approaches have been proposed including detection and resolution of the conflicting aspects. It appears that the majority of the existing approaches have basically focused on functional correctness, whereby orderings of aspects are evaluated with respect to assumed contractual specification. Although functional correctness is an important quality concern also other quality concerns such as evolvability, reuse and reliability can demand a specific ordering. As such, the resulting possible set of orderings might need to be further reduced. In this paper we discuss the impact of other quality concerns than functional correctness, on the required orderings of aspects. Based on a domain analysis of existing approaches we provide a feature model and complementary to this a metamodel for defining aspect interference management approaches for multiple quality concerns.\n\nTitle: Modeling Traceability Of Concerns For Synchronizing Architectural Views\nContent: Software architecture modeling includes the description of different views that represent the architectural concerns from different stakeholder perspectives. In case of evolution of the software system the related architectural views need to be adapted accordingly. To synchronize the architectural views it is necessary that the dependency links among the architectural concerns in the architectural views can be easily traced. Unfortunately, despite the ongoing efforts for modeling concerns in architectural views, the traceability of concerns remains a challenging issue in architecture design. We propose the concern traceability metamodel (CTM) that enables traceability of concerns within and across architectural views. The metamodel can be used for modeling the concerns, the architectural elements and the traceability links among the elements in architectural views. We have implemented CTM in the tool M-Trace, that uses XML-based representations of the models and XQuery queries to represent tracing information. CTM and M-Trace are illustrated for analyzing the impact of concerns of a Climate Control System case and synchronizing the architectural views.\n\nTitle: Decision support for adopting SPLE with Transit-PL\nContent: It is generally acknowledged that the decision to adopt a software product line engineering (SPLE) approach needs to be performed carefully due to the different risks involved in taking such an important decision. To mitigate the potential risks of the transition to SPLE, several studies have been proposed that include many different rules for analyzing the feasibility of the SPLE adoption and the selection of transition process. However, it is not easy to apply these manually and likewise provide a proper decision with the corresponding justification. In this paper, we propose the tool Transit-PL, a web based decision support system for analyzing the feasibility of SPLE for an organization and selecting the appropriate transition strategy. Transit-PL provides a framework to build particular decision support system for selected strategies using different types of questions and corresponding rules and set of answers.\n\n## Task\nCompose an abstract for the title 'Realizing chain-wide transparency in meat supply chains based on global standards and a reference architecture.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"A reference architecture for chain-wide transparency systems is presented.Concrete architectures that provide different transparency levels are discussed.A systematic approach for deriving concrete architectures is presented.The approach is illustrated by realizing a chain-wide transparency system. One of the key concerns in meat supply chains is to provide chain-wide transparency, whereby food operators capture and share transparency data across the supply chain. To meet this concern a chain-wide transparency software system is needed that is able to address the desired stakeholder requirements. Unfortunately, designing and implementing a chain-wide transparency system is not straightforward. In this paper we provide a systematic approach for designing and implementing chain wide transparency systems. To this end, we first present a reference architecture that represents a generic design of such systems. Secondly, we discuss the systematic approach for deriving concrete architectures from the reference architecture based on stakeholders' requirements. Finally, we illustrate our approach with the design and implementation of a transparency system for beef supply chains."
      }
    ]
  },
  {
    "tid":"151",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: The complexity of non-hierarchical clustering with instance and cluster level constraints\nContent: Recent work has looked at extending clustering algorithms with instance level must-link (ML) and cannot-link (CL) background information. Our work introduces \u9a74 and \u9a74 cluster level constraints that influence inter-cluster distances and cluster composition. The addition of background information, though useful at providing better clustering results, raises the important feasibility question: Given a collection of constraints and a set of data, does there exist at least one partition of the data set satisfying all the constraints? We study the complexity of the feasibility problem for each of the above constraints separately and also for combinations of constraints. Our results clearly delineate combinations of constraints for which the feasibility problem is computationally intractable (i.e., NP-complete) from those for which the problem is efficiently solvable (i.e., in the computational class P). We also consider the ML and CL constraints in conjunctive and disjunctive normal forms (CNF and DNF respectively). We show that for ML constraints, the feasibility problem is intractable for CNF but efficiently solvable for DNF. Unfortunately, for CL constraints, the feasibility problem is intractable for both CNF and DNF. This effectively means that CL-constraints in a non-trivial form cannot be efficiently incorporated into clustering algorithms. To overcome this, we introduce the notion of a choice-set of constraints and prove that the feasibility problem for choice-sets is efficiently solvable for both ML and CL constraints. We also present empirical results which indicate that the feasibility problem occurs extensively in real world problems.\n\nTitle: Incorporating SAT solvers into hierarchical clustering algorithms: an efficient and flexible approach\nContent: The area of constrained clustering has been actively pursued for the last decade. A more recent extension that will be the focus of this paper is constrained hierarchical clustering which allows building user-constrained dendrograms/trees. Like all forms of constrained clustering, previous work on hierarchical constrained clustering uses simple constraints that are typically implemented in a procedural language. However, there exists mature results and packages in the fields of constraint satisfaction languages and solvers that the constrained clustering field has yet to explore. This work marks the first steps towards introducing constraints satisfaction languages/solvers into hierarchical constrained clustering. We make several significant contributions. We show how many existing and new constraints for hierarchical clustering, can be modeled as a Horn-SAT problem that is easily solvable in polynomial time and which allows their implementation in any number of declarative languages or efficient solvers. We implement our own solver for efficiency reasons. We then show how to formulate constrained hierarchical clustering in a flexible manner so that any number of algorithms, whose output is a dendrogram, can make use of the constraints.\n\nTitle: A flexible ILP formulation for hierarchical clustering\nContent: Hierarchical clustering is a popular approach in a number of fields with many well known algorithms. However, all existing work to our knowledge implements a greedy heuristic algorithm with no explicit objective function. In this work we formalize hierarchical clustering as an integer linear programming (ILP) problem with a natural objective function and the dendrogram properties enforced as linear constraints. Our experimental work shows that even for small data sets finding the global optimum produces more accurate results. Formalizing hierarchical clustering as an ILP with constraints has several advantages beyond finding the global optima. Relaxing the dendrogram constraints such as transitivity can produce novel problem variations such as finding hierarchies with overlapping clusterings. It is also possible to add constraints to encode guidance such as must\u2013link, cannot\u2013link, must\u2013link\u2013before etc. Finally, though exact solvers exist for ILP we show that a simple randomized algorithm and a linear programming (LP) relaxation can be used to provide approximate solutions faster.\n\nTitle: Labels vs. Pairwise Constraints: A Unified View of Label Propagation and Constrained Spectral Clustering\nContent: In many real-world applications we can model the data as a graph with each node being an instance and the edges indicating a degree of similarity. Side information is often available in the form of labels for a small subset of instances, which gives rise to two problem settings and two types of algorithms. In the label propagation style algorithms, the known labels are propagated to the unlabeled nodes. In the constrained clustering style algorithms, known labels are first converted to pair wise constraints (Must-Link and Cannot-Link), then a constrained cut is computed as a tradeoff between minimizing the cut cost and maximizing the constraint satisfaction. Both techniques are evaluated by their ability to recover the ground truth labeling, i.e. by 0/1 loss function either directly on the labels or on the pair wise relations derived from the labels. These two fields have developed separately, but in this paper, we show that they are indeed related. This insight allows us to propose a novel way to generate constraints from the propagated labels, which our empirical study shows outperforms and is more stable than the state-of-the-art label propagation and constrained spectral clustering algorithms.\n\nTitle: Using instance-level constraints in agglomerative hierarchical clustering: theoretical and empirical results\nContent: Clustering with constraints is a powerful method that allows users to specify background knowledge and the expected cluster properties. Significant work has explored the incorporation of instance-level constraints into non-hierarchical clustering but not into hierarchical clustering algorithms. In this paper we present a formal complexity analysis of the problem and show that constraints can be used to not only improve the quality of the resultant dendrogram but also the efficiency of the algorithms. This is particularly important since many agglomerative style algorithms have running times that are quadratic (or faster growing) functions of the number of instances to be clustered. We present several bounds on the improvement in the running times of algorithms obtainable using constraints.\n\n## Task\nCompose an abstract for the title 'Unifying dependent clustering and disparate clustering for non-homogeneous data' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Modern data mining settings involve a combination of attribute-valued descriptors over entities as well as specified relationships between these entities. We present an approach to cluster such non-homogeneous datasets by using the relationships to impose either dependent clustering or disparate clustering constraints. Unlike prior work that views constraints as boolean criteria, we present a formulation that allows constraints to be satisfied or violated in a smooth manner. This enables us to achieve dependent clustering and disparate clustering using the same optimization framework by merely maximizing versus minimizing the objective function. We present results on both synthetic data as well as several real-world datasets."
      }
    ]
  },
  {
    "tid":"152",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Muse: Mapping Understanding and deSign by Example\nContent: A fundamental problem in information integration is that of designing the relationships, called schema mappings, between two schemas. The specification of a semantically correct schema mapping is typically a complex task. Automated tools can suggest potential mappings, but few tools are available for helping a designer understand mappings and design alternative mappings. We describe Muse, a mapping design wizard that uses data examples to assist designers in understanding and refining a schema mapping towards the desired specification. We present novel algorithms behind Muse and show how Muse systematically guides the designer on two important components of a mapping design: the specification of the desired grouping semantics for sets of data and the choice among alternative interpretations for semantically ambiguous mappings. In every component, Muse infers the desired semantics based on the designer's actions on a short sequence of small examples. Whenever possible, Muse draws examples from a familiar database, thus facilitating the design process even further. We report our experience with Muse on some publicly available schemas.\n\nTitle: A New Framework for Designing Schema Mappings.\nContent: One of the fundamental tasks in information integration is to specify the relationships, called schema mappings, between database schemas. Schema mappings specify how data structured under a source schema is to be transformed into data structured under a target schema. The design of schema mappings is usually a non-trivial and time-intensive process and the task of designing schema mappings is exacerbated by the fact that schemas that occur in real life tend to be large and heterogeneous. Traditional approaches for designing schema mappings are either manual or performed through a user interface from which a schema mapping is interpreted from correspondences between attributes of the source and target schemas. These correspondences are either specified by the user or automatically derived by applying schema matching on the two schemas.\n\nTitle: STBenchmark: towards a benchmark for mapping systems\nContent: A fundamental problem in information integration is to precisely specify the relationships, called mappings, between schemas. Designing mappings is a time-consuming process. To alleviate this problem, many mapping systems have been developed to assist the design of mappings. However, a benchmark for comparing and evaluating these systems has not yet been developed. We present STBenchmark, a solution towards a much needed benchmark for mapping systems. We first describe the challenges that are unique to the development of benchmarks for mapping systems. After this, we describe the three components of STBenchmark: (1) a basic suite of mapping scenarios that we believe represents a minimum set of transformations that should be readily supported by any mapping system, (2) a mapping scenario generator as well as an instance generator that can produce complex mapping scenarios and, respectively, instances of varying sizes of a given schema, (3) a simple usability model that can be used as a first-cut measure on the case of use of a mapping system. We use STBenchmark to evaluate four mapping systems and report our results, as well as describe some interesting observations.\n\nTitle: Approximation Algorithms for Schema-Mapping Discovery from Data Examples.\nContent: In recent years, data examples have been at the core of several different approaches to schema-mapping design. In particular, Gottlob and Senellart introduced a framework for schema-mapping discovery from a single data example, in which the derivation of a schema mapping is cast as an optimization problem. Our goal is to refine and study this framework in more depth. Among other results, we design a polynomial-time log(n)-approximation algorithm for computing optimal schema mappings from a given set of data examples (where n is the combined size of the given data examples) for a restricted class of schema mappings; moreover, we show that this approximation ratio cannot be improved. In addition to the complexity-theoretic results, we implemented the aforementioned log(n)-approximation algorithm and carried out an experimental evaluation in a real-world mapping scenario.\n\nTitle: Comparing and evaluating mapping systems with STBenchmark\nContent: Schema mappings are fundamental building blocks in many information integration applications. Designing mappings is a time-consuming process and for that reason many mapping systems have been developed to assist in the task of designing mappings. However, to the best of our knowledge, a benchmark for comparing and evaluating these systems has not yet been developed. We demonstrate STBenchmark, a benchmark that we have developed for evaluating mapping systems. Our demonstration will showcase the different aspects of mapping systems that STBenchmark evaluates, highlight the results of our comparison and evaluation of four mapping systems, as well as make a case for the need for a standard specification input mechanism to mapping systems in order to make progress towards the development of a uniform testbed or repository for schema mappings and data exchange tasks.\n\n## Task\nCompose an abstract for the title 'Muse: a system for understanding and designing mappings' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Schema mappings are logical assertions that specify the relationships between a source and a target schema in a declarative way. The specification of such mappings is a fundamental problem in information integration. Mappings can be generated by existing mapping systems (semi-)automatically from a visual specification between two schemas. In general, the well-known 80-20 rule applies for mapping generation tools. They can automate 80% of the work, covering common cases and creating a mapping that is close to correct. However, ensuring complete correctness can still require intricate manual work to perfect portions of the mapping. Previous research on mapping understanding and refinement and anecdotal evidence from mapping designers suggest that the mapping design process can be perfected by using data examples to explain the mapping and alternative mappings. We demonstrate Muse, a data example driven mapping design tool currently implemented on top of the Clio schema mapping system. Muse leverages data examples that are familiar to a designer to illustrate nuances of how a small change to a mapping specification changes its semantics. We demonstrate how Muse can differentiate between alternative mapping specifications and infer the desired mapping semantics based on the designer's actions on a short sequence of simple data examples."
      }
    ]
  },
  {
    "tid":"153",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Prioritized Flow Optimization With Multi-Path and Network Coding Based Routing for Scalable Multirate Multicasting\nContent: In this paper, we study performance optimization for scalable video coding and multicast over networks. Multi-path video streaming, network coding based routing, and network flow control are jointly optimized to maximize a network utility function defined over heterogeneous receivers. Content priority of video coding layers is considered during the flow routing to determine the optimal multicast paths and associated data rates for each layer. Our optimization scheme attempts to find content distribution meshes with minimum path costs for each video coding layer while satisfying the inter-layer dependency during scalable video coding. Based on primal decomposition and primal-dual analysis, we develop a decentralized algorithm with two optimization levels to solve the performance optimization problem. We also prove the stability and convergence of the proposed iterative algorithm using Lyapunov theories. Extensive experimental results demonstrate that the proposed algorithm not only achieves the max-flow throughput using network coding, but also provides better video quality with balanced layered access for heterogeneous receivers.\n\nTitle: A time-based adaptive retry strategy for video streaming in 802.11 WLANs\nContent: Video streaming over time-varying, error-prone wireless LANs (WLANs) poses many challenges. One problem is that WLANs are designed without awareness of the characteristics of application data, which causes performance degradation in a noisy or congested environment. In this paper, we propose a time-based adaptive retry (TAR) mechanism for MPEG-like video streaming over 802.11 wireless networks. TAR dynamically determines whether to send or discard a packet based on its retransmission deadline instead of adopting a static retry limit uniformly over all the packets. Our approach can adapt the retry limit for each individual packet, thus providing indirect unequal error protection over different types of video frames. Analytical and simulation results show that TAR significantly improves video quality and saves channel bandwidth. We also describe a preliminary software-based implementation of TAR and use it to demonstrate the practicality of the proposed approach. Copyright (c) 2007 John Wiley & Sons, Ltd.\n\nTitle: Time-Aware Opportunistic Relay For Video Streaming Over Wlans\nContent: Robust video streaming over time-varying, error-prone wireless LANs (WLANs) poses many challenges. In this paper, we present a Time-based Opportunistic Relay (TOR) scheme for high-quality video streaming over WLANs. The proposed scheme exploits path diversity in relaying packets with awareness of the time constraints of video data. Specifically, relay nodes continuously overhear and opportunistically forward packets to enhance end-to-end delivery rates. To relay a packet in a time-aware manner, a relay deadline is computed for each packet and used by relay nodes to determine whether they should relay or discard the packet. Simulation results show that TOR improves video quality by up to 9.76 dB.\n\nTitle: Robust wireless video streaming using hybrid spatial/temporal retransmission\nContent: Bandwidth demands and timing constraints are two major challenges for wireless video streaming applications. In this paper, we present a hybrid spatial/temporal retransmission protocol that tackles both of these challenges. To increase individual throughput as well as overall network capacity, the system uses an opportunistic retransmission protocol (PRO, Protocol for Retransmitting Opportunistically) that relies on overhearing nodes distributed in physical space to function as relays that retransmit failed packets on behalf of the source. Specifically, the best relay out of the set of nodes that currently have the copy of the packet is responsible for retransmitting (relaying) the packet. Relays with stronger connectivity to the destination have a higher chance of delivering packets successfully than the source, thereby resulting in a more efficient use of the channel. To meet timing constraints, a Time-based Adaptive Retransmission strategy (TAR) is applied by both the source and the relays. With TAR, the MAC dynamically determines whether to (re)transmit or discard a packet based on the retransmission deadline of the packet assigned by the video server. This significantly reduces the number of late packet arrivals at the receiver. Our extensive evaluation results both on a testbed and in the real world demonstrate that hybrid temporal/spatial retransmission can boost streaming performance in diverse wireless environments. The benefits are most pronounced for busy networks, under fading conditions, or for mobile users.\n\nTitle: Video Streaming Over 802.11 WLAN with Content-Aware Adaptive Retry\nContent: Robust video streaming over error-prone wireless LANs (WLANs) poses many challenges. In this paper, we propose a timestamp-based content-aware adaptive retry (CAR) mechanism for MPEG video streaming over 802.11 WLANs, where the MAC dynamically determines whether to send or discard a packet based on its retransmission deadline. The retransmission deadline is assigned to each packet according to its temporal relationship and error propagation characteristics with respect to other video packets in the same GOP. The proposed scheme avoids late packets by eliminating the impact of random backoff deference and co-channel interference with proper initial delay introduced at the receiver. Simulation results show CAR significantly improves video quality and saves channel bandwidth\n\n## Task\nCompose an abstract for the title 'Design, implementation and evaluation of an efficient opportunistic retransmission protocol' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper presents an efficient opportunistic retransmission protocol (PRO, Protocol for Retransmitting Opportunistically) to improve the performance of IEEE 802.11 WLANs. PRO is a link-layer protocol that allows overhearing nodes to function as relays that retransmit on behalf of the source after they learn about a failed transmission. Relays with stronger connectivity to the destination have a higher chance of delivering the packet than the source does, thereby resulting in a more efficient use of the channel. PRO has four main features. First, channel reciprocity coupled with a run-time calibration process is used to estimate the instantaneous link quality to the destination. Second, a local qualification process filters out poor relays early. Third, a distributed relay selection algorithm chooses the best set of eligible relays among all qualified relays and prioritizes them. Finally, 802.11e Enhanced Distributed Channel Access (EDCA) is leveraged to make sure high priority relays transmit with high probability. PRO is designed to coexist with legacy 802.11 stations. We have implemented PRO in the driver of a commodity wireless card. Our extensive evaluation on both a controlled testbed and in the real world shows that PRO boosts throughput in diverse wireless environments, and especially in when there is significant contention for the channels, under fading, and with user mobility."
      }
    ]
  },
  {
    "tid":"154",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Detail-Preserving and Content-Aware Variational Multi-View Stereo Reconstruction\nContent: Accurate recovery of 3D geometrical surfaces from calibrated 2D multi-view images is a fundamental yet active research area in computer vision. Despite the steady progress in multi-view stereo (MVS) reconstruction, many existing methods are still limited in recovering fine-scale details and sharp features while suppressing noises, and may fail in reconstructing regions with less textures. To address these limitations, this paper presents a detail-preserving and content-aware variational (DCV) MVS method, which reconstructs the 3D surface by alternating between reprojection error minimization and mesh denoising. In reprojection error minimization, we propose a novel inter-image similarity measure, which is effective to preserve fine-scale details of the reconstructed surface and builds a connection between guided image filtering and image registration. In mesh denoising, we propose a content-aware \u2113p-minimization algorithm by adaptively estimating the p value and regularization parameters. Compared with conventional isotropic mesh smoothing approaches, the proposed method is much more promising in suppressing noise while preserving sharp features. Experimental results on benchmark data sets demonstrate that our DCV method is capable of recovering more surface details, and obtains cleaner and more accurate reconstructions than the state-of-the-art methods. In particular, our method achieves the best results among all published methods on the Middlebury dino ring and dino sparse data sets in terms of both completeness and accuracy.\n\nTitle: Fast neighbourhood component analysis with spatially smooth regulariser for robust noisy face recognition\nContent: For the robust recognition of noisy face images, in this study, the authors improved the fast neighbourhood component analysis (FNCA) model by introducing a novel spatially smooth regulariser (SSR), resulting in the FNCA-SSR model. The SSR can enforce local spatial smoothness by penalising large differences between adjacent pixels, and makes FNCA-SSR model robust against noise in face image. Moreover, the gradient of SSR can be efficiently computed in image space, and thus the optimisation problem of FNCA-SSR can be conveniently solved by using the gradient descent algorithm. Experimental results on several face data sets show that, for the recognition of noisy face images, FNCA-SSR is robust against Gaussian noise and salt and pepper noise, and can achieve much higher recognition accuracy than FNCA and other competing methods.\n\nTitle: Robust recognition of noisy and partially occluded faces using iteratively reweighted fitting of eigenfaces\nContent: Robust recognition of noisy and partially occluded faces is essential for an automated face recognition system, but most appearance-based methods (e.g., Eigenfaces) are sensitive to these factors. In this paper, we propose to address this problem using an iteratively reweighted fitting of the Eigenfaces method (IRF-Eigenfaces). Unlike Eigenfaces fitting, in which a simple linear projection operation is used to extract the feature vector, the IRF-Eigenfaces method first defines a generalized objective function and then uses the iteratively reweighted least-squares (IRLS) fitting algorithm to extract the feature vector by minimizing the generalized objective function. Our simulated and experimental results on the AR database show that IRF-Eigenfaces is far superior to both Eigenfaces and to the local probabilistic method in recognizing noisy and partially occluded faces.\n\nTitle: A Mixed Gauss And Directional Distance Filter For Fiber Direction Tracking\nContent: Fiber directional tracking through diffusion tensor magnetic resonance imaging (DTMRI) is a promising research field in visualization and computer graphics and is widely applied in the reconstruction of fiber orientation and the structure of biological tissues. The filter technique used to blur noise in data is of critical importance for fiber directional tracking, particularly because existing tracking methods are very sensitive to impulsive noise. In this paper, a mixed filter of the 3D Gauss and directional distance filter (GDDF) is proposed to suppress noises in corrupted vector fields. Simulation results and objective evaluation of vector datasets demonstrate that GDDF not only possesses the capability of noise attenuation but also preserves vector directions. By validating the simulated vector data against experimental heart data, it is also shown that the GDDF is an effective and stable preprocessing method that accurately reconstructs fiber orientation.\n\nTitle: Multiview stereo and silhouette fusion via minimizing generalized reprojection error.\nContent: Accurate reconstruction of 3D geometrical shape from a set of calibrated 2D multiview images is an active yet challenging task in computer vision. The existing multiview stereo methods usually perform poorly in recovering deeply concave and thinly protruding structures, and suffer from several common problems like slow convergence, sensitivity to initial conditions, and high memory requirements. To address these issues, we propose a two-phase optimization method for generalized reprojection error minimization (TwGREM), where a generalized framework of reprojection error is proposed to integrate stereo and silhouette cues into a unified energy function. For the minimization of the function, we first introduce a convex relaxation on 3D volumetric grids which can be efficiently solved using variable splitting and Chambolle projection. Then, the resulting surface is parameterized as a triangle mesh and refined using surface evolution to obtain a high-quality 3D reconstruction. Our comparative experiments with several state-of-the-art methods show that the performance of TwGREM based 3D reconstruction is among the highest with respect to accuracy and efficiency, especially for data with smooth texture and sparsely sampled viewpoints.\n\n## Task\nCompose an abstract for the title 'Adaptive NormalHedge for robust visual tracking.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a novel visual tracking framework, based on a decision-theoretic online learning algorithm namely NormalHedge. To make NormalHedge more robust against noise, we propose an adaptive NormalHedge algorithm, which exploits the historic information of each expert to perform more accurate prediction than the standard NormalHedge. Technically, we use a set of weighted experts to predict the state of the target to be tracked over time. The weight of each expert is online learned by pushing the cumulative regret of the learner towards that of the expert. Our simulation experiments demonstrate the effectiveness of the proposed adaptive NormalHedge, compared to the standard NormalHedge method. Furthermore, the experimental results of several challenging video sequences show that the proposed tracking method outperforms several state-of-the-art methods."
      }
    ]
  },
  {
    "tid":"155",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Localization of off-the-shelf mobile devices using audible sound: architectures, protocols and performance assessment\nContent: Sound source localization will play a major role in the new location-aware applications envisioned in Ubiquitous Computing. We describe the design and performance of three architectures and corresponding protocols that use a variation of the Time-of-Flight method for localizing three different kinds of devices, namely 802.11-enabled PDAs, 3G cell phones, and PDAs without network connectivity. The quantitative assessment is based on the deployment made with 6 sensors in, a 20x9m room, sewing over 10,000 localization requests. Our experiments indicate that all architectures achieve localization within 70cm of the actual position 90% of the time. The accuracy is further improved to 40cm 90% of the time when geometric factors are taken into consideration. The effects of noise and obstructions are also analyzed. Within 1m localization error realistic noise degrades the accuracy by 6 to 10%. The presence of obstacles, such as humans and cement columns, has no observable effect on the performance.\n\nTitle: An Acoustic Identification Scheme for Location Systems\nContent: Many pervasive computing applications require location awareness in order to successfully integrate technology into our daily lives. A location system consists of a group of sensors that determine the position of a mobile user and provide the user with useful context-specific information. Locating the user requires signal exchanges between the user's mobile device and the sensors. This paper considers an acoustic-based location system for pervasive computing applications. The system is comprised of a set of microphones connected to a central server. Mobile users produce acoustic signals through standard speakers, which are already available in most mobile devices, to perform location operations through the system. We focus on the design of robust acoustic signals using multi-frequency symbols that serve both in locating and uniquely identifying the user. We conduct experiments at distances between 1 and 17 ft to explore the ability of the server to recognize and decode signals originated by different users in the same general area.\n\nTitle: U-MAC: a proactive and adaptive UWB medium access control protocol\nContent: Ultra wide band (UWB) technology has received increasing recognition in recent years for its potential applications beyond radar technology to communication networks. UWB is a spread spectrum technology that requires careful coordination among communicating nodes to jointly control link power and transmission rates. Here, we present ultra wide band MAC (U-MAC), an adaptive medium access control (MAC) protocol for UWB in which nodes periodically declare their current state, so that neighbors can proactively assign power and rate values for new links locally in order to optimize global network performance. Simulations comparing U-MAC to the reactive approach confirm that U-MAC lowers link setup latency and control overhead, doubles the throughput and adapts better to high network loads. Simulations also reveal that the basic form of U-MAC favors nodes that are closer to the receiver. As a result, we also introduce novel mechanisms that control the radius around a receiver within which nodes can have fair access to it. We show through simulations the effect of the mechanisms on the tradeoff between network throughput and fair access. Copyright (c) 2005 John Wiley & Sons, Ltd.\n\nTitle: Software-driven sensor networks for short-range shallow water applications\nContent: Most existing underwater networks target deep and long range oceanic environments, which has led to the design of power hungry and expensive underwater communication hardware. Because of prohibitive monetary and energy cost of currently over-engineered communication hardware, dense deployments of shallow water sensor networks remain an elusive goal. To enable dense shallow water networks, we propose a network architecture that builds on the success of terrestrial sensor motes and that relies on the coupling of software modems and widely available speakers and microphones in sensor motes to establish acoustic communication links. In this paper, we analytically and empirically explore the potential of this acoustic communication system for the underwater environment. Our experimental approach first profiles the hardware in water after waterproofing the components with elastic membranes. The medium profiling results expose the favorable frequencies of operation for the hardware, enabling us to design a software FSK modem. Subsequently, our experiments evaluate the data transfer capability of the underwater channel with 8-frequency FSK software modems. The experiments within a 17x8m controlled underwater environment yield an error-free channel capacity of 24bps, and they also demonstrate that the system supports date rates between 6 and 48bps with adaptive fidelity.\n\nTitle: Adaptive Low Power Listening for Wireless Sensor Networks\nContent: Most sensor networks require application-specific network-wide performance guarantees, suggesting the need for global and flexible network optimization. The dynamic and nonuniform local states of individual nodes in sensor networks complicate global optimization. Here, we present a cross-layer framework for optimizing global power consumption and balancing the load in sensor networks through greedy local decisions. Our framework enables each node to use its local and neighborhood state information to adapt its routing and MAC layer behavior. The framework employs a flexible cost function at the routing layer and adaptive duty cycles at the MAC layer in order to adapt a node's behavior to its local state. We identify three state aspects that impact energy consumption: 1) number of descendants in the routing tree, 2) radio duty cycle, and 3) role. We conduct experiments on a test-bed of 14 mica2 sensor nodes to compare the state representations and to evaluate the framework's energy benefits. The experiments show that the degree of load balancing increases for expanded state representations. The experiments also reveal that all state representations in our framework reduce global power consumption in the range of one-third for a time-driven monitoring network and in the range of one-fifth for an event-driven target tracking network.\n\n## Task\nCompose an abstract for the title 'Beep: 3D Indoor Positioning Using Audible Sound' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Rapid growth in the number of wireless enabled devices has led to an increased interest in location-aware applications. The backbone of such applications is provided by a location system. In this paper we present Beep, an indoor location system that senses audible sound. The use of audible sound makes our system cheap and easily deployable to most existing roaming devices. Unlike positioning systems using ultrasound and infrared signals, Beep does not require the user to carry any kind of specialized hardware. Our system is based on standard 3D multilateration algorithms. However, the requirement of being able to locate existing devices, whose sound cards were not designed for high-precision signaling, introduces additional challenges to the location problem. This paper describes how those problems were solved and presents experimental results. Beep works with an accuracy of about 2 feet in more than 97% cases. The paper also describes a sensor deployment strategy that requires low sensor density and consequently low installation costs."
      }
    ]
  },
  {
    "tid":"156",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Algorithms and data structures for flash memories\nContent: Flash memory is a type of electrically-erasable programmable read-only memory (EEPROM). Because flash memories are nonvolatile and relatively dense, they are now used to store files and other persistent objects in handheld computers, mobile phones, digital cameras, portable music players, and many other computer systems in which magnetic disks are inappropriate. Flash, like earlier EEPROM devices, suffers from two limitations. First, bits can only be cleared by erasing a large block of memory. Second, each block can only sustain a limited number of erasures, after which it can no longer reliably store data. Due to these limitations, sophisticated data structures and algorithms are required to effectively use flash memories. These algorithms and data structures support efficient not-in-place updates of data, reduce the number of erasures, and level the wear of the blocks in the device. This survey presents these algorithms and data structures, many of which have only been described in patents until now.\n\nTitle: Characterizing the performance of flash memory storage devices and its impact on algorithm design\nContent: Initially used in digital audio players, digital cameras, mobile phones, and USB memory sticks, flash memory may become the dominant form of end-user storage in mobile computing, either completely replacing the magnetic hard disks or being an additional secondary storage. We study the design of algorithms and data structures that can exploit the flash memory devices better. For this, we characterize the performance of NAND flash based storage devices, including many solid state disks. We show that these devices have better random read performance than hard disks, but much worse random write performance. We also analyze the effect of misalignments, aging and past I/O patterns etc. on the performance obtained on these devices. We show that despite the similarities between flash memory and RAM (fast random reads) and between flash disk and hard disk (both are block based devices), the algorithms designed in the RAM model or the external memory model do not realize the full potential of the flash memory devices. We later give some broad guidelines for designing algorithms which can exploit the comparative advantages of both a flash memory device and a hard disk, when used together.\n\nTitle: Competitive analysis of flash memory algorithms\nContent: Flash memories are widely used in computer systems ranging from embedded systems to workstations and servers to digital cameras and mobile phones. The memory cells of flash devices can only endure a limited number of write cycles, usually between 10,000 and 1,000,000. Furthermore, cells containing data must be erased before they can store new data, and erasure operations erase large blocks of memory, not individual cells. To maximize the endurance of the device (the amount of useful data that can be written to it before one of its cells wears out), flash-based systems move data around in an attempt to reduce the total number of erasures and to level the wear of the different erase blocks. This data movement introduces an interesting online problem called the wear-leveling problem. Wear-leveling algorithms have been used at least since 1993, but they have never been mathematically analyzed. In this article we analyze the two main wear-leveling problems. We show that a simple randomized algorithm for one of them is essentially optimal both in the competitive sense and in the absolute sense (our competitive result relies on an analysis of a nearly-optimal offline algorithm). We show that deterministic algorithms cannot achieve comparable endurance. We also analyze a more difficult problem and show that offline algorithms for it can improve upon naive approaches, but that online algorithms essentially cannot.\n\nTitle: Competitive analysis of flash-memory algorithms\nContent: The cells of flash memories can only endure a limited number of write cycles, usually between 10,000 and 1,000,000. Furthermore, cells containing data must be erased before they can store new data, and erasure operations erase large blocks of memory, not individual cells. To maximize the endurance of the device (the amount of useful data that can be written to it before one of its cells wears out), flash-based systems move data around in an attempt to reduce the total number of erasures and to level the wear of the different erase blocks. This data movement introduces interesting online problems called wear-leveling problems. We show that a simple randomized algorithm for one problem is essentially optimal. For a more difficult problem, we show that clever offline algorithms can improve upon naive approaches, but online algorithms essentially cannot.\n\nTitle: NANDFS: a flexible flash file system for RAM-constrained systems\nContent: NANDFS is a flash file system that exposes a memory-performance tradeoff to system integrators. The file system can be configured to use a large amount of RAM, in which case it delivers excellent performance. In particular, when NANDFS is configured with the same amount of RAM that YAFFS2 uses, the performance of the two file systems is comparable (YAFFS2 is a file system that is widely used in embedded Linux and other embedded environments). But YAFFS2 and other state-of-the-art flash file systems allocate RAM dynamically and do not provide the system builder with a way to limit the amount ofmemory that they allocate. NANDFS, on the other hand, allows the system builder to configure it to use a specific amount of RAM. The performance of NANDFS degrades when the amount of RAM it uses shrinks, but the degradation is graceful, not catastrophic. NANDFS is able to provide this flexibility thanks to a novel data structure that combines a coarsegrained logical-to-physical mapping with a log-structured file system.\n\n## Task\nCompose an abstract for the title 'Mapping Structures for Flash Memories: Techniques and Open Problems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Flash memory is a type of electrically erasable programmable read-only memory (EEPROM). Because flash memories are nonvolatile and relatively dense, they are now used to store files and other persistent objects in handheld computers, mobile phones, digital cameras, portable music players, and many other computer systems in which magnetic disks are inappropriate. Flash, like earlier EEPROM devices, suffers from two limitations. First, bits can only be cleared by erasing a large block of memory. Second, each block can only sustain a limited number of erasures, after which it can no longer reliably store data. Due to these limitations, sophisticated data structures and algorithms are required to effectively use flash memories. These algorithms and data structures support efficient not-in-place updates of data, reduce the number of erasures, and level the wear of the blocks in the device. This survey presents these algorithms and data structures as well as open theoretical problems that arise in this area."
      }
    ]
  },
  {
    "tid":"157",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Mobile Robot Localization Using Fusion Of Object Recognition And Range Information\nContent: Most present localization algorithms are either range or vision-based. In many environments, only one type of sensor cannot often ensure successful localization; furthermore, using low-priced range sensors instead of expensive, but accurate, laser scanners often lead to poor performance. This paper proposes an MCL-based localization method that robustly estimates the robot pose with fusion of the range information from a low-cost IR scanner and the SIFT based visual information gathered using a mono camera. With sensor fusion, the rough pose estimation from range-based sensors is compensated by the vision-based sensors and slow object recognition can be overcome by the frequent update of the range information. In order to synchronize the two sensors with different bandwidths, the encoder information gathered during object recognition is exploited. This paper also suggests a method for evaluating localization performance that is based on the normalized probability of a vision sensor model. Various experiments show that the proposed algorithm can estimate the robot pose reasonably well and can accurately evaluate the localization performance.\n\nTitle: Federated-filter-based unmanned ground vehicle localization using 3D range registration with digital elevation model in outdoor environments\nContent: An integrated GPS/INS does not guarantee localization robustness in outdoor environments, because GPS is vulnerable to external disturbances. However, a digital elevation model (DEM) contains 3D data on the terrain over a specified area and hence can provide in-depth localization information during GPS blockage. This paper proposes federated-filter-based localization using three-dimensional (3D) range registration with a DEM. A no-reset-feedback method is used and a 3D LIDAR sensor, magnetic compass, and odometer are used to correct INS errors in GPS blockage. For 3D range registration with DEM, this paper presents a framework based on the weighted registration scheme of two transformations, pairwise registration and registration with DEM, with the INS position and attitude information. The transformation is first determined by comparing the results of two registration methods with the INS position and is then modified to replace the orientation result of 3D registration with the INS attitude. A multilayered DEM approach using the height of the integrated system is also used to constrain the search range of DEM into three layers near the current unmanned ground vehicle (UGV) position when the corresponding point is searched for in the DEM. Experimental results show that the proposed localization algorithm can greatly enhance the robustness and accuracy of UGV localization in outdoor environments. \u00a9 2012 Wiley Periodicals, Inc. \u00a9 2012 Wiley Periodicals, Inc.\n\nTitle: Monocular Vision-Based SLAM in Indoor Environment Using Corner, Lamp, and Door Features From Upward-Looking Camera\nContent: We examine monocular vision-based simultaneous localization and mapping (SLAM) of a mobile robot using an upward-looking camera. Although a monocular camera looking up toward the ceiling can provide a low-cost solution to indoor SLAM, this approach is often unable to achieve dependable navigation due to a lack of reliable visual features on the ceiling. We propose a novel approach to monocular SLAM using corner, lamp, and door features simultaneously to achieve stable navigation in various environments. We use the corner features and the circular-shaped brightest parts of the ceiling image for detection of lamp features. Furthermore, vertical and horizontal lines are combined to robustly detect line-based door features to reduce the problem that line features can be easily misidentified due to nearby edges. The use of these three types of features as landmarks increases our ability to observe the features in various environments and maintains the stability of the SLAM process. A series of experiments in indoor environments showed that the proposed scheme resulted in dependable navigation.\n\nTitle: Sonar Sensor-Based Efficient Exploration Method Using Sonar Salient Features and Several Gains\nContent: This paper describes a sonar sensor-based exploration method. To build an accurate map in an unknown environment during exploration, a simultaneous localization and mapping problem must be solved. Therefore, a new type of sonar feature called a \"sonar salient feature\" (SS-feature), is proposed for robust data association. The key concept of an SS-feature is to extract circle feature clouds on salient convex objects from environments by associating sets of sonar data. The SS-feature is used as an observation in the extended Kalman filter (EKF)-based SLAM framework. A suitable strategy is needed to efficiently explore the environment. We used utilities of driving cost, expected information about an unknown area, and localization quality. Through this strategy, the exploration method can greatly reduce behavior that leads a robot to explore a previously visited place, and thus shorten the exploration distance. A robot can select a favorable path for localization by localization gain during exploration. Thus, the robot can estimate its pose more robustly than other methods that do not consider localizability during exploration. This proposed exploration method was verified by various experiments, and it ensures that a robot can build an accurate map fully autonomously with sonar sensors in various home environments.\n\nTitle: Robust mobile robot localization using optical flow sensors and encoders\nContent: Open-loop estimation methods are commonly used in mobile robot applications. Their strength lies in the speed and simplicity of an estimate. However, these methods can sometimes lead to inaccurate or unreliable positional estimates. Using one or more optical flow sensors, a method has been developed which can accurately track position in both ideal kinematic conditions and otherwise. Using optical flow techniques and available sensors, reliable positional estimates are made. The sensor provides accurate measurement of the movement at the sensor location. Even though the sensor does not provide angular displacement, the robot movement is estimated using only one sensor even with wheel slip. However, when the robot moves sideways due to external disturbance, redundant sensors are used in order to estimate the configuration of the robot. Pseudo inverse based estimation and the extended Kalman filter based estimation are presented to show the effectiveness of the proposed approach. Location of the sensors has also been investigated in order to minimize errors caused by inaccurate sensor readings. Finally, the method is implemented and tested using a potential field based navigation scheme.\n\n## Task\nCompose an abstract for the title 'Sensor fusion-based exploration in home environments using information, driving and localization gains' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We proposed a sensor fusion-based exploration scheme in home environments.The goal is to optimize the exploration efficiency and improve the map quality.In complex environments, the proposed method showed excellent performance.The localization gain enables the robot to accurately estimate its pose.The driving gain enables the robot to explore unknown environments very efficiently. Exploration is one of the most important functions for a mobile service robot because a map is required to carry out various tasks. A suitable strategy is needed to efficiently explore an environment and to build an accurate map. This study proposed the use of several gains (information, driving, localization) that, if considered during exploration, can simultaneously improve the efficiency of the exploration process and quality of the resulting map. Considering the information and driving gains reduces behavior that leads a robot to explore a previously visited place, and thus the exploration distance is reduced. In addition, the robot can select a favorable path for localization by considering the localization gain during exploration, and the robot can estimate its pose more robustly than other methods that do not consider localizability during exploration. This proposed exploration method was verified by various experiments, which verified that a robot can build an accurate map fully autonomously and efficiently in various home environments using the proposed method."
      }
    ]
  },
  {
    "tid":"158",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A novel hardware plane fitting implementation and applications for bionic vision.\nContent: This paper presents a high-speed real-time plane fitting implementation on a field-programmable gate array (FPGA) platform. A novel hardware-based least squares algorithm fits planes to patches of points within a depth image captured using a Microsoft Kinect v2 sensor. The validity of a plane fit and the plane parameters are reported for each patch of 11 by 11 depth pixels. The high level of parallelism of operations in the algorithm has allowed for a fast, low-latency hardware implementation on an FPGA that is capable of processing depth data at a rate of 480 frames per second. A hybrid hardware---software end-to-end system integrates the hardware solution with the Kinect v2 sensor via a computer and PCI express communication link to a Terasic TR4 FPGA development board. We have also implemented two proof-of-concept object detection applications as future candidates for bionic vision systems. We show that our complete end-to-end system is capable of running at 60 frames per second. An analysis and characterisation of the Kinect v2 sensor errors has been performed in order to specify logic precision requirements, statistical testing of the validity of a plane fit, and achievable plane fitting angle resolution.\n\nTitle: Psychophysics Testing Of Bionic Vision Image Processing Algorithms Using An Fpga Hatpack\nContent: Different image processing approaches are presented that are candidates for the Monash Vision Group prosthetic vision device. As described in a companion paper [6], the Monash Vision Group is developing a bionic eye based on the implantation of 7-11 stimulation tiles on the primary visual cortex of the brain. In the lead up to an expected 2014 first in-human trial of this device, potential image processing techniques and intuitive user interfaces need to be developed and evaluated. An FPGA Hatpack has been developed to give normally sighted people a similar binary limited resolution visual experience expected of a Bionic Eye recipient. The Hatpack has been used to generate new psychophysics results to evaluate and improve the performance and practicality of three different methods of luminance threshold selection. The results show an interesting difference in terms of performance and highlight areas of possible improvement for the algorithms used.\n\nTitle: Multi-Sensory synergies in Humanoid Robotics\nContent: Sensing is a key element for any intelligent robotic system. This paper describes current progress of a project in the Intelligent Robotics Research Center at Monash University that has the aim of developing a synergistic set of sensory systems for a humanoid robot. Currently sensing modes for colour vision, stereo vision, active range, smell and airflow are being developed in a size and form that is compatible with the humanoid appearance. Essential considerations are sensor calibration and the processing of sensor data to give reliable information about properties of the robot's environment. In order to demonstrate the synergistic use of all of the available sensory modes a high level supervisory control scheme is being developed for the robot. All time-stamped sensor data together with derived information about the robot's environment are organized in a blackboard system. Control action sequences are then derived from the blackboard data based on a task description. The paper presents details of each of the robot's sensory systems, sensor calibration, and supervisory control. Results are also presented of a demonstration project that involves identifying and selecting mugs containing household chemicals. Proposals for future development of the humanoid robot are also presented.\n\nTitle: Integration of robust visual perception and control for a domestic humanoid robot\nContent: This paper describes a complete vision-based framework that enables a humanoid robot to perform simple manipulations in a domestic environment. Our system empha- sizes autonomous operation with minimal a priori knowledge in an unstructured environment, with robustness to visual distractions and calibration errors. For each new task, the robot first acquires a dense 3D image of the scene using our novel stereoscopic light stripe scanner that rejects secondary reflections and cross-talk. A data-driven analysis of the range map identifies and models simple objects using geometric primitives. Objects are reliably tracked through clutter and occlusions by exploiting multimodal cues (colour, texture and edges). Finally, manipulations are performed by controlling the end-effector using a hybrid position-based visual servoing scheme that fuses visual and kinematic measurements and compensates for calibration errors. Two domestic tasks are implemented to evaluate the performance of the framework: identifying and grasping a yellow box without any prior knowledge of the object, and pouring rice from an inter- actively selected cup into a bowl. I. I NTRODUCTION\n\nTitle: Discovering and restoring changes in object positions using an autonomous robot with laser rangefinders\nContent: This paper describes an object rearrangement system for an autonomous mobile robot. The objective of the robot is to autonomously explore and learn about an environment, to detect changes in the environment on a later visit after object disturbances and finally, to move objects back to their original positions. In the implementation, it is assumed that the robot does not have any prior knowledge of the environment and the positions of the objects. The system exploits Simultaneous Localisation and Mapping (SLAM) and autonomous exploration techniques to achieve the task. These techniques allow the robot to perform localisation and mapping which is required to perform the object rearrangement task autonomously. The system includes an arrangement change detector, object tracking and map update that work with a Polar Scan Match (PSM) Extended Kalman Filter (EKF) SLAM system. In addition, a path planning technique for dragging and pushing an object is also presented in this paper. Experimental results of the integrated approach are shown to demonstrate that the proposed approach provides real-time autonomous object rearrangements by a mobile robot in an initially unknown real environment. Experiments also show the limits of the system by investigating failure modes.\n\n## Task\nCompose an abstract for the title 'Transformative Reality: improving bionic vision with robotic sensing.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Implanted visual prostheses provide bionic vision with very low spatial and intensity resolution when compared against healthy human vision. Vision processing converts camera video to low resolution imagery for bionic vision with the aim of preserving salient features such as edges. Transformative Reality extends and improves upon traditional vision processing in three ways. Firstly, a combination of visual and non-visual sensors are used to provide multi-modal data of a person's surroundings. This enables the sensing of features that are difficult to sense with only a camera. Secondly, robotic sensing algorithms construct models of the world in real time. This enables the detection of complex features such as navigable empty ground or people. Thirdly, models are visually rendered so that visually complex entities such as people can be effectively represented in low resolution. Preliminary simulated prosthetic vision trials, where a head mounted display is used to constrain a subject's vision to 25\u00d725 binary phosphenes, suggest that Transformative Reality provides functional bionic vision for tasks such as indoor navigation, object manipulation and people detection in scenes where traditional processing is unusable."
      }
    ]
  },
  {
    "tid":"159",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Iterative design of MOVE: A situationally appropriate vehicle navigation system\nContent: Drivers need assistance when navigating an unfamiliar route. In-vehicle navigation systems have improved in recent years due to the technology advances, but are sometimes problematic because of information overload while driving. To address the attentional demands of reading a map while driving, we have developed the maps optimized for vehicular environments (MOVE) in-car navigation display, which provides situationally appropriate navigation information to the driver through optimization of map information. In this paper, we describe the iterative design and evaluation process that shaped the MOVE system. We describe early map reading and navigation studies that led to early designs for our system. We present a study on visual search tasks that refined the renditions used for the system. Finally, we present a study on the effectiveness of several variations of a contextually optimized route map visualization with a desktop steering system. The result of this study shows that MOVE's contextually optimized navigation information can reduce the driver's perceptual load significantly. Our laboratory experiment shows that the total map display fixation time was decreased six-fold, and the number of glances to interpret the map display were decreased about threefold, when comparing the contextually optimized display to a static display.\n\nTitle: Companion: A Constraint-Optimizing Method For Person-Acceptable Navigation\nContent: This paper introduces the COMPANION framework: a Constraint-Optimizing Method for Person-Acceptable NavigatION. In this framework, human social conventions, such as personal space and tending to one side of hallways, are represented as constraints on the robot's navigation. These constraints are accounted for at the global planning level. In this paper, we present the rationale for, and implementation of, this framework, and we describe the experiments we have run in simulation to verify that the method produces human-like behavior in a mobile robot. Our approach is novel in that it can express an arbitrary number of social conventions and explicitly accounts for these conventions in the planning phase.\n\nTitle: Studying the effectiveness of MOVE: a contextually optimized in-vehicle navigation system\nContent: In-vehicle navigation has changed substantially in recent years, due to the advent of computer generated maps and directions. However, these maps are still problematic, due to a mismatch between the complexity of the maps and the attentional demands of driving. In response to this problem, we are developing the MOVE (Maps Optimized for Vehicular Environments) system. This system will provide situationally appropriate map information by presenting information that uses appropriate amounts of the driver's attention. In this paper, we describe our findings of studies to help shape the design of the MOVE system, including studies on map reading and in-vehicle navigation, and studies on the effectiveness of a variety of contextually optimized route map visualizations in a simulated driving context.Results show that contextually optimized displays designed for the MOVE system should significantly reduce perceptual load in the context of driving. In our laboratory experiment there was a six-fold decrease in the total map display fixation time and nearly threefold decrease in the number of glances needed to interpret the contextually optimized display compared to a static display.\n\nTitle: Exiting the Design Studio: Leveraging Online Participants for Early-Stage Design Feedback\nContent: Online collaboration tools enable developers of interactive systems to quickly reach potential users for usability testing. Can these technologies serve designers who seek feedback on user needs during the earliest stages of design? Online needfinding may help designers create products and services that can target a more diverse user population. To explore this, we conducted a feasibility study to compare face-to-face methods with online needfinding sessions. We found that video can sufficiently capture nuanced reactions to preliminary concept storyboards, but that feedback providers need guidance and structure. We then introduce a tool for collecting early-stage design feedback from online participants and conduct a case study with a professional design team. The team conducted needfinding activities with local participants, as well as a cost-equivalent number of online participants The case study demonstrates that combining online crowdsourcing with a video survey tool provides a simple and cost-efficient way to collect early-stage feedback.\n\nTitle: Understanding my data, myself: supporting self-reflection with ubicomp technologies\nContent: We live in a world where many kinds of data about us can be collected and more will be collected as Ubicomp technologies mature. People reflect on this data using different tools for personal informatics. However, current tools do not have sufficient understanding of users' self-reflection needs to appropriately leverage Ubicomp technologies. To design tools that effectively assist self-reflection, we need to comprehensively understand what kinds of questions people have about their data, why they ask these questions, how they answer them with current tools, and what kinds of problems they encounter. To explore this, we conducted interviews with people who use various kinds of tools for personal informatics. We found six kinds of questions that people asked about their data. We also found that certain kinds of questions are more important at certain times, which we call phases. We identified two phases of reflection: Discovery and Maintenance. We discuss the kinds of questions and the phases in detail and identify features that should be supported in personal informatics tools for which Ubicomp technologies can play an important role.\n\n## Task\nCompose an abstract for the title 'Where should i turn: moving from individual to collaborative navigation strategies to inform the interaction design of future navigation systems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The design of in-vehicle navigation systems fails to take into account the social nature of driving and automobile navigation. In this paper, we consider navigation as a social activity among drivers and navigators to improve design of such systems. We explore the implications of moving from a map-centered, individually-focused design paradigm to one based upon collaborative human interaction during the navigation task. We conducted a qualitative interaction design study of navigation among three types of teams: parents and their teenage children, couples, and unacquainted individuals. We found that collaboration varied among these different teams, and was influenced by social role, as well as the task role of driver or navigator. We also found that patterns of prompts, maneuvers, and confirmations varied among the three teams. We identify overarching practices that differ greatly from the literature on individual navigation. From these discoveries, we present design implications that can be used to inform future navigation systems."
      }
    ]
  },
  {
    "tid":"160",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Problems and Promises of Using the Cloud and Biometrics\nContent: This paper explores the mutual benefits of biometrics technology and cloud computing. Cloud computing can take advantage of the strong authentication property of biometrics in order to enhance the security of the cloud and to introduce new service models (i.e. Biometric authentication as a Service (BioAaaS)). On the other hand, biometrics technology can leverage the cloud's unbounded computational resources and attractive properties of flexibility, scalability, and cost reduction in order to reduce the cost of the biometrics system requirements of different computational resources (i.e. processing power or data storage) and to enhance the performance of biometrics systems' processes (i.e. biometric matching). In this paper, we explore five potential areas where there are mutual advantages between the cloud and biometrics. For each of these five areas, the current status and how biometrics/cloud can enhance or solve problems of the current state-of-the-art is investigated. Next, the challenges facing biometric/cloud applications in each area are discussed. Finally, the literature of applying biometrics/cloud in each area is surveyed.\n\nTitle: Revocable Fingerprint Biotokens: Accuracy and Security Analysis\nContent: This paper reviews the biometric dilemma, the pending threat that may limit the long-term value of biometrics in security applications. Unlike passwords, if a biometric database is ever compromised or improperly shared, the underlying biometric data cannot be changed. The concept of revocable or cancelable biometric-based identity tokens (biotokens), if properly implemented, can provide significant enhancements in both privacy and security and address the biometric dilemma.The key to effective revocable biotokens is the need to support the highly accurate approximate matching needed in any biometric system as well as protecting privacy/security of the underlying data. We briefly review prior work and show why it is insufficient in both accuracy and security.This paper adapts a recently introduced approach that separates each datum into two fields, one of which is encoded and one which is left to support the approximate matching. Previously applied to faces, this paper uses this approach to enhance an existing fingerprint system. Unlike previous work in privacy-enhanced biometrics, our approach improves the accuracy of the underlying system! The security analysis of these biotokens includes addressing the crucial issue of protection of small fields.The resulting algorithm is tested on three different fingerprint verification challenge datasets and shows an average decrease in the Equal Error Rate of over 30% - providing improved security and improved privacy.\n\nTitle: Secure remote matching with privacy: Scrambled support vector vaulted verification (S^2V^3)\nContent: As biometric authentication systems become common in everyday use, researchers are beginning to address privacy issues in biometric recognition. With the growing use of mobile devices, it is important to develop approaches that support remote mobile verification. This paper outlines the need for a mobile/remote SVM-based authentication system that does not compromise the privacy of the subject being recognized. We discuss limitations of earlier privacy-preserving authentication systems and present necessary privacy and security requirements that make a system attractive from both the server's security point of view and from the client's privacy-centric point of view. We then present a novel protocol we call \"Vaulted Verification\" that allows a server to remotely authenticate a client's biometric in a privacy preserving way. We conclude with a small evaluation of performance, discussion of security implications, and ideas for future work.\n\nTitle: Improving Variance Estimation in Biometric Systems\nContent: Measuring system performance seems conceptually straightforward. However, the interpretation of the results and predicting future performance remain as exceptional challenges in system evaluation. Robust experimental de- sign is critical in evaluation, but there have been very few techniques to check designs for either overlooked associa- tions or weak assumptions. For biometric & vision system evaluation, the complexity of the systems make a thorough exploration of the problem space impossible \u2014 this lack of verifiability in experimental design is a serious issue. In this paper, we present a new evaluation methodology that im- proves the accuracy of variance estimator via the discovery of false assumptions about the homogeneity of cofactors \u2014 i.e., when the data is not \"well mixed.\" The new method- ology is then applied in the context of a biometric system evaluation with highly influential cofactors.\n\nTitle: PRIVV: Private remote iris-authentication with Vaulted Verification\nContent: Iris biometrics are one of the strongest modalities in terms of performance, and as such, are used in several high-security scenarios. However, remote/network authentication creates its own class of problems that have not previously been considered. In this work, we adapt Vaulted Verification, a novel remote matching protocol, to work with iris biometrics in a network authentication setting. We give an overview of how Vaulted Verification works and show how to adapt it to the unique challenges of iris biometrics. We then investigate the algorithm's security and evaluate its performance, showing that Vaulted Verification is among the top-performing template protection algorithms for the CASIAv1 dataset. We conclude with a summary and ideas for future work.\n\n## Task\nCompose an abstract for the title 'id-Privacy in large scale biometric systems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Balancing privacy and security concerns in biometric systems is an area of growing importance. While important work has gone on in template protection and revocable biometric tokens, these avenues of research address only one aspect of the problem. Such research does not address a critical issue: balancing the need government and anti-fraud programs to do deduplication (ensure one identity per person) against the potential for abuse using that data. Any existing system capable of deduplication, even if using a template protection scheme, would allow function creep or abuse by searching with latent prints. This paper introduces the concept of id-privacy, requiring at least i items (e.g. fingers) to be provided to resolve identity to better than d above random chance. We show how using cross-finger representation on unsegmented fingerprint slap data, we can address what may be the single most important \u201cprivacy\u201d issue in biometrics, privacy enhanced deduplication. We prove we can achieve (2,0)-id-privacy for fingerprint-based deduplication while preventing searching with a latent print. We introduce the Forest Finger algorithm - an approach for matching unsegmented slaps and cross-finger representations. Our results on the largest public slap database shows superior accuracy when compared with existing NIST Bozorth matcher when tested on unsegmented slaps, segmented prints or fused rolled prints."
      }
    ]
  },
  {
    "tid":"161",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Forwarding metamorphosis: fast programmable match-action processing in hardware for SDN\nContent: In Software Defined Networking (SDN) the control plane is physically separate from the forwarding plane. Control software programs the forwarding plane (e.g., switches and routers) using an open interface, such as OpenFlow. This paper aims to overcomes two limitations in current switching chips and the OpenFlow protocol: i) current hardware switches are quite rigid, allowing ``Match-Action'' processing on only a fixed set of fields, and ii) the OpenFlow specification only defines a limited repertoire of packet processing actions. We propose the RMT (reconfigurable match tables) model, a new RISC-inspired pipelined architecture for switching chips, and we identify the essential minimal set of action primitives to specify how headers are processed in hardware. RMT allows the forwarding plane to be changed in the field without modifying hardware. As in OpenFlow, the programmer can specify multiple match tables of arbitrary width and depth, subject only to an overall resource limit, with each table configurable for matching on arbitrary fields. However, RMT allows the programmer to modify all header fields much more comprehensively than in OpenFlow. Our paper describes the design of a 64 port by 10 Gb/s switch chip implementing the RMT model. Our concrete design demonstrates, contrary to concerns within the community, that flexible OpenFlow hardware switch implementations are feasible at almost no additional cost or power.\n\nTitle: Fast and Scalable Conflict Detection for Packet Classifiers\nContent: Packet filters provide roles for classifying packets based on header fields. High speed packet classification has received much study. However, the twin problems of fast updates and fast conflict detection have not received much attention. A conflict occurs when two classifiers overlap, potentially creating ambiguity for packets that match both filters. For example, if Rule 1 specifies that all packets going to CNN be rote controlled and Rule 2 specifies that all packets coming from Walmart be given high priority, the roles conflict for traffic from Walmart to CNN. There has been prior work on efficient conflict detection for two dimensional classifiers. However, the best known algorithm for conflict detection for geneml classifiers is the naive O(N2) algorithm of comparing each pair of rules for a conflict. In this paper, we describe an efficient and scalable conflict detection algorithm for the general casethat is significantly faster. For example, for a database of 20,000 roles, our algorithm is 40 times faster. than the naive implementation. Even without considering conflicts, our algorithm also provides a packet classifier with fast updates and fast lookups that can be used for stateful packet filtering.\n\nTitle: Scalable high speed IP routing lookups\nContent: Internet address lookup is a challenging problem because of increasing routing table sizes, increased traffic, higher speed links, and the migration to 128 bit IPv6 addresses. IP routing lookup requires computing the best matching prefix, for which standard solutions like hashing were believed to be inapplicable. The best existing solution we know of, BSD radix tries, scales badly as IP moves to 128 bit addresses. Our paper describes a new algorithm for best matching prefix using binary search on hash tables organized by prefix lengths. Our scheme scales very well as address and routing table sizes increase: independent of the table size, it requires a worst case time of log2(address bits) hash lookups. Thus only 5 hash lookups are needed for IPv4 and 7 for IPv6. We also introduce Mutating Binary Search and other optimizations that, for a typical IPv4 backbone router with over 33,000 entries, considerably reduce the average number of hashes to less than 2, of which one hash can be simplified to an indexed array access. We expect similar average case behavior for IPv6.\n\nTitle: Fast packet classification for two-dimensional conflict-free filters\nContent: Routers can use packet classification to support advanced functions such as QoS routing, virtual private networks and access control. Unlike traditional routers, which forward packets based on destination address only, routers with packet classification capability can forward packets based on multiple header fields, such as source address, protocol type, or application port numbers. The destination-based forwarding can be thought of as one-dimensional packet classification.While several efficient solutions are known for the one-dimensional IP lookup problem, the multi-dimensional packet classification has proved to be far more difficult. While an O(logw) time scheme is known for the IP lookup, Srinivisan et al. IV. Srinivasan, S. Suri, G. Varghese, Packet classification using tuple space search, in: Proceedings of SIGCOMM'99, 1999] show a lower bound of \u03a9(wk-1) for k-dimensional filter lookup, where w is the number of bits in a header field. In particular, this lower bound precludes the possibility of a binary search like scheme even for two-dimensional filters (say, IP source and destination pairs).In this paper, we examine this lower bound more closely, and discover that the lower bound depends crucially on conflicts in the filter database. We then show that for two-dimensional conflict-free filters, a binary search scheme does work! Our lookup scheme requires O(log2 w) hashes in the worst case, and uses O(nlog2 w) memory. Alternatively, our algorithm can be viewed as making O(log w) calls to a prefix lookup scheme.It has been observed in practice that filter databases have very few conflicts, and these conflicts can be removed by adding additional filters (one per conflict). Thus, our scheme may also be quite practical. Our simulation and experimental results show that the proposed scheme also performs as good as or better than existing schemes. For example, on real firewall data sets with over 200 rules consisting of source and destination IP prefixes, our algorithm performs worst case 12 hashes. For filter sets containing arbitrarily many filtering rules with IP prefixes, the worst case search time guaranteed is utmost 25 hashes.\n\nTitle: Fast and scalable layer four switching\nContent: In Layer Four switching, the route and resources allocated to a packet are determined by the destination address as well as other header fields of the packet such as source address, TCP and UDP port numbers. Layer Four switching unifies firewall processing, RSVP style resource reservation filters, QoS Routing, and normal unicast and multicast forwarding into a single framework. In this framework, the forwarding database of a router consists of a potentially large number of filters on key header fields. A given packet header can match multiple filters, so each filter is given a cost, and the packet is forwarded using the least cost matching filter.In this paper, we describe two new algorithms for solving the least cost matching filter problem at high speeds. Our first algorithm is based on a grid-of-tries construction and works optimally for processing filters consisting of two prefix fields (such as destination-source filters) using linear space. Our second algorithm, cross-producting, provides fast lookup times for arbitrary filters but potentially requires large storage. We describe a combination scheme that combines the advantages of both schemes. The combination scheme can be optimized to handle pure destination prefix filters in 4 memory accesses, destination-source filters in 8 memory accesses worst case, and all other filters in 11 memory accesses in the typical case.\n\n## Task\nCompose an abstract for the title 'Fast firewall implementations for software and hardware-based routers' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Routers must perform packet classification at high speeds to efficiently implement functions such as firewalls and diffserv. Classification can be based on an arbitrary number of fields in the packet header. Performing classification quickly on an arbitrary number of fields is known to be difficult, and has poor worst-case complexity.In this paper, we re-examine two basic mechanisms that have been dismissed in the literature as being too inefficient: backtracking search and set pruning tries. We find using real databases that the time for backtracking search is much better than the worst-case bound; instead of \u03a9((logN)k-1), the search time is only roughly twice the optimal search time1. Similarly, we find that set pruning tries (using a DAG optimization) have much better storage costs than the worstcase bound. We also propose several new techniques to further improve the two basic mechanisms. Our major ideas are (i) backtracking search on a small memory budget, (ii) a novel compression algorithm, (iii) pipelining the search, (iv) the ability to trade-off smoothly between backtracking and set pruning, and (v) algorithms to effectively make use of hardware if hardware is available. We quantify the performance gain of each technique using real databases. We show that on real firewall databases our schemes, with the accompanying optimizations, are close to optimal in time and storage."
      }
    ]
  },
  {
    "tid":"162",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Exploiting High Dimensional Video Features Using Layered Gaussian Mixture Models\nContent: Analysis of video data usually requires training classifiers in high dimensional feature spaces. This paper proposes a layered Gaussian mixture model (LGMM) to exploit high dimensional features for classifying various shots in video. LGMM decomposes a high dimensional feature space by building a pyramid structure and estimating the distribution of local partitions in each layer using Gaussian mixtures from the bottom of the pyramid to the top. We reduce the dimension of features in each local region at a lower layer by projecting them onto the estimated Gaussian components. These projected feature vectors are then used to estimate the Gaussian mixture models at a upper layer. The final dimension of the feature is adjustable by choosing the number of Gaussians at the top layer of the pyramid. We demonstrate the proposed method using motion features to classify video shots. The proposed method is independent from low level features and can be extended to other classification tasks\n\nTitle: A discriminative learning framework with pairwise constraints for video object classification.\nContent: To deal with the problem of insufficient labeled data in video object classification, one solution is to utilize additional pairwise constraints that indicate the relationship between two examples, i.e., whether these examples belong to the same class or not. In this paper, we propose a discriminative learning approach which can incorporate pairwise constraints into a conventional margin-based learning framework. Different from previous work that usually attempts to learn better distance metrics or estimate the underlying data distribution, the proposed approach can directly model the decision boundary and, thus, require fewer model assumptions. Moreover, the proposed approach can handle both labeled data and pairwise constraints in a unified framework. In this work, we investigate two families of pairwise loss functions, namely, convex and nonconvex pairwise loss functions, and then derive three pairwise learning algorithms by plugging in the hinge loss and the logistic loss functions. The proposed learning algorithms were evaluated using a people identification task on two surveillance video data sets. The experiments demonstrated that the proposed pairwise learning algorithms considerably outperform the baseline classifiers using only labeled data and two other pairwise learning algorithms with the same amount of pairwise constraints.\n\nTitle: SmartLabel: an object labeling tool using iterated harmonic energy minimization\nContent: Labeling objects in images is an essential prerequisite for many visual learning and recognition applications that depend on training data, such as image retrieval, object detection and recognition. Manually creating labels in images is not only time-consuming but also subject to human labeling errors, and eventually, becomes impossible for a large scale image database. Semi-supervised learning (SSL)algorithms such as Gaussian random field (GRF)can be applied to labeling objects in images since they have the ability to include a large amount of unlabeled data while requiring only a small amount of labeled data. However, the one-shot property of GRF prevents it from achieving good labeling performance. In this paper, we presents a novel object labeling tool, SmartLabel, to semi-automatically label objects in images. The algorithm of SmartLabel has four innovations over GRF:1)soft labeling,2)graph construction with spatial constraints, 3)iterated harmonic energy minimization, and 4)using relevance feedback to incorporate human interaction in the loop. As demonstrated in datasets of six object categories, the proposed SmartLabel not only works effectively even with a very small amount of user input (e.g., 1 .5%of image size)but also achieves significant improvement over GRF.\n\nTitle: Semi-supervised learning of object categories from paired local features\nContent: This paper presents a semi-supervised learning (SSL) approach to find similarities of images using statistics of local matches. SSL algorithms are well known for leveraging a large amount of unlabeled data as well as a small amount of labeled data to boost classification performance. Our approach proposes to formulate the problem of matching two images as an SSL based classification problem of image pairs with a minimal amount of labeled pairs. We apply a Gaussian random field model to represent each image pair as vertices in a weighted graph and the optimal configuration of the field is obtained by harmonic energy minimization. A symmetrical feature selection criterion is first introduced to select robust matches of local keypoints between two images. The Mallows distance is then adopted to combine multiple cues from statistics of local matches. Our experiments confirm that our SSL based approach not only boost classification performance but also improve robustness of the learned category model using only simple local keypoint features.\n\nTitle: Modeling Background and Segmenting Moving Objects from Compressed Video\nContent: Modeling background and segmenting moving objects are significant techniques for video surveillance and other video processing applications. Most existing methods of modeling background and segmenting moving objects mainly operate in the spatial domain at pixel level. In this paper, we present three new algorithms (running average, median, mixture of Gaussians) modeling background directly from compressed video, and a two-stage segmentation approach based on the proposed background models. The proposed methods utilize discrete cosine transform (DCT) coefficients (including ac coefficients) at block level to represent background, and adapt the background by updating DCT coefficients. The proposed segmentation approach can extract foreground objects with pixel accuracy through a two-stage process. First a new background subtraction technique in the DCT domain is exploited to identify the block regions fully or partially occupied by foreground objects, and then pixels from these foreground blocks are further classified in the spatial domain. The experimental results show the proposed background modeling algorithms can achieve comparable accuracy to their counterparts in the spatial domain, and the associated segmentation scheme can visually generate good segmentation results with efficient computation. For instance, the computational cost of the proposed median and MoG algorithms are only 40.4% and 20.6% of their counterparts in the spatial domain for background construction.\n\n## Task\nCompose an abstract for the title 'Automatically Labeling Video Data Using Multi-class Active Learning' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Labeling video data is an essential prerequisite for many visionapplications that depend on training data, such as visualinformation retrieval, object recognition, and human activitymodeling. However, manually creating labels is not onlytime-consuming but also subject to human errors, and eventually,becomes impossible for a very large amount of data (e.g. 24/7surveillance video). To minimize the human effort in labeling, wepropose a unified multi-class active learning approach forautomatically labeling video data. The contributions of this paperinclude extending active learning from binary classes to multipleclasses and evaluating several practical sample selectionstrategies. The experimental results show that the proposedapproach works effectively even with a significantly reduced amountof labeled data. The best sample selection strategy can achievemore than a 50% error reduction over random sample selection."
      }
    ]
  },
  {
    "tid":"163",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Building Association-Rule Based Sequential Classifiers for Web-Document Prediction\nContent: Web servers keep track of web users' browsing behavior in web logs. From these logs, one can build statistical models that predict the users' next requests based on their current behavior. These data are complex due to their large size and sequential nature. In the past, researchers have proposed different methods for building association-rule based prediction models using the web logs, but there has been no systematic study on the relative merits of these methods. In this paper, we provide a comparative study on different kinds of sequential association rules for web document prediction. We show that the existing approaches can be cast under two important dimensions, namely the type of antecedents of rules and the criterion for selecting prediction rules. From this comparison we propose a best overall method and empirically test the proposed model on real web logs.\n\nTitle: Scalable collaborative filtering using incremental update and local link prediction\nContent: The traditional collaborative filtering approaches have been shown to suffer from two fundamental problems: data sparsity and difficulty in scalability. To address these problems, we present a novel scalable item-based collaborative filtering method by using incremental update and local link prediction. By subdividing the computations and analyzing the factors in different cases of item-to-item similarity, we design the incremental update strategies in item-based CF, which can make the recommender system more efficient and scalable. Based on the transitive structure of item similarity graph, we use the local link prediction method to find implicit candidates to alleviate the lack of neighbors in predictions and recommendations caused by the sparsity of data. The experiment results validate that our algorithm can improve the performance of traditional CF, and can increase the efficiency in recommendations.\n\nTitle: An effective clustering approach to web query log anonymization\nContent: Web query log data contain information useful to research; however, release of such data can re-identify the search engine users issuing the queries. These privacy concerns go far beyond removing explicitly identifying information such as name and address, since non-identifying personal data can be combined with publicly available information to pinpoint to an individual. In this work we model web query logs as unstructured transaction data and present a novel transaction anonymization technique based on clustering and generalization techniques to achieve the k-anonymity privacy. We conduct extensive experiments on the AOL query log data. Our results show that this method results in a higher data utility compared to the state-of-the-art transaction anonymization methods.\n\nTitle: Building Hierarchical Classifiers Using Class Proximity\nContent: In this paper, we address the need to automatically classify text documents into topic hierarchies like those in ACM Digital Library and Yahoo!. The existing local approach constructs a classifier at each split of the topic hierarchy. However, the local approach does not address the closeness of classification in hierarchical classification where the concern often is how close a classification is, rather than simply correct or wrong. Also, the local approach puts its bet on classification at higher levels where the classification structure often diminishes. To address these issues, we propose the notion of class proximity and cast the hierarchical classification as a flat classification with the class proximity modeling the closeness of classes. Our approach is global in that it constructs a single classifier based on the global information about all classes and class proximity. We leverage generalized association rules as the rule/feature space to address several other issues in hierarchical classification.\n\nTitle: RecTree: An Efficient Collaborative Filtering Method\nContent: Many people rely on the recommendations of trusted friends to find restaurants or movies, which match their tastes. But, what if your friends have not sampled the item of interest? Collaborative filtering (CF) seeks to increase the effectiveness of this process by automating the derivation of a recommendation, often from a clique of advisors that we have no prior personal relationship with. CF is a promising tool for dealing with the information overload that we face in the networked world. Prior works in CF have dealt with improving the accuracy of the predictions. However, it is still challenging to scale these methods to large databases. In this study, we develop an efficient collaborative filtering method, called RecTree (which stands for RECommendation Tree) that addresses the scalability problem with a divide-and-conquer approach. The method first performs an efficient k-means-like clustering to group data and creates neighborhood of similar users, and then performs subsequent clustering based on smaller, partitioned databases. Since the progressive partitioning reduces the search space dramatically, the search for an advisory clique will be faster than scanning the entire database of users. In addition, the partitions contain users that are more similar to each other than those in other partitions. This characteristic allows RecTree to avoid the dilution of opinions from good advisors by a multitude of poor advisors and thus yielding a higher overall accuracy. Based on our experiments and performance study, RecTree outperforms the well-known collaborative filter, CorrCF, in both execution time and accuracy. In particular, RecTree's execution time scales by O(nlog2(n)) with the dataset size while CorrCF scales quadratically.\n\n## Task\nCompose an abstract for the title 'Classification Pruning for Web-request Prediction' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"N-gram and repeating pattern based prediction rules have been used for next-web request prediction. However, there is no rigorous study of how to select the best rule for a given observation. The longer patter may not be the best pattern, because such patters are also more rare. In this paper, we propose several rule-pruning methods that enable us to build efficient, compact and high-quality classifiers for web- request prediction."
      }
    ]
  },
  {
    "tid":"164",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Privacy Preserving Record Linkage via grams Projections\nContent:   Record linkage has been extensively used in various data mining applications involving sharing data. While the amount of available data is growing, the concern of disclosing sensitive information poses the problem of utility vs privacy. In this paper, we study the problem of private record linkage via secure data transformations. In contrast to the existing techniques in this area, we propose a novel approach that provides strong privacy guarantees under the formal framework of differential privacy. We develop an embedding strategy based on frequent variable length grams mined in a private way from the original data. We also introduce personalized threshold for matching individual records in the embedded space which achieves better linkage accuracy than the existing global threshold approach. Compared with the state-of-the-art secure matching schema, our approach provides formal, provable privacy guarantees and achieves better scalability while providing comparable utility. \n\nTitle: Fusion: Privacy-Preserving Distributed Protocol for High-Dimensional Data Mashup\nContent: In the last decade, several approaches concerning private data release for data mining have been proposed. Data mashup, on the other hand, has recently emerged as a mechanism for integrating data from several data providers. Fusing both techniques to generate mashup data in a distributed environment while providing privacy and utility guarantees on the output involves several challenges. That is, how to ensure that no unnecessary information is leaked to the other parties during the mashup process, how to ensure the mashup data is protected against certain privacy threats, and how to handle the high-dimensional nature of the mashup data while guaranteeing high data utility. In this paper, we present Fusion, a privacy-preserving multi-party protocol for data mashup with guaranteed LKC-privacy for the purpose of data mining. Experiments on real-life data demonstrate that the anonymous mashup data provide better data utility, the approach can handle high dimensional data, and it is scalable with respect to the data size.\n\nTitle: Secure distributed framework for achieving \u03b5-differential privacy\nContent: Privacy-preserving data publishing addresses the problem of disclosing sensitive data when mining for useful information. Among the existing privacy models, \u03b5-differential privacy provides one of the strongest privacy guarantees. In this paper, we address the problem of private data publishing where data is horizontally divided among two parties over the same set of attributes. In particular, we present the first generalization-based algorithm for differentially private data release for horizontally-partitioned data between two parties in the semi-honest adversary model. The generalization algorithm correctly releases differentially-private data and protects the privacy of each party according to the definition of secure multi-party computation. To achieve this, we first present a two-party protocol for the exponential mechanism. This protocol can be used as a subprotocol by any other algorithm that requires exponential mechanism in a distributed setting. Experimental results on real-life data suggest that the proposed algorithm can effectively preserve information for a data mining task.\n\nTitle: Secure Two-Party Differentially Private Data Release for Vertically Partitioned Data\nContent: Privacy-preserving data publishing addresses the problem of disclosing sensitive data when mining for useful information. Among the existing privacy models, $(\\epsilon)$-differential privacy provides one of the strongest privacy guarantees. In this paper, we address the problem of private data publishing, where different attributes for the same set of individuals are held by two parties. In particular, we present an algorithm for differentially private data release for vertically partitioned data between two parties in the semihonest adversary model. To achieve this, we first present a two-party protocol for the exponential mechanism. This protocol can be used as a subprotocol by any other algorithm that requires the exponential mechanism in a distributed setting. Furthermore, we propose a two-party algorithm that releases differentially private data in a secure way according to the definition of secure multiparty computation. Experimental results on real-life data suggest that the proposed algorithm can effectively preserve information for a data mining task.\n\nTitle: Template-Based Privacy Preservation in Classification Problems\nContent: In this paper, we present a template-based privacy preservation to protect against the threats caused by data mining abilities. The problem has dual goals: preserve the information for a wanted classification analysis and limit the usefulness of unwanted sensitive inferences that may be derived from the data. Sensitive inferences are specified by a set of \"privacy templates\". Each template specifies the sensitive information to be protected, a set of identifying attributes, and the maximum association between the two. We show that suppressing the domain values is an effective way to eliminate sensitive inferences. For a large data set, finding an optimal suppression is hard, since it requires optimization over all suppressions. We present an approximate but scalable solution. We demonstrate the effectiveness of this approach on real life data sets.\n\n## Task\nCompose an abstract for the title 'Frequent grams based embedding for privacy preserving record linkage' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, we study the problem of privacy preserving record linkage which aims to perform record linkage without revealing anything about the non-linked records. We propose a new secure embedding strategy based on frequent variable length grams which allows record linkage on the embedded space. The frequent grams used for constructing the embedding base are mined from the original database under the framework of differential privacy. Compared with the state-of-the-art secure matching schema [15], our approach provides formal, provable privacy guarantees and achieves better scalability while providing comparable utility."
      }
    ]
  },
  {
    "tid":"165",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Cujo: efficient detection and prevention of drive-by-download attacks\nContent: The JavaScript language is a core component of active and dynamic web content in the Internet today. Besides its great success in enhancing web applications, however, JavaScript provides the basis for so-called drive-by downloads---attacks exploiting vulnerabilities in web browsers and their extensions for unnoticeably downloading malicious software. Due to the diversity and frequent use of obfuscation in these attacks, static code analysis is largely ineffective in practice. While dynamic analysis and honeypots provide means to identify drive-by-download attacks, current approaches induce a significant overhead which renders immediate prevention of attacks intractable. In this paper, we present Cujo, a system for automatic detection and prevention of drive-by-download attacks. Embedded in a web proxy, Cujo transparently inspects web pages and blocks delivery of malicious JavaScript code. Static and dynamic code features are extracted on-the-fly and analysed for malicious patterns using efficient techniques of machine learning. We demonstrate the efficacy of Cujo in different experiments, where it detects 94% of the drive-by downloads with few false alarms and a median run-time of 500 ms per web page---a quality that, to the best of our knowledge, has not been attained in previous work on detection of drive-by-download attacks.\n\nTitle: ASAP: automatic semantics-aware analysis of network payloads\nContent: Automatic inspection of network payloads is a prerequisite for effective analysis of network communication. Security research has largely focused on network analysis using protocol specifications, for example for intrusion detection, fuzz testing and forensic analysis. The specification of a protocol alone, however, is often not sufficient for accurate analysis of communication, as it fails to reflect individual semantics of network applications. We propose a framework for semantics-aware analysis of network payloads which automatically extracts semanticsaware components from recorded network traffic. Our method proceeds by mapping network payloads to a vector space and identifying communication templates corresponding to base directions in the vector space. We demonstrate the efficacy of semantics-aware analysis in different security applications: automatic discovery of patterns in honeypot data, analysis of malware communication and network intrusion detection.\n\nTitle: Efficient and Flexible Discovery of PHP Application Vulnerabilities\nContent: The Web today is a growing universe of pages and applications teeming with interactive content. The security of such applications is of the utmost importance, as exploits can have a devastating impact on personal and economic levels. The number one programming language in Web applications is PHP, powering more than 80% of the top ten million websites. Yet it was not designed with security in mind and, today, bears a patchwork of fixes and inconsistently designed functions with often unexpected and hardly predictable behavior that typically yield a large attack surface. Consequently, it is prone to different types of vulnerabilities, such as SQL Injection or Cross-Site Scripting. In this paper, we present an interprocedural analysis technique for PHP applications based on code property graphs that scales well to large amounts of code and is highly adaptable in its nature. We implement our prototype using the latest features of PHP 7, leverage an efficient graph database to store code property graphs for PHP, and subsequently identify different types of Web application vulnerabilities by means of programmable graph traversals. We show the efficacy and the scalability of our approach by reporting on an analysis of 1,854 popular open-source projects, comprising almost 80 million lines of code.\n\nTitle: Automatic Inference of Search Patterns for Taint-Style Vulnerabilities\nContent: Taint-style vulnerabilities are a persistent problem in software development, as the recently discovered \"Heart bleed\" vulnerability strikingly illustrates. In this class of vulnerabilities, attacker-controlled data is passed unsanitized from an input source to a sensitive sink. While simple instances of this vulnerability class can be detected automatically, more subtle defects involving data flow across several functions or project-specific APIs are mainly discovered by manual auditing. Different techniques have been proposed to accelerate this process by searching for typical patterns of vulnerable code. However, all of these approaches require a security expert to manually model and specify appropriate patterns in practice. In this paper, we propose a method for automatically inferring search patterns for taint-style vulnerabilities in C code. Given a security-sensitive sink, such as a memory function, our method automatically identifies corresponding source-sink systems and constructs patterns that model the data flow and sanitization in these systems. The inferred patterns are expressed as traversals in a code property graph and enable efficiently searching for unsanitized data flows -- across several functions as well as with project-specific APIs. We demonstrate the efficacy of this approach in different experiments with 5 open-source projects. The inferred search patterns reduce the amount of code to inspect for finding known vulnerabilities by 94.9% and also enable us to uncover 8 previously unknown vulnerabilities.\n\nTitle: Incorporation of Application Layer Protocol Syntax into Anomaly Detection\nContent: The syntax of application layer protocols carries valuable information for network intrusion detection. Hence, the majority of modern IDS perform some form of protocol analysis to refine their signatures with application layer context. Protocol analysis, however, has been mainly used for misuse detection, which limits its application for the detection of unknown and novel attacks. In this contribution we address the issue of incorporating application layer context into anomaly-based intrusion detection. We extend a payload-based anomaly detection method by incorporating structural information obtained from a protocol analyzer. The basis for our extension is computation of similarity between attributed tokens derived from a protocol grammar. The enhanced anomaly detection method is evaluated in experiments on detection of web attacks, yielding an improvement of detection accuracy of 49%. While byte-level anomaly detection is sufficient for detection of buffer overflow attacks, identification of recent attacks such as SQL and PHP code injection strongly depends on the availability of application layer context.\n\n## Task\nCompose an abstract for the title 'TokDoc: a self-healing web application firewall' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The growing amount of web-based attacks poses a severe threat to the security of web applications. Signature-based detection techniques increasingly fail to cope with the variety and complexity of novel attack instances. As a remedy, we introduce a protocol-aware reverse HTTP proxy TokDoc (the token doctor), which intercepts requests and decides on a per-token basis whether a token requires automatic \"healing\". In particular, we propose an intelligent mangling technique, which, based on the decision of previously trained anomaly detectors, replaces suspicious parts in requests by benign data the system has seen in the past. Evaluation of our system in terms of accuracy is performed on two real-world data sets and a large variety of recent attacks. In comparison to state-of-the-art anomaly detectors, TokDoc is not only capable of detecting most attacks, but also significantly outperforms the other methods in terms of false positives. Runtime measurements show that our implementation can be deployed as an inline intrusion prevention system."
      }
    ]
  },
  {
    "tid":"166",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Fostering Cooperation through Dynamic Coalition Formation and Partner Switching\nContent: In this article we tackle the problem of maximizing cooperation among self-interested agents in a resource exchange environment. Our main concern is the design of mechanisms for maximizing cooperation among self-interested agents in a way that their profits increase by exchanging or trading with resources. Although dynamic coalition formation and partner switching (rewiring) have been shown to promote the emergence and maintenance of cooperation for self-interested agents, no prior work in the literature has investigated whether merging both mechanisms exhibits positive synergies that lead to increase cooperation even further. Therefore, we introduce and analyze a novel dynamic coalition formation mechanism, that uses partner switching, to help self-interested agents to increase their profits in a resource exchange environment. Our experiments show the effectiveness of our mechanism at increasing the agents\u2019 profits, as well as the emergence of trading as the preferred behavior over different types of complex networks.\n\nTitle: Trust-based mechanisms for robust and efficient task allocation in the presence of execution uncertainty\nContent: Vickrey-Clarke-Groves (VCG) mechanisms are often used to allocate tasks to selfish and rational agents. VCG mechanisms are incentive compatible, direct mechanisms that are efficient (i.e., maximise social utility) and individually rational (i.e., agents prefer to join rather than opt out). However, an important assumption of these mechanisms is that the agents will always successfully complete their allocated tasks. Clearly, this assumption is unrealistic in many real-world applications, where agents can, and often do, fail in their endeavours. Moreover, whether an agent is deemed to have failed may be perceived differently by different agents. Such subjective perceptions about an agent's probability of succeeding at a given task are often captured and reasoned about using the notion of trust. Given this background, in this paper we investigate the design of novel mechanisms that take into account the trust between agents when allocating tasks. Specifically, we develop a new class of mechanisms, called trust-based mechanisms, that can take into account multiple subjective measures of the probability of an agent succeeding at a given task and produce allocations that maximise social utility, whilst ensuring that no agent obtains a negative utility. We then show that such mechanisms pose a challenging new combinatorial optimisation problem (that is NP-complete), devise a novel representation for solving the problem, and develop an effective integer programming solution (that can solve instances with about 2\u00d7105 possible allocations in 40 seconds).\n\nTitle: Efficient Inter-Team Task Allocation in RoboCup Rescue\nContent: The coordination of cooperative agents involved in rescue missions is an important open research problem. We consider the RoboCup Rescue Simulation (RCS) challenge, where teams of agents perform urban rescue operations. Previous approaches typically cast such problem as separate single-team allocation problems. However, different teams have complementary capabilities, and therefore some kind of inter-team coordination is desirable for high-quality solutions. Our contribution considers inter-team coordination using Max-Sum. We present a methodology that allows teams in RCS to efficiently assess joint allocations. Furthermore, we show how to reduce the algorithm's computational complexity from exponential to polynomial time by using Tractable High Order Potentials. To the best of our knowledge this is the first time where it has been shown that MS can be run in polynomial time in the RCS challenge without relaxing the problem. Experiments with fire brigades and police agents show that teams employing inter-team coordination are significantly more effective than uncoordinated teams. Moreover, the evaluation shows that our BMS and THOPs method achieves up to $2.5$ times better results than other state-of-the-art methods.\n\nTitle: Robust Regulation Adaptation in Multi-Agent Systems\nContent: Adaptive organisation-centred multi-agent systems can dynamically modify their organisational components to better accomplish their goals. Our research line proposes an abstract distributed architecture (2-LAMA) to endow an organisation with adaptation capabilities. This article focuses on regulation-adaptation based on a machine learning approach, in which adaptation is learned by applying a tailored case-based reasoning method. We evaluate the robustness of the system when it is populated by non compliant agents. The evaluation is performed in a peer-to-peer sharing network scenario. Results show that our proposal significantly improves system performance and can cope with regulation violators without incorporating any specific regulation-compliance enforcement mechanisms.\n\nTitle: Algorithms for Graph-Constrained Coalition Formation in the Real World.\nContent: Coalition formation typically involves the coming together of multiple, heterogeneous, agents to achieve both their individual and collective goals. In this article, we focus on a special case of coalition formation known as Graph-Constrained Coalition Formation (GCCF) whereby a network connecting the agents constrains the formation of coalitions. We focus on this type of problem given that in many real-world applications, agents may be connected by a communication network or only trust certain peers in their social network. We propose a novel representation of this problem based on the concept of edge contraction, which allows us to model the search space induced by the GCCF problem as a rooted tree. Then, we propose an anytime solution algorithm (Coalition Formation for Sparse Synergies (CFSS)), which is particularly efficient when applied to a general class of characteristic functions called m + a functions. Moreover, we show how CFSS can be efficiently parallelised to solve GCCF using a nonredundant partition of the search space. We benchmark CFSS on both synthetic and realistic scenarios, using a real-world dataset consisting of the energy consumption of a large number of households in the UK. Our results show that, in the best case, the serial version of CFSS is four orders of magnitude faster than the state of the art, while the parallel version is 9.44 times faster than the serial version on a 12-core machine. Moreover, CFSS is the first approach to provide anytime approximate solutions with quality guarantees for very large systems of agents (i.e., with more than 2,700 agents).\n\n## Task\nCompose an abstract for the title 'Using reputation and adaptive coalitions to support collaboration in competitive environments' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Internet-based scenarios, like co-working, e-freelancing, or crowdsourcing, usually need supporting collaboration among several actors that compete to service tasks. Moreover, the distribution of service requests, i.e., the arrival rate, varies over time, as well as the service workload required by each customer. In these scenarios, coalitions can be used to help agents to manage tasks they cannot tackle individually. In this paper we present a model to build and adapt coalitions with the goal of improving the quality and the quantity of tasks completed. The key contribution is a decision making mechanism that uses reputation and adaptation to allow agents in a competitive environment to autonomously enact and sustain coalitions, not only its composition, but also its number, i.e., how many coalitions are necessary. We provide empirical evidence showing that when agents employ our mechanism it is possible for them to maintain high levels of customer satisfaction. First, we show that coalitions keep a high percentage of tasks serviced on time despite a high percentage of unreliable workers. Second, coalitions and agents demonstrate that they successfully adapt to a varying distribution of customers' incoming tasks. This occurs because our decision making mechanism facilitates coalitions to disband when they become non-competitive, and individual agents detect opportunities to start new coalitions in scenarios with high task demand."
      }
    ]
  },
  {
    "tid":"167",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Retrieval of Time-Varying Mesh and motion capture data using 2D video queries based on silhouette shape descriptors\nContent: This paper presents a retrieval system for time-varying mesh (TVM) and motion capture data using 2D video queries. Previous approaches have used other TVM and motion capture data as queries and the cost for query generation was a significant issue. Instead, the proposed system uses 2D video queries, which can be easily captured by a single camera, enabling end users to retrieve 3D motion sequences such as TVM and motion capture data easily and interactively. We introduce the P-type Fourier descriptor, which is a feature of 2D contour images. TVM and computer graphics sequences rendered from motion capture data are silhouetted rendering from multiple viewpoints. Feature vectors for TVM and motion capture data are generated by applying the P-type Fourier descriptor to these silhouetted images. Experimental results using four TVM sequences and motion capture data demonstrated an average retrieval accuracy of 88% in terms of nearest neighbors.\n\nTitle: Hierarchical Mesh Decomposition And Motion Tracking For Time-Varying-Meshes\nContent: This paper proposes a system for automatic segmentation and motion tracking of Time-Varying-Meshes (TVM). Our approach is based on skeleton-based hierarchical mesh decomposition by distance calculation. The properties of the human skeleton structure are used to define the decomposition of each TVM frame. The proposed framework is a recursive system that iterates between automatic hierarchical decomposition on minimum distance satisfaction and skeleton realignment. This is done to achieve a stable segmentation and a refined skeleton. By utilizing color information, ill-defined meshes can be successfully segmented. Results show an average disparity of 1.39% of total surface area across all segmented parts, which indicate stability of the system. In addition, motion tracking is successfully performed through the use of refined skeletons.\n\nTitle: Content-Based Cross Search for Human Motion Data using Time-Varying Mesh and Motion Capture Data\nContent: This paper describes a content-based cross search scheme for two kinds of three-dimensional (3D) human motion data: time-varying mesh (TVM) and motion capture data. TVM is a sequence of 3D mesh models made for real-world 3D objects. TVM is generated using multiple-view images taken with multiple cameras. Since TVM can record shape, color, and motion of the real-world 3D objects, it has been drawing a lot of attention these days. In order to realize practical archiving systems for TVM, efficient retrieval systems are indispensable. The retrieval systems for TVM developed so far are based on query-by-example. This means additional TVM generation is required for constructing queries, which is computationally demanding and time consuming. On the other hand, motion capture systems are widely used to capture 3D human motion. However, the data structure is very different from that of TVM. Therefore, the two kinds of 3D human motion data are incompatible to each other. In this paper, we present a retrieval system that enables retrieving TVM using motion capture data as queries and vice versa using the modified shape distribution algorithm.\n\nTitle: Motion segmentation for time-varying mesh sequences based on spherical registration\nContent: A highly accurate motion segmentation technique for time-varying mesh (TVM) is presented. In conventional approaches, motion of the objects was analyzed using shape feature vectors extracted from TVM frames. This was because it was very difficult to locate and track feature points in the objects in the 3D space due to the fact that the number of vertices and connection varies each frame. In this study, we developed an algorithm to analyze the objects' motion in the 3D space using the spherical registration based on the iterative closest-point algorithm. Rough motion tracking is conducted and the degree of motion is robustly calculated by this method. Although the approach is straightforward, much better motion segmentation results than the conventional approaches are obtained by yielding such high precision and recall rates as 95% and 92% on average.\n\nTitle: Motion segmentation and retrieval for 3D video based on modified shape distribution\nContent: A similar motion search and retrieval system for 3D video are presented based on a modified shape distribution algorithm. 3D video is a sequence of 3D models made for a real-world object. In the present work, three fundamental functions for efficient retrieval have been developed: feature extraction, motion segmentation, and similarity evaluation. Stable-shape feature representation of 3D models has been realized by a modified shape distribution algorithm. Motion segmentation has been conducted by analyzing the degree of motion using the extracted feature vectors. Then, similar motion retrieval has been achieved employing the dynamic programming algorithm in the feature vector space. The experimental results using 3D video sequences of dances have demonstrated very promising results for motion segmentation and retrieval.\n\n## Task\nCompose an abstract for the title 'Fast And Robust Motion Tracking For Time-Varying Mesh Featuring Reeb-Graph-Based Skeleton Fitting And Its Application To Motion Retrieval' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, an algorithm for motion extraction from Time-Varying Mesh (TVM) is proposed. TVM is a sequence of 3D models made for real-world dynamic objects. In TVM, detailed information of 3D objects such as shape, color, and motion is stored. Therefore, TVM is an attractive technology for the next-generation multimedia. However, the management of TVM database for archiving and retrieving is a significant problem because it is difficult to locate and track feature points in TVM. For efficient and effective archive and retrieval systems, motion extraction and tracking is essential. In our approach, skeletons are extracted from TVM using Reeb graph, and motion tracking is achieved by fitting a reference skeleton to the others. We defined a geodesic function using principal component analysis (PCA) for fast and noiseless skeleton extraction. Robust fitting has been realized by an end node tracking strategy. In addition, we have developed an efficient motion retrieval system compatible to conventional motion capture (mocap) data using the extracted skeleton. As a result, low-cost query generation using mocap systems and cross search between TVM and mocap data have been achieved."
      }
    ]
  },
  {
    "tid":"168",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Schema versioning and database conversion techniques for bi-temporal databases\nContent: Schema evolution and schema versioning are two techniques used for managing database evolution. Schema evolution keeps only the current version of a schema and database after applying schema changes. Schema versioning creates new schema versions and converts the corresponding data while preserving the old schema versions and data. To provide the most generality, bi-temporal databases can be used to realize schema versioning, since they allow both retroactive and proactive updates to the schema and database. In this paper we first study two proposed database conversion approaches for supporting schema evolution and schema versioning: single table version approach and multiple table version approach. We then propose the partial table version approach to solve the problems encountered in these approaches when applied to bi-temporal databases.\n\nTitle: PMTV: a schema versioning approach for bi-temporal databases\nContent: Most of the existing schema versioning approaches are developed for single time-dimension temporal databases. For bi-temporal databases, which allow both retroactive and proactive updates to the schema and database, these approaches introduce many problems and perform inefficiently. We first review two schema versioning approaches, single table versioning (STV) and multiple table versioning (MTV) approaches, and also the proposed partial multiple table versioning (PMTV) approach. We prove the correctness of PMTV by showing it producing equivalent results to STV and then compare these three approaches in database conversion and temporal query processing. The comparison shows that the proposed PMTV approach not only solves the problems but also performs more efficiently than the other approaches when applied to bi-temporal databases\n\nTitle: Utilizing Object-Oriented Databases for Concurrency Control in Virtual Environments\nContent: Virtual Reality Modeling Language (VRML) is widely used to represent, create, and display virtual reality objects and their environment. Some VRML applications require concurrent interaction by multiple users in a real-time distributed fashion. Such applications need a method for users to share and update the VRML objects in real-time. To allow concurrent shared real-time access, our approach is to store the VRML objects in an object-oriented database system (ObjectStore) in order to utilize the concurrency control mechanisms of the system. In this paper, we present an architecture that allows multiple users to interact in a non-trivial way in such a shared VRML environment. We outline how the VRML world can be saved in ObjectStore and implement a series of test cases demonstrating concurrency issues arising from simultaneous updates. Our architecture uses ordinary Java enabled web browsers with a VRML plug-in. A commercial web server routes client requests to a custom application server, which interacts with the object-oriented database. As users change the VRML world, our application server orders the requests and updates the master copy in the database.Kewords: Object-Oriented Databases, Concurrency Control, Virtual Environments\n\nTitle: The Design and Evaluation of a Magnetic/Optical Access Structure for Temporal Databases\nContent: This paper describes a magnetic/optical access structure for append\u2013only temporal databases. We formally define the properties of an access structure, called the MonotonicB+ -Tree (MBT), that is well suited for Write-Once Read Many optical disks. We present an insertion algorithm for the MBT that does not require splitting of index nodes and give the time analysis for this algorithm. We also describe a storage architecture where optical disks work in tandem with magnetic disks. Magnetic disks are used for storing current versions and recent past versions, whereas optical disks are dedicated for archiving older past versions. Our archiving techniques: (1) allow temporal data and the MBT access structure to span magnetic disks and optical disks; (2) minimize the overhead of the migration process by taking advantage of append\u2013only nature of temporal databases; (3) gracefully handle object versions with very long time intervals so that the delay in the migration process is kept to minimum; and (4) ensure that no false magnetic or optical disk address lookup is performed during search operations by duplicating some closed versions on both magnetic and optical disks. To validate our claims for the efficiency of migration techniques, we analyze the performance of temporal access structures partitioned between magnetic and optical disks. We show that the migration process has a minimal effect on the search time. Our simulation identifies important parameters, and shows how they affect the performance of the temporal access structures. These include mean of version lifespan, block size, query time interval length, and total number of versions.\n\nTitle: An EER-Based Conceptual Model and Query Language for Time-Series Data\nContent: \n Temporal databases provide a complete history of all changes to a database and include the times when changes occurred. This\n permits users to query the current status of the database as well as the past states, and even future states that are planned\n to occur. Traditional temporal data models concentrated on describing temporal data based on versioning of objects, tuples\n or attributes. However, this approach does not effectively manage time-series data that is frequently found in real-world\n applications, such as sales, economic, and scientific data. In this paper, we first review and formalize a conceptual model\n that supports time-series objects as well as the traditional version-based objects. The proposed model, called integrated\n temporal data model (ITDM), is based on EER. It includes in it the concept of time and provides necessary constructs for modeling\n all different types of objects. We then propose a temporal query language for ITDM, that treats both version-based and time-series\n data in a uniform manner.\n \n \n\n## Task\nCompose an abstract for the title 'Study and Comparison of Schema Versioning and Database Conversion Techniques for Bi-Temporal Databases' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, we describe a new system of relations for reasoning about general, i.e., both convex and nonconvex, intervals of time. This system is essentially an integration of Allen's theory [1,2] and a one-dimensional version of the Region Connection ..."
      }
    ]
  },
  {
    "tid":"169",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A New Infinity of Distance Oracles for Sparse Graphs\nContent: Given a weighted undirected graph, our basic goal is to represent all pairwise distances using much less than quadratic space, such that we can estimate the distance between query vertices in constant time. We will study the inherent trade-off between space of the representation and the stretch (multiplicative approximation disallowing underestimates) of the estimates when the input graph is sparse with m = O\u0303(n) edges. In this paper, for any fixed positive integers k and \u2113, we obtain stretches = 2k + 1 \u00b1 2/\u2113 = 2k + 1 - 2/\u2113, 2k + 1 + 2/\u2113, using space S(\u03b1, m) = O\u0303(m1+2/(\u03b1+1)). The query time is O(k + \u2113) = O(1). For integer stretches, this coincides with the previous bounds (odd stretches with \u2113 = 1 and even stretches with \u2113 = 2). The infinity of fractional stretches between consecutive integers are all new (even though \u2113 is fixed as a constant independent of the input, the number of integers \u2113 is still countably infinite). We will argue that the new fractional points are not just arbitrary, but that they, at least for fixed stretches below 3, provide a complete picture of the inherent trade-off between stretch and space in m. Consider any fixed stretch \u03b1 <; 3. Based on the hardness of set intersection, we argue that if \u2113 is the largest integer such that 3-2/\u2113 \u2264 \u03b1, then \u03a9\u0303(S(3 - 2/\u2113, m)) space is needed for stretch . In particular, for fixed stretch below 22/3, we improve Patrascu and Roditty's lower bound from \u03a9\u0303(m3/2) to \u03a9\u0303(m5/3), thus matching their upper bound for stretch 2. For space in terms of m, this is the first hardness matching the space of a non-trivial/sub-quadratic distance oracle.\n\nTitle: Deterministic constructions of approximate distance oracles and spanners\nContent: Thorup and Zwick showed that for any integer k\u2265 1, it is possible to preprocess any positively weighted undirected graph G=(V,E), with |E|=m and |V|=n, in \u00d5(kmn$^{\\rm 1/{\\it k}}$) expected time and construct a data structure (a (2k\u20131)-approximate distance oracle) of size O(kn$^{\\rm 1+1/{\\it k}}$) capable of returning in O(k) time an approximation $\\hat{\\delta}(u,v)$ of the distance \u03b4(u,v) from u to v in G that satisfies $\\delta(u,v) \\leq \\hat{\\delta}(u,v) \\leq (2k -1)\\cdot \\delta(u,v)$, for any two vertices u,v\u2208 V. They also presented a much slower \u00d5(kmn) time deterministic algorithm for constructing approximate distance oracle with the slightly larger size of O(kn$^{\\rm 1+1/{\\it k}}$log n). We present here a deterministic \u00d5(kmn$^{\\rm 1/{\\it k}}$) time algorithm for constructing oracles of size O(kn$^{\\rm 1+1/{\\it k}}$). Our deterministic algorithm is slower than the randomized one by only a logarithmic factor. Using our derandomization technique we also obtain the first deterministic linear time algorithm for constructing optimal spanners of weighted graphs. We do that by derandomizing the O(km) expected time algorithm of Baswana and Sen (ICALP\u201903) for constructing (2k\u20131)-spanners of size O(kn$^{\\rm 1+1/{\\it k}}$) of weighted undirected graphs without incurring any asymptotic loss in the running time or in the size of the spanners produced.\n\nTitle: All-pairs shortest paths with a sublinear additive error\nContent: We show that, for every 0 \u2264 p \u2264 1, there is an O(n2.575\u2212p/(7.4\u22122.3p))-time algorithm that given a directed graph with small positive integer weights, estimates the length of the shortest path between every pair of vertices u, v in the graph to within an additive error \u03b4p(u, v), where \u03b4(u, v) is the exact length of the shortest path between u and v. This algorithm runs faster than the fastest algorithm for computing exact shortest paths for any 0 p \u2264 1. Previously the only way to \u201cbeat\u201d the running time of the exact shortest path algorithms was by applying an algorithm of Zwick [2002] that approximates the shortest path distances within a multiplicative error of (1 + &epsi;). Our algorithm thus gives a smooth qualitative and quantitative transition between the fastest exact shortest paths algorithm, and the fastest approximation algorithm with a linear additive error. In fact, the main ingredient we need in order to obtain the above result, which is also interesting in its own right, is an algorithm for computing (1 + &epsi;) multiplicative approximations for the shortest paths, whose running time is faster than the running time of Zwick's approximation algorithm when &epsi; &Lt; 1 and the graph has small integer weights.\n\nTitle: f-Sensitivity Distance Oracles and Routing Schemes\nContent: An f-sensitivity distance oracle for a weighted undirected graph G(V,E) is a data structure capable of answering restricted distance queries between vertex pairs, i.e., calculating distances on a subgraph avoiding some forbidden edges. This paper presents an efficiently constructible f-sensitivity distance oracle that given a triplet (s,t,F), where s and t are vertices and F is a set of forbidden edges such that |F|\u2264f, returns an estimate of the distance between s and t in G(V,E\u2216F). For an integer parameter k\u22651, the size of the data structure is O(fkn 1+1/k log\u2009(nW)), where W is the heaviest edge in G, the stretch (approximation ratio) of the returned distance is (8k\u22122)(f+1), and the query time is O(|F|\u22c5log\u20092 n\u22c5log\u2009log\u2009n\u22c5log\u2009log\u2009d), where d is the distance between s and t in G(V,E\u2216F). Our result differs from previous ones in two major respects: (1) it is the first to consider approximate oracles for general graphs (and thus obtain a succinct data structure); (2) our result holds for an arbitrary number of forbidden edges. In contrast, previous papers concern f-sensitive exact distance oracles, which consequently have size \u03a9(n 2). Moreover, those oracles support forbidden sets F of size |F|\u22642. The paper also considers f-sensitive compact routing schemes, namely, routing schemes that avoid a given set of forbidden (or failed) edges. It presents a scheme capable of withstanding up to two edge failures. Given a message M destined to t at a source vertex s, in the presence of a forbidden edge set F of size |F|\u22642 (unknown to s), our scheme routes M from s to t in a distributed manner, over a path of length at most O(k) times the length of the optimal path (avoiding F). The total amount of information stored in vertices of G is O(kn 1+1/k log\u2009(nW)log\u2009n). To the best of our knowledge, this is the first result obtaining an f-sensitive compact routing scheme for general graphs.\n\nTitle: Improved algorithms for fully dynamic geometric spanners and geometric routing\nContent: For a set S of points in \u211dd, a t-spanner is a sparse graph on the points of S such that between any pair of points there is a path in the spanner whose total length is at most t times the Euclidean distance between the points. In this paper, we show how to construct a (1 + \u03b5)-spanner with O(n/\u03b5d) edges and maximum degree O(1/\u03b5d) in time O(n log n). A spanner with similar properties was previously presented in [6, 8]. However, using our new construction (coupled with several other innovations) we obtain new results for two fundamental problems for constant doubling dimension metrics: The first result is an essentially optimal compact routing scheme. In particular, we show how to perform routing with a stretch of 1 + \u2208, where the label size is [log n] and the size of the table stored at each point is only O(log n/\u03b5d). This routing problem was first considered by Peleg and Hassin [11], who presented a routing scheme in the plane. Later, Chan et al. [6] and Abraham et al. [1] considered this problem for doubling dimension metric spaces. Abraham et al. [1] were the first to present a (1 + \u2208) routing scheme where the label size depends solely on the number of points. In their scheme labels are of size of [log n], and each point stores a table of size O(log2 n/\u03b5d). In our routing scheme, we achieve routing tables of size O(log n/\u03b5d), which is essentially the same size as a label (up to the factor of 1/\u03b5d). The second and main result of this paper is the first fully dynamic geometric spanner with poly-logarithmic update time for both insertions and deletions. We present an algorithm that allows points to be inserted into and deleted from S with an amortized update time of O(log3 n).\n\n## Task\nCompose an abstract for the title 'Distance Oracles beyond the Thorup-Zwick Bound' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We give the first improvement to the space/approximation trade-off of distance oracles since the seminal result of Thorup and Zwick [STOC'01]. For unweighted graphs, our distance oracle has size $O(n^{5/3}) = O(n^{1.66\\cdots})$ and, when queried about vertices at distance $d$, returns a path of length $2d+1$. For weighted graphs with $m=n^2/\\alpha$ edges, our distance oracle has size $O(n^2 / \\sqrt[3]{\\alpha})$ and returns a factor 2 approximation. Based on a plausible conjecture about the hardness of set intersection queries, we show that a 2-approximate distance oracle requires space $\\tOmega(n^2 / \\sqrt{\\alpha})$. For unweighted graphs, this implies a $\\tOmega(n^{1.5})$ space lower bound to achieve approximation $2d+1$."
      }
    ]
  },
  {
    "tid":"170",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Prioritized Petri Net Model and Its Application in Distributed Multimedia Systems\nContent: The achievement of media synchronization has been dealt with in the Object Composition Petri Net (OCPN) model and the extended OCPN (XOCPN) model. Yet these two models are not enough for synchronization of computers in a distributed environment. This paper proposes a new Petri Net model\u9a74Prioritized Petri Net (P-net). The modeling power and properties of P-nets are analyzed. We apply the P-net model to distributed multimedia synchronization, using our version of Distributed Object Composition Petri Net (DOCPN) model. Using the DOCPN model, we can coordinate operations among distributed computer sites. The scenario is like a conductor conducting an orchestra to perform a symphony.\n\nTitle: Modeling adaptable multimedia and self-modifying protocol execution\nContent: Over the years, researchers have tried to extend Petri net to model multimedia. The focus of the research flows from the synchronization of multimedia without user interactions (UIs), to interactions in distributed environments. The issues in concern are the flexibility and compactness of the model when applied to model a system under change. Most existing models lack the power to model a system under change during execution. Petri net extensions have been developed to facilitate UIs in distributed environments, however, they require sophisticated pre-planning to lay out detailed schedule changes. On the other hand, there has been active research on self-modifying protocols (SMPs) or adaptive protocols in recent years. Plenty of models have been developed to model communication protocol execution, to name a few, finite state machines, communicating finite state machines, Petri nets. However, there exist no suitable models to simulate protocols that are self-modifying or adaptive during execution. In this paper, we propose a Reconfigurable Petri net (RPN) for adaptable multimedia. A RPN comprises of a novel mechanism called modifier. This modifier can create a new change or delete an existing mechanism (e.g. arc, place, token, transition, etc.) of the net. In a way, modifier embraces controllability, reconfigurability, and programmability into the Petri net, and enhances the real-time adaptive modeling power. This development allows a RPN to have a greater modeling power over other extended Petri nets. The paper includes both the model and theory required to establish the technique's validity. Examples are also shown how RPN can be used to model interactive multimedia, and simulate SMPs. A simulator has been developed using Visual C++ under Windows NT to show that RPN is feasible.\n\nTitle: A golden-block-based self-refining scheme for repetitive patterned wafer inspections\nContent: This paper presents a novel technique for detecting possible defects in two-dimensional wafer images with repetitive patterns using prior knowledge. The technique has a learning ability that can create a golden-block database from the wafer image itself, then modify and refine its content when used in further inspections. The extracted building block is stored as a golden block for the detected pattern. When new wafer images with the same periodical pattern arrive, we do not have to recalculate their periods and building blocks. A new building block can be derived directly from the existing golden block after eliminating alignment differences. If the newly derived building block has better quality than the stored golden block, then the golden block is replaced with the new building block. With the proposed algorithm, our implementation shows that a significant amount of processing time is saved. Also, the storage overhead of golden templates is reduced significantly by storing golden blocks only.\n\nTitle: Recursive hybrid decomposition with reduced pattern training\nContent: When neural networks are applied to large scale real-world classification problems, a major drawback is its inefficiency in utilizing network resources. A natural approach to overcome this drawback is to decompose the problem into several smaller sub-problems based on the \"divide-and-conquer\" methodology. This paper presents a hybrid method of task decomposition - OP-RPHP (Output Parallelism with Recursive Percentage-based Hybrid Pattern training). OP-RPHP employs a combination of both class decomposition and domain decomposition in its architecture thereby incorporating the advantages of both methods. OP-RPHP can be grown and trained in parallel on separate processing units to improve training time. To further improve the training time, a reduced pattern training algorithm is introduced. The reduction parameter p associated with the reduced pattern training algorithm is optimized to obtain maximum reduction in training time without compromising classification accuracy. Our approach is tested on four benchmark classification problems retrieved from the UCI repository of machine learning databases. The results show that OP-RPHP with reduced pattern training outperformed conventional OP and RPHP algorithms in both classification accuracy and training times.\n\nTitle: Self-modifiable color Petri nets for modeling user manipulation and network event handling\nContent: A self-modifiable color Petri net (SMCPN) which has multimedia synchronization capability and the ability to model user manipulation and network event (i.e., network congestion, etc.) handling is proposed in this paper. In SMCPN, there are two types of tokens: resource tokens representing resources to be presented and color tokens with two subtypes: one associated with some commands to modify the net mechanism in operation, another associated with a number to decide iteration times. Also introduced is a new type of resource token, named reverse token, that moves in the opposite direction of arcs. When user manipulation/network event occurs, color tokens associated with the corresponding interrupt handling commands will be injected into places that contain resource tokens. These commands are then executed to handle the user manipulation/network event. SMCPN has the desired general programmability in the following sense: 1) it allows handling of user manipulations or prespecified events at any time while keeping the Petri net design simple and easy. 2) It allows the user to customize event handling beforehand. This means the system being modeled can handle not only commonly seen user interrupts (e.g., skip, reverse, freeze), the user is free to define new operations, including network event handling. 3) It has the power to simulate self-modifying protocols. A simulator has been built to demonstrate the feasibility of SMCPN.\n\n## Task\nCompose an abstract for the title 'A refined cut-through buffer management scheme for layered protocol stacks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Avoiding recopying of user data as it is passed between protocol layers helps break the protocol-execution bottleneck. The authors use a use a simple pointer-variable updating technique in the sending phase of their refined buffer cut-through scheme. Binary tree traversal is not required. Time is saved in allocating (and later deallocating) buffers for storing usually short data transfer protocol headers. Their scheme saves working buffer space for segmenting and linking of protocol data units. Because a simple linked list is constructed in the receiving phase, significant working buffer space can be saved. Each node of the linked list points to a user data segment. During the linked-list traversal, user data is copied directly to the user process when a node is visited.<>"
      }
    ]
  },
  {
    "tid":"171",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Word-Level Script Identification from Handwritten Multi-script Documents.\nContent: In this paper, a robust word-level handwritten script identification technique has been proposed. A combination of shape based and texture based features are used to identify the script of the handwritten word images written in any of five scripts namely, Bangla, Devnagari, Malayalam, Telugu and Roman. An 87-element feature set is designed to evaluate the present script recognition technique. The technique has been tested on 3000 handwritten words in which each script contributes about 600 words. Based on the identification accuracies of multiple classifiers, Multi Layer Perceptron (MLP) has been chosen as the best classifier for the present work. For 5-fold cross validation and epoch size of 500, MLP classifier produces the best recognition accuracy of 91.79% which is quite impressive considering the shape variations of the said scripts.\n\nTitle: A Study of Different Classifier Combination Approaches for Handwritten Indic Script Recognition.\nContent: Script identification is an essential step in document image processing especially when the environment is multi-script/multilingual. Till date researchers have developed several methods for the said problem. For this kind of complex pattern recognition problem, it is always difficult to decide which classifier would be the best choice. Moreover, it is also true that different classifiers offer complementary information about the patterns to be classified. Therefore, combining classifiers, in an intelligent way, can be beneficial compared to using any single classifier. Keeping these facts in mind, in this paper, information provided by one shape based and two texture based features are combined using classifier combination techniques for script recognition (word-level) purpose from the handwritten document images. CMATERdb8.4.1 contains 7200 handwritten word samples belonging to 12 Indic scripts (600 per script) and the database is made freely available at https://code.google.com/p/cmaterdb/. The word samples from the mentioned database are classified based on the confidence scores provided by Multi-Layer Perceptron (MLP) classifier. Major classifier combination techniques including majority voting, Borda count, sum rule, product rule, max rule, Dempster-Shafer (DS) rule of combination and secondary classifiers are evaluated for this pattern recognition problem. Maximum accuracy of 98.45% is achieved with an improvement of 7% over the best performing individual classifier being reported on the validation set.\n\nTitle: Statistical Validation Of Multiple Classifiers Over Multiple Datasets In The Field Of Pattern Recognition\nContent: Classification is a machine learning technique which is used to categorise the different input patterns into different classes. To select the best classifier for a given dataset is one of the critical issues in pattern classification. Using cross-validation approach, it is possible to apply candidate algorithms on a given dataset and best classifier is selected by considering various evaluation measures of classification. But computational cost associated with this approach is trivial in nature. There are a lot of tests described in the literature but they are specifically used on different and non-related datasets. But, in pattern recognition domain the datasets are taken from the same testing samples and hence are related in nature. In this paper, we propose the use of two non-parametric tests, namely Mann-Whitney U test for comparison of two classifiers and Kruskal-Wallis H test with the corresponding post-hoc tests for comparison of multiple classifiers on multiple datasets. The tests have been fruitfully applied on the datasets of handwritten digit recognition problem taken from UCI Machine Learning Database Repository (http://www.ics.uci.edu/similar to mlearn). The results show that both the tests provide a better solution than previously proposed tests.\n\nTitle: Feature Selection Using Harmony Search for Script Identification from Handwritten Document Images.\nContent: The feature selection process can be considered a problem of global combinatorial optimization in machine learning, which reduces the irrelevant, noisy, and non-contributing features, resulting in acceptable classification accuracy. Harmony search algorithm (HSA) is an evolutionary algorithm that is applied to various optimization problems such as scheduling, text summarization, water distribution networks, vehicle routing, etc. This paper presents a hybrid approach based on support vector machine and HSA for wrapper feature subset selection. This approach is used to select an optimized set of features from an initial set of features obtained by applying Modified log-Gabor filters on prepartitioned rectangular blocks of handwritten document images written in either of 12 official Indic scripts. The assessment justifies the need of feature selection for handwritten script identification where local and global features are computed without knowing the exact importance of features. The proposed approach is also compared with four well-known evolutionary algorithms, namely genetic algorithm, particle swarm optimization, tabu search, ant colony optimization, and two statistical feature dimensionality reduction techniques, namely greedy attribute search and principal component analysis. The acquired results show that the optimal set of features selected using HSA gives better accuracy in handwritten script recognition.\n\nTitle: Offline Script Identification from multilingual Indic -script documents: A state-of-the-art\nContent: Offline Script Identification (OSI) facilitates many important applications such as automatic archiving of multilingual documents, searching online/offline archives of document images and for the selection of script specific Optical Character Recognition (OCR) in a multilingual environment. In a multilingual country like India, a document containing text words in more than one language is a common scenario. A state-of-the-art survey about the techniques available in the area of OSI for Indic scripts would be of a great aid to the researchers. Hence, a sincere attempt is made in this article to discuss the advancements reported in the literature during the last few decades. Various feature extraction and classification techniques associated with the OSI of the Indic scripts are discussed in this survey. We hope that this survey will serve as a compendium not only for researchers in India, but also for policymakers and practitioners in India. It will also help to accomplish a target of bringing the researchers working on different Indic scripts together. Taking the recent developments in OSI of Indian regional scripts into consideration, this article will provide a better platform for future research activities.\n\n## Task\nCompose an abstract for the title 'Statistical Comparison Of Classifiers For Script Identification From Multi-Script Handwritten Documents' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Script identification for handwritten document image is an open document analysis problem especially for multilingual optical character recognition (OCR) system. To design the OCR system for multi-script document pages, it is essential to recognise different scripts before running a particular OCR system of a script. The present work reports an intelligent feature-based technique for word-level script identification in multi-script handwritten document pages. At first, the text lines and then the words are extracted from the document pages. A set of 39 distinctive features have been designed of which eight features are topological and the rest (31) are based on convex hull for each word image. For selection of a suitable classifier, performances of multiple classifiers are evaluated with the designed feature set on multiple subsets of freely available database CMATERdb1.5.1 (http://www. code. google. com/p/cmaterdb), which comprises of 150 handwritten document pages containing both Devnagari and Roman script words. Statistical significance tests on these performance measures declare MLP to be the best performing one. The overall word-level script identification accuracy with MLP classifier on the said database is observed as 99.74%."
      }
    ]
  },
  {
    "tid":"172",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Adaptive multi-task monitoring system based on overhead prediction\nContent: The remarkable growth of the Internet infrastructure, the tremendous success of the Internet-based applications and their rapidly changing characteristics have made the management and monitoring of ISP networks a complex process. The design of a new monitoring system that takes into account the requirements of multiple monitoring tasks and variations in the traffic is becoming an inevitable trend. In this paper, we propose a monitoring system that adaptively adjusts its configuration according to network conditions and measurement accuracy. Our system relies on an optimization method consisting of: (i) overhead prediction to track short-term and long-term changes in the traffic, and (ii) a global weighted utility function to deal with multiple monitoring tasks. To assess the performance of our system, we propose an exhaustive experimental methodology for the evaluation of monitoring applications. We present our experimental platform and provide a global study of the operation of the proposed system and the impact of the different parameters on its behavior.\n\nTitle: Reformulating the Monitor Placement Problem: Optimal Network-Wide Sampling\nContent: Confronted with the generalization of monitoring in operational networks, researchers have proposed placement algorithms that can help ISPs deploy their monitoring infrastructure in a cost effective way, while maximizing the benefits of their infrastructure. However, a static placement of monitors cannot be optimal given the short-term and long-term variations in traffic due to re-routing events, anomalies and the normal network evolution. In addition, most ISPs already deploy router embedded monitoring functionalities. Despite some limitations (inherent to being part of a router), these monitoring tools give greater visibility on the network traffic but raise the question on how to configure a network-wide monitoring infrastructure that may contain hundreds of monitoring points. We reformulate the placement problem as follows. Given a network where all links can be monitored, which monitors should be activated and which sampling rate should be set on these monitors in order to achieve a given measurement task with high accuracy and low resource consumption? We provide a formulation of the problem, an optimal algorithm to solve it, and we study its performance on a real backbone network.\n\nTitle: Network-wide monitoring through self-configuring adaptive system.\nContent: The remarkable growth of the Internet infrastructure and the increasing heterogeneity of applications and users' behavior make more complex the manageability and monitoring of ISP networks and raises the cost of any new deployment. The main consequence of this trend is an inherent disagreement between existing monitoring solutions and the increasing needs of management applications. In this context, we present the design of an adaptive centralized architecture that provides visibility over the entire network through a network-wide cognitive monitoring system. Practically, given a measurement task and a constraint on the volume of collected information, the proposed architecture drives the sampling rates on the interfaces of network routers to achieve the maximum possible accuracy, while adapting itself to any change in network traffic conditions. We illustrate our work with an accounting application whose purpose is to estimate the volume of aggregate flows across a backbone transit network. The paper provides a global study of the functioning of the proposed system and the impact of the different parameters on its behavior. The performance of our system is validated in typical scenarios over an experimental platform we developed for the purpose of the study.\n\nTitle: On Active Sampling of Controlled Experiments for QoE Modeling\nContent: For internet applications, measuring, modeling and predicting the quality experienced by end users as a function of network conditions is challenging. A common approach for building application specific Quality of Experience (QoE) models is to rely on controlled experimentation. For accurate QoE modeling, this approach can result in a large number of experiments to carry out because of the multiplicity of the network features, their large span (e.g., bandwidth, delay) and the time needed to setup the experiments themselves. However, most often, the space of network features in which experimentations are carried out shows a high degree of uniformity in the training labels of QoE. This uniformity, difficult to predict beforehand, amplifies the training cost with little or no improvement in QoE modeling accuracy. So, in this paper, we aim to exploit this uniformity, and propose a methodology based on active learning, to sample the experimental space intelligently, so that the training cost of experimentation is reduced. We prove the feasibility of our methodology by validating it over a particular case of YouTube streaming, where QoE is modeled both in terms of interruptions and stalling duration.\n\nTitle: Temporal resolution adaptation in real-time bitrate estimation from packed sampled traffic\nContent: Packet sampling can greatly reduce traffic measurement overhead in high-speed networks. At the same time, this operation introduces estimation errors that have to be carefully handled to ensure a reasonable measurement accuracy. Recently, a frequency-based approach has been proposed to catch the impact of such errors in bitrate estimation of IP flows at various time-scales. In particular, a closed-form expression for the signal-to-noise ratio has been derived as a function of the packet sampling probability, the bin size, and some basic information about the flow (i.e., first and second order moments of the packet size, and long term average packet-rate). In this work, we adopt this model to design a real-time algorithm that properly tunes the temporal resolution of bitrate estimates available at a router, with the goal of assuring a target measurement accuracy. The effectiveness of the proposed approach has been demonstrated by computer simulations, carried out using real packet traces.\n\n## Task\nCompose an abstract for the title 'A multi-task adaptive monitoring system combining different sampling primitives' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Traffic measurement and analysis are crucial management activities for network operators. With the increase in traffic volume, operators resort to sampling primitives to reduce the measurement load. Unfortunately, existing systems use sampling primitives separately and configure them statically to achieve some performance objective. It becomes then important to design a new system that combines different existing sampling primitives together to support a large spectrum of monitoring tasks while providing the best possible accuracy by spatially correlating measurements and adapting the configuration to traffic variability. In this paper, and to prove the interest of the joint approach, we introduce an adaptive system that combines two sampling primitives, packet sampling and flow sampling, and that is able to satisfy multiple monitoring tasks. Our system consists of two main functions: (i) a global estimator that investigates measurements done by the different sampling primitives in order to deal with multiple monitoring tasks and to construct a more reliable global estimator while providing visibility over the entire network; (ii) an optimization method based on overhead prediction that allows to reconfigure monitors according to accuracy requirements and monitoring constraints. We present an exhaustive experimental methodology with different monitoring tasks in order to assess the performance of our system. Our experimentations are done on our MonLab platform that we developed for the purpose of this research.\n\n"
      }
    ]
  },
  {
    "tid":"173",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Integrating a Boolean Satisfiability Checker and BDDs for Combinational Equivalence Checking\nContent: There has been much interest in techniques which combine the advantages of function-based methods, such as BDDs, with structure-based methods, such as ATPG, for verifying the equivalence of combinational circuits. However, most existing efforts have focused on exploiting circuit similarity through use of learning and/or ATPG-based methods rather than on making the integration between BDDs and ATPG techniques efficient. This paper presents a new technique, where the focus is on improving the equivalence check itself, thereby making it more robust in the absence of circuit similarity. It is based on tight integration of a Boolean Satisfiability Checker with BDDs, whereby BDDs are effectively used to reduce both the problem size and the number of backtracks for the satisfiability problem. This methodology does not preclude exploitation of circuit similarity, when it exists, since the improved check can be easily incorporated as the inner loop of the well-known iterative framework involving search and replacement of internally equivalent nodes. We demonstrate the significance of our contributions with practical results on the ISCAS benchmark circuits.\n\nTitle: Completeness bounds and sequentialization for model checking of interacting firmware and hardware\nContent: An emerging trend in system design is to implement complex system management functions in firmware (FW). This changing design scenario requires support for verifying FW in the context of its hardware (HW) environment. As shown in previous work, there is value in a unified HW-FW model for driving the verification tasks. This model can help identify specific commonly-occurring interaction patterns between the HW and FW. These patterns enable pruning the verification search space as demonstrated in previous work in automating FW test generation using concolic testing.\n\nIn this work, we introduce a bounded model checking (BMC)-based methodology for FW verification. Although BMC is effective for finding bugs by unrolling the underlying transition system up to some bound, it requires a completeness threshold on the bound to ensure complete verification. We show how commonly occurring FW code patterns can be exploited, using inexpensive static analysis techniques, to determine this completeness bound. Further, we show how this bound analysis, combined with the interaction patterns in the unified HW-FW model, is used to sequentialize the concurrent FW and HW code, i.e., to derive a sequential program that represents the parallel interaction of the FW and HW. This enables the direct application of standard software model checkers such as CBMC on this sequentialized program.\n\nWe have automated this process by implementing: (i) a static completeness bound analyzer on top of the tool Frama-C, and (ii) a sequentializer to generate code for verification by the CBMC model checker. We evaluate the resulting tool using three real FW benchmarks, each consisting of a Linux device driver and its interacting QEMU-emulated HW code with multiple correctness properties. We successfully computed the BMC completeness bounds for 41 out of 46 properties and completed model checking for 12 out of 16 FW transactions.\n\n\n\nTitle: Efficient SAT-based unbounded symbolic model checking using circuit cofactoring\nContent: We describe an efficient approach for SAT-based quantifier elimination that significantly improves the performance of pre-image and fixed-point computation in SAT-based unbounded symbolic model checking (UMC). The proposed method captures a larger set of new states per SAT-based enumeration step during quantifier elimination, in comparison to previous approaches. The novelty of our approach is in the use of circuit-based cofactoring to capture a large set of states, and in the use of a functional hashing based simplified circuit graph to represent the captured states. We also propose a number of heuristics to further enlarge the state set represented per enumeration, thereby reducing the number of enumeration steps. We have implemented our techniques in a SAT-based UMC framework where we show the effectiveness of SAT-based existential quantification on public benchmarks, and on a number of large industry designs that were hard to model check using purely BDD-based techniques. We show several orders of improvement in time and space using our approach over previous CNF-based approaches. We also present controlled experiments to demonstrate the role of several heuristics proposed in the paper. Importantly, we were able to prove using our method the correctness of a safety property in an industry design that could not be proved using other known approaches.\n\nTitle: Completeness in SMT-based BMC for software programs\nContent: Bounded Model Checking (BMC) is incomplete without a completeness threshold (CT) bound. Previous methods, using recurrence diameter for obtaining CT, check for existence of a longest loop-free path at every depth k. For terminating software programs, we propose an efficient method for obtaining CT that requires solving a formula of size O(k) at some depths only, as compared to previous methods that require solving a formula of O(k2) (or O(klogk)) size at every depth. We augment previous methods for BMC simplifications using model transformation and control flow information, with context-sensitive analysis. This results in more BMC simplifications and further reduction in the number of CT checks. We have implemented our techniques in a Satisfiability Modulo Theory (SMT)-based BMC framework. Our controlled experiments on real-world software programs show that our proposed formulation provides significant improvements over previous approaches.\n\nTitle: Efficient Distributed SAT and SAT-Based Distributed Bounded Model Checking\nContent: SAT-based Bounded Model Checking (BMC), though a robust and scalable verification approach, still is computationally intensive, requiring large memory and time. Even with the recent development of improved SAT solvers, the memory limitation of a single server rather than time can become a bottleneck for doing deeper BMC search for large designs. Distributing computing requirements of BMC over a network of workstations can overcome the memory limitation of a single server, albeit at increased communication cost. In this paper, we present (a) a method for distributed SAT over a network of workstations using a Master/Client model where each Client workstation has an exclusive partition of the SAT problem and uses knowledge of partition topology to communicate with other Clients, (b) a method for distributing SAT-based BMC using the distributed SAT. For the sake of scalability, at no point in the BMC computation does a single workstation have all the information. We experimented on a network of heterogeneous workstations interconnected with a standard Ethernet LAN. To illustrate, on an industrial design with \u223c13 K FFs and \u223c0.5 million gates, the non-distributed BMC on a single workstation (with 4 GB memory) ran out of memory after reaching a depth of 120; on the other hand, our SAT-based distributed BMC over 5 similar workstations was able to go up to 323 steps with a communication overhead of only 30%.\n\n## Task\nCompose an abstract for the title 'Using complete-1-distinguishability for FSM equivalence checking' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper introduces the use of the Complete-1-Distinguishability (C-1-D) property for simplifying FSM verification. This property eliminates the need for a traversal of the product machine for the implementation and the specification. Instead, a much simpler check suffices. This check consists of first obtaining a 1-equivalence mapping between states of the two machines, and then checking that it is a bisimulation relation. The C-1-D property can be used directly on specifications for which it naturally holds -- a condition that has not been exploited thus far in FSM verification. We also show how this property can be enforced on arbitrary FSMs by exposing some of the latch outputs as pseudo-primary outputs during synthesis and verification. In this sense, our synthesis/verification methodology provides another point in the tradeoff curve between constraints-on-synthesis versus complexity-of-verification. Practical experiences with using this methodology have resulted in success with several examples for which it is not possible to complete verification using existing implicit state space traversal techniques."
      }
    ]
  },
  {
    "tid":"174",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Negative Correlation Discovery for Big Multimedia Data Semantic Concept Mining and Retrieval\nContent: With massive amounts of data producing each day in almost every field, traditional data processing techniques have become more and more inadequate. However, the research of effectively managing and retrieving these big data is still under development. Multimedia high-level semantic concept mining and retrieval in big data is one of the most challenging research topics, which requires joint efforts from researchers in both big data mining and multimedia domains. In order to bridge the semantic gap between high-level concepts and low-level visual features, correlation discovery in semantic concept mining is worth exploring. Meanwhile, correlation discovery is a computationally intensive task in the sense that it requires a deep analysis of very large and growing repositories. This paper presents a novel system of discovering negative correlation for semantic concept mining and retrieval. It is designed to adapt to Hadoop MapReduce framework, which is further extended to utilize Spark, a more efficient and general cluster computing engine. The experimental results demonstrate the feasibility of utilizing big data technologies in negative correlation discovery.\n\nTitle: Negative-Based Sampling for Multimedia Retrieval\nContent: Nowadays, in such a high-tech living lifestyle, profusion of multimedia data are produced and propagated around the world. To identify meaningful semantic concepts from the large amount of data, one of the major challenges is called the data imbalance problem. Data imbalance occurs when the number of positive instances (i.e., instances which contain the target concept) is greatly less than the number of negative instances (i.e., instances which do not contain the target concept). In other words, the ratio between positive and negative instances is extremely low. Rebalancing the dataset is usually proposed to resolve the problem by sampling or data pruning. In this paper, we propose a sampling method which consists of three stages, namely selecting features to identify the negative instances, producing negative ranking scores, and performing sampling. The method is compared with some other existing methods on the TRECVID dataset and is demonstrated to have better performance.\n\nTitle: Utilizing Indirect Associations in Multimedia Semantic Retrieval\nContent: Technological developments have lead to the propagation of massive amounts of data in the form of text, image, audio, and video. The unstoppable trend draws researchers' attention to develop approaches to efficiently retrieve and manage multimedia data. The inadequacy of keyword-based search in multimedia data retrieval due to non-existent or incomplete text annotations has called for the development of a contentbased multimedia data management framework. Specifically, detecting high-level semantic concepts is one of the rapidly growing topics in this regard. In order to thoroughly identify semantic concepts in data which have different representations and are derived from different modalities, both positive and negative inter-concept correlations have been recently studied and explored to enhance the re-ranking performance. In this paper, an indirect association rule mining (IARM) approach is introduced to reveal the hidden correlation among semantic concepts. The effectiveness of IARM is evaluated by Multiple Correspondence Analysis (MCA). Furthermore, normalization and score integration are performed to achieve the optimal classification results. The TRECVID 2011 benchmark dataset is used to show the effectiveness of the proposed IARM factor in the re-ranking process.\n\nTitle: Utilization of Co-occurrence Relationships between Semantic Concepts in Re-ranking for Information Retrieval\nContent: Semantic information retrieval is a popular research topic in the multimedia area. The goal of the retrieval is to provide the end users with as relevant results as possible. Many research efforts have been done to build ranking models for different semantic concepts (or classes). While some of them have been proven to be effective, others are still far from satisfactory. Our observation that certain target semantic concepts have high co-occurrence relationships with those easy-to-retrieve semantic concepts (or called reference semantics) has motivated us to utilize such co-occurrence relationships between semantic concepts in information retrieval and re-ranking. In this paper, we propose a novel semantic retrieval and re-ranking framework that takes advantage of the co-occurrence relationships between a target semantic concept and a reference semantic concept to re-rank the retrieved results. The proposed framework discretizes the training data into a set of feature-value pairs and employs Multiple Correspondence Analysis (MCA) to capture the correlation in terms of the impact weight between feature-value pairs and the positive-positive class in which the data instances belong to both the target semantic concept and the reference semantic concept. A combination of all these impact weights is utilized to re-rank the retrieved results for the target semantic concept. Comparative experiments are designed and evaluated on TRECVID 2005 and TRECVID 2010 video collections with public-available ranking scores. Experimental results on different retrieval scales demonstrate that our proposed framework can enhance the retrieval results for the target semantic concepts in terms of average precision, and the improvements for some semantic concepts are promising.\n\nTitle: Sparse Linear Integration of Content and Context Modalities for Semantic Concept Retrieval\nContent: The semantic gap between low-level visual features and high-level semantics is a well-known challenge in content-based multimedia information retrieval. With the rapid popularization of social media, which allows users to assign tags to describe images and videos, attention is naturally drawn to take advantage of these metadata in order to bridge the semantic gap. This paper proposes a sparse linear integration (SLI) model that focuses on integrating visual content and its associated metadata, which are referred to as the content and the context modalities, respectively, for semantic concept retrieval. An optimization problem is formulated to approximate an instance using a sparse linear combination of other instances and minimize the difference between them. The prediction score of a concept for a test instance measures how well it can be reconstructed by the positive instances of that concept. Two benchmark image data sets and their associated tags are used to evaluate the SLI model. Experimental results show promising performance by comparing with the approaches based on a single modality and approaches based on popular fusion methods.\n\n## Task\nCompose an abstract for the title 'Supporting Semantic Concept Retrieval With Negative Correlations In A Multimedia Big Data Mining System' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"With the extensive use of smart devices and blooming popularity of social media websites such as Flickr, YouTube, Twitter, and Facebook, we have witnessed an explosion of multimedia data. The amount of data nowadays is formidable without effective big data technologies. It is well-acknowledged that multimedia high-level semantic concept mining and retrieval has become an important research topic; while the semantic gap (i.e., the gap between the low-level features and high-level concepts) makes it even more challenging. To address these challenges, it requires the joint research efforts from both big data mining and multimedia areas. In particular, the correlations among the classes can provide important context cues to help bridge the semantic gap. However, correlation discovery is computationally expensive due to the huge amount of data. In this paper, a novel multimedia big data mining system based on the MapReduce framework is proposed to discover negative correlations for semantic concept mining and retrieval. Furthermore, the proposed multimedia big data mining system consists of a big data processing platform with Mesos for efficient resource management and with Cassandra for handling data across multiple data centers. Experimental results on the TRECVID benchmark datasets demonstrate the feasibility and the effectiveness of the proposed multimedia big data mining system with negative correlation discovery for semantic concept mining and retrieval."
      }
    ]
  },
  {
    "tid":"175",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Semidefinite Relaxation Approach for Beamforming in Cooperative Clustered Multicell Systems With Novel Limited Feedback Scheme\nContent: In this paper, we consider suboptimal cooperative downlink beamforming strategies to maximize the user sum rate in a clustered multicell system with limited feedback. We decouple the problem into multiple independent subproblems through the uplink-downlink duality theorem. Semidefinite relaxation is applied to transform the original nonconvex problem into a convex semidefinite programming problem, which can be efficiently solved in polynomial time. We propose a novel limited feedback scheme based on a compressive sensing technique that can yield high-quality channel state information (CSI) feedback. We also consider a limited feedback scenario where base stations within one cluster have different estimates of the same CSI knowledge. We investigate the channel quantization criterion by choosing the codeword vector that has the minimum norm of the difference between the codeword and the normalized channel. Numerical results demonstrate the effectiveness of the proposed beamforming algorithm and limited feedback schemes.\n\nTitle: Downlink beamforming in clustered cooperative multicell MIMO systems\nContent: Base station cooperation has been shown to be an effective way to increase the system capacity and to suppress interference in mobile communications. A clustered multicell system is studied in this paper, where the mobile stations (MSs) will encounter intra-cluster and inter-cluster interference. By sharing the user data among base stations (BSs), one MS can be served by several BSs simultaneously, which enhances the sum rate at the MSs. The BS needs the channel state information (CSI) feedback from the MS to design its beamforming scheme. Herein, we analyze the distributed cooperation where each BS has only local CSI. A distributed beamforming technique based on the uplink-downlink duality is applied. Imperfect CSI through limited feedback is also investigated. Simulation results show that the system performance in terms of the quantized channel is close to the performance given full CSI.\n\nTitle: Novel channel covariance matrix feedback strategy in coordinated multicell MISO networks\nContent: This work studies the channel covariance matrix feedback problem in the coordinated multicell network, where a couple of base stations (BSs) collaborate with each other to transmit signals to the intended users. The design of transmit beamforming scheme requires the knowledge of channel state information (CSI), and the quality of CSI feedback will affect the system performance. We propose a novel channel covariance matrix feedback scheme based on matrix completion theory, in which the channel covariance matrix can be recovered with high accuracy via solving a nuclear-norm minimization problem, if a small number of covariance matrix entries contaminated by noise are fed back to BSs. Numerical results show that the proposed feedback scheme can yield high quality statistical channel knowledge at BSs.\n\nTitle: Diversity-multiplexing tradeoff of opportunistic relay system with multiple-antenna destination\nContent: In this study, the authors investigate the diversity-multiplexing tradeoff (DMT) of a wireless system in which user equipments (UEs) are equipped with only one antenna while the destination has multiple antennas. Each UE is assisted by M half-duplex cooperative users serving as relay nodes. They combine the opportunistic relaying scheme and the cooperative spatial multiplexing (C-SM) scheme to produce an opportunistic C-SM (OC-SM) scheme. The fraction of time allocated to the source-relay link and the relay-destination link is treated as a design parameter to minimise the outage probability of the system. At high signal-to-noise ratio regime, the optimal time allocated factor is obtained in a closed-form expression through DMT analysis. Based on the DMT result of the proposed OC-SM scheme, an adaptive OC-SM (AOC-SM) approach in which the system can switch between different C-SM models adaptively based on the rate transmitted is proposed. To reduce the complexity of the relay nodes, an AOC-SM scheme with relay selection (SAOC-SM) in which a constant number of relay nodes is selected from the set of relay nodes that have successfully decoded the source message is investigated. The DMT results of the SAOC-SM scheme and the AOC-SM scheme are obtained and compared.\n\nTitle: An Efficient Successive Relaying Protocol for Multiple-Relay Cooperative Networks\nContent: We propose an efficient successive relaying (SR) transmission protocol for multiple-relay cooperative systems under the half-duplex constraint. In this protocol, the process of relay selection is conducted once the instantaneous channel state information changes. During each channel coherent interval, the source keeps transmitting newly generated messages, and two selected relays successively decode and forward the source's information to the destination. In particular, in one time slot, a selected relay jointly decodes both signals transmitted from the source and the other selected relay. In the next time slot, this relay forwards the superposition of these two decoded signals. The destination jointly decodes messages at the end of each interval. The lower bound of outage probability is derived in a closed-form expression and the diversity-multiplexing trade-off (DMT) performance is characterized as well. It is demonstrated by the numerical results that the proposed SR protocol is capable of achieving both full diversity gain and high multiplexing gain.\n\n## Task\nCompose an abstract for the title 'Semidefinite relaxation based beamforming in clustered cooperative multicell MISO systems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a novel coordinated downlink beamforming strategy based on the semidefinite relaxation (SDR) approach for a clustered multicell cellular system. As base station (BS) cooperation can provide significant performance improvement for the system capacity and interference suppression, we divide the whole network into a number of clusters. In each cluster, several BSs share the common user data and cooperatively design their signaling strategies. In this system, maximizing the sum-rate requires solving a global nonconvex optimization problem. We apply the semidefinite programming (SDP) relaxation to convert the original problem into distributed convex SDP which can be solved efficiently and optimally with convex optimization techniques. Numerical results show that the proposed beamforming algorithm provides a remarkable sum-rate enhancement and outperforms some existing schemes."
      }
    ]
  },
  {
    "tid":"176",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Improved Combinatorial Algorithms for the Facility Location and k-Median Problems\nContent: We present improved combinatorial approximation algorithms for the uncapacitated facility location and k-median problems. Two central ideas in most of our results are cost scaling and greedy improvement. We present a simple greedy local search algorithm which achieves an approximation ratio of \\math in \\math time. This also yields a bicriteria approximation tradeoff of \\math for facility cost versus service cost which is better than previously known tradeoffs and close to the best possible. Combining greedy improvement and cost scaling with a recent primal dual algorithm for facility location due to Jain and Vazirani, we get an approximation ratio of 1.853 in \\math time. This is already very close to the approximation guarantee of the best known algorithm which is LP-based. Further, combined with the best known LP-based algorithm for facility location, we get a very slight improvement in the approximation factor for facility location, achieving 1.728. We present improved approximation algorithms for capacitated facility location and a variant.We also present a 4-approximation for the k-median problem, using similar ideas, building on the 6-approximation of Jain and Vazirani. The algorithm runs in \\math time.\n\nTitle: A dependent LP-rounding approach for the k-median problem\nContent: In this paper, we revisit the classical k-median problem. Using the standard LP relaxation for k-median, we give an efficient algorithm to construct a probability distribution on sets of k centers that matches the marginals specified by the optimal LP solution. Analyzing the approximation ratio of our algorithm presents significant technical difficulties: we are able to show an upper bound of 3.25. While this is worse than the current best known 3+\u03b5 guarantee of [2], because: (1) it leads to 3.25 approximation algorithms for some generalizations of the k-median problem, including the k-facility location problem introduced in [10], (2) our algorithm runs in $\\tilde{O}(k^3 n^2/\\delta^2)$ time to achieve 3.25(1+\u03b4)-approximation compared to the O(n8) time required by the local search algorithm of [2] to guarantee a 3.25 approximation, and (3) our approach has the potential to beat the decade old bound of 3+\u03b5 for k-median. We also give a 34-approximation for the knapsack median problem, which greatly improves the approximation constant in [13]. Using the same technique, we also give a 9-approximation for matroid median problem introduced in [11], improving on their 16-approximation.\n\nTitle: Almost 3-Approximate Correlation Clustering in Constant Rounds\nContent: We study parallel algorithms for correlation clustering. Each pair among n objects is labeled as either \u201csimilar\u201d or \u201cdissimilar\u201d. The goal is to partition the objects into arbitrarily many clusters while minimizing the number of disagreements with the labels.Our main result is an algorithm that for any $\\varepsilon>0$ obtains a (3 + $\\varepsilon$)-approximation in $O(1/\\varepsilon$) rounds (of models such as massively parallel computation, local, and semi-streaming). This is a culminating point for the rich literature on parallel correlation clustering. On the one hand, the approximation (almost) matches a natural barrier of 3 for combinatorial algorithms. On the other hand, the algorithm\u2019s round-complexity is essentially constant.To achieve this result, we introduce a simple $O(1/\\varepsilon$)-round parallel algorithm. Our main result is to provide an analysis of this algorithm, showing that it achieves a (3 + $\\varepsilon$)-approximation. Our analysis draws on new connections to sublinear-time algorithms. Specifically, it builds on the work of Yoshida, Yamamoto, and Ito [1] on bounding the \u201cquery complexity\u201d of greedy maximal independent set. To our knowledge, this is the first application of this method in analyzing the approximation ratio of any algorithm.Full version. Due to the page limit, this version of the paper does not include all the proofs. The full version of the paper is available at [2].\n\nTitle: Approximating min-sum k-clustering in metric spaces\nContent: The min-sum k-clustering problem in a metric space is to find a partition of the space into k clusters as to minimize the total sum of distances between pairs of points assigned to the same cluster. We give the first polynomial time non-trivial approximation algorithm for this problem. The algorithm provides an $\\ratio$ approximation to the min-sum k-clustering problem in general metric spaces, with running time $\\runtime$. The result is based on embedding of metric spaces into hierarchically separated trees. We also provide a bicriteria approximation result that provides a constant approximation factor solution with only a constant factor increase in the number of clusters. This result is obtained by modifying and drawing ideas from recently developed primal dual approximation algorithms for facility location.\n\nTitle: Detecting High Log-Densities -- an O(n^1/4) Approximation for Densest k-Subgraph\nContent: In the Densest k-Subgraph problem, given a graph G and a parameter k, one needs to find a subgraph of G induced on k vertices that contains the largest number of edges. There is a significant gap between the best known upper and lower bounds for this problem. It is NP-hard, and does not have a PTAS unless NP has subexponential time algorithms. On the other hand, the current best known algorithm of Feige, Kortsarz and Peleg [10], gives an approximation ratio of n(1/3-epsilon) for some fixed epsilon > 0 (later estimated at around epsilon = 1/90). We present an algorithm that for every epsilon > 0 approximates the Densest k-Subgraph problem within a ratio of n(1/4+epsilon) in time n(O(1/epsilon)). If allowed to run for time n(O(log n)), the algorithm achieves an approximation ratio of O(n(1/4)). Our algorithm is inspired by studying an average-case version of the problem where the goal is to distinguish random graphs from random graphs with planted dense subgraphs - the approximation, ratio we achieve for the general case matches the \"distinguishing ratio\" we obtain for this planted Problem. At a high level, our algorithms involve cleverly counting appropriately defined trees of constant size in G, and using these counts to identify the vertices of the dense subgraph. We say that a graph G(V, E) has log-density alpha if its average degree is Theta(vertical bar V vertical bar(alpha))in The algorithmic core of our result is a procedure to output a k-subgraph of 'nontrivial' density whenever the log-density of the densest k-subgraph is larger than the log-density of the host graph. We outline an extension to our approximation algorithm which achieves an O(n(1/4-epsilon))-approximation in O(2(nO(epsilon))) time. We also show that, for certain parameter ranges, eigenvalue and SDP based techniques can outperform our basic distinguishing algorithm for random instances (in polynomial time), though without improving upon the O(n(1/4)) guarantee overall. We outline an extension to our approximation algorithm which achieves an O(n(1/4-epsilon))-approximation in O(2(nO(epsilon))) time. We also show that, for certain parameter ranges, eigenvalue and SDP based techniques can outperform our basic distinguishing algorithm for random instances (in polynomial time), though without improving upon the O(n(1/4)) guarantee overall.\n\n## Task\nCompose an abstract for the title 'A constant-factor approximation algorithm for the k-median problem' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We present the first constant-factor approximation algorithm for the metric k-median problem. The k-median problem is one of the most well-studied clustering problems, i.e., those problems in which the aim is to partition a given set of points into clusters so that the points within a cluster are relatively close with respect to some measure. For the metric k-median problem, we are given n points in a metric space. We select k of these to be cluster centers and then assign each point to its closest selected center. If point j is assigned to a center i, the cost incurred is proportional to the distance between i and j. The goal is to select the k centers that minimize the sum of the assignment costs. We give a 62/3-approximation algorithm for this problem. This improves upon the best previously known result of O(log k log log k), which was obtained by refining and derandomizing a randomized O(log n log log n)-approximation algorithm of Bartal."
      }
    ]
  },
  {
    "tid":"177",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Two conditions for equivalence of 0-norm solution and 1-norm solution in sparse representation.\nContent: In sparse representation, two important sparse solutions, the 0-norm and 1-norm solutions, have been receiving much of attention. The 0-norm solution is the sparsest, however it is not easy to obtain. Although the 1-norm solution may not be the sparsest, it can be easily obtained by the linear programming method. In many cases, the 0-norm solution can be obtained through finding the 1-norm solution. Many discussions exist on the equivalence of the two sparse solutions. This paper analyzes two conditions for the equivalence of the two sparse solutions. The first condition is necessary and sufficient, however, difficult to verify. Although the second is necessary but is not sufficient, it is easy to verify. In this paper, we analyze the second condition within the stochastic framework and propose a variant. We then prove that the equivalence of the two sparse solutions holds with high probability under the variant of the second condition. Furthermore, in the limit case where the 0-norm solution is extremely sparse, the second condition is also a sufficient condition with probability 1.\n\nTitle: Probability estimation for recoverability analysis of blind source separation based on sparse representation\nContent: An important application of sparse representation is underdetermined blind source separation (BSS), where the number of sources is greater than the number of observations. Within the stochastic framework, this paper discusses recoverability of underdetermined BSS based on a two-stage sparse representation approach. The two-stage approach is effective when the source matrix is sufficiently sparse. The first stage of the two-stage approach is to estimate the mixing matrix, and the second is to estimate the source matrix by minimizing the 1-norms of the source vectors subject to some constraints. After estimating the mixing matrix and fixing the number of nonzero entries of a source vector, we estimate the recoverability probability (i.e., the probability that the source vector can be recovered). A general case is then considered where the number of nonzero entries of the source vector is fixed and the mixing matrix is drawn from a specific probability distribution. The corresponding probability estimate on recoverability is also obtained. Based on this result, we further estimate the recoverability probability when the sources are also drawn from a distribution (e.g., Laplacian distribution). These probability estimates not only reflect the relationship between the recoverability and sparseness of sources, but also indicate the overall performance and confidence of the two-stage sparse representation approach for solving BSS problems. Several simulation results have demonstrated the validity of the probability estimation approach.\n\nTitle: Underdetermined blind source separation based on sparse representation\nContent: This paper discusses underdetermined (i.e., with more sources than sensors) blind source separation (BSS) using a two-stage sparse representation approach. The first challenging task of this approach is to estimate precisely the unknown mixing matrix. In this paper, an algorithm for estimating the mixing matrix that can be viewed as an extension of the DUET and the TIFROM methods is first developed. Standard clustering algorithms (e.g., K-means method) also can be used for estimating the mixing matrix if the sources are sufficiently sparse. Compared with the DUET, the TIFROM methods, and standard clustering algorithms, with the authors' proposed method, a broader class of problems can be solved, because the required key condition on sparsity of the sources can be considerably relaxed. The second task of the two-stage approach is to estimate the source matrix using a standard linear programming algorithm. Another main contribution of the work described in this paper is the development of a recoverability analysis. After extending the results in , a necessary and sufficient condition for recoverability of a source vector is obtained. Based on this condition and various types of source sparsity, several probability inequalities and probability estimates for the recoverability issue are established. Finally, simulation results that illustrate the effectiveness of the theoretical results are presented.\n\nTitle: Deterministic construction of sparse binary matrices via incremental integer optimization.\nContent: A central problem in compressed sensing (CS) is the design of measurement matrices. Compared with the conventional random matrices, sparse binary matrices have some attractive properties, such as lower storage/encoding cost and easy hardware implementation. In this paper, we formulate the construction of sparse binary measurement matrices from an optimization standpoint. A new algorithm is presented to construct arbitrary-size sparse binary measurement matrices through relaxing the resultant optimization model to an incremental integer programming problem. The proposed method in general outputs sparse binary matrices with optimal mutual coherence. In addition, we prove that the constructed matrices can be almost completely incoherent with the conventional wavelet dictionary. Extensive simulation results show that the sparse binary matrices constructed by the proposed algorithm significantly outperform Gaussian random matrices, random sparse binary matrices and two well-performing deterministic sparse binary matrices.\n\nTitle: Noisy Sparse Recovery Based on Parameterized Quadratic Programming by Thresholding\nContent: Parameterized quadratic programming (Lasso) is a powerful tool for the recovery of sparse signals based on underdetermined observations contaminated by noise. In this paper, we study the problem of simultaneous sparsity pattern recovery and approximation recovery based on the Lasso. An extended Lasso method is proposed with the following main contributions: (1) we analyze the recovery accuracy of Lasso under the condition of guaranteeing the recovery of nonzero entries positions. Specifically, an upper bound of the tuning parameter h of Lasso is derived. If h exceeds this bound, the recovery error will increase with h; (2) an extended Lasso algorithm is developed by choosing the tuning parameter according to the bound and at the same time deriving a threshold to recover zero entries from the output of the Lasso. The simulation results validate that our method produces higher probability of sparsity pattern recovery and better approximation recovery compared to two state-of-the-art Lasso methods.\n\n## Task\nCompose an abstract for the title 'Equivalence probability and sparsity of two sparse solutions in sparse representation.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper discusses the estimation and numerical calculation of the probability that the 0-norm and 1-norm solutions of underdetermined linear equations are equivalent in the case of sparse representation. First, we define the sparsity degree of a signal. Two equivalence probability estimates are obtained when the entries of the 0-norm solution have different sparsity degrees. One is for the case in which the basis matrix is given or estimated, and the other is for the case in which the basis matrix is random. However, the computational burden to calculate these probabilities increases exponentially as the number of columns of the basis matrix increases. This computational complexity problem can be avoided through a sampling method. Next, we analyze the sparsity degree of mixtures and establish the relationship between the equivalence probability and the sparsity degree of the mixtures. This relationship can be used to analyze the performance of blind source separation (BSS). Furthermore, we extend the equivalence probability estimates to the small noise case. Finally, we illustrate how to use these theoretical results to guarantee a satisfactory performance in underdetermined BSS."
      }
    ]
  },
  {
    "tid":"178",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: The effectiveness of an optimized EPMcreate as a creativity enhancement technique for Web site requirements elicitation\nContent: Creativity is often needed in requirements elicitation, i.e., requirement idea generation; and techniques to enhance creativity are believed to be useful. This paper describes two controlled experiments to compare the requirements-elicitation effectiveness of three creativity enhancement techniques (CET): (1) full EPMcreate; (2) Power-Only EPMcreate, an optimization of full EPMcreate; and (3) traditional brainstorming. In each experiment, one team of university students applied one of the two or three CETs under study in the experiment to generate ideas for requirements for enhancing a high school's public Web site. The results of the first experiment indicate that Power-Only EPMcreate is more effective, by the quantity and quality of the ideas generated, than the full EPMcreate, which is, in turn, more effective than brainstorming. The results of the second experiment confirm that Power-Only EPMcreate is more effective, by the same measures, than full EPMcreate. In each experiment, for the sake of uniform, reproducible evaluation, a requirement idea is considered high quality if it is both new and useful.\n\nTitle: Creativity Techniques for Requirements Elicitation: Comparing Four-Step EPMcreate-Based Processes\nContent: [Context and motivation] The Elementary Pragmatic Model Creativity Technique, a.k.a EPMcreate, is a method for creative requirements discovery. It includes 16 steps corresponding to all the possible combinations of two classes of stakeholders' viewpoints. The feasibility and effectiveness of the full 16-step process have been confirmed by a number of experiments. The need to reduce the number of steps was observed in the very first experiments run in 2003. To address that problem, a few years later, a four-step creativity technique, POEPMcreate (Power-Only EPMcreate), was defined and tested. [Question/problem] More recently, the general problem of downsizing EPMcreate to define lighter weight processes based on sub-sets of the 16 steps has been theoretically investigated. The theoretical analysis demanded experimental confirmation. [Principal idea/Goal] This paper describes an experiment applying and comparing two four-step techniques, POEPMcreate and a new technique, ROS-EPMcreate (Redundant, Odd Step EPMcreate), resembling traditional requirements elicitation with brainstorming. [Contribution] The results of the experiment, even if preliminary, seem to indicate that ROSEPMcreate is at least as feasible and effective as POEPMcreate. The results also give new insights and suggest issues to be considered in future experiments. The paper also offers Kano categories as a new way to evaluate the innovativeness of generated requirement ideas.\n\nTitle: An industrial case study of the impact of domain ignorance on the effectiveness of requirements idea generation during requirements elicitation\nContent: One of the factors that is supposed to have a significant effect on an individual's effectiveness during requirements engineering activities is knowledge of the problem being solved by the system to be built, i.e., domain knowledge. Nevertheless, domain knowledge is a double-edged sword. While in-depth domain knowledge facilitates understanding the details of the problem, in-depth domain knowledge can promote falling for tacit assumptions of the domain and overlooking the obvious. On the other hand, lack of domain knowledge can facilitate more innovative out-of-the-domain-box idea generation. This paper describes a case study carried out in industry of the idea generation part of a requirements idea brainstorming session conducted by a team deliberately constructed with four domain experts supplied by the company participating in the case study and with four domain ignorants supplied by the authors. The results support the conclusion that having a team consisting of a mix of domain experts and domain ignorants improves the effectiveness of the idea generation part of requirements idea brainstorming.\n\nTitle: Improving the identification of hedonic quality in user requirements: a second controlled experiment.\nContent: Systematically engineering a good user experience (UX) into a computer-based system under development demands that the user requirements of the system reflect all needs, including emotional, of all stakeholders. User requirements address two different types of qualities: pragmatic qualities (PQs), that address system functionality and usability, and hedonic qualities (HQs) that address the stakeholder\u2019s psychological well-being. Studies show that users tend to describe such satisfying UXes mainly with PQs and that some users seem to believe that they are describing an HQ when they are actually describing a PQ. The problem is to see if classification of any user requirement as PQ-related or HQ-related is difficult, and if so, why. We conducted two controlled experiments involving the same twelve requirements-engineering and UX professionals, hereinafter called \u201canalysts.\u201d The first experiment, which had the twelve analysts classifying each of 105 user requirements as PQ-related or HQ-related, shows that neither (1) an analyst\u2019s involvement in the project from which the requirements came nor (2) the analyst\u2019s use of a detailed model of the qualities in addition to the standard definitions of \u201cPQ\u201d and \u201cHQ\u201d has a positive effect on the consistency of the analyst\u2019s classification with that of others. The second experiment, which had the twelve analysts classifying each of a set of 50 user requirements, derived from the 105 of the first experiment, showed that difficulties seem to be caused both by the analyst\u2019s lacking skill in applying the definitions of \u201cPQ\u201d and \u201cHQ\u201d and by poorly written user requirement specifications. The first experiment revealed that classification of user requirements is a lot harder than initially assumed. The second experiment provided evidence that the difficulties can be mitigated by the combination of (1) training analysts in applying the definitions of \u201cPQ\u201d and \u201cHQ\u201d and (2) casting user requirement specifications in a new template that forces provision of the information needed for reliable classification. The experiment shows also that neither training analysts nor casting user requirement specifications in the new template, by itself, mitigates the difficulty in classifying user requirements.\n\nTitle: The design of SREE: a prototype potential ambiguity finder for requirements specifications and lessons learned\nContent: [Context and Motivation] Many a tool for finding ambiguities in natural language (NL) requirements specifications (RSs) is based on a parser and a parts-of-speech identifier, which are inherently imperfect on real NL text. Therefore, any such tool inherently has less than 100% recall. Consequently, running such a tool on a NL RS for a highly critical system does not eliminate the need for a complete manual search for ambiguity in the RS. [Question/Problem] Can an ambiguity-finding tool (AFT) be built that has 100% recall on the types of ambiguities that are in the AFT's scope such that a manual search in an RS for ambiguities outside the AFT's scope is significantly easier than a manual search of the RS for all ambiguities? [Principal Ideas/Results] This paper presents the design of a prototype AFT, SREE (Systemized Requirements Engineering Environment), whose goal is achieving a 100% recall rate for the ambiguities in its scope, even at the cost of a precision rate of less than 100%. The ambiguities that SREE searches for by lexical analysis are the ones whose keyword indicators are found in SREE's ambiguity-indicator corpus that was constructed based on studies of several industrial strength RSs. SREE was run on two of these industrial strength RSs, and the time to do a completely manual search of these RSs is compared to the time to reject the false positives in SREE's output plus the time to do a manual search of these RSs for only ambiguities not in SREE's scope. [Contribution] SREE does not achieve its goals. However, the time comparison shows that the approach to divide ambiguity finding between an AFT with 100% recall for some types of ambiguity and a manual search for only the other types of ambiguity is promising enough to justify more work to improve the implementation of the approach. Some specific improvement suggestions are offered.\n\n## Task\nCompose an abstract for the title 'Group versus individual use of power-only EPMcreate as a creativity enhancement technique for requirements elicitation.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Creativity is often needed in requirements elicitation, i.e., generating ideas for requirements, and therefore, techniques to enhance creativity are believed to be useful. How does the size of a group using the Power-Only EPMcreate (POEPMcreate) creativity enhancement technique affect the group's and each member of the group's effectiveness in generating requirement ideas? This paper describes an experiment in which individuals and two-person and four-person groups used POEPMcreate to generate ideas for requirements for enhancing a high school's public Web site. The data of this experiment combined with the data of two previous experiments involving two-person and four-person groups using POEPMcreate show that, similar to what has been observed for brainstorming, the size of a group using POEPMcreate does affect the number of raw and new requirement ideas generated by the group and by the average member of the group. The data allow concluding that a two-person group using POEPMcreate generates more raw and new requirement ideas, both per group and per group member or individual, than does a four-person group and than does an individual. This conclusion is partially corroborated by qualitative data gathered from a survey of professional business or requirements analysts about group sizes and creativity enhancement techniques."
      }
    ]
  },
  {
    "tid":"179",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: An Intelligent Fingerprint-Biometric Image Scrambling Scheme\nContent: To obstruct the attacks, and to hamper with the liveness and retransmission issues of biometrics images, we have researched on the challenge/response-based biometrics scrambled image transmission. We proposed an intelligent biometrics sensor, which has computational power to receive challenges from the authentication server and generate response against the challenge with the encrypted biometric image. We utilized the FRT for biometric image encryption and used its scaling factors and random phase mask as the additional secret keys. In addition, we chaotically generated the random phase masks by a chaotic map to further improve the encryption security. Experimental and simulation results have shown that the presented system is secure, robust, and deters the risks of attacks of biometrics image transmission.\n\nTitle: Enhancing the transmission security of content-based hidden biometric data\nContent: This paper presents a secure scheme to enhance the transmission security of content-based hidden biometric data over insecure network. Encryp-tion, coding, spread spectrum modulation, and data hiding techniques are used to improve the security and secrecy of the transmitted templates. Secret keys are generated by the biometric image and used as the parameter value and initial condition of the chaotic maps, and each transaction session has different secret keys to protect from the attacks. Templates are encrypted by chaotic encryption, encoded by BCH encoding method, modulated by chaotic neural network parameter modulation (CNNPM), and then hid into the cover image. Experi-mental results show that the security, performance, and accuracy of the presented scheme is encouraging comparable with other methods found in the current literature.\n\nTitle: Privacy-preserving and tokenless chaotic revocable face authentication scheme\nContent: With the large-scale proliferation of biometric systems, privacy and irrevocability issues of their data have become a hot research issue. Recently, some researchers proposed a couple of schemes to generate BioHash, e.g. PalmHash, however, in this paper, we point out that the previous schemes are costly in terms of computational complexity and possession of USB tokens to generate pseudorandom numbers. To overcome the problems, we propose a novel chaotic FaceHashing scheme which preserves privacy of a biometric user. The presented scheme does not need users to posses USB tokens in generating pseudorandom sequences, which is a cost-effective solution. Besides, our scheme minimizes the system complexity with simple operations to attain the FaceHash. Experimental results show that the proposed scheme is efficient, secure, and revocable in case FaceHash is theft or compromised.\n\nTitle: Dual-key-binding cancelable palmprint cryptosystem for palmprint protection and information security\nContent: Biometric cryptosystems and cancelable biometrics are both practical and promising schemes to enhance the security and privacy of biometric systems. Though a number of bio-crypto algorithms have been proposed, they have limited practical applicability because they lack of cancelability. Since biometrics are immutable, the users whose biometrics are stolen cannot use bio-crypto systems anymore. Cancelable biometric schemes are of cancelability; however, they are difficult to compromise the conflicts between the security and performance. By embedded a novel cancelable palmprint template, namely ''two dimensional (2D) Palmprint Phasor'', the proposed palmprint cryptosystem overcomes the lack of cancelability in existing biometric cryptosystems. Besides, the authentication performance is enhanced when users have different tokens/keys. Furthermore, we develop a novel dual-key-binding cancelable palmprint cryptosystem to enhance the security and privacy of palmprint biometric. 2D Palmprint Phasor template is scrambled by the scrambling transformation based on the chaotic sequence that is generated by both the user's token/key and strong key extracted from palmprint. Dual-key-binding scrambling not only has more robustness to resist against chosen plain text attack, but also enhances the secure requirement of non-invertibility. 2D Palmprint Phasor algorithm and dual-key-binding scrambling both increase the difficulty of adversary's statistical analysis. The experimental results and security analysis confirm the efficiency of the proposed scheme.\n\nTitle: Securing biometric templates for reliable identity authentication\nContent: The large-scale implementation and deployment of biometric systems demand the concentration on the security holes, by which a reliable system can loose its integrity and acceptance. Like the passwords or PIN codes, biometric systems also suffer from inherent security threats and it is important to pay attention on the security issues before deploying a biometric system. To solve these problems, this paper proposes a novel chaotic encryption method to protect and secure biometric templates. To enhance the security of the templates, this research uses two chaotic maps for the encryption/decryption process. One chaotic map generates a pseudorandom sequence, which is used as private key. While on the other hand, another chaotic map encrypts the biometric data. Experimental results show that the proposed method is secure, fast, and easy to implement for achieving the security of biometric templates.\n\n## Task\nCompose an abstract for the title 'Challenge-response-based biometric image scrambling for secure personal identification' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper presents a challenge/response-based biometric image scrambling scheme which attempts to overcome the liveness and retransmission issues of biometric image transmission over insecure communication channel. An intelligent biometric sensor is proposed which has computational power to receive challenges from authentication server and generates response against the challenge with encrypted biometric image. The fractional Fourier transform (FRT) has been used for a biometric image encryption process with its scaling factors and random phase masks as the secret keys. The random phase masks of FRT are chaotically generated by piecewise linear chaotic map (PWLCM) to further improve the encryption security. Experimental and simulation results have demonstrated that the proposed system is secure, robust, and deters the risks of attacks for secure biometric image transmission."
      }
    ]
  },
  {
    "tid":"180",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: An Accelerated Rendering Algorithm For Stereoscopic Display\nContent: With the development of the scientific visualization and the virtual environment techniques, stereo viewing systems have now been used extensively. In this paper, we present an accelerated rendering algorithm for stereoscopic display. As the difference between the left view and the right view is slight, we generate the right view by a transformation of the left view conforming to the stereo disparity. The problem of visibility change of a few polygons during the transformation is discussed and an efficient algorithm is developed for filling the holes that may arise in the right view after the transformation. This method makes full use of the coherence between the left view and the right view. Experiments prove its efficiency. (C) 1996 Elsevier Science Ltd\n\nTitle: Robust Metric Reconstruction from Challenging Video Sequences\nContent: Although camera self-calibration and metric reconstruction have been extensively studied during the past decades, automatic metric reconstruction from long video sequences with varying focal length is still very challenging. Several critical issues in practical implementations are not adequately addressed. For example, how to select the initial frames for initializing the projective reconstruction? What criteria should be used? How to handle the large zooming problem? How to choose an appropriate moment for upgrading the projective reconstruction to a metric one? This paper gives a careful investigation of all these issues. Practical and effective approaches are proposed. In particular, we show that existing image-based distance is not an adequate measurement for selecting the initial frames. We propose a novel measurement to take into account the zoom degree, the self-calibration quality, as well as image-based distance. We then introduce a new strategy to decide when to upgrade the projective reconstruction to a metric one. Finally, to alleviate the heavy computational cost in the bundle adjustment, a local on-demand approach is proposed. Our method is also extensively compared with the state-of-the-art commercial software to evidence its robustness and stability.\n\nTitle: High-Quality Depth Recovery via Interactive Multi-view Stereo\nContent: Although multi-view stereo has been extensively studied during the past decades, automatically computing high-quality dense depth information from captured images/videos is still quite difficult. Many factors, such as serious occlusion, large texture less regions and strong reflection, easily cause erroneous depth recovery. In this paper, we present a novel semi-automatic multi-view stereo system, which can quickly create and repair depth from a monocular sequence taken by a freely moving camera. One of our main contributions is that we propose a novel multi-view stereo model incorporating prior constraints indicated by user interaction, which makes it possible to even handle Non-Lambertian surface that surely violates the photo-consistency constraint. Users only need to provide a coarse segmentation and a few user interactions, our system can automatically correct depth and refine boundary. With other priors and occlusion handling, the erroneous depth can be effectively corrected even for very challenging examples that are difficult for state-of-the-art methods.\n\nTitle: Consistent depth maps recovery from a video sequence.\nContent: This paper presents a novel method for recovering consistent depth maps from a video sequence. We propose a bundle optimization framework to address the major difficulties in stereo reconstruction, such as dealing with image noise, occlusions, and outliers. Different from the typical multi-view stereo methods, our approach not only imposes the photo-consistency constraint, but also explicitly associates the geometric coherence with multiple frames in a statistical way. It thus can naturally maintain the temporal coherence of the recovered dense depth maps without over-smoothing. To make the inference tractable, we introduce an iterative optimization scheme by first initializing the disparity maps using a segmentation prior and then refining the disparities by means of bundle optimization. Instead of defining the visibility parameters, our method implicitly models the reconstruction noise as well as the probabilistic visibility. After bundle optimization, we introduce an efficient space-time fusion algorithm to further reduce the reconstruction noise. Our automatic depth recovery is evaluated using a variety of challenging video examples.\n\nTitle: Video object matching across multiple non-overlapping camera views based on multi-feature fusion and incremental learning.\nContent: Matching objects across multiple cameras with non-overlapping views is a necessary but difficult task in the wide area video surveillance. Owing to the lack of spatio-temporal information, only the visual information can be used in some scenarios, especially when the cameras are widely separated. This paper proposes a novel framework based on multi-feature fusion and incremental learning to match the objects across disjoint views in the absence of space\u2013time cues. We first develop a competitive major feature histogram fusion representation (CMFH11CMFH is the abbreviation of Competitive Major Feature Histogram fusion representation.) to formulate the appearance model for characterizing the potentially matching objects. The appearances of the objects can change over time and hence the models should be continuously updated. We then adopt an improved incremental general multicategory support vector machine algorithm (IGMSVM22IGMSVM is the abbreviation of Incremental General Multicategory Support Vector Machine learning algorithm.) to update the appearance models online and match the objects based on a classification method. Only a small amount of samples are needed for building an accurate classification model in our method. Several tests are performed on CAVIAR, ISCAPS and VIPeR databases where the objects change significantly due to variations in the viewpoint, illumination and poses. Experimental results demonstrate the advantages of the proposed methodology in terms of computational efficiency, computation storage, and matching accuracy over that of other state-of-the-art classification-based matching approaches. The system developed in this research can be used in real-time video surveillance applications.\n\n## Task\nCompose an abstract for the title 'Stereoscopic video synthesis from a monocular video.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper presents an automatic and robust approach to synthesize stereoscopic videos from ordinary monocular videos acquired by commodity video cameras. Instead of recovering the depth map, the proposed method synthesizes the binocular parallax in stereoscopic video directly from the motion parallax in monocular video. The synthesis is formulated as an optimization problem via introducing a cost function of the stereoscopic effects, the similarity, and the smoothness constraints. The optimization selects the most suitable frames in the input video for generating the stereoscopic video frames. With the optimized selection, convincing and smooth stereoscopic video can be synthesized even by simple constant-depth warping. No user interaction is required. We demonstrate the visually plausible results obtained given the input clips acquired by ordinary handheld video camera."
      }
    ]
  },
  {
    "tid":"181",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Style Transfer Functions For Illustrative Volume Rendering\nContent: Illustrative volume visualization frequently employs non-photorealistic rendering techniques to enhance important features or to suppress unwanted details. However it is difficult to integrate multiple non-photorealistic rendering approaches into a single framework due to great differences in the individual methods and their parameters. In this paper, we present the concept of style transfer functions. Our approach enables flexible data-driven illumination which goes beyond using the transfer function to just assign colors and opacities. An image-based lighting model uses sphere maps to represent non-photorealistic rendering styles. Style transfer functions allow us to combine a multitude of different shading styles in a single rendering. We extend this concept with a technique for curvature-controlled style contours and an illustrative transparency model. Our implementation of the presented methods allows interactive generation of high-quality volumetric illustrations.\n\nTitle: Flexible Direct Multi-Volume Rendering in Dynamic Scenes\nContent: In this paper we describe methods to efficiently vi- sualize multiple intersecting volumetric objects. We introduce the concept of V-Objects. V-Objects rep- resent abstract properties of an object connected to a volumetric data source. We present a method to perform direct volume rendering of a scene com- prised of an arbitrary number of possibly intersect- ing V-Objects. The idea of our approach is to distin- guish between regions of intersection, which need costly multi-volume processing, and regions con- taining only one V-Object, which can be processed using a highly efficient brick-wise volume traversal scheme. Using this method, we achieve significant performance gains for multi-volume rendering. We show possible medical applications, such as surgi- cal planning, diagnosis, and education.\n\nTitle: A multidirectional occlusion shading model for direct volume rendering\nContent: In this paper, we present a novel technique which simulates directional light scattering for more realistic interactive visualization of volume data. Our method extends the recent directional occlusion shading model by enabling light source positioning with practically no performance penalty. Light transport is approximated using a tilted cone-shaped function which leaves elliptic footprints in the opacity buffer during slice-based volume rendering. We perform an incremental blurring operation on the opacity buffer for each slice in front-to-back order. This buffer is then used to define the degree of occlusion for the subsequent slice. Our method is capable of generating high-quality soft shadowing effects, allows interactive modification of all illumination and rendering parameters, and requires no pre-computation.\n\nTitle: Interaction-dependent semantics for illustrative volume rendering\nContent: In traditional illustration the choice of appropriate styles and rendering techniques is guided by the intention of the artist. For illustrative volume visualizations it is difficult to specify the mapping between the 3D data and the visual representation that preserves the intention of the user. The semantic layers concept establishes this mapping with a linguistic formulation of rules that directly map data features to rendering styles. With semantic layers fuzzy logic is used to evaluate the user defined illustration rules in a preprocessing step. In this paper we introduce interaction-dependent rules that are evaluated for each frame and are therefore computationally more expensive. Enabling interaction-dependent rules, however, allows the use of a new class of semantics, resulting in more expressive interactive illustrations. We show that the evaluation of the fuzzy logic can be done on the graphics hardware enabling the efficient use of interaction-dependent semantics. Further we introduce the flat rendering mode and discuss how different rendering parameters are influenced by the rule base. Our approach provides high quality illustrative volume renderings at interactive frame rates, guided by the specification of illustration rules.\n\nTitle: Obscurance-based volume rendering framework\nContent: Obscurances, from which ambient occlusion is a particular case, is a technology that produces natural-looking lighting effects in a faster way than global illumination. Its application in volume visualization is of special interest since it permits us to generate a high quality rendering at a low cost. In this paper, we propose an obscurancebased framework that allows us to obtain realistic and illustrative volume visualizations in an interactive manner. Obscurances can include color bleeding effects without additional cost. Moreover, we obtain a saliency map from the gradient of obscurances and we show its application to enhance volume visualization and to select the most salient views.\n\n## Task\nCompose an abstract for the title 'Hybrid visibility compositing and masking for illustrative rendering.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, we introduce a novel framework for the compositing of interactively rendered 3D layers tailored to the needs of scientific illustration. Currently, traditional scientific illustrations are produced in a series of composition stages, combining different pictorial elements using 2D digital layering. Our approach extends the layer metaphor into 3D without giving up the advantages of 2D methods. The new compositing approach allows for effects such as selective transparency, occlusion overrides, and soft depth buffering. Furthermore, we show how common manipulation techniques such as masking can be integrated into this concept. These tools behave just like in 2D, but their influence extends beyond a single viewpoint. Since the presented approach makes no assumptions about the underlying rendering algorithms, layers can be generated based on polygonal geometry, volumetric data, point-based representations, or others. Our implementation exploits current graphics hardware and permits real-time interaction and rendering."
      }
    ]
  },
  {
    "tid":"182",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Path Space Regularization for Holistic and Robust Light Transport.\nContent: We propose a simple yet powerful regularization framework for robust light transport simulation. It builds on top of existing unbiased methods and resorts to a consistent estimation using regularization only for paths which cannot be sampled in an unbiased way. To introduce as little bias as possible, we selectively regularize individual interactions along paths, and also derive the regularization consistency conditions. Our approach is compatible with the majority of unbiased methods, e. g. (bidirectional) path tracing and Metropolis light transport (MLT), and only a simple modification is required to adapt existing renderers. We compare to recent unbiased and consistent methods and show examples of scenes with difficult light paths, where regularization is required to account for all illumination features. When coupled with MLT we are able to sample all phenomena, like recent consistent methods, while achieving superior convergence.\n\nTitle: Path-space manipulation of physically-based light transport\nContent: Industry-quality content creation relies on tools for lighting artists to quickly prototype, iterate, and refine final renders. As industry-leading studios quickly adopt physically-based rendering (PBR) across their art generation pipelines, many existing tools have become unsuitable as they address only simple effects without considering underlying PBR concepts and constraints. We present a novel light transport manipulation technique that operates directly on path-space solutions of the rendering equation. We expose intuitive direct and indirect manipulation approaches to edit complex effects such as (multi-refracted) caustics, diffuse and glossy indirect bounces, and direct/indirect shadows. With our sketch- and object-space selection, all built atop a parameterized regular expression engine, artists can search and isolate shading effects to inspect and edit. We classify and filter paths on the fly and visualize the selected transport phenomena. We survey artists who used our tool to manipulate complex phenomena on both static and animated scenes.\n\nTitle: Selective Inspection and Interactive Visualization of Light Transport in Virtual Scenes\nContent: This paper presents novel interactive visualization techniques for inspecting the global light transport in virtual scenes. First, we propose a simple extension to photon mapping to gather required lighting information. We then introduce a set of five light inspection tools which process this data to provide further insights. Corresponding visualizations help the user to comprehend how light travels within a scene, how the lighting affects the appearance of a surface, and how objects cause lighting effects such as caustics. We implemented all tools for direct usage in real production environments. Rendering is based on progressive photon mapping, providing interactivity and immediate visual feedback. We conducted a user study to evaluate all techniques in various application scenarios and hence discuss their individual strengths and weaknesses. Moreover, we present feedback from domain experts. \u00a9 2012 Wiley Periodicals, Inc.\n\nTitle: Selective guided sampling with complete light transport paths.\nContent: Finding good global importance sampling strategies for Monte Carlo light transport is challenging. While estimators using local methods (such as BSDF sampling or next event estimation) often work well in the majority of a scene, small regions in path space can be sampled insufficiently (e.g. a reflected caustic). We propose a novel data-driven guided sampling method which selectively adapts to such problematic regions and complements the unguided estimator. It is based on complete transport paths, i.e. is able to resolve the correlation due to BSDFs and free flight distances in participating media. It is conceptually simple and places anisotropic truncated Gaussian distributions around guide paths to reconstruct a continuous probability density function (guided PDF). Guide paths are iteratively sampled from the guided as well as the unguided PDF and only recorded if they cause high variance in the current estimator. While plain Monte Carlo samples paths independently and Markov chain-based methods perturb a single current sample, we determine the reconstruction kernels by a set of neighbouring paths. This enables local exploration of the integrand without detailed balance constraints or the need for analytic derivatives. We show that our method can decompose the path space into a region that is well sampled by the unguided estimator and one that is handled by the new guided sampler. In realistic scenarios, we show 4\u00d7 speedups over the unguided sampler.\n\n\n\nTitle: Microfacet-based normal mapping for robust Monte Carlo path tracing.\nContent: Normal mapping enhances the amount of visual detail of surfaces by using shading normals that deviate from the geometric normal. However, the resulting surface model is geometrically impossible and normal mapping is thus often considered a fundamentally flawed approach with unavoidable problems for Monte Carlo path tracing, such as asymmetry, back-facing normals, and energy loss arising from this incoherence. These problems are usually sidestepped in real-time renderers, but they cannot be fixed robustly in a path tracer: normal mapping breaks either the appearance (black fringes, energy loss) or the integrator (different forward and backward light transport); in practice, workarounds and tweaked normal maps are often required to hide artifacts.\n\nWe present microfacet-based normal mapping, an alternative way of faking geometric details without corrupting the robustness of Monte Carlo path tracing. It takes the same input data as classic normal mapping and works with any input BRDF. Our idea is to construct a geometrically valid microfacet surface made of two facets per shading point: the one given by the normal map at the shading point and an additional facet that compensates for it such that the average normal of the microsurface equals the geometric normal. We derive the resulting microfacet BRDF and show that it mimics geometric detail in a plausible way, although it does not replicate the appearance of classic normal mapping. However, our microfacet-based normal mapping model is well-defined, symmetric, and energy conserving, and thus yields identical results with any path tracing algorithm (forward, backward, or bidirectional).\n\n\n\n## Task\nCompose an abstract for the title 'The natural-constraint representation of the path space for efficient light transport simulation' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The path integral formulation of light transport is the basis for (Markov chain) Monte Carlo global illumination methods. In this paper we present half vector space light transport (HSLT), a novel approach to sampling and integrating light transport paths on surfaces. The key is a partitioning of the path space into subspaces in which a path is represented by its start and end point constraints and a sequence of generalized half vectors. We show that this representation has several benefits. It enables importance sampling of all interactions along paths in between two endpoints. Based on this, we propose a new mutation strategy, to be used with Markov chain Monte Carlo methods such as Metropolis light transport (MLT), which is well-suited for all types of surface transport paths (diffuse/glossy/specular interaction). One important characteristic of our approach is that the Fourier-domain properties of the path integral can be easily estimated. These can be used to achieve optimal correlation of the samples due to well-chosen mutation step sizes, leading to more efficient exploration of light transport features. We also propose a novel approach to control stratification in MLT with our mutation strategy."
      }
    ]
  },
  {
    "tid":"183",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A formula for incorporating weights into scoring rules\nContent: A \u201cscoring rule\u201d is an assignment of a value to every tuple (of varying sizes). This paper is concerned with the issue of how to modify a scoring rule to apply to the case where weights are assigned to the importance of each argument. We give an explicit formula for incorporating weights that can be applied no matter what the underlying scoring rule is. The formula is surprisingly simple, in that it involves far fewer terms than one might have guessed. It has three further desirable properties. The first desirable property is that when all of the weights are equal, then the result is obtained by simply using the underlying scoring rule. Intuitively, this says that when all of the weights are equal, then this is the same as considering the unweighted case. The second desirable property is that if a particular argument has zero weight, then that argument can be dropped without affecting the value of the result. The third desirable property is that the value of the result is a continuous function of the weights. We show that if these three desirable properties hold, then under one additional assumption (a type of local linearity), our formula gives the unique possible answer.\n\nTitle: Efficient similarity search and classification via rank aggregation\nContent: We propose a novel approach to performing efficient similarity search and classification in high dimensional data. In this framework, the database elements are vectors in a Euclidean space. Given a query vector in the same space, the goal is to find elements of the database that are similar to the query. In our approach, a small number of independent \"voters\" rank the database elements based on similarity to the query. These rankings are then combined by a highly efficient aggregation algorithm. Our methodology leads both to techniques for computing approximate nearest neighbors and to a conceptually rich alternative to nearest neighbors.One instantiation of our methodology is as follows. Each voter projects all the vectors (database elements and the query) on a random line (different for each voter), and ranks the database elements based on the proximity of the projections to the projection of the query. The aggregation rule picks the database element that has the best median rank. This combination has several appealing features. On the theoretical side, we prove that with high probability, it produces a result that is a (1 + \u03b5) factor approximation to the Euclidean nearest neighbor. On the practical side, it turns out to be extremely efficient, often exploring no more than 5% of the data to obtain very high-quality results. This method is also database-friendly, in that it accesses data primarily in a pre-defined order without random accesses, and, unlike other methods for approximate nearest neighbors, requires almost no extra storage. Also, we extend our approach to deal with the k nearest neighbors.We conduct two sets of experiments to evaluate the efficacy of our methods. Our experiments include two scenarios where nearest neighbors are typically employed---similarity search and classification problems. In both cases, we study the performance of our methods with respect to several evaluation criteria, and conclude that they are uniformly excellent, both in terms of quality of results and in terms of efficiency.\n\nTitle: Understanding queries in a search database system\nContent: It is well known that a search engine can significantly benefit from an auxiliary database, which can suggest interpretations of the search query by means of the involved concepts and their interrelationship. The difficulty is to translate abstract notions like concept and interpretation into a concrete search algorithm that operates over the auxiliary database. To surpass existing heuristics, there is a need for a formal basis, which is realized in this paper through the framework of a search database system, where an interpretation is identified as a parse. It is shown that the parses of a query can be generated in polynomial time in the combined size of the input and the output, even if parses are restricted to those having a nonempty evaluation. Identifying that one parse is more specific than another is important for ranking answers, and this framework captures the precise semantics of being more specific; moreover, performing this comparison between parses is tractable. Lastly, the paper studies the problem of finding the most specific parses. Unfortunately, this problem turns out to be intractable in the general case. However, under reasonable assumptions, the parses can be enumerated in an order of decreasing specificity, with polynomial delay and polynomial space.\n\nTitle: Cleaning inconsistencies in information extraction via prioritized repairs\nContent: The population of a predefined relational schema from textual content, commonly known as Information Extraction (IE), is a pervasive task in contemporary computational challenges associated with Big Data. Since the textual content varies widely in nature and structure (from machine logs to informal natural language), it is notoriously difficult to write IE programs that extract the sought information without any inconsistencies (e.g., a substring should not be annotated as both an address and a person name). Dealing with inconsistencies is hence of crucial importance in IE systems. Industrial-strength IE systems like GATE and IBM SystemT therefore provide a built-in collection of cleaning operations to remove inconsistencies from extracted relations. These operations, however, are collected in an ad-hoc fashion through use cases. Ideally, we would like to allow IE developers to declare their own policies. But existing cleaning operations are defined in an algorithmic way and, hence, it is not clear how to extend the built-in operations without requiring low-level coding of internal or external functions. We embark on the establishment of a framework for declarative cleaning of inconsistencies in IE, though principles of database theory. Specifically, building upon the formalism of document spanners for IE, we adopt the concept of prioritized repairs, which has been recently proposed as an extension of the traditional database repairs to incorporate priorities among conflicting facts. We show that our framework captures the popular cleaning policies, as well as the POSIX semantics for extraction through regular expressions. We explore the problem of determining whether a cleaning declaration is unambiguous (i.e., always results in a single repair), and whether it increases the expressive power of the extraction language. We give both positive and negative results, some of which are general, and some of which apply to policies used in practice.\n\nTitle: Static index pruning for information retrieval systems\nContent: We introduce static index pruning methods that significantly reduce the index size in information retrieval systems.We investigate uniform and term-based methods that each remove selected entries from the index and yet have only a minor effect on retrieval results. In uniform pruning, there is a fixed cutoff threshold, and all index entries whose contribution to relevance scores is bounded above by a given threshold are removed from the index. In term-based pruning, the cutoff threshold is determined for each term, and thus may vary from term to term. We give experimental evidence that for each level of compression, term-based pruning outperforms uniform pruning, under various measures of precision. We present theoretical and experimental evidence that under our term-based pruning scheme, it is possible to prune the index greatly and still get retrieval results that are almost as good as those based on the full index.\n\n## Task\nCompose an abstract for the title 'Allowing users to weight search terms' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":" Information retrieval systems typically weight the importance of search terms according todocument and collection statistics (such as by using tf \\Theta idf scores, where less common termsare given higher weight). We consider here the scenario where a user can express her ownsubjective weighting of the importance of the terms that form the query on top of the systemgeneratedweighting, and show how this should modify the relevance scores of documents.This has been allowed before, but... "
      }
    ]
  },
  {
    "tid":"184",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Distributed computing with rules of thumb\nContent: We present our recent work (ICS 2011) on dynamic environments in which computational nodes, or decision makers, follow simple and unsophisticated rules of behavior (e.g., repeatedly \"best replying\" to others' actions, and minimizing \"regret\") that have been extensively studied in game theory and economics. We aim to understand when convergence of the resulting dynamics to an equilibrium point is guaranteed if nodes' interaction is not synchronized (e.g., as in Internet protocols and large-scale markets). We take the first steps of this research agenda. We exhibit a general non-convergence result and consider its implications across a wide variety of interesting and timely applications: routing, congestion control, game theory, social networks and circuit design. We also consider the relationship between classical nontermination results in distributed computing theory and our result, explore the impact of scheduling on convergence, study the computational and communication complexity of asynchronous dynamics and present some basic observations regarding the effects of asynchrony on no-regret dynamics.\n\nTitle: Privacy-preserving Bayesian network structure computation on distributed heterogeneous data\nContent: As more and more activities are carried out using computers and computer networks, the amount of potentially sensitive data stored by business, governments, and other parties increases. Different parties may wish to benefit from cooperative use of their data, but privacy regulations and other privacy concerns may prevent the parties from sharing their data. Privacy-preserving data mining provides a solution by creating distributed data mining algorithms in which the underlying data is not revealed.In this paper, we present a privacy-preserving protocol for a particular data mining task: learning the Bayesian network structure for distributed heterogeneous data. In this setting, two parties owning confidential databases wish to learn the structure of Bayesian network on the combination of their databases without revealing anything about their data to each other. We give an efficient and privacy-preserving version of the K2 algorithm to construct the structure of a Bayesian network for the parties' joint data.\n\nTitle: Dynamics at the Boundary of Game Theory and Distributed Computing\nContent: We use ideas from distributed computing and game theory to study dynamic and decentralized environments in which computational nodes, or decision makers, interact strategically and with limited information. In such environments, which arise in many real-world settings, the participants act as both economic and computational entities. We exhibit a general non-convergence result for a broad class of dynamics in asynchronous settings. We consider implications of our result across a wide variety of interesting and timely applications: game dynamics, circuit design, social networks, Internet routing, and congestion control. We also study the computational and communication complexity of testing the convergence of asynchronous dynamics. Our work opens a new avenue for research at the intersection of distributed computing and game theory.\n\nTitle: Privacy-preserving distributed k-means clustering over arbitrarily partitioned data\nContent: Advances in computer networking and database technologies have enabled the collection and storage of vast quantities of data. Data mining can extract valuable knowledge from this data, and organizations have realized that they can often obtain better results by pooling their data together. However, the collected data may contain sensitive or private information about the organizations or their customers, and privacy concerns are exacerbated if data is shared between multiple organizations.Distributed data mining is concerned with the computation of models from data that is distributed among multiple participants. Privacy-preserving distributed data mining seeks to allow for the cooperative computation of such models without the cooperating parties revealing any of their individual data items. Our paper makes two contributions in privacy-preserving data mining. First, we introduce the concept of arbitrarily partitioned data, which is a generalization of both horizontally and vertically partitioned data. Second, we provide an efficient privacy-preserving protocol for k-means clustering in the setting of arbitrarily partitioned data.\n\nTitle: Privacy-Preserving Publish/Subscribe: Efficient Protocols in a Distributed Model.\nContent: We consider the problem of modeling and designing efficient and privacy-preserving publish/subscribe protocols in a distributed model where parties can act as publishers or subscribers or both, and there are no brokers or other types of parties. The problem is particularly challenging as privacy demands on such protocols come with efficiency limitations; most notably, the publisher must send messages as long as the publications to all parties, and the cryptographic techniques to perform the publish/subscribe match need to be based on asymmetric cryptographic operation which are known to be less efficient than their symmetric counterpart. Our main result is a distributed publish/subscribe protocol which addresses and essentially nullifies the impact of both efficiency limitations, without sacrificing the required privacy properties. Our construction is based on very efficient design of a novel cryptographic tool, of independent interest, called 'hybrid conditional oblivious transfer protocol', as it resembles hybrid encryption, where asymmetric encryption is only used to transfer a short key, which enables (much more efficient) symmetric encryption of a long message.\n\n## Task\nCompose an abstract for the title 'Distributed Computing with Adaptive Heuristics.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"  We use ideas from distributed computing to study dynamic environments in which computational nodes, or decision makers, follow adaptive heuristics (Hart 2005), i.e., simple and unsophisticated rules of behavior, e.g., repeatedly \"best replying\" to others' actions, and minimizing \"regret\", that have been extensively studied in game theory and economics. We explore when convergence of such simple dynamics to an equilibrium is guaranteed in asynchronous computational environments, where nodes can act at any time. Our research agenda, distributed computing with adaptive heuristics, lies on the borderline of computer science (including distributed computing and learning) and game theory (including game dynamics and adaptive heuristics). We exhibit a general non-termination result for a broad class of heuristics with bounded recall---that is, simple rules of behavior that depend only on recent history of interaction between nodes. We consider implications of our result across a wide variety of interesting and timely applications: game theory, circuit design, social networks, routing and congestion control. We also study the computational and communication complexity of asynchronous dynamics and present some basic observations regarding the effects of asynchrony on no-regret dynamics. We believe that our work opens a new avenue for research in both distributed computing and game theory. "
      }
    ]
  },
  {
    "tid":"185",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: When Hashes Met Wedges: A Distributed Algorithm for Finding High Similarity Vectors.\nContent: Finding similar user pairs is a fundamental task in social networks, with numerous applications in ranking and personalization tasks such as link prediction and tie strength detection. A common manifestation of user similarity is based upon network structure: each user is represented by a vector that represents the user's network connections, where pairwise cosine similarity among these vectors defines user similarity. The predominant task for user similarity applications is to discover all similar pairs that have a pairwise cosine similarity value larger than a given threshold \u03c4. In contrast to previous work where \u03c4 is assumed to be quite close to 1, we focus on recommendation applications where \u03c4 is small, but still meaningful. The all pairs cosine similarity problem is computationally challenging on networks with billions of edges, and especially so for settings with small \u03c4. To the best of our knowledge, there is no practical solution for computing all user pairs with, say \u03c4 = 0.2 on large social networks, even using the power of distributed algorithms. Our work directly addresses this challenge by introducing a new algorithm --- WHIMP --- that solves this problem efficiently in the MapReduce model. The key insight in WHIMP is to combine the ``wedge-sampling\\\" approach of Cohen-Lewis for approximate matrix multiplication with the SimHash random projection techniques of Charikar. We provide a theoretical analysis of WHIMP, proving that it has near optimal communication costs while maintaining computation cost comparable with the state of the art. We also empirically demonstrate WHIMP's scalability by computing all highly similar pairs on four massive data sets, and show that it accurately finds high similarity pairs. In particular, we note that WHIMP successfully processes the entire Twitter network, which has tens of billions of edges.\n\nTitle: Finding Forbidden Minors in Sublinear Time: A n^1/2+o(1)-Query One-Sided Tester for Minor Closed Properties on Bounded Degree Graphs\nContent: Let G be an undirected, bounded degree graph with n vertices. Fix a finite graph H, and suppose one must remove \u03b5 n edges from G to make it H-minor free (for some small constant \u03b5 > 0). We give an n\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1/2+o(1)</sup>\n-time randomized procedure that, with high probability, finds an H-minor in such a graph. As an application, suppose one must remove \u03b5 n edges from a bounded degree graph G to make it planar. This result implies an algorithm, with the same running time, that produces a K\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">3,3</sub>\n or K\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">5</sub>\n minor in G. No prior sublinear time bound was known for this problem. By the graph minor theorem, we get an analogous result for any minor-closed property. Up to n\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">o(1)</sup>\n factors, this resolves a conjecture of Benjamini-Schramm-Shapira (STOC 2008) on the existence of one-sided property testers for minor-closed properties. Furthermore, our algorithm is nearly optimal, by an \u03a9(\u221an) lower bound of Czumaj et al (RSA 2014). Prior to this work, the only graphs H for which non-trivial one-sided property testers were known for H-minor freeness are the following: H being a forest or a cycle (Czumaj et al, RSA 2014), K\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub>\n,\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">k</sub>\n, (k\u00d7 2)-grid, and the k-circus (Fichtenberger et al, Arxiv 2017).\n\nTitle: Trigger detection for adaptive scientific workflows using percentile sampling\nContent:   Increasing complexity of scientific simulations and HPC architectures are driving the need for adaptive workflows, where the composition and execution of computational and data manipulation steps dynamically depend on the evolutionary state of the simulation itself. Consider for example, the frequency of data storage. Critical phases of the simulation should be captured with high frequency and with high fidelity for post-analysis, however we cannot afford to retain the same frequency for the full simulation due to the high cost of data movement. We can instead look for triggers, indicators that the simulation will be entering a critical phase and adapt the workflow accordingly.   We present a method for detecting triggers and demonstrate its use in direct numerical simulations of turbulent combustion using S3D. We show that chemical explosive mode analysis (CEMA) can be used to devise a noise-tolerant indicator for rapid increase in heat release. However, exhaustive computation of CEMA values dominates the total simulation, thus is prohibitively expensive. To overcome this bottleneck, we propose a quantile-sampling approach. Our algorithm comes with provable error/confidence bounds, as a function of the number of samples. Most importantly, the number of samples is independent of the problem size, thus our proposed algorithm offers perfect scalability. Our experiments on homogeneous charge compression ignition (HCCI) and reactivity controlled compression ignition (RCCI) simulations show that the proposed method can detect rapid increases in heat release, and its computational overhead is negligible. Our results will be used for dynamic workflow decisions about data storage and mesh resolution in future combustion simulations. Proposed framework is generalizable and we detail how it could be applied to a broad class of scientific simulation workflows. \n\nTitle: A Space-Efficient Streaming Algorithm for Estimating Transitivity and Triangle Counts Using the Birthday Paradox\nContent: We design a space-efficient algorithm that approximates the transitivity (global clustering coefficient) and total triangle count with only a single pass through a graph given as a stream of edges. Our procedure is based on the classic probabilistic result, the birthday paradox. When the transitivity is constant and there are more edges than wedges (common properties for social networks), we can prove that our algorithm requires O(\u221an) space (n is the number of vertices) to provide accurate estimates. We run a detailed set of experiments on a variety of real graphs and demonstrate that the memory requirement of the algorithm is a tiny fraction of the graph. For example, even for a graph with 200 million edges, our algorithm stores just 40,000 edges to give accurate results. Being a single pass streaming algorithm, our procedure also maintains a real-time estimate of the transitivity/number of triangles of a graph by storing a minuscule fraction of edges.\n\nTitle: ESCAPE: Efficiently Counting All 5-Vertex Subgraphs.\nContent: Counting the frequency of small subgraphs is a fundamental technique in network analysis across various domains, most notably in bioinformatics and social networks. The special case of triangle counting has received much attention. Getting results for 4-vertex or 5-vertex patterns is highly challenging, and there are few practical results known that can scale to massive sizes. We introduce an algorithmic framework that can be adopted to count any small pattern in a graph and apply this framework to compute exact counts for all 5-vertex subgraphs. Our framework is built on cutting a pattern into smaller ones, and using counts of smaller patterns to get larger counts. Furthermore, we exploit degree orientations of the graph to reduce runtimes even further. These methods avoid the combinatorial explosion that typical subgraph counting algorithms face. We prove that it suffices to enumerate only four specific subgraphs (three of them have less than 5 vertices) to exactly count all 5-vertex patterns. We perform extensive empirical experiments on a variety of real-world graphs. We are able to compute counts of graphs with tens of millions of edges in minutes on a commodity machine. To the best of our knowledge, this is the first practical algorithm for 5-vertex pattern counting that runs at this scale. A stepping stone to our main algorithm is a fast method for counting all 4-vertex patterns. This algorithm is typically ten times faster than the state of the art 4-vertex counters.\n\n## Task\nCompose an abstract for the title 'Catching the Head, Tail, and Everything in Between: A Streaming Algorithm for the Degree Distribution' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The degree distribution is one of the most fundamental graph properties of interest for real-world graphs. It has been widely observed in numerous domains that graphs typically have a tailed or scale-free degree distribution. While the average degree is usually quite small, the variance is quite high and there are vertices with degrees at all scales. We focus on the problem of approximating the degree distribution of a large streaming graph, with small storage. We design an algorithm headtail, whose main novelty is a new estimator of infrequent degrees using truncated geometric random variables. We give a mathematical analysis of headtail and show that it has excellent behavior in practice. We can process streams will millions of edges with storage less than 1% and get extremely accurate approximations for all scales in the degree distribution. We also introduce a new notion of Relative Hausdorff distance between tailed histograms. Existing notions of distances between distributions are not suitable, since they ignore infrequent degrees in the tail. The Relative Hausdorff distance measures deviations at all scales, and is a more suitable distance for comparing degree distributions. By tracking this new measure, we are able to give strong empirical evidence of the convergence of headtail."
      }
    ]
  },
  {
    "tid":"186",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Fast and reliable anomaly detection in categorical data\nContent: Spotting anomalies in large multi-dimensional databases is a crucial task with many applications in finance, health care, security, etc. We introduce COMPREX, a new approach for identifying anomalies using pattern-based compression. Informally, our method finds a collection of dictionaries that describe the norm of a database succinctly, and subsequently flags those points dissimilar to the norm---with high compression cost---as anomalies. Our approach exhibits four key features: 1) it is parameter-free; it builds dictionaries directly from data, and requires no user-specified parameters such as distance functions or density and similarity thresholds, 2) it is general; we show it works for a broad range of complex databases, including graph, image and relational databases that may contain both categorical and numerical features, 3) it is scalable; its running time grows linearly with respect to both database size as well as number of dimensions, and 4) it is effective; experiments on a broad range of datasets show large improvements in both compression, as well as precision in anomaly detection, outperforming its state-of-the-art competitors.\n\nTitle: Cartification: A Neighborhood Preserving Transformation for Mining High Dimensional Data\nContent: The analysis of high dimensional data comes with many intrinsic challenges. In particular, cluster structures become increasingly hard to detect when the data includes dimensions irrelevant to the individual clusters. With increasing dimensionality, distances between pairs of objects become very similar, and hence, meaningless for knowledge discovery. In this paper we propose Cartification, a new transformation to circumvent this problem. We transform each object into an item set, which represents the neighborhood of the object. We do this for multiple views on the data, resulting in multiple neighborhoods per object. This transformation enables us to preserve the essential pair wise-similarities of objects over multiple views, and hence, to improve knowledge discovery in high dimensional data. Our experiments show that frequent item set mining on the certified data outperforms competing clustering approaches on the original data space, including traditional clustering, random projections, principle component analysis, subspace clustering, and clustering ensemble.\n\nTitle: Efficiently Summarising Event Sequences with Rich Interleaving Patterns.\nContent: Discovering the key structure of a database is one of the main goals of data mining. In pattern set mining we do so by discovering a small set of patterns that together describe the data well. The richer the class of patterns we consider, and the more powerful our description language, the better we will be able to summarise the data. In this paper we propose ourmethod, a novel greedy MDL-based method for summarising sequential data using rich patterns that are allowed to interleave. Experiments show ourmethod is orders of magnitude faster than the state of the art, results in better models, as well as discovers meaningful semantics in the form patterns that identify multiple choices of values.\n\nTitle: Keeping it Short and Simple: Summarising Complex Event Sequences with Multivariate Patterns\nContent: We study how to obtain concise descriptions of discrete multivariate sequential data. In particular, how to do so in terms of rich multivariate sequential patterns that can capture potentially highly interesting (cor)relations between sequences. To this end we allow our pattern language to span over the domains (alphabets) of all sequences, allow patterns to overlap temporally, as well as allow for gaps in their occurrences. We formalise our goal by the Minimum Description Length principle, by which our objective is to discover the set of patterns that provides the most succinct description of the data. To discover high-quality pattern sets directly from data, we introduce Ditto, a highly efficient algorithm that approximates the ideal result very well. Experiments show that Ditto correctly discovers the patterns planted in synthetic data. Moreover, it scales favourably with the length of the data, the number of attributes, the alphabet sizes. On real data, ranging from sensor networks to annotated text, Ditto discovers easily interpretable summaries that provide clear insight in both the univariate and multivariate structure.\n\nTitle: Telling Cause from Effect Using MDL-Based Local and Global Regression\nContent: We consider the fundamental problem of inferring the causal direction between two univariate numeric random variables X and Y from observational data. The two-variable case is especially difficult to solve since it is not possible to use standard conditional independence tests between the variables. To tackle this problem, we follow an information theoretic approach based on Kolmogorov complexity and use the Minimum Description Length (MDL) principle to provide a practical solution. In particular, we propose a compression scheme to encode local and global functional relations using MDL-based regression. We infer X causes Y in case it is shorter to describe Y as a function of X than the inverse direction. In addition, we introduce Slope, an efficient linear-time algorithm that through thorough empirical evaluation on both synthetic and real world data we show outperforms the state of the art by a wide margin.\n\n## Task\nCompose an abstract for the title 'Linear-time Detection of Non-linear Changes in Massively High Dimensional Time Series.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Change detection in multivariate time series has applications in many domains, including health care and network monitoring. A common approach to detect changes is to compare the divergence between the distributions of a reference window and a test window. When the number of dimensions is very large, however, the naive approach has both quality and efficiency issues: to ensure robustness the window size needs to be large, which not only leads to missed alarms but also increases runtime.  this end, we propose LIGHT, a linear-time algorithm for robustly detecting non-linear changes in massively high dimensional time series. Importantly, LIGHT provides high flexibility in choosing the window size, allowing the domain expert to fit the level of details required. To do such, we 1) perform scalable PCA to reduce dimensionality, 2) perform scalable factorization of the joint distribution, and 3) scalably compute divergences between these lower dimensional distributions. Extensive empirical evaluation on both synthetic and real-world data show that LIGHT outperforms state of the art with up to 100% improvement in both quality and efficiency."
      }
    ]
  },
  {
    "tid":"187",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Integrating Meta-modelling Aspects with Graph Transformation for Efficient Visual Language Definition and Model Manipulation\nContent: Visual languages (VLs) play a central role in modelling various system aspects. Besides standard languages like UML, a variety of domain-specific languages exist which are the more used the more tool support is available for them. Different kinds of generators have been developed which produce visual modelling environments based on VL specifications. To define a VL, declarative as well as constructive approaches are used. The meta modelling approach is a declarative one where classes of symbols and relations are defined and associated to each other. Constraints describe additional language properties. Defining a VL by a graph grammar, the constructive way is followed where graphs describe the abstract syntax of models and graph rules formulate the language grammar. In this paper, we extend algebraic graph grammars by a node type inheritance concept which opens up the possibility to integrate both approaches by identifying symbol classes with node types and associations with edge types of some graph class. In this way, declarative as well as constructive elements may be used for language definition and model manipulation. Two concrete approaches, the GENGED and the AToM(3) approach, illustrate how VLs can be defined and models can be manipulated by the techniques described above.\n\nTitle: Generation of Animation Views for Petri Nets in GenGED\nContent: Formal specification techniques like Petri nets allow for the formal description and analysis of systems. Tool support exists for many different Petri net classes for editing, simulating and analyzing formal models. A domain-specific animation of net behavior going beyond the well-known token game, however, is not yet supported in most cases. In this paper, we present a formal approach for the generic specification of animation views for different Petri net classes based on GENCED and graph transformation. The GENGED approach, developed at the TU Berlin, allows for the generic description of visual modeling languages including different Petri net classes. In our framework, the animation view of a system modeled as a Petri net consists of a domain-specific visual layout and an animation view according to the firing behavior of the Petri net class. The basic idea is to generate visual animation rules based on visual syntax rules defining the corresponding Petri net language. We propose a view transformation from the classical Petri net layout to the animation layout. The well-known producer/consumer system modeled as an elementary Petri net serves as running example. We provide an animation view for the application domain kitchen where producing and consuming is visualized by icons for baking and eating cakes, respectively.\n\nTitle: Formal Integration of Inheritance with Typed Attributed Graph Transformation for Efficient VL Definition and Model Manipulation\nContent: Several approaches exist to define a visual language (VL). Among those the meta-modeling approach used to define the Unified Modeling Language (UML), and the graph transformation approach are very popular. Especially the combination of both, using meta-modeling to define the syntax of a VL and graph transformation for specifying model transformations has been considered conceptually and explored in a number of applications. A formal integration of both approaches has just been started by integrating classical algebraic graph grammars with a node type inheritance concept. In this paper, the integration of inheritance is extending to attributed graph transformation. More precisely, we define attributed type graphs with inheritance leading to a formal integration of inheritance with typed attributed graph transformation.\n\nTitle: A Typed Attributed Graph Grammar with Inheritance for the Abstract Syntax of UML Class and Sequence Diagrams\nContent: According to the UML Standard 2.0 class and sequence diagrams are defined in a descriptive way by a MOF meta-model and semi-formal constraints. This paper presents a formal and constructive definition of the abstract syntax of UML class and sequence diagrams based on the well-defined theory of typed attributed graph transformation with inheritance and application conditions. The generated language covers all important features of these parts of UML diagrams and is shown to satisfy all of the corresponding constraints by construction. An explicit model transformation demonstrates the close correspondence between the graph grammar and the MOF definition of UML class and sequence diagrams. The graph grammar is validated by well-established benchmarks showing that all important features of the MOF definition of UML are covered. This formal constructive syntax definition of UML class and sequence diagrams is the basis for syntax directed editing, formal analysis, formal operational and denotational semantics and correctness of model transformations.\n\nTitle: A Transformation-Based Component Framework For A Generic Integrated Modeling Technique\nContent: This paper is based on two general ideas. The first one is the integration paradigm for data type and process modeling techniques developed by the first two authors within the last five years. The second one is a transformation-based component framework for system modeling presented at ETAPS 2002 in Grenoble. The aim of this paper is to join both ideas leading to a component framework for a generic integrated modeling technique. This component framework is based on transformations and is especially useful to be instantiated by graph- and net-based techniques. The main concepts are a self-contained semantics and internal correctness of components, based on a new idea of high-level constraints. Two main results concerning compositionality show that semantics and correctness for a system can be inferred from that of its components. The concepts are illustrated by a running example on modeling Java threads by high-level nets.\n\n## Task\nCompose an abstract for the title 'Conceptual Model of the Graphical Editor GENGED for the Visual Definition of Visual Languages' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This contribution presents a conceptual model ofGENGED, an editor supporting the visual definition of visual lan- guages (VLs). As usual a VL consists of an alphabet and a gramm ar. Given a grammar of a specific VLGENGED generates a syntax-directed graphical editor allowing themanipulation of visual sentences over this language. The conceptual framework of GENGED is based on algebraic graph transformation and algebraic sp ecification techniques, especially typed algebraic specifications. Starting with a type signatureMETAVI SUAL for visual alpha- bets, the user of GENGED can define a specific VL-alphabet and some graphical constrai nts. The VL-alphabet and the constraints are transformed into an algebraic speci fication whose signature is typed over METAVI SUAL. The VL-grammar the user can define on top of the VL-alphabet consi sts of a start sentence and a set of VL-rules. Each VL-rule is defined by a graph grammar rule and comprises a left - as well as a right-hand-side. Both sides are again visual sentences. Like the VL-alphabet these visual s entences can be transformed into algebraic specifica- tions whose signature is typed over the VL-alphabet. Becaus e the typing is transitiv, all visual sentences are also typed over METAVI SUAL. The same is true according to the visual sentences which can be manipulated using a gen- erated graphical editor."
      }
    ]
  },
  {
    "tid":"188",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Novel Parallel QR Algorithm for Hybrid Distributed Memory HPC Systems\nContent: A novel variant of the parallel QR algorithm for solving dense nonsymmetric eigenvalue problems on hybrid distributed high performance computing systems is presented. For this purpose, we introduce the concept of multiwindow bulge chain chasing and parallelize aggressive early deflation. The multiwindow approach ensures that most computations when chasing chains of bulges are performed in level 3 BLAS operations, while the aim of aggressive early deflation is to speed up the convergence of the QR algorithm. Mixed MPI-OpenMP coding techniques are utilized for porting the codes to distributed memory platforms with multithreaded nodes, such as multicore processors. Numerous numerical experiments confirm the superior performance of our parallel QR algorithm in comparison with the existing ScaLAPACK code, leading to an implementation that is one to two orders of magnitude faster for sufficiently large problems, including a number of examples from applications.\n\nTitle: Parallel variants of the multishift QZ algorithm with advanced deflation techniques\nContent: The QZ algorithm reduces a regular matrix pair to generalized Schur form, which can be used to address the generalized eigenvalue problem. This paper summarizes recent work on improving the performance of the QZ algorithm on serial machines and work in progress on a novel parallel implementation. In both cases, the QZ iterations are based on chasing chains of tiny bulges. This allows to formulate the majority of the computation in terms of matrix-matrix multiplications, resulting in natural parallelism and better performance on modern computing systems with memory hierarchies. In addition, advanced deflation strategies are used, specifically the so called aggressive early deflation, leading to a considerable convergence acceleration and consequently to a reduction of floating point operations and computing time.\n\nTitle: Fine-grained bulge-chasing kernels for strongly scalable parallel QR algorithms\nContent: The bulge-chasing kernel in the small-bulge multi-shift QR algorithm for the non-symmetric dense eigenvalue problem becomes a sequential bottleneck when the QR algorithm is run in parallel on a multicore platform with shared memory. The duration of each kernel invocation is short, but the critical path of the QR algorithm contains a long sequence of calls to the bulge-chasing kernel. We study the problem of parallelizing the bulge-chasing kernel itself across a handful of processor cores in order to reduce the execution time of the critical path. We propose and evaluate a sequence of four algorithms with varying degrees of complexity and verify that a pipelined algorithm with a slowly shifting block column distribution of the Hessenberg matrix is superior. The load-balancing problem is non-trivial and computational experiments show that the load-balancing scheme has a large impact on the overall performance. We propose two heuristics for the load-balancing problem and also an effective optimization method based on local search. Numerical experiments show that speed-ups are obtained for problems as small as 40x40 on two different multicore architectures.\n\nTitle: Algorithm 953: Parallel Library Software for the Multishift QR Algorithm with Aggressive Early Deflation\nContent: Library software implementing a parallel small-bulge multishift QR algorithm with Aggressive Early Deflation (AED) targeting distributed memory high-performance computing systems is presented. Starting from recent developments of the parallel multishift QR algorithm [Granat et al., SIAM J. Sci. Comput. 32(4), 2010], we describe a number of algorithmic and implementation improvements. These include communication avoiding algorithms via data redistribution and a refined strategy for balancing between multishift QR sweeps and AED. Guidelines concerning several important tunable algorithmic parameters are provided. As a result of these improvements, a computational bottleneck within AED has been removed in the parallel multishift QR algorithm. A performance model is established to explain the scalability behavior of the new parallel multishift QR algorithm. Numerous computational experiments confirm that our new implementation significantly outperforms previous parallel implementations of the QR algorithm.\n\nTitle: A Hierarchical Approach for Performance Analysis of ScaLAPACK-Based Routines Using the Distributed Linear Algebra Machine\nContent: Performance models are important in the design and analysis of linear algebra software for scalable high performance computer systems. They can be used for estimation of the overhead in a parallel algorithm and measuring the impact of machine characteristics and block sizes on the execution time. We present an hierarchical approach for design of performance models for parallel algorithms in linear algebra based on a parallel machine model and the hierarchical structure of the ScaLAPACK library. This suggests three levels of performance models corresponding to existing ScaLAPACK routines. As a proof of the concept a performance model of the high level QR factorization routine pdgeqrf is presented. We also derive performance models of lower level ScaLAPACK building blocks such as pdgeqr2, pdlarft, pdlarfb, pdlarfg, pdlarf, pdnrm2, and pdscal, which are used in the high level model for pdgeqrf. Predicted performance results are compared to measurements on an Intel Paragon XP/S system. The accuracy of the top level model is over 90% for measured matrix and block sizes and different process grid configurations.\n\n## Task\nCompose an abstract for the title 'A Parallel QZ Algorithm for Distributed Memory HPC Systems.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Appearing frequently in applications, generalized eigenvalue problems represent one of the core problems in numerical linear algebra. The QZ algorithm of Moler and Stewart is the most widely used algorithm for addressing such problems. Despite its importance, little attention has been paid to the parallelization of the QZ algorithm. The purpose of this work is to fill this gap. We propose a parallelization of the QZ algorithm that incorporates all modern ingredients of dense eigensolvers, such as multishift and aggressive early deflation techniques. To deal with (possibly many) infinite eigenvalues, a new parallel deflation strategy is developed. Numerical experiments for several random and application examples demonstrate the effectiveness of our algorithm on two different distributed memory HPC systems."
      }
    ]
  },
  {
    "tid":"189",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Making RDF presentable: integrated global and local semantic Web browsing\nContent: This paper discusses generating document structure from annotated media repositories in a domain-independent manner. This approaches the vision of a universal RDF browser. We start by applying the search-and-browse paradigm established for the WWW to RDF presentation. Furthermore, this paper adds to this paradigm the clustering-based derivation of document structure from search returns, providing simple but domain-independent hypermedia generation from RDF stores. While such generated presentations hardly meet the standards of those written by humans, they provide quick access to media repositories when the required document has not yet been written. The resulting system allows a user to specify a topic for which it generates a hypermedia document providing guided navigation through virtually any RDF repository. The impact for content providers is that as soon as one adds new media items and their annotations to a repository, they become immediately available for automatic integration into subsequently requested presentations.\n\nTitle: Supporting subject matter annotation using heterogeneous thesauri: A user study in Web data reuse\nContent: We performed a user experiment in which museum professionals used vocabularies from the Web for annotating the subject matter of museum objects. In this paper, we study the requirements on the underlying RDF dataset, search algorithms and user interface design in a real world setting. We identify the advantages of reusing vocabularies from the Web and discuss how and to what extent the disadvantages can be overcome. The study is performed at the Print Room of the Rijksmuseum Amsterdam, where currently a large collection of prints, photographs and drawings is being catalogued. We report on the analysis of the current practice of professional cataloguers, the iterative design of an annotation tool and a qualitative evaluation of this tool with a user experiment in a realistic annotation environment. We discuss our findings in terms of their impact on the RDF data, the semantic search functionality and the user interface.\n\nTitle: ClioPatria: A SWI-Prolog infrastructure for the Semantic Web\nContent: ClioPatria is a comprehensive semantic web development framework based on SWI-Prolog. SWI-Prolog provides an efficient C-based main-memory RDF store that is designed to cooperate naturally and efficiently with Prolog, realizing a flexible RDF-based environment for rule based programming. ClioPatria extends this core with a SPARQL and LOD server, an extensible web frontend to manage the server, browse the data, query the data using SPARQL and Prolog and a Git-based plugin manager. The ability to query RDF using Prolog provides query composition and smooth integration with application logic. ClioPatria is primarily positioned as a prototyping platform for exploring novel ways of reasoning with RDF data. It has been used in several research projects in order to perform tasks such as data integration and enrichment and semantic search.\n\nTitle: Multimedia Annotations on the Semantic Web\nContent: Multimedia in all forms (images, video, graphics, music, speech) is exploding on the Web.The content needs to be annotated and indexed to enable effective search and retrieval.However, recent standards and best practices for multimedia metadata don't provide semantically rich descriptions of multimedia content.On the other hand, the World Wide Web Consortium's (W3C's) Semantic Web effort has been making great progress in advancing techniques for annotating semantics of Web resources.To bridge this gap, a new W3C task force has been created to investigate multimedia annotations on the Semantic Web. This article examines the problems of semantically annotating multimedia and describes the integration of multimedia metadata with Semantic Web.\n\nTitle: Towards Ontology-Driven Discourse: From Semantic Graphs to Multimedia Presentations\nContent: Traditionally, research in applying Semantic Web technology to multimedia information systems has focused on using annotations and ontologies to improve the retrieval process. This paper concentrates on improving the presentation of the retrieval results. First, our approach uses ontological domain knowledge to select and organize the content relevant to the topic the user is interested in. Domain ontologies are valuable in the presentation generation process, because effective presentations are those that succeed in conveying the relevant domain semantics to the user. Explicit discourse and narrative knowledge allows selection of appropriate presentation genres and creation of narrative structures, which are used for conveying these domain relations. In addition, knowledge of graphic design and media characteristics is essential to transform abstract presentation structures into real multimedia presentations. Design knowledge determines how the semantics and presentation structure are expressed in the multimedia presentation. In traditional Web environments, this type of design knowledge remains implicit, hidden in style sheets and other document transformation code. Our second use of Semantic Web technology is to model design knowledge explicitly, and to enable it to drive the transformations needed to turn annotated media items into structured presentations.\n\n## Task\nCompose an abstract for the title '/facet: a browser for heterogeneous semantic web repositories' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Facet browsing has become popular as a user friendly interface to data repositories. The Semantic Web raises new challenges due to the heterogeneous character of the data. First, users should be able to select and navigate through facets of resources of any type and to make selections based on properties of other, semantically related, types. Second, where traditional facet browsers require manual configuration of the software, a semantic web browser should be able to handle any RDFS dataset without any additional configuration. Third, hierarchical data on the semantic web is not designed for browsing: complementary techniques, such as search, should be available to overcome this problem. We address these requirements in our browser, /facet. Additionally, the interface allows the inclusion of facet-specific display options that go beyond the hierarchical navigation that characterizes current facet browsing. /facet is a tool for Semantic Web developers as an instant interface to their complete dataset. The automatic facet configuration generated by the system can then be further refined to configure it as a tool for end users. The implementation is based on current Web standards and open source software. The new functionality is motivated using a scenario from the cultural heritage domain."
      }
    ]
  },
  {
    "tid":"190",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Conjunctive queries for a tractable fragment of OWL 1.1\nContent: Despite the success of the Web Ontology Language OWL, the development of expressive means for querying OWL knowledge bases is still an open issue. In this paper, we investigate how a very natural and desirable form of queries--namely conjunctive ones--can be used in conjunction with OWL such that one of the major design criteria of the latter--namely decidability--can be retained. More precisely, we show that querying the tractable fragment \u0190L++ of OWL 1.1 is decidable. We also provide a complexity analysis and show that querying unrestricted \u0190L++ is undecidable.\n\nTitle: The Complexity of Answering Conjunctive and Navigational Queries over OWL 2 EL Knowledge Bases.\nContent: OWL 2 EL is a popular ontology language that supports role inclusions--axioms of the form S1...Sn \u2286 S that capture compositional properties of roles. Role inclusions closely correspond to context-free grammars, which was used to show that answering conjunctive queries (CQs) over OWL 2 EL knowledge bases with unrestricted role inclusions is undecidable. However, OWL 2 EL inherits from OWL 2 DL the syntactic regularity restriction on role inclusions, which ensures that role chains implying a particular role can be described using a finite automaton (FA). This is sufficient to ensure decidability of CQ answering; however, the FAs can be worst-case exponential in size so the known approaches do not provide a tight upper complexity bound. In this paper, we solve this open problem and show that answering CQs over OWL 2 EL knowledge bases is PSpace-complete in combined complexity (i.e., the complexity measured in the total size of the input). To this end, we use a novel encoding of regular role inclusions using bounded-stack pushdown automata--that is, FAs extended with a stack of bounded size. Apart from theoretical interest, our encoding can be used in practical tableau algorithms to avoid the exponential blowup due to role inclusions. In addition, we sharpen the lower complexity bound and show that the problem is PSPACE-hard even if we consider only role inclusions as part of the input (i.e., the query and all other parts of the knowledge base are fxed). Finally, we turn our attention to navigational queries over OWL 2 EL knowledge bases, and we show that answering positive, converse-free conjunctive graph XPath queries is PSPACE-complete as well; this is interesting since allowing the converse operator in queries is known to make the problem ExpTime-hard. Thus, in this paper we present several important contributions to the landscape of the complexity of answering expressive queries over description logic knowledge bases.\n\nTitle: Mixing Materialization and Query Rewriting for Existential Rules.\nContent: Ontology-Based Data Access (OBDA) is a recent paradigm aiming at enhancing data access by taking ontological knowledge into account. When using existential rules as ontological language, query answering is an undecidable problem, whence numerous decidable classes of ontologies have been defined, ranging from classes with very good computational complexities (AC(0) in data complexity) to classes with much larger expressivity. However, actually implementable algorithms have been proposed only for very restricted classes (typically those coinciding with lightweight description logics). The aim of this paper is to show how to deal with more expressive ontologies by proposing an algorithm that performs both materialization and rewriting and is applicable for a significant generalization of lightweight description logics. To this end, we first modify an existing algorithm previously proposed for a very generic class of rules, namely greedy bounded treewidth sets of rules. We then exhibit a special case, called pattern oblivious rule sets, which significantly generalizes the epsilon LHdr description logic, which underlies the OWL 2 EL ontology standard, while keeping the beneficial worst-case computational complexity. We last define a subclass of pattern oblivious rules that is recognizable in polynomial time.\n\nTitle: Top-k Exploration of Query Candidates for Efficient Keyword Search on Graph-Shaped (RDF) Data\nContent: Keyword queries enjoy widespread usage as they represent an intuitive way of specifying information needs. Recently, answering keyword queries on graph-structured data has emerged as an important research topic. The prevalent approaches build on dedicated indexing techniques as well as search algorithms aiming at finding substructures that connect the data elements matching the keywords. In this paper, we introduce a novel keyword search paradigm for graph-structured data, focusing in particular on the RDF data model. Instead of computing answers directly as in previous approaches, we first compute queries from the keywords, allowing the user to choose the appropriate query, and finally, process the query using the underlying database engine. Thereby, the full range of database optimization techniques can be leveraged for query processing. For the computation of queries, we propose a novel algorithm for the exploration of top-k matching subgraphs. While related techniques search the best answer trees, our algorithm is guaranteed to compute all k subgraphs with lowest costs, including cyclic graphs. By performing exploration only on a summary data structure derived from the data graph, we achieve promising performance improvements compared to other approaches.\n\nTitle: Schema-Agnostic Query Rewriting in SPARQL 1.1.\nContent: SPARQL 1.1 supports the use of ontologies to enrich query results with logical entailments, and OWL 2 provides a dedicated fragment OWL QL for this purpose. Typical implementations use the OWL QL schema to rewrite a conjunctive query into an equivalent set of queries, to be answered against the non-schema part of the data. With the adoption of the recent SPARQL 1.1 standard, however, RDF databases are capable of answering much more expressive queries directly, and we ask how this can be exploited in query rewriting. We find that SPARQL 1.1 is powerful enough to \"implement\" a full-fledged OWL QL reasoner in a single query. Using additional SPARQL 1.1 features, we develop a new method of schema-agnostic query rewriting, where arbitrary conjunctive queries over OWL QL are rewritten into equivalent SPARQL 1.1 queries in a way that is fully independent of the actual schema. This allows us to query RDF data under OWL QL entailment without extracting or preprocessing OWL axioms.\n\n## Task\nCompose an abstract for the title 'Nominals, inverses, counting, and conjunctive queries or: why infinity is your friend!' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Description Logics are knowledge representation formalisms that provide, for example, the logical underpinning of the W3C OWL standards. Conjunctive queries, the standard query language in databases, have recently gained significant attention as an expressive formalism for querying Description Logic knowledge bases. Several different techniques for deciding conjunctive query entailment are available for a wide range of DLs. Nevertheless, the combination of nominals, inverse roles, and number restrictions in OWL 1 and OWL 2 DL causes unsolvable problems for the techniques hitherto available. We tackle this problem and present a decidability result for entailment of unions of conjunctive queries in the DL ALCHOIQb that contains all three problematic constructors simultaneously. Provided that queries contain only simple roles, our result also shows decidability of entailment of (unions of) conjunctive queries in the logic that underpins OWL 1 DL and we believe that the presented results will pave the way for further progress towards conjunctive query entailment decision procedures for the Description Logics underlying the OWL standards."
      }
    ]
  },
  {
    "tid":"191",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Complex Pipelined Executions in OpenMP Parallel Applications\nContent: Abstract: This paper proposes a set of extensions to the OpenMP programming model to express complex pipelined computations. This is accomplished by defining, in the form of directives, precedence relations among the tasks originated from work-sharing constructs. The proposal is based on the definition of a name space that identifies the work parceled out by these work-sharing constructs. Then the programmer defines the precedence relations using this name space. This relieves the programmer from the burden of defining complex synchronization data structures and the insertion of explicit synchronization actions in the program that make the program difficult to understand and maintain. This work is transparently done by the compiler with the support of the OpenMP runtime library. The proposal is motivated and evaluated with a synthetic multi-block example. The paper also includes a description of the compiler and run- time support in the framework of the NanosCompiler for OpenMP.\n\nTitle: Improving the Integration of Task Nesting and Dependencies in OpenMP\nContent: The tasking model of OpenMP 4.0 supports both nesting and the definition of dependences between sibling tasks. A natural way to parallelize many codes with tasks is to first taskify the high-level functions and then to further refine these tasks with additional subtasks. However, this top-down approach has some drawbacks since combining nesting with dependencies usually requires additional measures to enforce the correct coordination of dependencies across nesting levels. For instance, most non-leaf tasks need to include a taskwait at the end of their code. While these measures enforce the correct order of execution, as a side effect, they also limit the discovery of parallelism. In this paper we extend the OpenMP tasking model to improve the integration of nesting and dependencies. Our proposal builds on both formulas, nesting and dependencies, and benefits from their individual strengths. On one hand, it encourages a top-down approach to parallelizing codes that also enables the parallel instantiation of tasks. On the other hand, it allows the runtime to control dependencies at a fine grain that until now was only possible using a single domain of dependencies. Our proposal is realized through additions to the OpenMP task directive that ensure backward compatibility with current codes. We have implemented a new runtime with these extensions and used it to evaluate the impact on several benchmarks. Our initial findings show that our extensions improve performance in three areas. First, they expose more parallelism. Second, they uncover dependencies across nesting levels, which allows the runtime to make better scheduling decisions. And third, they allow the parallel instantiation of tasks with dependencies between them.\n\nTitle: Nanoscompiler: Supporting Flexible Multilevel Parallelism Exploitation In Openmp\nContent: This paper describes the support provided by the NanosCompiler to nested parallelism in OpenMP. The NanosCompiler is a source-to-source parallelizing compiler implemented around a hierarchical internal program representation that captures the parallelism expressed by the user (through OpenMP directives and extensions) and the parallelism automatically discovered by the compiler through a detailed analysis of data and control dependences. The compiler is finally responsible for encapsulating work into threads, establishing their execution precedences and selecting the mechanisms to execute them in parallel, The NanosCompiler enables the experimentation with different work allocation strategies for nested parallel constructs. Some OpenMP extensions are proposed to allow the specification of thread groups and precedence relations among them. Copyright (C) 2000 John Wiley & Sons, Ltd.\n\nTitle: Exploiting multilevel parallelism using OpenMP on a massive multithreaded architecture\nContent: This paper evaluates and analyzes multilevel parallelism on a chip multiprocessor (CMP) architecture. The environment is based on the experimental IBM BG/Cyclops architecture, where we have run the multi-zone parallel benchmarks. Multilevel parallelism is spawned using the Nanos OpenMP execution environment. We have performed the analysis with different execution parameters in order to evaluate different hardware threads distributions, cache utilization, and thread grouping configurations. Our results demonstrate that a large number of thread groups and good balancing algorithms are critical for high performance. We also show that a small number of threads can share the same data cache to increase the performance, but a large number of threads should better not share the same data caches.\n\nTitle: Exploring dynamic parallelism in OpenMP\nContent: GPU devices are becoming a common element in current HPC platforms due to their high performance-per-Watt ratio. However, developing applications able to exploit their dazzling performance is not a trivial task, which becomes even harder when they have irregular data access patterns or control flows. Dynamic Parallelism (DP) has been introduced in the most recent GPU architecture as a mechanism to improve applicability of GPU computing in these situations, resource utilization and execution performance. DP allows to launch a kernel within a kernel without intervention of the CPU. Current experiences reveal that DP is offered to programmers at the expenses of an excessive overhead which, together with its architecture dependency, makes it difficult to see the benefits in real applications. In this paper, we propose how to extend the current OpenMP accelerator model to make the use of DP easy and effective. The proposal is based on nesting of teams constructs and conditional clauses, showing how it is possible for the compiler to generate code that is then efficiently executed under dynamic runtime scheduling. The proposal has been implemented on the MACC compiler supporting the OmpSs task--based programming model and evaluated using three kernels with data access and computation patterns commonly found in real applications: sparse matrix vector multiplication, breadth-first search and divide--and--conquer Mandelbrot. Performance results show speed-ups in the 40x range relative to versions not using DP.\n\n## Task\nCompose an abstract for the title 'Defining and Supporting Pipelined Executions in OpenMP' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper proposes a set of extensions to the OpenMP programming model to express complex pipelined computations. This is accomplished by defining, in the form of directives, precedence relations among the tasks originated from work-sharing constructs. The proposal is based on the definition of a name space that identifies the work parceled out by these work-sharing constructs. Then the programmer defines the precedence relations using this name space. This relieves the programmer from the burden of defining complex synchronization data structures and the insertion of explicit synchronization actions in the program that make the program difficult to understand and maintain. The paper focuses on the runtime support required to support this feature and the code generated by the NanosCompiler."
      }
    ]
  },
  {
    "tid":"192",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: DSP address optimization using evolutionary algorithms\nContent: Offset assignment has been studied as a highly effective approach to code optimization in modern digital signal processors (DSPs). In this paper, we propose two evolutionary algorithms to solve the general offset assignment problem with k address registers and an arbitrary auto-modify range. These algorithms differ from previous algorithms by having the capability of visiting the entire search space. We implement and analyze a variety of existing general offset assignment algorithms and test them on a set of standard benchmarks. The algorithms we propose can achieve a performance improvement of up to 31% over the best existing algorithm. We also achieve an average of 14% improvement over the union of recently proposed algorithms.\n\nTitle: Modified genetic algorithm-based feature selection combined with pre-trained deep neural network for demand forecasting in outpatient department.\nContent: A deep learning approach to model the patients demand for different key resources.Normal GA is improved for feature selection tasks by redesigning its key operators.An SAE-based pre-training process is designed to improve the initialization of DNN.The case study is implemented with real data collected from a hospital.Proposed approach can be used as a universal model for demand forecasting in hospital. A well-performed demand forecasting can provide outpatient department (OPD) managers with essential information for staff scheduling and rostering, considering the non-reservation policy of OPD in China. Based on the results reported by relevant studies, most approaches have focused on forecasting the overall amount of patient flow and ignored the demand for other key resources in OPD or similar department. Moreover, few studies have conducted feature selection before training a forecast model, which is a significant pre-processing operation of data mining and widely applied for knowledge discovery in expert and intelligent system. This study develops a novel hybrid methodology to forecast the patients demand for different key resources in OPD, by combining a new feature selection method and a deep learning approach. A modified version of genetic algorithm (MGA) is proposed for feature selection. The key operators of normal genetic algorithm are redesigned to utilize useful information provided by filter-based feature selection and feature combinations. A feedforward deep neural network is introduced as the forecast model, and the initial parameter set is generated from a stacked autoencoder-based pre-training process to overcome the optimization challenges in constructing deep architectures. In order to evaluate the performance of our methodology, it is applied to an OPD located at Northeast China. The results are compared with those obtained from combinations of other feature selection methods and demand forecasting models. Compared with GA and PCA, MGA improves the quality and efficiency of feature selection, with less selected features to get higher forecast accuracy. Pre-trained DNN optimally strengthens the advantage of MGA, compared with MLR, ARIMAX and SANN. The combination of MGA and pre-trained DNN possesses strongest predictive power among all involved combinations. Furthermore, the results of proposed methodology are crucial prerequisites for staff scheduling and resource allocation in OPD. Elite features obtained by MGA can provide practical insights on potential association between manifold feature combinations and demand variance.\n\nTitle: CASPER: An Integrated Energy-Driven Approach for Task Graph Scheduling on Distributed Embedded Systems\nContent: For multiprocessor embedded systems, the dynamic voltage scaling (DVS) technique can be applied to scheduled applications for energy reduction. DVS utilizes slack in the schedule to slow down processes and save energy. Therefore, it is generally believed that the maximal energy saving is achieved on a schedule with the minimum makespan (maximal slack). Most current approaches treat task assignment, scheduling, and DVS separately. In this paper, we present a framework called CASPER (Combined Assignment, Scheduling, and PowER-management) that challenges this common belief by integrating task scheduling and DVS under a single iterative optimization loop via genetic algorithm. We have conducted extensive experiments to validate the energy ef?ciency of CASPER. For homogeneous multiprocessor systems (in which all processors are of the same type), we consider a recently proposed slack distribution algorithm (PDP-SPM) [3]: applying PDP-SPM on the schedule with the minimal makespan gives an average of 53.8% energy saving; CASPER ?nds schedules with slightly larger makespan but a 57.3% energy saving, a 7.8% improvement. For heterogeneous systems, we consider the power variation DVS (PV-DVS) algorithm [13], CASPER improves its energy ef?ciency by 8.2%. Finally, our results also show that the proposed single loop CASPER framework saves 23.3% more energy over GMA+EE-GLSA [12], the only other known integrated approach with a nested loop that combines scheduling and power management in the inner loop but leaves assignment in the outer loop.\n\nTitle: Power Management of Multicore Multiple Voltage Embedded Systems by Task Scheduling\nContent: We study the role of task-level scheduling in power management on multicore multiple voltage embedded systems. Multicore on-a-chip, in particular DSP systems, can greatly improve performance through parallelism. On the other hand, dynamic voltage scaling (DVS) has been shown to be one of the most effective low power design techniques and multiple supply voltage system is among the most practical and well-studied DVS systems. The integration of multiple cores/processors on a chip naturally makes the system suitable for DVS with new innovations such as voltage island. In this paper, we discuss how to reduce multicore system's power consumption by utilizing multiple supply voltages. We first formulate the power management problem for multicore multi-voltage embedded systems and show that the problem is NP-hard, but the NPhardness can be removed for several real life applications. More specific, we develop polynomial algorithms to find the optimal solutions for two special cases. The first case is when preemption is allowed, and the second one is when the system uses the first-come-first- serve (FCFS) service strategy. We prove both algorithms' optimality and analyze their run-time complexity. Simulation on reallife and randomly generated tasks show that the optimal preemption scheduler provides significant energy saving. Our goal in this paper is to build a solid foundation for the power management problem on multicore multiple voltage systems and to study real life applications where the problem can be solved optimally.\n\nTitle: Energy-efficient dual-voltage soft real-time system with (m,k)-firm deadline guarantee\nContent: Soft real-time systems can tolerate some occasional deadline misses. This feature provides a unique opportunity to reduce system's energy consumption. In this paper, we study the system with (m,k)-firm deadline, a popular model for soft real-time systems. It basically requires at least m successful completions in any k consecutive executions. Our goal is to design such system with dual supply voltages for energy efficiency. To reach this goal, we first propose an on-line greedy deterministic scheduler that provides the (m,k)-firm guarantee with the provably minimum energy consumption. We then develop a novel exact method to compute the scheduler's average energy consumption per iteration. This leads us to the numerical solution to the voltage set-up problem, which seeks for the values of the two supply voltages to achieve the most energy efficiency with (m,k)-firm guarantee. Simulation shows that dual-voltage system can reduce significant amount of energy over single voltage system. Our numerical method finds the best voltage set-ups in seconds, while it takes hours to obtain almost identical solutions by simulation.\n\n## Task\nCompose an abstract for the title 'A genetic algorithm for solving patient-priority- based elective surgery scheduling problem' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Surgery generates the largest cost and revenue in the hospital. The quality of operation directly affects the level of patient satisfaction and economic benefit. This paper focuses on partitioning patients into different priorities according to the state of illness, an optimization model with the aim of maximizing customer satisfaction is established under the consideration of a three-dimensional parameter constraint related patients, operating rooms and medical staffs. An Genetic algorithm is proposed with two-dimensional 0-1 encoding for solving the surgery scheduling problem with the data derived from an upper first-class hospital, the experimental results show the efficiency of the model and algorithm."
      }
    ]
  },
  {
    "tid":"193",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Parallel I/O Behavior Model for HPC Applications Using Serial I/O Libraries\nContent: Analyzing and understanding an application's Input/Output (I/O) access patterns provides key information to gain insight into how the behavior of an application affects its performance in different systems. In this paper, we propose a portable model to represent the I/O behavior of parallel applications that use serial I/O libraries, as part of a more holistic model for I/O of parallel applications. The model allows actions such as replication of the application behavior on different HPC systems and evaluation of the I/O performance without running the real application. In this paper, we evaluate the portability of the proposed model for MADbench2 and ABYSS- P in four HPC systems. We analyze the impact of the parallel system configuration in a Cloud environment for the ABYSS-P application by using the proposed I/O behavior model.\n\nTitle: PIOM-PX: A Framework for Modeling the I/O Behavior of Parallel Scientific Applications.\nContent: Current parallel scientific applications generate a huge amount of data that must be managed efficiently for the HPC storage systems. However, the I/O performance depends on the application I/O behavior and the configuration of the underlying I/O system. To understand the I/O behavior in the software stack and its impact on the I/O operations defined in the application logic, we propose a design framework named PIOM-PX, which allows to define an I/O behavior model based on the I/O phases of HPC applications at POSIX-IO level. We validate our framework using the IOR benchmark for four I/O patterns and we analyze the I/O behavior of NAS BT-IO.\n\nTitle: Analyzing the Parallel I/O Severity of MPI Applications.\nContent: Performance evaluation of parallel applications plays an important role in High Performance Computing (HPC). This is also applied to parallel I/O performance evaluation, which requires understanding the I/O pattern of the application and having knowledge about the performance capacity of the HPC I/O system. In this paper, we present a methodology to evaluate the I/O performance of parallel applications based on the I/O severity degree. We define the I/O severity concept taking into account the I/O requirements of a parallel application, the mapping of I/O processes and the configuration of the I/O subsystem. Requirements are expressed in units denominated I/O phases, which are defined using the temporal and spatial pattern of different files of the application. Our approach is applied to the I/O kernels of scientific applications such as S3DIO, FLASH-IO and BT-IO on the SuperMUC supercomputer. Experimental results show that our methodology allows us to identify if a parallel application is limited by the I/O subsystem and identifying possible root causes of the I/O problems.\n\nTitle: Extraction of Parallel Application Signatures for Performance Prediction\nContent: Predicting performance of parallel applications is becoming increasingly complex and the best performance predictor is the application itself, but the time required to run it thoroughly is a onerous requirement. We seek to characterize the behavior of message-passing applications on different systems by extracting a signature which will allow us to predict what system will allow the application to perform best. To achieve this goal, we have developed a method we called Parallel Application Signatures for Performance Prediction (PAS2P) that strives to describe an application based on its behavior. Based on the application\u2019s message-passing activity, we have been able to identify and extract representative phases, with which we created a Parallel Application Signature that has allowed us to predict the application\u2019s performance. We have experimented with different signature-extraction algorithms and found a reduction in the prediction error using different scientific applications on different clusters. We were able to predict execution times with an average accuracy of over 98%.\n\nTitle: Methodology for Efficient Execution of SPMD Applications on Multicore Environments\nContent: The need to efficiently execute applications in heterogeneous environments is a current challenge for parallel computing programmers. The communication heterogeneities found in multicore clusters need to be addressed to improve efficiency and speedup. This work presents a methodology developed for SPMD applications, which is centered on managing communication heterogeneities and improving system efficiency on multicore clusters. The methodology is composed of three phases: characterization, mapping strategy, and scheduling policy. We focus on SPMD applications which are designed through a message-passing library for communication, and selected according to their synchronicity and communications volume. The novel contribution of this methodology is it determines the approximate number of cores necessary to achieve a suitable solution with a good execution time, while the efficiency level is maintained over a threshold defined by users. Applying this methodology gave results showing a maximum improvement in efficiency of around 43% in the SPMD applications tested.\n\n## Task\nCompose an abstract for the title 'Modeling Parallel Scientific Applications through their Input/Output Phases' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The increase in computational power in processing units and the complexity of scientific applications that use high performance computing require more efficient Input/Output (I/O) systems. To use the I/O systems more efficiently it is necessary to know its performance capacity to determine if it fulfills applications' I/O requirements. Evaluating the I/O system performance capacity is difficult due to the diversity of I/O architectures and the complexity of its I/O software stack. Furthermore, parallel scientific applications have different behavior depending on their access patterns. Then, it is necessary to have some method to evaluate the I/O subsystem capacity taking into account the applications access patterns without executing the application in each I/O subsystem. Here, we propose a methodology to evaluate the I/O subsystem performance capacity through an I/O model of the parallel application independent of the I/O subsystem. This I/O model is composed of I/O phases representing \"where\" and \"when\" the I/O operations are performed into application logic. This approach encompasses the I/O subsystem evaluation at I/O library level for the application I/O model. The I/O phases are replicated by benchmark IOR which is executed in the target subsystem. This approach was used to estimate the I/O time of an application in different subsystems. The results show an relative error of estimation lower than 10%. This approach was also utilized to select the I/O subsystem that provide less I/O time for the application."
      }
    ]
  },
  {
    "tid":"194",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Usability constructs: a cross-cultural study of how users and developers experience their use of information systems\nContent: Whereas research on usability predominantly employs universal definitions of the aspects that comprise usability, people experience their use of information systems through personal constructs. Based on 48 repertory-grid interviews, this study investigates how such personal constructs are affected by two factors crucial to the international development and uptake of information systems: cultural background (Chinese, Danish, or Indian) and stakeholder group (developer or user). We find that for the user group frustrating and useful systems are experienced similarly, whereas for the developers frustrating systems are experienced similarly to easy-to-use systems. Looking at the most characteristic construct for each participant we find that Chinese participants use constructs related to security, task types, training, and system issues, whereas Danish and to some extent Indian participants make more use of constructs traditionally associated with usability (e.g., easy-to-use, intuitive, and liked). Further analysis of the data is ongoing.\n\nTitle: Organisational implementation: a complex but under-recognised aspect of information-system design\nContent: The development of information technology (IT) is often seen as consisting of analysis, design, technical implementation, and testing. While this may involve user input at several stages --- to support the software engineers in devising and revising the system --- this view on systems development marginalises organisational implementation. Organisational implementation comprises the activities that prepare organisations and users for a new system as well as the activities that prepare the system for the transition period during which it enters into operation and takes over from previous systems and artefacts. This study analyses three organisational-implementation activities undertaken during the complete redesign of a large information system: the information plan, the data conversion, and the release strategy. It is found that these activities must be carefully aligned with the technical implementation of the system and that they involve the development of additional system facilities. In sum, this study provides evidence of the complexity and importance of organisational implementation and, thereby, argues that it must be recognised as a first-rate constituent of the design process.\n\nTitle: Effects-driven IT development: an instrument for supporting sustained participatory design\nContent: We present effects-driven IT development as an instrument for pursuing and reinforcing Participatory Design (PD) when it is applied in commercial information technology (IT) projects. Effects-driven IT development supports the management of a sustained PD process throughout design and organizational implementation. The focus is on the effects to be achieved by users through their adoption and use of a system. The overall idea is to (a) specify the purpose of a system as effects that are both measurable and meaningful to the users, and (b) evaluate the absence or presence of these effects during real use of the system. Effects are formulated in a user-oriented terminology, and they can be evaluated and revised with users in an iterative and incremental systems-development process that involves pilot implementations. In this paper we investigate the design, pilot implementation, and effects assessment of an electronic patient record. Effects concerning, among other things, clinicians' mental workload were specified and measured, but apart from the planned changes associated with these effects the pilot implementation also gave rise to emergent, opportunity-based, and curtailed changes. We discuss our experiences regarding conditions for making the specification of effects and their real-use evaluation central activities in IT projects.\n\nTitle: The information-seeking practices of engineers: searching for documents as well as for people\nContent: Engineers get most of their information from colleagues and internal reports. This study investigates how engineers\u2019 information-seeking practices intertwine looking for informing documents with looking for informed people. Based on case studies in two product-development organisations we find that engineers search for documents to find people, search for people to get documents, and interact socially to get information without engaging in explicit searches. This intricate interplay between document and people sources can be explained by the nature of the design task. Many possible solutions are normally available to the designer and in choosing one over the others the designer must take into account a complex set of issues involving both the product as such and its context. However, design documentation seems to be biased toward technical aspects of the chosen solution, while information about the context of the design process is typically not available. Hence, people become a critical source of information because they can explain and argue about why specific decisions were made and what purpose is served by individual parts of the design. While document retrieval is a well-established field, this study concludes by briefly outlining how computer systems could support searches for people. Given the immense practical importance of searches for people there seems to be a large need for such systems and, consequently, for addressing the open research questions involved in designing them.\n\nTitle: Collaborative information seeking and expertise seeking: different discourses about similar issues.\nContent: Purpose - The purpose of this paper is to compare and contrast research on collaborative information seeking (CIS) and expertise seeking (EXS) to identify focal themes, blind spots, and possibilities for cross-fertilization. Design/methodology/approach - Existing research was reviewed. The review consisted of a content analysis of 70 (CIS) and 72 (EXS) studies with respect to the context, scope, process, and setting of CIS and EXS, supplemented with a bibliometric analysis of the references in the reviewed studies. Findings - In CIS, the context is a group of actors collaborating on a shared task. In EXS, the information need is held by an individual but resolved by consulting other people. While the typical scope of EXS studies is source selection, CIS studies mostly concern the consultation of the sources and the use of the obtained information. CIS and EXS studies also attend differentially to the information-seeking process. Only 4 percent of the references in the reviewed studies are cited in both CIS and EXS research. The author concludes that, at present, CIS and EXS are different discourses about similar issues. Research limitations/implications - Increased interaction between CIS and EXS will advance research in both areas and prevent duplication of effort. Topics for future research are identified. It should be noted that the findings are limited to the 142 studies reviewed. Originality/value - By analyzing CIS in the context of EXS, and vice versa, this study provides a fresh look at the information-seeking research that attends to collaboration.\n\n## Task\nCompose an abstract for the title 'Configuring information systems and work practices for each other: What competences are needed locally?' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"\u2022Design is increasingly completed locally as system configurations after go-live.\u2022The configuration of systems and practices for each other is demanding in competences.\u2022Understanding practice is the largest competence issue in the studied project.\u2022Preparing and making change are probably the competences most thinly spread locally.\u2022The catalog of competences needed locally can inform training and project staffing."
      }
    ]
  },
  {
    "tid":"195",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Laser Fault Attack on Physically Unclonable Functions.\nContent: Physically Unclonable Functions (PUFs) are introduced to remedy the shortcomings of traditional methods of secure key storage and random key generation on Integrated Circuits (ICs). Due to their effective and low-cost implementations, intrinsic PUFs are popular PUF instances employed to improve the security of different applications on reconfigurable hardware. In this work we introduce a novel laser fault injection attack on intrinsic PUFs by manipulating the configuration of logic cells in a programable logic device. We present two fault attack scenarios, where not only the effectiveness of modeling attacks can be dramatically increased, but also the entropy of the targeted PUF responses are drastically decreased. In both cases, we conduct detailed theoretical analyses by considering XOR arbiter PUFs and RO PUFs as the examples of PUF-based authenticators and PUF-based random key generators, respectively. Finally we present our experimental results based on conducting laser fault injection on real PUFs, implemented on a common complex programmable logic device manufactured in 180 nm technology.\n\nTitle: Cloning Physically Unclonable Functions\nContent: As system security demands continue to evolve, Physically Unclonable Functions (PUFs) are a promising solution for secure storage on Integrated Circuits (ICs). SRAM PUFs are among the most popular types of PUFs, since they require no additional circuitry and can be implemented with on-die memories such as caches and data memory that are readily available on both ASICs and FPGAs. This work demonstrates that SRAM PUFs are not well suited as PUFs, as they do not meet several requirements that constitute an ideal PUF. The compact nature of SRAM, standard interconnects and resiliency to environmental effects make SRAM PUFs particularly easy to clone. We consider several ways in which SRAM PUFs can be characterized and demonstrate a Focused Ion Beam circuit edit with which we were able to produce a physical clone of our Proof-of-Concept SRAM PUF implementation. As a result of the circuit edit, when challenged, the physical clone produced an identical physical response to the original device. To the best of our knowledge, this is the first work in which a physical clone of a Physically Unclonable Function was produced.\n\nTitle: On the Power of Optical Contactless Probing: Attacking Bitstream Encryption of FPGAs.\nContent: Modern Integrated Circuits (ICs) employ several classes of countermeasures to mitigate physical attacks. Recently, a powerful semi-invasive attack relying on optical contactless probing has been introduced, which can assist the attacker in circumventing the integrated countermeasures and probe the secret data on a chip. This attack can be mounted using IC debug tools from the backside of the chip. The first published attack based on this technique was conducted against a proof-of-concept hardware implementation on a Field Programmable Gate Array (FPGA). Therefore, the success of optical probing techniques against a real commercial device without any knowledge of the hardware implementation is still questionable. The aim of this work is to assess the threat of optical contactless probing in a real attack scenario. To this end, we conduct an optical probing attack against the bitstream encryption feature of a common FPGA. We demonstrate that the adversary is able to extract the plaintext data containing sensitive design information and intellectual property (IP). In contrast to previous optical attacks from the IC backside, our attack does not require any device preparation or silicon polishing, which makes it a non-invasive attack. Additionally, we debunk the myth that small technology sizes are unsusceptible to optical attacks, as we use an optical resolution of about 1 um to successfully attack a 28 nm device. Based on our time measurements, an attacker needs less than 10 working days to conduct the optical analysis and reverse-engineer the security-related parts of the hardware. Finally, we propose and discuss potential countermeasures, which could make the attack more challenging.\n\n\n\nTitle: Why Attackers Win: On the Learnability of XOR Arbiter PUFs.\nContent: Aiming to find an ultimate solution to the problem of secure storage and hardware authentication, Physically Unclonable Functions (PUFs) appear to be promising primitives. While arbiter PUFs utilized in cryptographic protocols are becoming one of the most popular PUF instances, their vulnerabilities to Machine Learning (ML) attacks have been observed earlier. These attacks, as cost-effective approaches, can clone the challenge-response behavior of an arbiter PUF by collecting a subset of challenge-response pairs (CRPs). As a countermeasure against this type of attacks, PUF manufacturers shifted their focus to nonlinear architectures, such as XOR arbiter PUFs with a large number of arbiter PUF chains. However, the natural question arises whether an XOR arbiter PUF with an arbitrarily large number of parallel arbiter chains can be considered secure. On the other hand, even if a mature ML approach with a significantly high accuracy is adopted, the eventual delivery of a model for an XOR arbiter PUF should be ensured. To address these issues, this paper presents a respective PAC learning framework. Regarding our framework, we are able to establish a theoretical limit on the number of arbiter chains, where an XOR arbiter PUF can be learned in polynomial time, with given levels of accuracy and confidence. In addition, we state how an XOR arbiter PUF with noisy responses can be provably PAC learned. Finally, on the basis of learning theory concepts, we conclude that no secure XOR arbiter PUF relying on current IC technologies can be manufactured.\n\nTitle: Photonic Side-Channel Analysis of Arbiter PUFs.\nContent: As intended by its name, physically unclonable functions (PUFs) are considered as an ultimate solution to deal with insecure storage, hardware counterfeiting, and many other security problems. However, many different successful attacks have already revealed vulnerabilities of certain digital intrinsic PUFs. This paper demonstrates that legacy arbiter PUF and its popular extended versions (i.e., feed-forward and XOR-enhanced) can be completely and linearly characterized by means of photonic emission analysis. Our experimental setup is capable of measuring every PUF internal delay with a resolution of 6 ps. Due to this resolution, we indeed require only the theoretical minimum number of linear independent equations (i.e., physical measurements) to directly solve the underlying inhomogeneous linear system. Moreover, it is not required to know the actual PUF responses for our physical delay extraction. We present our practical results for an arbiter PUF implementation on a complex programmable logic device manufactured with a 180 nm process. Finally, we give an insight into photonic emission analysis of arbiter PUF on smaller chip architectures by performing experiments on a field programmable gate array manufactured with a 60 nm process.\n\n## Task\nCompose an abstract for the title 'Lattice Basis Reduction Attack against Physically Unclonable Functions' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Due to successful modeling attacks against arbiter PUFs (Physically Unclonable Functions), the trend towards consideration of XOR arbiter PUFs has emerged. Nevertheless, it has already been demonstrated that even this new non-linear structure, with a restricted number of parallel arbiter chains, is still vulnerable to more advanced modeling attacks and side channel analyses. However, so far the security of XOR arbiter PUFs with a large number of parallel arbiter chains has not been appropriately assessed. Furthermore, as another countermeasure against modeling and physical attacks, the concept of controlled PUFs, i.e., with a limited access to challenges and responses, has also been developed. Towards a better understanding of the security of XOR arbiter PUFs, the present paper simultaneously addresses all above mentioned countermeasures by introducing a novel attack, which is a combination of a lattice basis reduction attack and a photonic side channel analysis. We present how our new attack can be successfully launched against XOR arbiter PUFs with an arbitrarily large number of parallel arbiter chains. Most interestingly, our attack does not require any access to challenges or responses. Finally, by conducting an exhaustive discussion on our experimental results, the practical feasibility of our attack scenario is proved as well."
      }
    ]
  },
  {
    "tid":"196",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Sparse communication networks and efficient routing in the plane (extended abstract)\nContent: Traditional approaches to network design separate the issues of designing the network itself and designing its management and control subsystems. This paper proposes an approach termed routing-oriented network design, which is based on designing the network topology and its routing scheme together, attempting to optimize some of the relevant parameters of both simultaneously.This approach is explored by considering the design of communication networks supporting efficient routing in the special case of points located in the Euclidean plane. The desirable network parameters considered include low degree and small number of communication links. The desirable routing parameters considered include small routing tables, small number of hops and low routing stretch.Two rather different schemes are presented, one based on direct navigation in the plane and the other based on efficient hierarchical tree covers. On a collection of n sites with diameter D, these methods yield networks with maximum degree O(log D) (hence a total of O(n log D) communication links), coupled with routing schemes with constant routing stretch, O(log n log D) memory bits per vertex and routes with at most log n or log D hops.\n\nTitle: Proximity-Preserving Labeling Schemes and Their Applications\nContent: This paper considers informative labeling schemes for graphs. Specifically, the question introduced is whether it is possible to label the vertices of a graph with short labels in such a way that the distance between any two vertices can be inferred from inspecting their labels. A labeling scheme enjoying this property is termed a proximity-preserving labeling scheme. It is shown that for the class of n-vertex weighted trees with M-bit edge weights, there exists such a proximity-preserving labeling scheme using O(M log n + log2n) bit labels. For the family of all n-vertex unweighted graphs, a labeling scheme is proposed that using O(log2 n \u010b \u03f0 \u010b n1/\u03f0) bit labels can provide approximate estimates to the distance, which are accurate up to a factor of \u221a8\u03f0. In particular, using O(log3 n) bit labels the scheme can provide estimates accurate up to a factor of \u221a2 log n. (For weighted graphs, one of the log n factors in the label size is replaced by a factor logarithmic in the network's diameter.) In addition to their theoretical interest, proximity-preserving labeling systems seem to have some relevance in the context of communication networks. We illustrate this by proposing a potential application of our labeling schemes to efficient distributed connection setup in circuit-switched networks.\n\nTitle: Exact Algorithms and Approximation Schemes for Base Station Placement Problems\nContent: This paper concerns geometric disk problems motivated by base station placement problems arising in wireless network design. We first study problems that involve maximizing the coverage under various interference-avoidance constraints. A representative problem for this type is the maximum weight independent set problem on unit disk graphs, for which we present an exact solution whose complexity is exponential but with a sublinear exponent. Specifically, our algorithm has time complexity 2O(\u9a74mlogm), where m is the number of disks. We then study the problem of covering all the clients by a collection of disks of variable radii while minimizing the sum of radii, and present a PTAS for this problem.\n\nTitle: Dynamic routing schemes for graphs with low local density\nContent: This article studies approximate distributed routing schemes on dynamic communication networks. The work focuses on dynamic weighted general graphs where the vertices of the graph are fixed, but the weights of the edges may change. Our main contribution concerns bounding the cost of adapting to dynamic changes. The update efficiency of a routing scheme is measured by the time needed in order to update the routing scheme following a weight change. A naive dynamic routing scheme, which updates all vertices following a weight change, requires \u03a9(Diam) time in order to perform the updates after every weight change, where Diam is the diameter of the underlying graph. In contrast, this article presents approximate dynamic routing schemes with average time complexity \u0398\u02dc(D) per topological change, where D is the local density parameter of the underlying graph. Following a weight change, our scheme never incurs more than Diam time; thus, our scheme is particularly efficient on graphs which have low local density and large diameter. The article also establishes upper and lower bounds on the size of the databases required by the scheme at each site.\n\nTitle: Sparse communication networks and efficient routing in the plane\nContent: Traditional approaches to network design separate the issues of designing the network itself and designing its management and control subsystems. This paper proposes an approach termed routing-oriented network design, which is based on designing the network topology and its routing scheme together, attempting to optimize some of the relevant parameters of both simultaneously. This approach is explored by considering the design of communication networks supporting efficient routing in the special case of points located in the Euclidean plane. The desirable network, parameters considered include low degree and small number of communication links. The desirable routing parameters considered include small routing tables, small number of hops and low routing stretch. Two rather different schemes are presented, one based on direct navigation in the plane and the other based on efficient hierarchical tree covers. On a collection of it sites with diameter D, these methods yield networks with a total of O(n log D) communication links and some bounds on the degree, coupled with routing schemes with constant routing stretch, O(log log D) memory bits per vertex and routes with at most log n or log D hops.\n\n## Task\nCompose an abstract for the title 'f-Sensitivity Distance Oracles and Routing Schemes' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"An f-sensitivity distance oracle for a weighted undirected graph G(V,E) is a data structure capable of answering restricted distance queries between vertex pairs, i.e., calculating distances on a subgraph avoiding some forbidden edges. This paper presents an efficiently constructible f-sensitivity distance oracle that given a triplet (s,t,F), where s and t are vertices and F is a set of forbidden edges such that |F|\u2264f, returns an estimate of the distance between s and t in G(V,E\u2216F). For an integer parameter k\u22651, the size of the data structure is O(fkn 1+1/k log\u2009(nW)), where W is the heaviest edge in G, the stretch (approximation ratio) of the returned distance is (8k\u22122)(f+1), and the query time is O(|F|\u22c5log\u20092 n\u22c5log\u2009log\u2009n\u22c5log\u2009log\u2009d), where d is the distance between s and t in G(V,E\u2216F). Our result differs from previous ones in two major respects: (1) it is the first to consider approximate oracles for general graphs (and thus obtain a succinct data structure); (2) our result holds for an arbitrary number of forbidden edges. In contrast, previous papers concern f-sensitive exact distance oracles, which consequently have size \u03a9(n 2). Moreover, those oracles support forbidden sets F of size |F|\u22642. The paper also considers f-sensitive compact routing schemes, namely, routing schemes that avoid a given set of forbidden (or failed) edges. It presents a scheme capable of withstanding up to two edge failures. Given a message M destined to t at a source vertex s, in the presence of a forbidden edge set F of size |F|\u22642 (unknown to s), our scheme routes M from s to t in a distributed manner, over a path of length at most O(k) times the length of the optimal path (avoiding F). The total amount of information stored in vertices of G is O(kn 1+1/k log\u2009(nW)log\u2009n). To the best of our knowledge, this is the first result obtaining an f-sensitive compact routing scheme for general graphs."
      }
    ]
  },
  {
    "tid":"197",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Fast deterministic distributed algorithms for sparse spanners\nContent: This paper concerns the efficient construction of sparse and low stretch spanners for unweighted arbitrary graphs with n nodes. All previous deterministic distributed algorithms, for constant stretch spanners of o(n^2) edges, have a running time @W(n^@e) for some constant @e0 depending on the stretch. Our deterministic distributed algorithms construct constant stretch spanners of o(n^2) edges in o(n^@e) time for any constant @e0. More precisely, in Linial's free model a.k.a LOCAL model, we construct in n^O^(^1^/^l^o^g^n^) time, for every graph, a (3,2)-spanner of O(n^3^/^2) edges, i.e., a spanning subgraph in which the distance is at most 3 times the distance of the original graph plus 2. The result is extended to (@a\"k,@b\"k)-spanners with O(n^1^+^1^/^klogk) edges for every integer parameter k=1, where @a\"k+@b\"k=O(k^l^o^g^\"^2^5). If the minimum degree of the graph is @W(n), then, in the same time complexity, a (5,4)-spanner with O(n) edges can be constructed.\n\nTitle: Polylogarithmic network navigability using compact metrics with small stretch\nContent: Graph augmentation theory is a general framework for analyzing navigability in social networks. It is known that, for large classes of graphs, there exist augmentations of these graphs such that greedy routing according to the shortest path metric performs in polylogarithmic expected number of steps. However, it is also known that there are classes of graphs for which no augmentations can enable greedy routing according to the shortest path metric to perform better than \u03a9(n1/\u221alog n) expected number of steps. In fact, the best known universal bound on the greedy diameter of arbitrary graph is essentially n1/3. That is, for any graph, there is an augmentation such that greedy routing according to the shortest path metric performs in \u00d5(n1/3) expected number of steps. Hence, greedy routing according to the shortest path metric has at least two drawbacks. First, it is in general space-consuming to encode locally the shortest path distances to all the other nodes, and, second, greedy routing according to the shortest path metric performs poorly in some graphs. We prove that, using semimetrics of small stretch results in a huge positive impact, in both encoding space and efficiency of greedy routing. More precisely, we show that, for any connected n-node graph G and any integer k \u2265 1, there exist an augmentation \u03c6 of G and a semimetric \u03bc on G with stretch 2/k-1 such that greedy routing according to \u03bc performs in O(k2 n2/klog2n) expected number of steps. As a corollary, we get that for any connected n-node graph G, there exist an augmentation \u03c6 of G and a semimetric \u03bc on G with stretch O(log n) such that greedy routing according to \u03bc performs in polylogarithmic expected number of steps. This latter semimetric can be encoded locally at every node using only a polylogarithmic number of bits.\n\nTitle: On the locality of distributed sparse spanner construction\nContent: The paper presents a deterministic distributed algorithm that, given k \u2265 1, constructs in k rounds a (2k-1,0)-spanner of O(k n1+1/k) edges for every n-node unweighted graph. (If n is not available to the nodes, then our algorithm executes in 3k-2 rounds, and still returns a (2k-1,0)-spanner with O(k n1+1/k) edges.) Previous distributed solutions achieving such optimal stretch-size trade-off either make use of randomization providing performance guarantees in expectation only, or perform in log\u03a9(1)n rounds, and all require a priori knowledge of n. Based on this algorithm, we propose a second deterministic distributed algorithm that, for every \u03b5 0, constructs a (1+\u03b5,2)-spanner of O(\u03b5-1 n3/2) edges in O(\u03b5-1) rounds, without any prior knowledge on the graph. Our algorithms are complemented with lower bounds, which hold even under the assumption that n is known to the nodes. It is shown that any (randomized) distributed algorithm requires k rounds in expectation to compute a (2k-1,0)-spanner of o(n1+1/(k-1)) edges for k \u2208 {2,3,5}. It is also shown that for every k1, any (randomized) distributed algorithm that constructs a spanner with fewer than n1+1/k + \u03b5 edges in at most n\u03b5 expected rounds must stretch some distances by an additive factor of n\u03a9(\u03b5). In other words, while additive stretched spanners with O(n1+1/k) edges may exist, e.g., for k=2,3, they cannot be computed distributively in a sub-polynomial number of rounds in expectation.\n\nTitle: Local computation of nearly additive spanners\nContent: An (\u03b1, \u03b2)-spanner of a graph G is a subgraph H that approximates distances in G within a multiplicative factor \u03b1 and an additive error \u03b2, ensuring that for any two nodes u, v, dH(u, v) \u2264 \u03b1 \u010b dG(u, v)+\u03b2. This paper concerns algorithms for the distributed deterministic construction of a sparse (\u03b1, \u03b2)-spanner H for a given graph G and distortion parameters \u03b1 and \u03b2. It first presents a generic distributed algorithm that in constant number of rounds constructs, for every n-node graph and integer k \u2265 1, an (\u03b1, \u03b2)-spanner of O(\u03b2n1+1/k) edges, where \u03b1 and \u03b2 are constants depending on k. For suitable parameters, this algorithm provides a (2k - 1, 0)-spanner of at most kn1+1/k edges in k rounds, matching the performances of the best known distributed algorithm by Derbel et al. (PODC '08). For k = 2 and constant \u0190 0, it can also produce a (1 + \u0190, 2 - \u0190)-spanner of O(n3/2) edges in constant time. More interestingly, for every integer k 1, it can construct in constant time a (1 + \u0190, O(1/\u0190)k-2)-spanner of O(\u0190-k+1n1+1/k) edges. Such deterministic construction was not previously known. The paper also presents a second generic deterministic and distributed algorithm based on the construction of small dominating sets and maximal independent sets. After computing such sets in sub-polynomial time, it constructs at its best a (1 + \u0190, \u03b2)-spanner with O(\u03b2n1+1/k) edges, where \u03b2 = klog(log k/\u0190)+O(1). For k = 3, it provides a (1 + \u0190, 6 - \u0190)-spanner with O(\u0190-1n4/3) edges. The additive terms \u03b2 = \u03b2(k, \u0190) in the stretch of our constructions yield the best trade-off currently known between k and \u0190, due to Elkin and Peleg (STOC '01). Our distributed algorithms are rather short, and can be viewed as a unification and simplification of previous constructions.\n\nTitle: On approximate distance labels and routing schemes with affine stretch\nContent: For every integral parameter k 1, given an unweighted graph G, we construct in polynomial time, for each vertex u, a distance label L(u) of size \u00d5(n2/(2k-1)). For any u, v \u03b5 G, given L(u), L(v) we can return in time O(k) an affine approximation d(u, v) on the distance d(u, v) between u and v in G such that d(u, v) \u2264 d(u, v) \u2264 (2k - 2)d(u, v) + 1. Hence we say that our distance label scheme has affine stretch of (2k - 2)d + 1. For k = 2 our construction is comparable to the O(n5/3) size, 2d + 1 affine stretch of the distance oracle of P\u0103trascu and Roditty (FOCS '10), it incurs a o(log n) storage overhead while providing the benefits of a distance label. For any k 1, given a restriction of o(n1+1/(k-1)) on the total size of the data structure, our construction provides distance labels with affine stretch of (2k - 2)d+1 which is better than the stretch (2k - 1)d scheme of Thorup and Zwick (J. ACM '05). Our second contribution is a compact routing scheme with poly-logarithmic addresses that provides affine stretch guarantees. With \u00d5(n3/(3k-2))-bit routing tables we obtain affine stretch of (4k - 6)d+1, for any k 1. Given a restriction of o(n1/(k-1)) on the table size, our routing scheme provides affine stretch which is better than the stretch (4k - 5)d routing scheme of Thorup and Zwick (SPAA '01).\n\n## Task\nCompose an abstract for the title 'Deterministic distributed construction of linear stretch spanners in polylogarithmic time' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The paper presents a deterministic distributed algorithm that given an n node unweighted graph constructs an O(n3/2) edge 3- spanner for it in O(log n) time. This algorithm is then extended into a deterministic algorithm for computing an O(k n1+1/k) edge O(k)-spanner in 2O(k) logk-1 n time for every integer parameter k \u2265 1. This establishes that the problem of the deterministic construction of a linear (in k) stretch spanner with few edges can be solved in the distributed setting in polylogarithmic time. The paper also investigates the distributed construction of sparse spanners with almost pure additive stretch (1 + \u03b5, \u03b2), i.e., such that the distance in the spanner is at most 1 + \u03b5 times the original distance plus \u03b2. It is shown, for every \u03b5 0, that in O(\u03b5-1 log n) time one can deterministically construct a spanner with O(n3/2) edges that is both a 3-spanner and a (1+\u03b5, 8 log n)-spanner. Furthermore, it is shown that in nO(1/\u221alog n) + O(1/\u03b5) time one can deterministically construct a spanner with O(n3/2) edges which is both a 3-spanner and a (1 + \u03b5, 4)-spanner. This algorithm can be transformed into a Las Vegas randomized algorithm with guarantees on the stretch and time, running in O(\u03b5-1 + log n) expected time."
      }
    ]
  },
  {
    "tid":"198",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Solomon: seeking the truth via copying detection\nContent: We live in the Information Era: the Web has enabled the availability of a huge amount of useful information and eased sharing of data among sources. Despite the richness of information surrounding us, an information user is often overwhelmed by the huge volume of raw, heterogeneous, and even conflicting data. Data sources can be of different qualities, providing information of different levels of accuracy, freshness, and completeness, and data can flow between data sources, being copied, reformatted, verified, and modified. There is an increasing need to help users find the information and the sources that are of highest quality and authority, to help data producers understand how their data are being used (and possibly protect their rights), and to help analysts and auditors understand how information has been disseminated and how rumors have been propagated [1].\n\nTitle: Global detection of complex copying relationships between sources\nContent: Web technologies have enabled data sharing between sources but also simplified copying (and often publishing without proper attribution). The copying relationships can be complex: some sources copy from multiple sources on different subsets of data; some co-copy from the same source, and some transitively copy from another. Understanding such copying relationships is desirable both for business purposes and for improving many key components in data integration, such as resolving conflicts across various sources, reconciling distinct references to the same real-world entity, and efficiently answering queries over multiple sources. Recent works have studied how to detect copying between a pair of sources, but the techniques can fall short in the presence of complex copying relationships. In this paper we describe techniques that discover global copying relationships between a set of structured sources. Towards this goal we make two contributions. First, we propose a global detection algorithm that identifies co-copying and transitive copying, returning only source pairs with direct copying. Second, global detection requires accurate decisions on copying direction; we significantly improve over previous techniques on this by considering various types of evidence for copying and correlation of copying on different data items. Experimental results on real-world data and synthetic data show high effectiveness and efficiency of our techniques.\n\nTitle: Scaling up Copy Detection.\nContent:   Recent research shows that copying is prevalent for Deep-Web data and considering copying can significantly improve truth finding from conflicting values. However, existing copy detection techniques do not scale for large sizes and numbers of data sources, so truth finding can be slowed down by one to two orders of magnitude compared with the corresponding techniques that do not consider copying. In this paper, we study {\\em how to improve scalability of copy detection on structured data}.   Our algorithm builds an inverted index for each \\emph{shared} value and processes the index entries in decreasing order of how much the shared value can contribute to the conclusion of copying. We show how we use the index to prune the data items we consider for each pair of sources, and to incrementally refine our results in iterative copy detection. We also apply a sampling strategy with which we are able to further reduce copy-detection time while still obtaining very similar results as on the whole data set. Experiments on various real data sets show that our algorithm can reduce the time for copy detection by two to three orders of magnitude; in other words, truth finding can benefit from copy detection with very little overhead. \n\nTitle: Detecting Clones, Copying and Reuse on the Web\nContent: The Web has enabled the availability of a vast amount of useful information in recent years. However, the web technologies that have enabled sources to share their information have also made it easy for sources to copy from each other and often publish without proper attribution. Understanding the copying relationships between sources has many benefits, including helping data providers protect their own rights, improving various aspects of data integration, and facilitating in-depth analysis of information flow. The importance of copy detection has led to a substantial amount of research in many disciplines of Computer Science, based on the type of information considered, such as text, images, videos, software code, and structured data. This seminar explores the similarities and differences between the techniques proposed for copy detection across the different types of information. We also examine the computational challenges associated with large-scale copy detection, indicating how they could be detected efficiently, and identify a range of open problems for the community.\n\nTitle: Reference reconciliation in complex information spaces\nContent: Reference reconciliation is the problem of identifying when different references (i.e., sets of attribute values) in a dataset correspond to the same real-world entity. Most previous literature assumed references to a single class that had a fair number of attributes (e.g., research publications). We consider complex information spaces: our references belong to multiple related classes and each reference may have very few attribute values. A prime example of such a space is Personal Information Management, where the goal is to provide a coherent view of all the information on one's desktop.Our reconciliation algorithm has three principal features. First, we exploit the associations between references to design new methods for reference comparison. Second, we propagate information between reconciliation decisions to accumulate positive and negative evidences. Third, we gradually enrich references by merging attribute values. Our experiments show that (1) we considerably improve precision and recall over standard methods on a diverse set of personal information datasets, and (2) there are advantages to using our algorithm even on a standard citation dataset benchmark.\n\n## Task\nCompose an abstract for the title 'Truth discovery and copying detection in a dynamic world' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Modern information management applications often require integrating data from a variety of data sources, some of which may copy or buy data from other sources. When these data sources model a dynamically changing world (e.g., people's contact information changes over time, restaurants open and go out of business), sources often provide out-of-date data. Errors can also creep into data when sources are updated often. Given out-of-date and erroneous data provided by different, possibly dependent, sources, it is challenging for data integration systems to provide the true values. Straightforward ways to resolve such inconsistencies (e.g., voting) may lead to noisy results, often with detrimental consequences. In this paper, we study the problem of finding true values and determining the copying relationship between sources, when the update history of the sources is known. We model the quality of sources over time by their coverage, exactness and freshness. Based on these measures, we conduct a probabilistic analysis. First, we develop a Hidden Markov Model that decides whether a source is a copier of another source and identifies the specific moments at which it copies. Second, we develop a Bayesian model that aggregates information from the sources to decide the true value for a data item, and the evolution of the true values over time. Experimental results on both real-world and synthetic data show high accuracy and scalability of our techniques."
      }
    ]
  },
  {
    "tid":"199",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Building descriptive and discriminative visual codebook for large-scale image applications\nContent: Inspired by the success of textual words in large-scale textual information processing, researchers are trying to extract visual words from images which function similar as textual words. Visual words are commonly generated by clustering a large amount of image local features and the cluster centers are taken as visual words. This approach is simple and scalable, but results in noisy visual words. Lots of works are reported trying to improve the descriptive and discriminative ability of visual words. This paper gives a comprehensive survey on visual vocabulary and details several state-of-the-art algorithms. A comprehensive review and summarization of the related works on visual vocabulary is first presented. Then, we introduce our recent algorithms on descriptive and discriminative visual word generation, i.e., latent visual context analysis for descriptive visual word identification [74], descriptive visual words and visual phrases generation [68], contextual visual vocabulary which combines both semantic contexts and spatial contexts [69], and visual vocabulary hierarchy optimization [18]. Additionally, we introduce two interesting post processing strategies to further improve the performance of visual vocabulary, i.e., spatial coding [73] is proposed to efficiently remove the mismatched visual words between images for more reasonable image similarity computation; user preference based visual word weighting [44] is developed to make the image similarity computed based on visual words more consistent with users' preferences or habits.\n\nTitle: Image Classification by Selective Regularized Subspace Learning\nContent: Feature learning is an intensively studied research topic in image classification. Although existing methods like sparse coding, locality-constrained linear coding, fisher vector encoding, etc., have shown their effectiveness in image representation, most of them overlook a phenomenon called thesmall sample size problem, where the number of training samples is relatively smaller than the dimensionality of the features, which may limit the predictive power of the classifier. Subspace learning is a strategy to mitigate this problem by reducing the dimensionality of the features. However, most conventional subspace learning methods attempt to learn a global subspace to discriminate all the classes, which proves to be difficult and ineffective in multi-class classification task. To this end, we propose to learn a local subspace for each sample instead of learning a global subspace for all samples. Our key observation is that, in multi-class image classification, the label of each testing sample is only confused by a few classes which have very similar visual appearance to it. Thus, in this work, we propose a coarse-to-fine strategy, which first picks out such classes, and then conducts a local subspace learning to discriminate them. As the subspace learning method is regularized and conducted within some selected classes, we term it selective regularized subspace learning (SRSL), and we term our classification pipeline selective regularized subspace learning based multi-class image classification (SRSL_MIC). Experimental results on four representative datasets (Caltech-101, Indoor-67, ORL Faces and AR Faces) demonstrate the effectiveness of the proposed method.\n\nTitle: On random hyper-class random forest for visual classification\nContent: Random forest is an effective ensemble classifier. It has shown to be efficient with good generalization performance thanks to the randomness in training. However, for visual classification problems, where high dimensions and large number of classes exist, the impurity measurement in training the tree nodes split function not only neglects the strong conditional dependencies among visual attributes, but also leads to rather weak base classifiers, which may not reflect enough discriminative capability. Addressing this, we develop a random hyper-class random forest (RHC-RF) for visual classification tasks in this paper. During training each tree node, the entire class space is randomly partitioned into two hyper-classes and a weak learner targeting at separating the two hyper-classes is trained, where a linear classifier is learned with multiple feature attributes. This novel training scheme well preserves the randomness by random partition of the label space, and at the same time, the objective of training each node directly targets at discriminating classes in a divide-and-conquer-like manner. The proposed method is expected to benefit from the improved discrimination for visual classification compared with conventional random forests while preserving good generalization. Extensive experiments on various multi-class and high-dimensional visual classification tasks (including scene and object classification, image-based food identification, handwritten digit recognition and face recognition) demonstrate the superior accuracy, as well as its compactness and robustness achieved by the proposed method.\n\nTitle: Efficient region-aware large graph construction towards scalable multi-label propagation\nContent: With fast growing number of images on photo-sharing websites such as Flickr and Picasa, it is in urgent need to develop scalable multi-label propagation algorithms for image indexing, management and retrieval. It has been well acknowledged that analysis in semantic region level may greatly improve image annotation performance compared to that in the holistic image level. However, region level approach increases the data scale to several orders of magnitude and proposes new challenges to most existing algorithms. In this work, we present a novel framework to effectively compute pairwise image similarity by accumulating the information of semantic image regions. Firstly, each image is encoded as Bag-of-Regions based on multiple image segmentations. Secondly, all image regions are separated into buckets with efficient locality-sensitive hashing (LSH) method, which guarantees high collision probabilities for similar regions. The k-nearest neighbors of each image and the corresponding similarities can be efficiently approximated with these indexed patches. Lastly, the sparse and region-aware image similarity matrix is fed into the multi-label extension of the entropic graph regularized semi-supervised learning algorithm [1]. In combination they naturally yield the capability of handling large-scale dataset. Extensive experiments on NUS-WIDE (260k images) and COREL-5k datasets validate the effectiveness and efficiency of our proposed framework for region-aware and scalable multi-label propagation.\n\nTitle: Multiple Granularity Modeling: A Coarse-to-Fine Framework for Fine-grained Action Analysis.\nContent: Detecting fine-grained human action from video sequence is challenging. In this work, we propose to decompose this difficult analytic problem into two sequential tasks with increasing granularity. Firstly, we infer the coarse interaction status, i.e., which object is being manipulated and where the interaction occurs. To address the issue of frequent mutual occlusions during manipulation, we propose an interaction tracking framework in which hand (object) position and interaction status are jointly tracked by explicitly modeling the occlusion context. Secondly, for a given query sequence, the inferred interaction status is utilized to efficiently identify a small set of candidate matching sequences from the annotated training set. Frame-level action labels are then transferred to the query sequence by setting up the matching between the query and candidate sequences. Comprehensive experiments on two challenging fine-grained activity datasets show that: (1) the proposed interaction tracking approach achieves high tracking accuracy for multiple mutually occluded objects (hands) during manipulation action; and (2) the proposed multiple granularity analysis framework achieves superior action detection performance improvement over state-of-the-art methods.\n\n## Task\nCompose an abstract for the title 'A unified supervised codebook learning framework for classification.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, we investigate a discriminative visual dictionary learning method for boosting the classification performance. Tied to the K-means clustering philosophy, those popular algorithms for visual dictionary learning cannot guarantee the well-separation of the normalized visual word frequency vectors from distinctive classes or large label distances. The rationale of this work is to harness sample label information for learning visual dictionary in a supervised manner, and this target is then formulated as an objective function, where each sample element, e.g., SIFT descriptor, is expected to be close to its assigned visual word, and at the same time the normalized aggregative visual word frequency vectors are expected to possess the property that kindred samples shall be close to each other while inhomogeneous samples shall be far away. By relaxing the hard binary constraints to soft nonnegative ones, a multiplicative nonnegative update procedure is proposed to optimize the objective function along with theoretic convergence proof. Extensive experiments on classification tasks (i.e., natural scene and sports event classifications) all demonstrate the superiority of this proposed framework over conventional clustering based visual dictionary learning."
      }
    ]
  },
  {
    "tid":"200",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Instrumenting Network Simulators for Evaluating Energy Consumption in Power-Aware Ad-Hoc Network Protocols\nContent: In this paper, we describe our work on instrumenting network simulators to enable them to adequately and accurately account for the energy consumed by ad hoc network protocols\u00fd communication-related tasks. This is accomplished by explicitly accounting for low-power radio modes and considering the different energy costs associated with each possible radio state, i.e, transmitting, receiving, overhearing, idle, sensing, and sleeping. Our energy consumption instrumentation also allows the energy accounting to be done automatically by the simulator irrespective of what layer of the stack the protocol designer is working. To validate our model, we compare (1) simulation results using the GloMoSim/QualNet simulation platform with and without our instrumentation for the IEEE 802.11 DCF, (2) analytical results for both 802.11 and S-MAC (a power-aware MAC designed for sensor networks), and (3) simulation results reproducing testbed experiments obtained for the S-MAC protocol. Finally, by comparing S-MAC against 802.11 and AODV against DSR, we showcase the ability of a network simulation platform instrumented with our energy consumption model to evaluate energy consumption in ad-hoc network protocols.\n\nTitle: Characterizing Energy Consumption In A Visual Sensor Network Testbed\nContent: In this work we characterize the energy consumption of a visual sensor network testbed. Each node in the testbed consists of a \"single-board computer\", namely Crossbow's Stargate, equipped with a wireless network card and a webcam. We assess energy consumption of activities representative of the target application (e.g., perimeter surveillance) using a benchmark that runs (individual and combinations of) \"basic\" tasks such as processing, flash memory access, image acquisition, and communication over the network. In our characterization, we consider the various hardware states the system switches through as it executes these benchmarks, e.g., different radio modes (sleep, idle, transmission, reception), and webcam modes (off, on, and acquiring image). We report both steady-state and transient energy consumption behavior obtained by direct measurements of current with a digital multimeter. We validate our measurements against results obtained using the Stargate's on-board energy consumption measuring capabilities.\n\nTitle: Power-aware routing based on the energy drain rate for mobile ad hoc networks\nContent: Mobile ad hoc networks' (MANETs) inherent power limitation makes power-awareness a critical requirement for MANET protocols. We propose a new routing metric, the drain rate, which predicts the lifetime of a node as a function of current traffic conditions. We describe the minimum drain rate (MDR) mechanism which uses a combination of the drain rate with remaining battery capacity to establish routes. MDR can be employed by any existing MANET routing protocol to achieve a dual goal: extend both nodal battery life and connection lifetime. Using the ns-2 simulator and the dynamic source routing (DSR) protocol, we compared MDR to the minimum total transmission power routing (MTPR) scheme and the min-max battery cost routing (MM-BCR) scheme and proved that MDR is the best approach to achieve the dual goal.\n\nTitle: Isolines: Energy-Efficient Mapping in Sensor Networks\nContent: This paper introduces a novel energy efficient data aggregation algorithm that targets spatially correlated data in sensor networks. Isolines aggregation works by detecting isolines which are the lines in a contour map. Energy efficiency is achieved by having only the nodes that detect the isoline report to the sink. Simulation results show that isoline aggregation can lead to significant energy savings (some scenarios reported that no aggregation can send close to 150% more bytes than isolines aggregation) with adequate data accuracy. We also compared iso-lines against polygon aggregation, our implementation of an approach representing existing spatially-correlated data aggregation mechanisms. Our results report that isolines exhibit higher accuracy with a slight advantage in energy efficiency.\n\nTitle: Routing mechanisms for mobile ad hoc networks based on the energy drain rate\nContent: Untethered nodes in mobile ad hoc networks strongly depend on the efficient use of their batteries. In this paper, we propose a new metric, the drain rate, to forecast the lifetime of nodes according to current traffic conditions. This metric is combined with the value of the remaining battery capacity to determine which nodes can be part of an active route. We describe new route selection mechanisms for MANET routing protocols, which we call the minimum drain rate (MDR) and the conditional minimum drain rate (CMDR). MDR extends nodal battery life and the duration of paths, while CMDR also minimizes the total transmission energy consumed per packet. Using the ns-2 simulator and the dynamic source routing (DSR) protocol, we compare MDR and CMDR against prior proposals for energy-aware routing and show that using the drain rate for energy-aware route selection offers superior performance results. Methods keywords are system design and simulations.\n\n## Task\nCompose an abstract for the title 'Modeling Energy Consumption in Single-Hop IEEE 802.11 Ad Hoc Networks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper presents an analytical model to predict energy consumption in saturated IEEE 802.11 single-hop ad hoc networks under ideal channel conditions. The model we introduce takes into account the different operational modes of the IEEE 802.11 DCF MAC, and is validated against packet-level simulations. In contrast to previous works that attempted to characterize the energy consumption of IEEE 802.11 cards in isolated, contention-free channels (i.e., single sender/receiver pair), this paper investigates the extreme opposite case, i.e., when nodes need to contend for channel access under saturation conditions. In such scenarios, our main findings include: (1) contrary to what most previous results indicate, the radio's transmit mode has marginal impact on overall energy consumption, while other modes (receive, idle, etc.) are responsible for most of the energy consumed; (2) the energy cost to transmit useful data increases almost linearly with the network size; and (3) transmitting large payloads is more energy efficient under saturation conditions."
      }
    ]
  },
  {
    "tid":"201",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Distributed Top-k Keyword Search over Very Large Databases with MapReduce\nContent: In the last decade, keyword search over relational databases has been extensively studied because it promises to allow users lacking knowledge of structured query languages or unaware of the database schema to query the database in an intuitive way. The existing works about keyword search on databases proposed many approaches and have gain remarkable results. However, most of these approaches are designed for the centralized setting where keyword search is processed by only a single server. In reality, the scale of databases increases sharply and centralized methods hardly can handle keyword queries over these large databases. Moreover, processing keyword search over relational databases is a very time-consuming task, and the efficiency of the existing centralized approaches will degrade notably because the single server cannot provide enough computation power for the keyword search over very large databases. To address these challenges, we propose a distributed keyword search (DKS) approach with MapReduce and this approach can be well deployed on a cluster of servers to deal with keyword search over large databases in a parallel way.\n\nTitle: Scalable Distributed Processing of K Nearest Neighbor Queries over Moving Objects\nContent: Central to many applications involving moving objects is the task of processing k-nearest neighbor (k-NN) queries. Most of the existing approaches to this problem are designed for the centralized setting where query processing takes place on a single server; it is difficult, if not impossible, for them to scale to a distributed setting to handle the vast volume of data and concurrent queries that are increasingly common in those applications. To address this problem, we propose a suite of solutions that can support scalable distributed processing of k-NN queries. We first present a new index structure called Dynamic Strip Index (DSI), which can better adapt to different data distributions than exiting grid indexes. Moreover, it can be naturally distributed across the cluster, therefore lending itself well to distributed processing. We further propose a distributed k-NN search (DKNN) algorithm based on DSI. DKNN avoids having an uncertain number of potentially expensive iterations, and is thus more efficient and more predictable than existing approaches. DSI and DKNN are implemented on Apache S4, an open-source platform for distributed stream processing. We perform extensive experiments to study the characteristics of DSI and DKNN, and compare them with three baseline methods. Experimental results show that our proposal scales well and significantly outperforms the alternative methods.\n\nTitle: An efficient index structure for distributed k-nearest neighbours query processing\nContent: Many location-based services are supported by the moving k-nearest neighbour (k-NN) query, which continuously returns the k-nearest data objects for a query point. Most of existing approaches to this problem have focused on a centralized setting, which show poor scalability to work around massive-scale and distributed data sets. In this paper, we propose an efficient distributed solution for k-NN query over moving objects to tackle the increasingly large scale of data. This approach includes a new grid-based index called Block Grid Index (BGI), and a distributed k-NN query algorithm based on BGI. There are three advantages of our approach: (1) BGI can be easily constructed and maintained in a distributed setting; (2) the algorithm is able to return the results set in only two iterations. (3) the efficiency of k-NN query is improved. The efficiency of our solution is verified by extensive experiments with millions of nodes.\n\nTitle: Towards Efficient KNN Joins on Data Streams\nContent: We study the problem of efficient processing of kNN joins over high-dimensional data streams, which is an operation required by many big data applications. Specifically, we are concerned with the continuous evaluation of a set of k nearest neighbor queries Q on streams of high-dimensional items at consecutive snapshots of those streams. While one possible solution is to evaluate the kNN joins starting from scratch at each snapshot, it is too expensive for large volumes of data we encounter in big data applications. We consider the data stream on a time window and maintain the join results for Q at every snapshot in main memory. Our approach to this problem is to build indexes on Q, and only update the results of the queries affected by the changes in the streams at each snapshot. We propose a main-memory structure called the High-dimensional R-tree (HDR-tree) to index the queries, which is efficient in finding affected queries with reasonable maintenance cost. HDR-tree takes advantage of the benefit of clustering and the principle component analysis (PCA) technique. Preliminary experimental results show that our index structures significantly outperform baseline methods.\n\nTitle: CSR+-tree: Cache-conscious Indexing for High-dimensional Similarity Search\nContent: In this paper, we propose a novel index structure, the CSR+-tree, to support efficient high-dimensional similarity search in main memory. We introduce quantized bounding spheres (QBSs) that approximate bounding spheres (BSs) or data points. We analyze the respective pros and cons of both QBSs and the previously proposed quantized bounding rectangles (QBRs), and take the best of both worlds by carefully incorporating both of them into the CSR+-tree. We further propose a novel distance computation scheme that eliminates the need for decompressing QBSs or QBRs, which results in significant cost savings. We present an extensive experimental evaluation and analysis of the CSR+-tree, and compare its performance against that of other representative indexes in the literature. Our results show that the CSR+-tree consistently outperforms other index structures.\n\n## Task\nCompose an abstract for the title 'Pollux: towards scalable distributed real-time search on microblogs' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The last few years have witnessed a meteoric rise of microblogging platforms, such as Twitter and Tumblr. The sheer volume of the microblog data and its highly dynamic nature present unique technical challenges for the platforms that provide search services. In particular, the search service must provide real-time response to queries, and continuously update the results as new microblogs are posted. Conventional approaches either cannot keep up with the high update rate, or cannot scale well to handle the large volume of data. We propose Pollux, a system that provides distributed real-time indexing and search service on microblogs. It adopts the distributed stream processing paradigm advocated by the recently developed platforms that are designed for real-time processing of large volume of data, such as Apache S4 and Twitter Storm. Although those open-source platforms have found successful applications in production environments, they lack some critical features required for real-time search. In particular: (1) they only implement partial fault tolerance, and do not provide lossless recovery in the event of a node failure, and (2) they do not have a facility for storing global data, which is necessary in efficiently ranking search results. Addressing those problems, Pollux extends current platforms in two important ways. First, we propose a failover strategy that can ensure high system availability and no data/state loss in the event of a node failure. Second, Pollux adds a global storage facility that supports convenient, efficient, and reliable data storage for shared data. We describe how to apply Pollux to the task of real-time search. We implement Pollux based on Apache S4, and show through extensive experiments on a Twitter dataset that the proposed solutions are effective, and Pollux can achieve excellent scalability."
      }
    ]
  },
  {
    "tid":"202",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: An Adaptive And Distance-Based Resource Allocation Scheme For Interdependent Tasks In Mobile Ad Hoc Computational Grids\nContent: Two key components contribute to task completion time: execution cost and communication cost. The communication cost is induced by data transfers between tasks residing on separate nodes. The communication is always expensive and unreliable in mobile ad hoc Grids and therefore plays a critical role in application performance. To reduce communication cost, interdependent tasks are allocated to nodes located close to one another. However, once the tasks have been allocated, nodes can move within a Grid. The movement of nodes within a Grid may result in multi-hop communication between nodes executing dependent tasks. In order to deal with node mobility within a Grid, an effective resource allocation scheme is required, but the design of such a scheme for mobile ad hoc computational Grids is challenging due to the constrained communication environment, node mobility, and infrastructure-less network environment. In this paper, we have developed an adaptive and distance-based resource allocation scheme which takes into account the characteristics of an application and nodes and applies migration heuristics to address the local node mobility problem. The scheme is validated in a simulated environment using various workloads and parameters.\n\nTitle: Energy efficient resource allocation in mobile ad hoc computational grids\nContent: This paper addresses the problem of energy efficient resource allocation to interdependent tasks in mobile ad hoc computational grids. The interdependent tasks may exchange large quantity of data and consume the energy proportional to amount of data exchanged among the tasks. If tasks wouldn't be allocated effectively, then there can be a significant increase in the energy consumption and communication cost. The increased energy consumption limit the life time of a node and may result in a power failure which not only affects the task executing on a node but also affects other tasks in various ways. In addition, the mobile nodes within a grid are battery driven and thus have a limited power that should be utilized effectively. In this paper, we propose a hybrid power-based resource allocation scheme to reduce the energy consumption and communication cost among the interdependent tasks. The basic idea is to exploit the dependency and task types and allocate interdependent tasks to nodes accessible at a minimum transmission power level. We also propose a power-based algorithm to find a group of closest nodes to allocate a set of interdependent tasks. Compared to the traditional algorithms, the complexity of proposed algorithm depends on the number of transmission power levels rather than the number of nodes within a grid.\n\nTitle: Resource Allocation Scheme to Minimize Communication Cost in Mobile Ad Hoc Computational Grids\nContent: Recent advancements in mobile computing and communication technologies has given rise to development of mobile ad hoc computational grids. The mobile ad hoc computational grids provide a software infrastructure to share computing resources in mobile ad hoc environments. The adaptation of computational grids in mobile ad hoc environments is not straightforward, and presents many research challenges due to node mobility and infrastructure-less network environment. In order to improve utilization of shared computing resources and application performance, one of the key services is a resource allocation. In this paper, we propose a centralized resource allocation scheme that allocates dependent tasks among nodes in mobile ad hoc computational grids. First, we classify tasks and dependencies, and then exploit them to reduce communication cost among interdependent tasks, and thus application completion time. The effectiveness of proposed scheme is demonstrated through simulations.\n\nTitle: An effective and robust two-phase resource allocation scheme for interdependent tasks in mobile ad hoc computational Grids\nContent: This paper addresses the problem of resource allocation to interdependent tasks in mobile ad hoc computational Grids. Dependencies between tasks imply that there can be heavy communication induced by data transfers between tasks executed on separate nodes. The communication in mobile ad hoc Grids is always expensive and unreliable, and therefore plays a critical role in application performance. There are several factors that contribute to communication cost. Unreliable and short-term connectivity can increase communication cost due to frequent failure and activation of links, and ineffective resource allocation can increase communication cost due to multi hop communication between dependent tasks. To reduce communication cost, an effective and robust resource allocation scheme is required. However, the design of such a scheme for mobile ad hoc computational Grids exhibits numerous difficulties due to the constrained communication environment, node mobility, and lack of pre-existing network infrastructure. In this paper, we propose a two-phase resource allocation scheme to reduce communication cost between dependent tasks. The scheme is divided into two phases. The first phase exploits the history of user mobility patterns to select nodes that provide long-term connectivity and the second phase takes into account the task and dependency types, and uses the distance information among the nodes selected in the first phase to reduce communication costs. The scheme is validated in a simulation environment using various workloads and parameters.\n\nTitle: Network Aware and Power-Based Resource Allocation in Mobile Ad Hoc Computational Grid\nContent: Three resource allocation schemes are described for allocation of data-intensive dependent tasks on mobile ad hoc computational Grid. The first resource scheme exploits transmission power control mechanism in order to improve energy efficiency and network capacity while the second resource allocation scheme takes into account the connection quality and network traffic in order to reduce data transfer time. To balance the trade-off between first two schemes a network-aware and power-based resource allocation scheme is proposed which takes into account transmission power and network characteristics in order to make allocation decisions. The relationship between data transfer time and transmission energy consumption has also been investigated in detail. The schemes are validated in a simulated environment using various workloads and parameters.\n\n## Task\nCompose an abstract for the title 'An Energy-Efficient Resource Allocation Scheme for Mobile Ad Hoc Computational Grids' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Due to recent advancements in mobile computing and communication technologies, mobile ad hoc computational Grids are emerging as a new computing paradigm, enabling innovative applications through sharing of computing resources among mobile devices without any pre-existing network infrastructure. Energy-efficient resource allocation is one of the key issues in mobile ad hoc computational Grids due to limited battery life of mobile nodes. To reduce energy consumption, we propose a hybrid power-based resource allocation scheme for allocation of interdependent tasks to nodes within mobile ad hoc computational Grid. The basic idea is to exploit dependencies and task type, and allocate interdependent tasks to nodes accessible at minimum transmission power. We also propose a power-based algorithm to search a group of closest nodes to allocate a set of interdependent tasks. Compared to traditional algorithms, complexity of proposed algorithm depends on number of transmission power levels rather than number of nodes within a Grid. The scheme is validated in a simulation environment using various workloads and parameters."
      }
    ]
  },
  {
    "tid":"203",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Two Perspectives To Investigate The Intrinsic Organization Of The Dynamic And Ongoing Spontaneous Brain Activity In Humans\nContent: Spontaneous brain activity studies have revealed 'small-world' property in functional networks based on correlated or positively-correlated relationships. However, studies neither investigated negatively-correlated functional networks, nor checked the 'dynamic' properties of the whole functional organization. After subjects performed a specific task, what changes will be caused in the intrinsic organization? In this study, we examined pre-task and post-task resting brains using functional MRI (fMRI). Then we suggested two perspectives with positively-correlated brain functional network (PCBFN) and negatively-correlated brain functional network (NCBFN), by testing whether correlation coefficients were positive or negative. The major findings were: 1) the PCBFN was with small-world architecture but the NCBFN was not, with different motifs, and the PCBFN showed stronger small-world effect at the post-task resting state; 2) Both PCBFN and NCBFN followed an exponentially truncated power law degree distribution. This study may offer a framework to investigate the intrinsic organization of spontaneous brain activity.\n\nTitle: WI based multi-aspect data analysis in a brain informatics portal\nContent: In order to investigate human information processing mechanism systematically, Web intelligence (WI) based portal techniques are required for brain data measurement, management and analysis. Building a brain informatics portal is, in fact, to develop a data mining grid centric multi-layer grid system on the Wisdom Web, on which various data mining agents are deployed, for multi-aspect data analysis. We propose an approach for collecting, modeling, transforming, managing, and mining multiple human brain data obtained from systematic fMRI/EEG experiments. The proposed approach provides a new way in Brain Informatics (BI) for automatic analysis and understanding of human brain data to replace human-expert centric visualization. We attempt to change the perspective of cognitive scientists from a single type of experimental data analysis towards a holistic view at a long-term, global field of vision to understand the principle, models and mechanisms of human information processing system.\n\nTitle: Constructing a New-Style Conceptual Model of Brain Data for Systematic Brain Informatics\nContent: The development of brain science has led to a vast increase of brain data. To meet requirements of a systematic methodology of Brain Informatics (BI), this paper proposes a new conceptual model of brain data, namely Data-Brain, which explicitly represents various relationships among multiple human brain data sources, with respect to all major aspects and capabilities of human information processing systems (HIPS). A multidimension framework and a BI methodology-based ontological modeling approach have been developed to implement a Data-Brain. The Data-Brain, Data-Brain-based BI provenances, and heterogeneous brain data can be used to construct a Data-Brain-based brain data center which provides a global framework to integrate data, information, and knowledge coming from the whole research process for systematic BI study. Such a Data-Brain modeling approach represents a radically new way for domain-driven conceptual modeling of brain data, which models a whole process of systematically investigating human information processing mechanisms.\n\nTitle: Towards human level web intelligence: a brain informatics perspective\nContent: In this talk, we outline a vision of Web Intelligence (WI) research from the viewpoint of Brain Informatics (BI), a new interdisciplinary\n field that studies the mechanisms of human information processing from both the macro and micro viewpoints by combining experimental\n cognitive neuroscience with advanced information technology. BI can be regarded as brain sciences in WI centric IT age and\n emphasizes on a systematic approach for investigating human information processing mechanism. Advances in instruments like\n fMRI and information technologies offer more opportunities for research in both Web intelligence and brain sciences. Further\n understanding of human intelligence through brain sciences fosters innovative Web intelligence research and development. Web\n intelligence portal techniques provide a powerful new platform for brain sciences. The synergy between WI and BI advances\n our ways of analyzing and understanding of data, knowledge, intelligence and wisdom, as well as their relationship, organization\n and creation process. Web intelligence is becoming a central field that changes information technologies, in general, and\n artificial intelligence, in particular, towards human-level Web intelligence.\n \n\nTitle: Building A Data-Mining Grid For Multiple Human Brain Data Analysis\nContent: E-science is about global collaboration in key areas of science such as cognitive science and brain science, and the next generation of infrastructure such as the Wisdom Web and Knowledge Grids. As a case study, we investigate human multiperception mechanism by cooperatively using various psychological experiments, physiological measurements, and data mining techniques for developing artificial systems which match human ability in specific aspects. In particular, we observe fMRI (functional magnetic resonance imaging) and EEG (electroencephalogram) brain activations from the viewpoint of peculiarity oriented mining and propose a way of peculiarity oriented mining for knowledge discovery in multiple human brain data. Based on such experience and needs, we concentrate on the architectural aspect of a brain-informatics portal from the perspective of the Wisdom Web and Knowledge Grids. We describe how to build a data-mining grid on the Wisdom Web for multiaspect human brain data analysis. The proposed methodology attempts to change the perspective of cognitive scientists from a single type of experimental data analysis toward a holistic view at a long-term, global field of vision.\n\n## Task\nCompose an abstract for the title 'Robust and stable small-world topology of brain intrinsic organization during pre- and post-task resting states' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Brain functional network studies have demonstrated the small-world topology as the nature of large-scale spontaneous brain activity. Studies have also revealed that the temporal coherence of spontaneous activity could be reshaped during task-dependent (or post-task) resting states within local spatial patterns such as task-related and the default-mode networks. However, to our best knowledge, it is still a lack of rigorous investigations that whether the small-world topology of spontaneous intrinsic organization remains robust and stable during different resting states. To address the problem, we recorded blood oxygen level-dependent (BOLD) signals from two rests (namely, pre- and posttask resting states) before and after a simple semantic-matching task, and investigated the preceding task influences on the topology of the large-scale spontaneous intrinsic organization during the post-task resting state. The major findings are that the small-world configuration of spontaneous intrinsic organization remains robust and stable during resting states regardless of preceding task influences."
      }
    ]
  },
  {
    "tid":"204",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Recovering Sparse Signals With a Certain Family of Nonconvex Penalties and DC Programming\nContent: This paper considers the problem of recovering a sparse signal representation according to a signal dictionary. This problem could be formalized as a penalized least-squares problem in which sparsity is usually induced by a lscr1-norm penalty on the coefficients. Such an approach known as the Lasso or Basis Pursuit Denoising has been shown to perform reasonably well in some situations. However, it was also proved that nonconvex penalties like the pseudo lscrq-norm with q < 1 or smoothly clipped absolute deviation (SCAD) penalty are able to recover sparsity in a more efficient way than the Lasso. Several algorithms have been proposed for solving the resulting nonconvex least-squares problem. This paper proposes a generic algorithm to address such a sparsity recovery problem for some class of nonconvex penalties. Our main contribution is that the proposed methodology is based on an iterative algorithm which solves at each iteration a convex weighted Lasso problem. It relies on the family of nonconvex penalties which can be decomposed as a difference of convex functions (DC). This allows us to apply DC programming which is a generic and principled way for solving nonsmooth and nonconvex optimization problem. We also show that several algorithms in the literature dealing with nonconvex penalties are particular instances of our algorithm. Experimental results demonstrate the effectiveness of the proposed generic framework compared to existing algorithms, including iterative reweighted least-squares methods.\n\nTitle: More efficient sparsity-inducing algorithms using inexact gradient\nContent: In this paper, we tackle the problem of adapting a set of classic sparsity-inducing methods to cases when the gradient of the objective function is either difficult or very expensive to compute. Our contributions are two-fold: first, we propose methodologies for computing fair estimations of inexact gradients, second we propose novel stopping criteria for computing these gradients. For each contribution we provide theoretical backgrounds and justifications. In the experimental part, we study the impact of the proposed methods for two well-known algorithms, Frank-Wolfe and Orthogonal Matching Pursuit. Results on toy datasets show that inexact gradients can be as useful as exact ones provided the appropriate stopping criterion is used.\n\nTitle: Screening Rules for Lasso with Non-Convex Sparse Regularizers.\nContent: Leveraging on the convexity of the Lasso problem , screening rules help in accelerating solvers by discarding irrelevant variables, during the optimization process. However, because they provide better theoretical guarantees in identifying relevant variables, several non-convex regulariz-ers for the Lasso have been proposed in the literature. This work is the first that introduces a screening rule strategy into a non-convex Lasso solver. The approach we propose is based on a iterative majorization-minimization (MM) strategy that includes a screening rule in the inner solver and a condition for propagating screened variables between iterations of MM. In addition to improve efficiency of solvers, we also provide guarantees that the inner solver is able to identify the zeros components of its critical point in finite time. Our experimental analysis illustrates the significant computational gain brought by the new screening rule compared to classical coordinate-descent or proximal gradient descent methods.\n\nTitle: Surveying and comparing simultaneous sparse approximation (or group-lasso) algorithms\nContent: In this paper, we survey and compare different algorithms that, given an overcomplete dictionary of elementary functions, solve the problem of simultaneous sparse signal approximation, with common sparsity profile induced by a \u2113p\u2212\u2113q mixed-norm. Such a problem is also known in the statistical learning community as the group lasso problem. We have gathered and detailed different algorithmic results concerning these two equivalent approximation problems. We have also enriched the discussion by providing relations between several algorithms. Experimental comparisons of the detailed algorithms have also been carried out. The main lesson learned from these experiments is that depending on the performance measure, greedy approaches and iterative reweighted algorithms are the most efficient algorithms either in term of computational complexities, sparsity recovery or mean-square error.\n\nTitle: DC Proximal Newton for Nonconvex Optimization Problems.\nContent: We introduce a novel algorithm for solving learning problems where both the loss function and the regularizer are nonconvex but belong to the class of difference of convex (DC) functions. Our contribution is a new general purpose proximal Newton algorithm that is able to deal with such a situation. The algorithm consists in obtaining a descent direction from an approximation of the loss function and then in performing a line search to ensure a sufficient descent. A theoretical analysis is provided showing that the iterates of the proposed algorithm admit as limit points stationary points of the DC objective function. Numerical experiments show that our approach is more efficient than the current state of the art for a problem with a convex loss function and a nonconvex regularizer. We have also illustrated the benefit of our algorithm in high-dimensional transductive learning problem where both the loss function and regularizers are nonconvex.\n\n## Task\nCompose an abstract for the title 'Importance sampling strategy for non-convex randomized block-coordinate descent' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"As the number of samples and dimensionality of optimization problems related to statistics and machine learning explode, block coordinate descent algorithms have gained popularity since they reduce the original problem to several smaller ones. Coordinates to be optimized are usually selected randomly according to a given probability distribution. We introduce an importance sampling strategy that helps randomized coordinate descent algorithms to focus on blocks that are still far from convergence. The framework applies to problems composed of the sum of two possibly non-convex terms, one being separable and non-smooth. We have compared our algorithm to a full gradient proximal approach as well as to a randomized block coordinate algorithm that considers uniform sampling and cyclic block coordinate descent. Experimental evidences show the clear benefit of using an importance sampling strategy."
      }
    ]
  },
  {
    "tid":"205",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Distortion is Fixed Parameter Tractable\nContent: We study low-distortion embedding of metric spaces into the line, and more generally, into the shortest path metric of trees, from the parameterized complexity perspective. Let M = M (G ) be the shortest path metric of an edge weighted graph G , with the vertex set V (G ) and the edge set E (G ), on n vertices. We give the first fixed parameter tractable algorithm that for an unweighted graph metric M and integer d either constructs an embedding of M into the line with distortion at most d , or concludes that no such embedding exists. Our algorithm requires O (nd 4(2d + 1)2d ) time which is a significant improvement over the best previous algorithm of B\u0103doiu et al. that runs in time O (n 4d + 2 d O (1)). We find it surprising that this problem turns out to be fixed parameter tractable, because of its apparent similarity to the notoriously hard Bandwidth Minimization problem. We extend our results on embedding unweighted graph metric into the line in two ways. First, we give an algorithm to construct small distortion embeddings of weighted graph metrics. The running time of our algorithm is O (n (dW )4(2d + 1)2dW ) where W is the largest edge weight of the input graph. To complement this result, we show that the exponential dependence on the maximum edge weight is unavoidable. In particular, we show that deciding whether a weighted graph metric M (G ) with maximum weight W V (G )| can be embedded into the line with distortion at most d is NP-Complete for every fixed rational d *** 2. This rules out any possibility of an algorithm with running time O ((nW ) h (d )) where h is a function of d alone. Secondly, we consider more general host metrics for which analogous results hold. In particular, we prove that for any tree T with maximum degree \u0394 , embedding M into a shortest path metric of T is fixed parameter tractable, parameterized by (\u0394 ,d ).\n\nTitle: Fixed-Parameter Tractable Canonization and Isomorphism Test for Graphs of Bounded Treewidth\nContent: We give a fixed-parameter tractable algorithm that, given a parameter k and two graphs G1, G2, either concludes that one of these graphs has treewidth at least k, or determines whether G1 and G2 are isomorphic. The running time of the algorithm on an n-vertex graph is 2O(k5 log k) \u00b7 n5, and this is the first fixed-parameter algorithm for Graph Isomorphism parameterized by treewidth. Our algorithm in fact solves the more general canonization problem. We namely design a procedure working in 2OO(k5 log k) \u00b7 n5 time that, for a given graph G on n vertices, either concludes that the treewidth of G is at least k, or finds an isomorphism-invariant construction term - an algebraic expression that encodes G together with a tree decomposition of G of width O(k4). Hence, a canonical graph isomorphic to G can be constructed by simply evaluating the obtained construction term, while the isomorphism test reduces to verifying whether the computed construction terms for G1 and G2 are equal.\n\nTitle: Fixed Parameter Set Splitting, Linear Kernel and Improved Running Time\nContent: We study the problem k-SET SPLITTING in fixed parameter complexity. We show that the problem can be solved in time O\u2044(2:6494k), improving on the best currently known running time of O\u2044(8k). This is done by showing that a non-trivial instance must have a small minimal SET COVER, and using this to reduce the problem to a series of small instances of MAX SAT. We also give a linear kernel containing 2k elements and 2k sets. This is done by reducing the problem to a bipartite graph problem where we use crown decomposition to reduce the graph. We show that this result also gives a good kernel for MAX CUT.\n\nTitle: Parameterized tractability of multiway cut with parity constraints\nContent: In this paper, we study a parity based generalization of the classical Multiway Cut problem. Formally, we study the Parity Multiway Cut problem, where the input is a graph G, vertex subsets Te and To (T=Te\u222aTo) called terminals, a positive integer k and the objective is to test whether there exists a k-sized vertex subset S such that S intersects all odd paths from v\u2208To to T\u2216{v} and all even paths from v\u2208Te to T\u2216{v}. When Te=To, this is precisely the classical Multiway Cut problem. If To=\u2205 then this is the Even Multiway Cut problem and if Te=\u2205 then this is the Odd Multiway Cut problem. We remark that even the problem of deciding whether there is a set of at most k vertices that intersects all odd paths between a pair of vertices s and t is NP-complete. Our primary motivation for studying this problem is the recently initiated parameterized study of parity versions of graphs minors (Kawarabayashi, Reed and Wollan, FOCS 2011) and separation problems similar to Multiway Cut. The area of design of parameterized algorithms for graph separation problems has seen a lot of recent activity, which includes algorithms for Multi-Cut on undirected graphs (Marx and Razgon, STOC 2011, Bousquet, Daligault and Thomass\u00e9, STOC 2011), k-way cut (Kawarabayashi and Thorup, FOCS 2011), and Multiway Cut on directed graphs (Chitnis, Hajiaghayi and Marx, SODA 2012). A second motivation is that this problem serves as a good example to illustrate the application of a generalization of important separators which we introduce, and can be applied even when most of the recently develped tools fail to apply. We believe that this could be a useful tool for several other separation problems as well. We obtain this generalization by dividing the graph into slices with small boundaries and applying a divide and conquer paradigm over these slices. We show that Parity Multiway Cut is fixed parameter tractable (FPT) by giving an algorithm that runs in time $f(k)n^{{\\mathcal{O}}(1)}$. More precisely, we show that instances of this problem with solutions of size ${\\cal O}(\\log \\log n)$ can be solved in polynomial time. Along with this new notion of generalized important separators, our algorithm also combines several ideas used in previous parameterized algorithms for graph separation problems including the notion of important separators and randomized selection of important sets to simplify the input instance.\n\nTitle: Simultaneous Feedback Vertex Set: A Parameterized Perspective\nContent: For a family F of graphs, a graph G, and a positive integer k, the F-Deletion problem asks whether we can delete at most k vertices from G to obtain a graph in F. F-Deletion generalizes many classical graph problems such as Vertex Cover, Feedback Vertex Set, and Odd Cycle Transversal. For an integer \u03b1 \u2265 1, an n-vertex (multi) graph G = (V, \u22c3i=1\u03b1 Ei), where the edge set of G is partitioned into \u03b1 color classes, is called an \u03b1-edge-colored (multi) graph. A natural extension of the F-Deletion problem to edge-colored graphs is the Simultaneous F-Deletion problem. In the latter problem, we are given an \u03b1-edge-colored graph G and the goal is to find a set S of at most k vertices such that each graph Gi \u2212 S, where Gi = (V, Ei) and 1 \u2264 i \u2264 \u03b1, is in F. In this work, we study Simultaneous F-Deletion for F being the family of forests. In other words, we focus on the Simultaneous Feedback Vertex Set (SimFVS) problem. Algorithmically, we show that, like its classical counterpart, SimFVS parameterized by k is fixed-parameter tractable (FPT) and admits a polynomial kernel, for any fixed constant \u03b1. In particular, we give an algorithm running in 2O(\u03b1 k)nO(1) time and a kernel with O(\u03b1k3(\u03b1+1)) vertices. The running time of our algorithm implies that SimFVS is FPT even when \u03b1 \u2208 o(log n). We complement this positive result by showing that if we allow \u03b1 to be in O(log n), where n is the number of vertices in the input graph, then SimFVS becomes W[1]-hard. In particular, when \u03b1 is roughly equal to c log n, for a non-zero positive constant c, the problem becomes W[1]-hard. Our positive results answer one of the open problems posed by Cai and Ye (MFCS 2014).\n\n\n\n## Task\nCompose an abstract for the title 'Imbalance is fixed parameter tractable' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In the Imbalance Minimization problem we are given a graph G=(V,E) and an integer b and asked whether there is an ordering v\"1...v\"n of V such that the sum of the imbalance of all the vertices is at most b. The imbalance of a vertex v\"i is the absolute value of the difference between the number of neighbors to the left and right of v\"i. The problem is also known as the Balanced Vertex Ordering problem and it finds many applications in graph drawing. We show that this problem is fixed parameter tractable and provide an algorithm that runs in time 2^O^(^b^l^o^g^b^)@?n^O^(^1^). This resolves an open problem of Kara et al. [On the complexity of the balanced vertex ordering problem, in: COCOON, in: Lecture Notes in Comput. Sci., vol. 3595, 2005, pp. 849-858]."
      }
    ]
  },
  {
    "tid":"206",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Fast Parallel Algorithm for Selected Inversion of Structured Sparse Matrices with Application to 2D Electronic Structure Calculations\nContent: An efficient parallel algorithm is presented for computing selected components of $A^{-1}$ where $A$ is a structured symmetric sparse matrix. Calculations of this type are useful for several applications, including electronic structure analysis of materials in which the diagonal elements of the Green's functions are needed. The algorithm proposed here is a direct method based on a block $LDL^T$ factorization. The selected elements of $A^{-1}$ we compute lie in the nonzero positions of $L+L^T$. We use the elimination tree associated with the block $LDL^T$ factorization to organize the parallel algorithm, and reduce the synchronization overhead by passing the data level by level along this tree using the technique of local buffers and relative indices. We demonstrate the efficiency of our parallel implementation by applying it to a discretized two dimensional Hamiltonian matrix. We analyze the performance of the parallel algorithm by examining its load balance and communication overhead, and show that our parallel implementation exhibits an excellent weak scaling on a large-scale high performance distributed-memory parallel machine.\n\nTitle: Sparsify and Sweep: An Efficient Preconditioner for the Lippmann-Schwinger Equation.\nContent: This paper presents an efficient preconditioner for the Lippmann-Schwinger equation that combines the ideas of the sparsifying and the sweeping preconditioners. Following first the idea of the sparsifying preconditioner, this new preconditioner starts by transforming the dense linear system of the Lippmann-Schwinger equation into a nearly sparse system. The key novelty is a newly designed perfectly matched layer (PML) stencil for the boundary degrees of freedoms. The resulting sparse system gives rise to fairly accurate solutions and hence can be viewed as an accurate discretization of the Helmholtz equation. This new PML stencil also paves the way for applying the moving PML sweeping preconditioner to invert the resulting sparse system approximately. When combined with the standard GMRES solver, this new preconditioner for the Lippmann-Schwinger equation takes only a few iterations to converge for both two-dimensional and three-dimensional (3D) problems, where the iteration numbers are almost independent of the frequency. To the best of our knowledge, this is the first method that achieves near-linear cost to solve the 3D Lippmann-Schwinger equation in high-frequency cases.\n\nTitle: Sparsifying Preconditioner for Pseudospectral Approximations of Indefinite Systems on Periodic Structures\nContent: This paper introduces the sparsifying preconditioner for the pseudospectral approximation of highly indefinite systems on periodic structures, which include the frequency-domain response problems of the Helmholtz equation and the Schrodinger equation as examples. This approach transforms the dense system of the pseudospectral discretization approximately into a sparse system via an equivalent integral reformulation and a specially designed sparsifying operator. The resulting sparse system is then solved efficiently with sparse linear algebra algorithms and serves as a reasonably accurate preconditioner. When combined with standard iterative methods, this new preconditioner results in small iteration counts. Numerical results are provided for the Helmholtz equation and the Schrodinger in both two and three dimensions to demonstrate the effectiveness of this new preconditioner.\n\nTitle: Sparse Fourier Transform via Butterfly Algorithm\nContent: This paper introduces a fast algorithm for computing sparse Fourier transforms with spatial and Fourier data supported on curves or surfaces. This problem appears naturally in several important applications of wave scattering, digital signal processing, and reflection seismology. The main idea of the algorithm is that the interaction between a frequency region and a spatial region is approximately low rank if the product of their widths are bounded by the maximum frequency. Based on this property, we can approximate the interaction between these two boxes accurately and compactly using a small number of equivalent sources. The overall structure of the algorithm follows the butterfly algorithm. The computation is further accelerated by exploiting the tensor-product property of the Fourier kernel in two and three dimensions. The proposed algorithm is accurate and has the optimal complexity. We present numerical results in both two and three dimensions.\n\nTitle: Fast Directional Multilevel Algorithms for Oscillatory Kernels\nContent: This paper introduces a new directional multilevel algorithm for solving $N$-body or $N$-point problems with highly oscillatory kernels. These systems often result from the boundary integral formulations of scattering problems and are difficult due to the oscillatory nature of the kernel and the non-uniformity of the particle distribution. We address the problem by first proving that the interaction between a ball of radius $r$ and a well-separated region has an approximate low rank representation, as long as the well-separated region belongs to a cone with a spanning angle of $O(1/r)$ and is at a distance which is at least $O(r^2)$ away from from the ball. We then propose an efficient and accurate procedure which utilizes random sampling to generate such a separated, low rank representation. Based on the resulting representations, our new algorithm organizes the high frequency far field computation by a multidirectional and multiscale strategy to achieve maximum efficiency. The algorithm performs well on a large group of highly oscillatory kernels. Our algorithm is proved to have $O(N\\log N)$ computational complexity for any given accuracy when the points are sampled from a two dimensional surface. We also provide numerical results to demonstrate these properties.\n\n## Task\nCompose an abstract for the title 'SelInv---An Algorithm for Selected Inversion of a Sparse Symmetric Matrix' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We describe an efficient implementation of an algorithm for computing selected elements of a general sparse symmetric matrix A that can be decomposed as A = LDLT, where L is lower triangular and D is diagonal. Our implementation, which is called SelInv, is built on top of an efficient supernodal left-looking LDLT factorization of A. We discuss how computational efficiency can be gained by making use of a relative index array to handle indirect addressing. We report the performance of SelInv on a collection of sparse matrices of various sizes and nonzero structures. We also demonstrate how SelInv can be used in electronic structure calculations."
      }
    ]
  },
  {
    "tid":"207",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Activity analysis in crowded environments using social cues for group discovery and human interaction modeling\nContent: We propose a graph based algorithm with social cues to discover interacting groups.We propose a novel descriptor to recognize group activity in top-down approach.The descriptor capturing the motion and interaction of people in discovered group.The advantage of social cue in better understanding of activities in crowded scene. This paper presents a novel and efficient framework for group activity analysis. People in a scene can be intuitively represented by an undirected graph where vertices are people and the edges between two people are weighted by how much they are interacting. Social signaling cues are used to describe the degree of interaction between people. We propose a graph-based clustering algorithm to discover interacting groups in crowded scenes. Two social signaling cues are presented and compared for group discovery. The grouping of people in the scene serves to isolate the groups engaged in the dominant activity, effectively eliminating dataset contamination. Using discovered interacting groups, we create a descriptor capturing the motion and interaction of people within it. A bag-of-words approach is used to represent group activity and a SVM classifier is used for activity recognition. The proposed framework is evaluated in its ability to discover interacting groups and perform group activity recognition using two public datasets. The overall recognition system is compared to a baseline top-down model to understand the impact of social cues for activity recognition. The results of both the steps show that our method outperforms state-of-the-art methods for group discovery and achieves recognition rates comparable to state-of-the-art methods for group activity recognition.\n\nTitle: Modeling local behavior for predicting social interactions towards human tracking\nContent: Human interaction dynamics are known to play an important role in the development of robust pedestrian trackers that are needed for a variety of applications in video surveillance. Traditional approaches to pedestrian tracking assume that each pedestrian walks independently and the tracker predicts the location based on an underlying motion model, such as a constant velocity or autoregressive model. Recent approaches have begun to leverage interaction, especially by modeling the repulsion forces among pedestrians to improve motion predictions. However, human interaction is more complex and is influenced by multiple social effects. This motivates the use of a more complex human interaction model for pedestrian tracking. In this paper, we propose a novel human tracking method by modeling complex social interactions. We present an algorithm that decomposes social interactions into multiple potential interaction modes. We integrate these multiple social interaction modes into an interactive Markov Chain Monte Carlo tracker and demonstrate how the developed method translates into a more informed motion prediction, resulting in robust tracking performance. We test our method on videos from unconstrained outdoor environments and evaluate it against common multi-object trackers.\n\nTitle: Model-based estimation of 3D human motion with occlusion based on active multi-viewpoint selection\nContent: Dept. of Comput. & Inf. Sci., Pennsylvania Univ., Philadelphia, PA, USA Abstract: We present a new method for the 3D model-based tracking of human body parts. To mitigate the difficulties arising due to occlusion among body parts, we employ multiple calibrated cameras in a mutually orthogonal configuration. In addition, we develop criteria for a time varying active selection of a set of cameras to track the motion of a particular human part. In particular, at every frame, each camera tracks a number of parts depending on the visibility of these parts and the observability of their predicted motion from the specific camera. To relate points on the occluding contours of the parts to points on their models we apply concepts from projective geometry. Then, within the physics-based framework we compute the generalized forces applied from the parts' occluding contours to model points of the body parts. These forces update the translational and rotational degrees of freedom of the model, such as to minimize the discrepancy between the sensory data and the estimated model state. We present initial tracking results from a series of experiments involving the recovery of complex 3D motions in the presence of significant occlusion.\n\nTitle: A Review of Human Activity Recognition Methods\nContent: Recognizing human activities from video sequences or still images is a challenging task due to problems, such as background clutter, partial occlusion, changes in scale, viewpoint, lighting, and appearance. Many applications, including video surveillance systems, human-computer interaction, and robotics for human behavior characterization, require a multiple activity recognition system. In this work, we provide a detailed review of recent and state-of-the-art research advances in the field of human activity classification. We propose a categorization of human activity methodologies and discuss their advantages and limitations. In particular, we divide human activity classification methods into two large categories according to whether they use data from different modalities or not. Then, each of these categories is further analyzed into sub-categories, which reflect how they model human activities and what type of activities they are interested in. Moreover, we provide a comprehensive analysis of the existing, publicly available human activity classification datasets and examine the requirements for an ideal human activity recognition dataset. Finally, we report the characteristics of future research directions and present some open issues on human activity recognition.\n\nTitle: Fusion of human posture features for continuous action recognition\nContent: This paper presents a real-time or online system for continuous recognition of human actions. The system recognizes actions such as walking, bending, jumping, waving, and falling and relies on spatial features computed to characterize human posture. The paper evaluates the utility of these features based on its joint or independent treatment within the context of the Hidden Markov Model (HMM) framework. A baseline approach wherein disparate spatial features are treated as an input vector to trained HMMs is used to compare three different independent feature models. In addition, an action transition constraints is introduced to stabilize the developed models and allow for continuity in recognized actions. The system is evaluated across a dataset of videos and results reported in terms of frame error rate, the frame delay in recognizing an action, action recognition rate, and the missed and false recognition rates. Experimental results shows the effectiveness of the proposed treatment of input features and the corresponding HMM formulations.\n\n## Task\nCompose an abstract for the title 'A Hybrid Approach for Individual and Group Activity Analysis in Crowded Scene.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper presents an efficient hybrid (top-down and bottom-up) framework for activity recognition based on analyzing group context in crowded scenes. The approach presented starts by discovering interacting groups of people using a graph based clustering algorithm. Given the interacting groups, a novel group context activity descriptor is computed that captures not only the focal person's activity but also the behaviors of neighbors in the group. Finally, for a high-level of understanding of human activities, we propose a bottom-up approach using a random field model to encode activity relationships between people in the scene. We evaluate our approach on two public benchmark datasets and compare the utility of our proposed descriptor with other descriptors using the same baseline recognition framework. The results of both the steps show that our approach with the proposed descriptor achieves recognition rates comparable to state-of-the-art methods for activity recognition in crowded scenes."
      }
    ]
  },
  {
    "tid":"208",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Distributed Privacy-Preserving Secure Aggregation in Vehicular Communication\nContent: Vehicular ad hoc networks (VANETs), formed by computers embedded in vehicles and the traffic infrastructure, are expected to develop in the near future to improve traffic safety and efficiency. To this end, VANETs should be designed to be resistant against various abuses and attacks. In this paper, we first review the existing proposals to provide security, privacy, and data aggregation in vehicle-to-vehicle communication. We then address the fundamental issue of achieving these conflicting properties in a unified solution, having observed that separate efforts cannot fulfill the VANET design objectives. A set of new mechanisms are suggested for efficiently managing identities and securely compressing cryptographic witnesses, which are among the major obstacles to the deployment of strong security mechanisms in VANETs.\n\nTitle: Practical privacy for value-added applications in vehicular ad hoc networks\nContent: Advances in mobile networking and information processing technologies have triggered vehicular ad hoc networks (VANETs) for traffic safety and value-added applications. Most efforts have been made to address the security concerns while little work has been done to investigate security and privacy for value-added applications in VANETs. To fill this gap, we propose a value-added application, specifically, a security and privacy preserving location-based service (LBS) scheme for VANETs. For each LBS transaction, the scheme provides authentication, integrity and non-repudiation for both the service provider and the user. A user can obtain the service in an anonymous way and hence user privacy is well protected. However, a tracing procedure can be invoked to find malicious users, thereby efficiently preventing users from abusing the anonymity provided by the system.\n\nTitle: Secure compression of privacy-preserving witnesses in vehicular ad hoc networks\nContent: Vehicular ad hoc networks (VANETs) are designed to improve traffic safety and efficiency. To this end, the traffic communication must be authenticated to guarantee trustworthiness for guiding drivers and establishing liability in case of traffic accident investigation. Cryptographic authentication techniques have been extensively exploited to secure VANETs. Applying cryptographic authentication techniques such as digital signatures raises challenges to efficiently store signatures on messages growing with time. To alleviate the conflict between traffic liability investigation and limited storage capacity in vehicles, this paper proposes to aggregate signatures in VANETs. Our proposal can preserve privacy for honest vehicles and trace misbehaving ones, and provides a practical balance between security and privacy in VANETs. With our proposal, cryptographic witnesses of safety-related traffic messages can be significantly compressed so that they can be stored for a long period for liability investigation. Our proposal allows a large number of traffic messages to be verified as if they were a single one, which speeds up the response of vehicles to traffic reports.\n\nTitle: Practical secure and privacy-preserving scheme for value-added applications in VANETs\nContent: Advances in mobile networking and information processing technologies have triggered vehicular ad hoc networks (VANETs) for traffic safety and value-added applications. Most efforts have been made to address the security concerns while little work has been done to investigate security and privacy for value-added applications in VANETs. To fill this gap, we propose a secure and privacy-preserving scheme for value-added applications in VANETs; specifically, a vehicular secure and privacy-preserving location-based service (LBS). For each LBS transaction, the scheme provides authentication, integrity and non-repudiation for both the service provider and the vehicle. A vehicle can obtain the service in an anonymous way and hence vehicle privacy is well protected. However, if a vehicle maliciously uses the service, a tracing procedure can be invoked to find the malicious vehicle, thereby efficiently preventing vehicles from abusing the anonymity provided by the system. These features are achieved by efficiently exploiting the available infrastructure of VANETs and without requiring cooperation from other vehicles, which might be reluctant to cooperate or otherwise unavailable. Although public-key cryptosystems are employed as building blocks, no complicated certificate management is required by our system.\n\nTitle: Privacy-Preserving Traffic Monitoring in Vehicular Ad Hoc Networks\nContent: Most modern metropolitan cities have suffered from increasing traffic accidents and jams. Vehicular ad hoc network (VANET), consisting of information collecting, processing and transmitting units embedded in vehicles assisted by roadside infrastructures, has been proposed as one the most promising solution to problems introduced by the increasing number of vehicles in modern cities. To achieve the goal, it is crucial to allow the transportation administration center to collect information about the traffic and road status through VANET. A major obstacle in this scenario is the privacy concern on the vehicles. To address this issue, this paper proposes a generic privacy-preserving traffic monitoring framework which allows individual vehicle driving status and the road usage information are collected while the privacy of the vehicles is well preserved. This goal is achieved by the novel technology of distinguishing individual vehicles with their spatio-temporal occupations. The continual change of spatio-temporal identities provides privacy for vehicles in a natural way, which remains nonetheless traceable by a trusted authority to prevent misbehaving vehicles from abuse the privacy-preserving mechanism provided by the system.\n\n## Task\nCompose an abstract for the title 'Versatile lightweight key distribution for big data privacy in vehicular ad hoc networks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Vehicular ad hoc networks VANETs continually produce large scale of data shared among vehicles nearby to improve driving experience and safety. The vehicular communications contain a large amount of private information. It is well known that group key management is a fundamental cryptographic primitive for providing secure group communication. Although many proposals with regard to group key management have been introduced, they cannot well support secure subgroup and intergroup communications in VANETs. In this paper, we exploit the specific features of vehicular communications and propose two group key distribution schemes providing efficient solutions to these two problems. Our schemes do not require interaction between users. Storage and computation analyses show that compared with existing schemes, our proposals are more efficient and versatile. Based on these basic schemes, we present extensions for multipartite groups, by which the efficiency is greatly improved in this scenario. We also provide the extensions for two-level multipartite groups, which enable fine-grained real-world applications. The experimental results confirm that our group key distribution schemes are practical in security, computation, and communication. Copyright \u00a9 2015 John Wiley & Sons, Ltd."
      }
    ]
  },
  {
    "tid":"209",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Secure One-To-Group Communications Escrow-Free Id-Based Asymmetric Group Key Agreement\nContent: Group key agreement (GKA) is widely employed for secure group communications. Yet there is an increasing demand for secure one-to-group communications in distributed computing applications. Asymmetric group key agreement (AGKA) is a handy tool to answer this need. In AGKA, a group of members can establish a group public key while each member has a different secret key. Any sender can encrypt under this group key such that any of the members who hold the secret key can decrypt. This paper proposes an identity-based AGKA protocol which is secure against active attackers, with an emphasis on optimal round efficiency, sender dynamics, and escrow freeness. The last feature offers security of the previously established ciphertexts even when either all the involved participants or the key generation center of the identity-based cryptosystem are compromised. The proposed protocol is shown to be secure under the k-Bilinear Diffie-Hellman exponent assumption in the random oracle model. Regarding performance, our protocol is comparable to the state-of-the-art AGKA protocols.\n\nTitle: Distributed Privacy-Preserving Secure Aggregation in Vehicular Communication\nContent: Vehicular ad hoc networks (VANETs), formed by computers embedded in vehicles and the traffic infrastructure, are expected to develop in the near future to improve traffic safety and efficiency. To this end, VANETs should be designed to be resistant against various abuses and attacks. In this paper, we first review the existing proposals to provide security, privacy, and data aggregation in vehicle-to-vehicle communication. We then address the fundamental issue of achieving these conflicting properties in a unified solution, having observed that separate efforts cannot fulfill the VANET design objectives. A set of new mechanisms are suggested for efficiently managing identities and securely compressing cryptographic witnesses, which are among the major obstacles to the deployment of strong security mechanisms in VANETs.\n\nTitle: Provably secure threshold public-key encryption with adaptive security and short ciphertexts\nContent: Threshold public-key encryption is a cryptographic primitive allowing decryption control in group-oriented encryption applications. Existing TPKE schemes suffer from long ciphertexts with size linear in the number of authorized users or can only achieve non-adaptive security, which is too weak to capture the capacity of the attackers in the real world. In this paper, we propose an efficient TPKE scheme with constant-size ciphertexts and adaptive security. Security is proven under the decision Bilinear Diffie-Hellman Exponentiation assumption in the standard model. Then we extend our basic construction with efficient trade-offs between the key size and the ciphertext size. Finally, we illustrate improvements to transmit multiple secret session keys in one session with almost no extra cost.\n\nTitle: Provably secure one-round identity-based authenticated asymmetric group key agreement protocol\nContent: The newly introduced notion of asymmetric group key agreement (AGKA) enables external users to securely send messages to group members. The existing AGKA is only secure against passive attacks which are too weak to capture the attacks in the real world. In this paper, we formalize an active security model for identity-based authenticated asymmetric group key agreement (IB-AAGKA). We then present an efficient identity-based batch multi-signature, from which we construct an IB-AAGKA protocol. Our protocol is proven secure under the Bilinear Diffie-Hellman Exponent (BDHE) assumption. The active security feature implies that the protocol can withstand more realistic attacks. The identity-based feature eliminates the need of certificates and solves the certificate management problem in traditional public-key cryptosystems. Finally, an effective trade-off is provided to balance the protocol transcript size and the ciphertext size.\n\nTitle: Asymmetric Cross-cryptosystem Re-encryption Applicable to Efficient and Secure Mobile Access to Outsourced Data\nContent: With the increasing development of pervasive computing and wireless bandwidth communication, more mobile devices are used to access sensitive data stored in remote servers. In such applications, a practical issue emerges such as how to exploit the sufficient resource of a server so that the file owners can enforce fine-grained access control over the remotely stored files, while enable resource-limited mobile devices to easily access the protected data, especially if the storage server maintained by a third party is untrusted. This challenge mainly arises from the asymmetric capacity among the participants, i.e., the capacity limited mobile devices and the resource abundant server (and file owners equipped with fixed computers). To meet the security requirements in mobile access to sensitive data, we propose a new encryption paradigm, referred to as asymmetric cross-cryptosystem re-encryption (ACCRE) by leveraging the asymmetric capacity of the participants. In ACCRE, relatively light-weight identity-based encryption (IBE) is deployed in mobile devices, while resource-consuming but versatile identity-based broadcast encryption (IBBE) is deployed in servers and fixed computers of the file owners. The core of ACCRE is a novel ciphertext conversion mechanism that allows an authorized proxy to convert a complicated IBBE ciphertext into a simple IBE ciphertext affordable to mobile devices, without leaking any sensitive information to the proxy. Following this paradigm, we propose an efficient ACCRE scheme with its security formally reduced to the security of the underlying IBE and IBBE schemes. Thorough theoretical analyses and extensive experiments confirm that the scheme takes very small cost for mobile devices to access encrypted data and is practical to secure mobile computing applications.\n\n## Task\nCompose an abstract for the title 'Publicly Verifiable Privacy-Preserving Group Decryption' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Anonymity is one of the main concerns in group-oriented cryptography. However, most efforts, for instance, group signatures and ring signatures, are only made to provide anonymity on the sender's side. There are merely few works done to ensure anonymity in a cryptographic sense on the recipient's side in group-oriented communications. This paper formalizes the notion of group decryption (GD) . It can be viewed as an analog of group signatures in the context of public key encryptions. In this notion, a sender can encrypt a committed message intended to any member of a group, managed by a group manager, while the recipient of the ciphertext remains anonymous . The sender can convince a verifier about this fact without leaking the plaintext or the identity of the recipient. If required, the group manager can verifiably open the identity of the recipient. We propose an efficient GD scheme that is proven secure in the random oracle model. The overhead in both computation and communication is independent of the group size . A full ciphertext is about 0.2K bytes in a typical implementation and the scheme is practical."
      }
    ]
  },
  {
    "tid":"210",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Study of Spatial Interaction Behaviour for Improved Delivery of Web-Based Maps\nContent: As access to large quantities of detailed spatial information becomes more commonplace through popular Web-based map browsing systems, the need for automatic spatial information filtering processes grows. Particulary in the case of wireless access, a suitable filtering mechanism can assist the user by personalising the map contents, giving priority to the most relevant content and eliding less relevant content. We have developed a generic solution for implicitly obtaining an insight into a user's interests and intentions based on their mouse interactions with a Web-based map browsing interface. Results of a previously reported experiment showed visual patterns in interface interaction across multiple users. This has led us to develop a new categorisation of the user interactions that can serve as a basis for more accurately weighting user interest profiles. This paper proposes a classification scheme for user interactions according to their functional properties by describing a `mouse movement hierarchy'. The value of such a weighting is demonstrated by employing the hierarchy to make a new experimental analysis of a dataset of previously logged user interactions. By properly understanding sequences of interactions, implicit inferences about the user's interests can be made, leading to improved map personalisation, ultimately improving the user's experience by delivering the most relevant content to the user in a timely fashion.\n\nTitle: A Multimodal Interface for Personalising Spatial Data in Mobile GIS\nContent: Recently the availability and usage of more advanced mobile devices has significantly increased, with many users accessing information and applications while on the go. However, for users to truly accept and adopt such technologies it is necessary to address human-computer interaction challenges associated with such devices. We are interested in exploring these issues within the context of mobile GIS applications. Current mobile GIS interfaces suffer from two major problems: interface complexity and information overload. We have developed a novel system that addresses both of these issues. Firstly, our system allows GIS users to interact multimodally, providing them with the flexibility of choosing their preferred mode of interaction for specific tasks in specific contexts. Secondly, it records all such interactions, analyses them and uses them to build individual user profiles. Based on these, our system returns personalised spatial data to users, and hence eliminates superfluous information that might be otherwise presented to them. In this paper we describe the system we have developed that combines multimodal interaction with personalised services, and that can be used by mobile users, whether they are novices or professionals within the field of GIS. The advantages of our multimodal GIS interface approach are demonstrated through a user interaction study.\n\nTitle: Improving Spatial Data Usability By Capturing User Interactions\nContent: In this paper we present a flexible architecture for both desktop and mobile systems designed to measure users' implicit levels of interest in spatial information, based on their mouse movements. Users interact with spatial objects through a GIS interface. Our system monitors their actions and compares the spatial location of their mouse interactions during a session to the underlying spatial information displayed to the user. By making this comparison we implicitly determine a user's interests and disinterests. This information can be used to build up a user profile on a session by session basis, allowing the system to personalise future datasets to present to the user based on his profile, reducing information overload, and increasing the ratio of user-relevant content. The case study for our system concerns archaeological information for burial tombs in the ancient city of Tarquinia.\n\nTitle: Personalised maps in multimodal mobile GIS\nContent: Information overload is a critical problem in mobile GIS where user interactions are constrained by device limitations. Identifying relevant map content and generating personalised map versions, tailored towards users' preferences, can improve download times as well as perception on small screens. Whereas some existing applications propose solutions that require explicit user input, we adopt an implicit profiling approach that does not demand off-task input. Modelling preferences in this manner allows us to recommend personalised context-aware spatial content to users whenever they request maps. Providing a multimodal interface further improves a user's mobile geospatial experience as each user has the ability to freely switch between different input modalities, including speech and gesture, depending on which mode best suits their current task and environment. This article provides a description and evaluation of our approach as implemented in CoMPASS, a multimodal mobile GIS that implicitly records user movements and interactions to infer persistent spatial preferences and recommend relevant map content.\n\nTitle: Delivering personalized context-aware spatial information to mobile devices\nContent: When attempting to locate specific spatial information online users face the burden of having to differentiate between relevant and extraneous spatial content. This problem is more evident in the mobile environment where users are impeded by several device limitations. One way of overcoming this is to automatically profile users' spatial content preferences by recording all interactions users have with maps and monitoring users' movements in the field as they interact with maps. We describe a multimodal mobile GIS that implicitly records all user movements, as well as interactions between users and maps, to dynamically recommend information and to infer persistent spatial information preferences. A search engine, prefetching context-aware information, is incorporated to enhance the users' experiences. Modeling preferences in this manner allows us to recommend personalized context-aware spatial content to users whenever they request maps. A specific case study has been developed around subjects working on surveying tasks where spatial information is required in the field.\n\n## Task\nCompose an abstract for the title 'Personalizing map content to improve task completion efficiency' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Significant interaction challenges arise in both developing and using interactive map applications. Users encounter problems of information overload in using interactive maps to complete tasks. This is further exacerbated by device limitations and interaction constraints in increasingly popular mobile platforms. Application developers must then address restrictions related to screen size and limited bandwidth in order to effectively display maps on mobile devices. In order to address issues of user information overload and application efficiency in interactive map applications, we have developed a novel approach for delivering personalized vector maps. Ongoing task interactions between users and maps are monitored and captured implicitly in order to infer individual and group preferences related to specific map feature content. Personalized interactive maps that contain spatial feature content tailored specifically to users' individual preferences are then generated. Our approach addresses spatial information overload by providing only the map information necessary and sufficient to suit user interaction preferences, thus simplifying the completion of tasks performed with interactive maps. In turn, tailoring map content to specific user preferences considerably reduces the size of vector data sets necessary to transmit and render maps on mobile devices. We have developed a geographic information system prototype, MAPPER (MAP PERsonalization), that implements our approach. Experimental evaluations show that the use of personalized maps helps users complete their tasks more efficiently and can reduce information overload."
      }
    ]
  },
  {
    "tid":"211",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Blind separation of audio signals using trigonometric transforms and Kalman filtering\nContent: This paper deals with the problem of blind separation of audio signals from noisy mixtures. It proposes the application of a blind separation algorithm on the Discrete Cosine Transform (DCT) or the Discrete Sine Transform (DST) of the mixed signals, instead of performing the separation on the mixtures in the time domain. Kalman Filtering of the noisy separated signals is recommended in this paper as a post-processing step for noise reduction. Both the DCT and the DST have an energy compaction property, which concentrates most of the signal energy in a few coefficients in the transform domain, leaving the rest of the transform-domain coefficients close to zero. As a result, the separation is performed on a few coefficients in the transform domain. Another advantage of signal separation in transform domains is that the effect of noise on the signals in the transform domains is smaller than that in the time domain due to the averaging effect of the transform equations. The simulation results confirm the effectiveness of transform-domain signal separation and the feasibility of the post-processing Kalman filtering step.\n\nTitle: An efficient singular value decomposition algorithm for digital audio watermarking\nContent: The singular value decomposition (SVD) mathematical technique is utilized, in this paper, for audio watermarking in time and\n transform domains. Firstly, the audio signal in time or an appropriate transform domain is transformed to a 2-D format. The\n SVD algorithm is applied on this 2-D matrix, and an image watermark is added to the matrix of singular values (SVs) with a\n small weight, to guarantee the possible extraction of the watermark without introducing harmful distortions to the audio signal.\n The transformation of the audio signal between the 1-D and 2-D formats is performed in the well-known lexicographic ordering\n method used in image processing. A comparison study is presented in the paper between the time and transform domains as possible\n hosting media for watermark embedding. Experimental results are in favor of watermark embedding in the time domain if the\n distortion level in the audio signal is to be kept as low as possible with a high detection probability. The proposed algorithm\n is utilized also for embedding chaotic encrypted watermarks to increase the level of security. Experimental results show that\n watermarks embedded with the proposed algorithm can survive several attacks. A\u00a0segment-by-segment implementation of the proposed\n SVD audio watermarking algorithm is also presented to enhance the detectability of the watermark in the presence of severe\n attacks.\n\nTitle: Automatic Modulation Recognition in Wireless Multi-carrier Wireless Systems with Cepstral Features\nContent: utomatic digital modulation recognition (ADMR) has become an interesting problem in wireless communication systems with various civil and military applications. In this paper, an ADMR algorithm is proposed for both orthogonal frequency division multiplexing and multi-carrier code division multiple access systems using discrete transforms and mel-frequency cepstral coefficients (MFCCs). The proposed algorithm uses one of the discrete cosine transform, discrete sine transform, and discrete wavelet transform with MFCCs to extract the modulated signal coefficients, and uses also either a support vector machine (SVM) or an artificial neural network (ANN) for modulation classification. Simulation results show that the proposed algorithm provides higher recognition rates than those obtained in previous studies, in addition to a superiority of SVM performance compared to ANN performance at low signal-to-noise ratios.\n\nTitle: Efficient audio cryptosystem based on chaotic maps and double random phase encoding\nContent: This paper presents an efficient audio cryptosystem based on combining chaotic maps with optical encryption. The proposed audio cryptosystem is designed with the potential of enforcing the security level for digital speech communication via shared networks. The proposed audio cryptosystem is built using two security phases. In the first phase, it utilizes a chaotic system using either Baker map or cat map for providing the first security level. In the second phase, it utilizes optical encryption using double random phase encoding (DRPE) for providing the second security level. This second security level represented with DRPE is a physical security that is very hard to attack. The proposed audio cryptosystem is implemented and its performance is evaluated using different audio encryption/decryption quality metrics. The results demonstrated that the proposed audio cryptosystem increases the level of voice security with high degree of confidentiality.\n\nTitle: Transceiver scheme for single-carrier frequency division multiple access implementing the wavelet transform and peak-to-average-power ratio reduction methods\nContent: Single-carrier frequency division multiple access (SC-FDMA) has been adopted as a possible air interface for future wireless networks. It combines most of the advantages of orthogonal frequency division multiple access (OFDMA) and the low peak-to-average-power ratio (PAPR) of single-carrier transmission. This study proposes a new transceiver scheme for SC-FDMA systems implementing the wavelet transform to decompose the transmitted signal into approximation and detail components. The approximation component can be clipped or companded whereas the detail component is left unchanged because of its sensitivity to noise. Wavelet filter banks at the transmitter and the receiver demonstrate the ability to reduce the distortion in the reconstructed signal while retaining all the significant features present in the signal. The performance of the proposed scheme is investigated with different PAPR reduction methods. Simulation results show that the proposed scheme with the hybrid clipping and companding method provides a significant performance enhancement when compared with the conventional SC-FDMA system, while the complexity of the system is slightly increased.\n\n## Task\nCompose an abstract for the title 'Blind separation of audio signals using trigonometric transforms and wavelet denoising' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper deals with the problem of blind separation of audio signals from noisy mixtures. It proposes the application of\n a blind separation algorithm on the discrete cosine transform (DCT) or the discrete sine transform (DST) of the mixed signals,\n instead of performing the separation on the mixtures in the time domain. Wavelet denoising of the noisy mixtures is recommended\n in this paper as a preprocessing step for noise reduction. Both the DCT and the DST have an energy compaction property, which\n concentrates most of the signal energy in a few coefficients in the transform domain, leaving most of the transform domain\n coefficients close to zero. As a result, the separation is performed on a few coefficients in the transform domain. Another\n advantage of signal separation in transform domains is that the effect of noise on the signals in the transform domains is\n smaller than that in the time domain due to the averaging effect of the transform equations, especially when the separation\n algorithm is preceded by a wavelet denoising step. The simulation results confirm the superiority of transform domain separation\n to time domain separation and the importance of the wavelet denoising step."
      }
    ]
  },
  {
    "tid":"212",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Nearest hyperdisk methods for high-dimensional classification\nContent: In high-dimensional classification problems it is infeasible to include enough training samples to cover the class regions densely. Irregularities in the resulting sparse sample distributions cause local classifiers such as Nearest Neighbors (NN) and kernel methods to have irregular decision boundaries. One solution is to \"fill in the holes\" by building a convex model of the region spanned by the training samples of each class and classifying examples based on their distances to these approximate models. Methods of this kind based on affine and convex hulls and bounding hyperspheres have already been studied. Here we propose a method based on the bounding hyperdisk of each class - the intersection of the affine hull and the smallest bounding hypersphere of its training samples. We argue that in many cases hyperdisks are preferable to affine and convex hulls and hyperspheres: they bound the classes more tightly than affine hulls or hyperspheres while avoiding much of the sample overfitting and computational complexity that is inherent in high-dimensional convex hulls. We show that the hyperdisk method can be kernelized to provide nonlinear classifiers based on non-Euclidean distance metrics. Experiments on several classification problems show promising results.\n\nTitle: SAR-Based Terrain Classification Using Weakly Supervised Hierarchical Markov Aspect Models\nContent: We introduce the hierarchical Markov aspect model (HMAM), a computationally efficient graphical model for densely labeling large remote sensing images with their underlying terrain classes. HMAM resolves local ambiguities efficiently by combining the benefits of quadtree representations and aspect models\u2014the former incorporate multiscale visual features and hierarchical smoothing to provide improved local label consistency, while the latter sharpen the labelings by focusing them on the classes that are most relevant for the broader local image context. The full HMAM model takes a grid of local hierarchical Markov quadtrees over image patches and augments it by incorporating a probabilistic latent semantic analysis aspect model over a larger local image tile at each level of the quadtree forest. Bag-of-word visual features are extracted for each level and patch, and given these, the parent\u2013child transition probabilities from the quadtree and the label probabilities from the tile-level aspect models, an efficient forwards\u2013backwards inference pass allows local posteriors for the class labels to be obtained for each patch. Variational expectation-maximization is then used to train the complete model from either pixel-level or tile-keyword-level labelings. Experiments on a complete TerraSAR-X synthetic aperture radar terrain map with pixel-level ground truth show that HMAM is both accurate and efficient, providing significantly better results than comparable single-scale aspect models with only a modest increase in training and test complexity. Keyword-level training greatly reduces the cost of providing training data with little loss of accuracy relative to pixel-level training.\n\nTitle: Large margin classifiers based on affine hulls\nContent: This paper introduces a geometrically inspired large margin classifier that can be a better alternative to the support vector machines (SVMs) for the classification problems with limited number of training samples. In contrast to the SVM classifier, we approximate classes with affine hulls of their class samples rather than convex hulls. For any pair of classes approximated with affine hulls, we introduce two solutions to find the best separating hyperplane between them. In the first proposed formulation, we compute the closest points on the affine hulls of classes and connect these two points with a line segment. The optimal separating hyperplane between the two classes is chosen to be the hyperplane that is orthogonal to the line segment and bisects the line. The second formulation is derived by modifying the @n-SVM formulation. Both formulations are extended to the nonlinear case by using the kernel trick. Based on our findings, we also develop a geometric interpretation of the least squares SVM classifier and show that it is a special case of the proposed method. Multi-class classification problems are dealt with constructing and combining several binary classifiers as in SVM. The experiments on several databases show that the proposed methods work as good as the SVM classifier if not any better.\n\nTitle: Hyperdisk based large margin classifier\nContent: We introduce a large margin linear binary classification framework that approximates each class with a hyperdisk - the intersection of the affine support and the bounding hypersphere of its training samples in feature space - and then finds the linear classifier that maximizes the margin separating the two hyperdisks. We contrast this with Support Vector Machines (SVMs), which find the maximum-margin separator of the pointwise convex hulls of the training samples, arguing that replacing convex hulls with looser convex class models such as hyperdisks provides safer margin estimates that improve the accuracy on some problems. Both the hyperdisks and their separators are found by solving simple quadratic programs. The method is extended to nonlinear feature spaces using the kernel trick, and multi-class problems are dealt with by combining binary classifiers in the same ways as for SVMs. Experiments on a range of data sets show that the method compares favourably with other popular large margin classifiers.\n\nTitle: Margin-based discriminant dimensionality reduction for visual recognition\nContent: Nearest neighbour classifiers and related kernel meth- ods often perform poorly in high dimensional problems be- cause it is infeasible to include enough training samples to cover the class regions densely. In such cases, test sam- ples often fall into gaps between training samples where the nearest neighbours are too distant to be good indica- tors of class membership. One solution is to project the data onto a discriminative lower dimensional subspace. We propose a gap-resistant nonparametric method for finding such subspaces: first the gaps are filled by building a con- vex model of the region spanned by each class - we test the affine and convex hulls and the bounding disk of the class training samples - then a set of highly discriminative direc- tions is found by building and decomposing a scatter matrix of weighted displacement vectors from training examples to nearby rival class regions. The weights are chosen to focus attention on narrow margin cases while still allowing more diversity and hence more discriminability than the 1D lin- ear Support Vector Machine (SVM) projection. Experimen- tal results on several face and object recognition datasets show that the method finds effective projections, allowing simple classifiers such as nearest neighbours to work well in the low dimensional reduced space.\n\n## Task\nCompose an abstract for the title 'Manifold Based Local Classifiers: Linear and Nonlinear Approaches' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In case of insufficient data samples in high-dimensional classification problems, sparse scatters of samples tend to have\n many \u2018holes\u2019\u2014regions that have few or no nearby training samples from the class. When such regions lie close to inter-class\n boundaries, the nearest neighbors of a query may lie in the wrong class, thus leading to errors in the Nearest Neighbor classification\n rule. The K-local hyperplane distance nearest neighbor (HKNN) algorithm tackles this problem by approximating each class with\n a smooth nonlinear manifold, which is considered to be locally linear. The method takes advantage of the local linearity assumption\n by using the distances from a query sample to the affine hulls of query\u2019s nearest neighbors for decision making. However,\n HKNN is limited to using the Euclidean distance metric, which is a significant limitation in practice. In this paper we reformulate\n HKNN in terms of subspaces, and propose a variant, the Local Discriminative Common Vector (LDCV) method, that is more suitable\n for classification tasks where the classes have similar intra-class variations. We then extend both methods to the nonlinear\n case by mapping the nearest neighbors into a higher-dimensional space where the linear manifolds are constructed. This procedure\n allows us to use a wide variety of distance functions in the process, while computing distances between the query sample and\n the nonlinear manifolds remains straightforward owing to the linear nature of the manifolds in the mapped space. We tested\n the proposed methods on several classification tasks, obtaining better results than both the Support Vector Machines (SVMs)\n and their local counterpart SVM-KNN on the USPS and Image segmentation databases, and outperforming the local SVM-KNN on the\n Caltech visual recognition database."
      }
    ]
  },
  {
    "tid":"213",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Data envelopment analysis of mutual funds based on second-order stochastic dominance\nContent: Although data envelopment analysis (DEA) has been extensively used to assess the performance of mutual funds (MF), most of the approaches overestimate the risk associated to the endogenous benchmark portfolio. This is because in the conventional DEA technology the risk of the target portfolio is computed as a linear combination of the risk of the assessed MF. This neglects the important effects of portfolio diversification. Other approaches based on mean\u2013variance or mean\u2013variance\u2013skewness are non-linear. We propose to combine DEA with stochastic dominance criteria. Thus, in this paper, six distinct DEA-like linear programming (LP) models are proposed for computing relative efficiency scores consistent (in the sense of necessity) with second-order stochastic dominance (SSD). The aim is that, being SSD efficient, the obtained target portfolio should be an optimal benchmark for any rational risk-averse investor. The proposed models are compared with several related approaches from the literature.\n\nTitle: A fuzzy expected value approach under generalized data envelopment analysis\nContent: Fuzzy data envelopment analysis (DEA) models emerge as another class of DEA models to account for imprecise inputs and outputs for decision making units (DMUs). Although several approaches for solving fuzzy DEA models have been developed, there are some drawbacks, ranging from the inability to provide satisfactory discrimination power to simplistic numerical examples that handles only triangular fuzzy numbers or symmetrical fuzzy numbers. To address these drawbacks, this paper proposes using the concept of expected value in generalized DEA (GDEA) model. This allows the unification of three models - fuzzy expected CCR, fuzzy expected BCC, and fuzzy expected FDH models - and the ability of these models to handle both symmetrical and asymmetrical fuzzy numbers. We also explored the role of fuzzy GDEA model as a ranking method and compared it to existing super-efficiency evaluation models. Our proposed model is always feasible, while infeasibility problems remain in certain cases under existing super-efficiency models. In order to illustrate the performance of the proposed method, it is first tested using two established numerical examples and compared with the results obtained from alternative methods. A third example on energy dependency among 23 European Union (EU) member countries is further used to validate and describe the efficacy of our approach under asymmetric fuzzy numbers.\n\nTitle: Benchmarking Formula One auto racing circuits: a two stage DEA approach\nContent: Formula One (F1) World Championship has become one of the most successful sport tournaments over the last decade. Races take place in modern-day closed racing circuits, whose design plays a key role in racing results. This paper proposes a framework for the design efficiency assessment of the more representative racing circuits that hosted Grands Prix during the recent F1 seasons. The proposed approach considers two basic circuit features (namely, circuit length and number of turns) and combines car performance and race safety data. The methodology used is based on data envelopment analysis (DEA). The number of inefficient circuits is small, five in the case of variable returns to scale and nine (out of 21) when constant returns to scale are assumed. Potential improvements in terms of speed, fuel consumption and safety targets are computed. For each inefficient circuit its reference set is identified. Also, a second-stage DEA fractional regression analysis is carried out to study the influence of the circuit type (race or street circuit), the track orientation (clockwise or anticlockwise) and the number of red-flagged races due to rainfall on the circuits\u2019 efficiency. The results indicate that all three variables are significant. The implications of the results for track designers and F1 organizers are also discussed.\n\nTitle: Multiobjective target setting in data envelopment analysis using AHP\nContent: In this paper two new target setting DEA approaches are proposed. The first one is an interactive multiobjective method that at each step of the process asks the decision maker (DM) which inputs and outputs he wishes to improve, which ones are allowed to worsen and which ones should stay at their current level. The local relative priorities of these inputs and outputs changes are computed using the analytic hierarchy process (AHP). After obtaining the candidate target, the DM can update his preferences for improving, worsening or maintaining current inputs and outputs levels and obtain a new candidate target. Thus continuing, until a satisfactory operating point is computed. The second method proposed uses a lexicographic multiobjective approach in which the DM specifies a priori a set of priority levels and, using AHP, the relative importance given to the improvements of the inputs and outputs at each priority level. This second approach requires solving a series of models in order, one model for each priority level. The models do not allow for worsening of neither inputs nor outputs. After the lowest priority model has been solved the corresponding target operating point is obtained. The application of the proposed approach to a port logistics problem is presented.\n\nTitle: Data envelopment analysis of systems with multiple modes of functioning\nContent: Many systems can operate in different modes of functioning. Conventional data envelopment analysis (DEA) would ignore that fact and consider instead that the system is a black box, paying attention just to the overall input consumption and output production. In this paper a more fine-grained approach is proposed consisting of explicitly modelling the different modes of functioning as specific processes and using the observed data on the input consumption and output production in each of the modes of functioning to infer the corresponding mode-specific technology. The system technology results from composing these mode-specific technologies according to the corresponding time allocations. The proposed approach allows computing efficient operating points for every mode of functioning, looking for improvements in the overall system performance. Two efficiency assessment DEA models are presented depending on whether the observed time allocation is maintained or the model is free to modify it. An application of the proposed approach to assessing the efficiency of NFL teams, operating in defence and offence modes in a given game, is presented.\n\n## Task\nCompose an abstract for the title 'A competing risks analysis of the duration of federal target funds rates' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The behaviour of short term interest rates has been the focus of extensive studies in economics and finance. This paper analyses the duration between changes in the Intended Federal Funds Rate (IFFR) taking into account different factors and the possibility of two outcomes (rate increase and rate decrease). Due to the lack of independence between these outcomes, the use of traditional survival analysis was ruled out and a novel approach based on competing risks is used. The estimation results show the influence of the current interest rate level, US GDP growth rate, US inflation rate, inflation rate differential with Euro-zone and the sign of the previous interest rate change. Test results also confirm that the duration previous to small changes (of a quarter point) and to large changes (above a quarter point) are statistically different for both rate hikes and rate cuts. Accurate confidence intervals for the Cumulative Incidence Function (CIF) are provided even with small sample size under non-normality."
      }
    ]
  },
  {
    "tid":"214",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Convex enclosures for the reachable sets of nonlinear parametric ordinary differential equations\nContent: This article describes convex enclosures for the reachable sets of nonlinear parametric ordinary differential equations (ODEs). Such equations arise from the control parametrization of nonlinear control systems, and the computation of reachable sets for such systems is central to many control and verification problems. The method presented computes convex and concave relaxations of the ODE solutions with respect to the parameters, which are then used to compute a convex enclosure of the reachable set. This enclosure is expressed as an infinite intersection of halfspaces, so that a convex polyhedral enclosure of the reachable set can be computed by considering any finite m of these halfspaces. The method requires the solution of m dynamic optimization problems which are guaranteed to be convex, even when the reachable set is itself nonconvex.\n\nTitle: Bounds on the reachable sets of nonlinear control systems\nContent: The computation of rigorous enclosures of the reachable sets of nonlinear control systems is considered, with a focus on applications for which speed is crucial. Low computational costs make interval methods based on differential inequalities an attractive option. Unfortunately, such methods are prone to large overestimation and often produce useless results in practice. From physical considerations, however, it is common that some crude set is known to contain the reachable set. We establish a general bounding result, based on differential inequalities, which enables the effective use of such sets during the bounding procedure. In the case where this set is a convex polyhedron, an efficient implementation using interval computations is developed. Using readily available physical information from practical examples, this method is shown to provide significant advantages over alternative methods in terms of both efficiency and accuracy.\n\nTitle: Outer approximation algorithms for separable nonconvex mixed-integer nonlinear programs\nContent: A rigorous decomposition approach to solve separable mixed-integer nonlinear programs where the participating functions are nonconvex is presented. The proposed algorithms consist of solving an alternating sequence of Relaxed Master Problems (mixed-integer linear program) and two nonlinear programming problems (NLPs). A sequence of valid nondecreasing lower bounds and upper bounds is generated by the algorithms which converge in a finite number of iterations. A Primal Bounding Problem is introduced, which is a convex NLP solved at each iteration to derive valid outer approximations of the nonconvex functions in the continuous space. Two decomposition algorithms are presented in this work. On finite termination, the first yields the global solution to the original nonconvex MINLP and the second finds a rigorous bound to the global solution. Convergence and optimality properties, and refinement of the algorithms for efficient implementation are presented. Finally, numerical results are compared with currently available algorithms for example problems, illuminating the potential benefits of the proposed algorithm.\n\nTitle: Efficient solution of ordinary differential equations with a parametric lexicographic linear program embedded\nContent: This work analyzes the initial value problem in ordinary differential equations with a parametric lexicographic linear program (LP) embedded. The LP is said to be embedded since the dynamics depend on the solution of the LP, which is in turn parameterized by the dynamic states. This problem formulation finds application in dynamic flux balance analysis, which serves as a modeling framework for industrial fermentation reactions. It is shown that the problem formulation can be intractable numerically, which arises from the fact that the LP induces an effective domain that may not be open. A numerical method is developed which reformulates the system so that it is defined on an open set. The result is a system of semi-explicit index-one differential algebraic equations, which can be solved with efficient and accurate methods. It is shown that this method addresses many of the issues stemming from the original problem's intractability. The application of the method to examples of industrial fermentation processes demonstrates its effectiveness and efficiency.\n\nTitle: Convex relaxations for nonconvex optimal control problems\nContent: An optimal control problem with a nonlinear control system embedded is considered. Using the endpoint map of the control system, such problems can be written as nonlinear programs on the set of admissible controls. Though necessary optimality conditions exist for such problems, they are often nonconvex and such conditions are not sufficient. A relaxation procedure is outlined which generates a convex program whose solution value is a guaranteed lower bound on the solution value of the original problem. This result is a crucial step towards developing deterministic global optimization techniques for optimal control problems using a branch-and-bound framework. The major contribution is that, unlike other developments along these lines, the convex underestimating program derived here is valid on the original function space; i.e. there is no need to discretize the control.\n\n## Task\nCompose an abstract for the title 'Efficient polyhedral enclosures for the reachable set of nonlinear control systems.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This work presents a general theory for the construction of a polyhedral outer approximation of the reachable set (\u201cpolyhedral bounds\u201d) of a dynamic system subject to time-varying inputs and uncertain initial conditions. This theory is inspired by the efficient methods for the construction of interval bounds based on comparison theorems. A numerically implementable instance of this theory leads to an auxiliary system of differential equations which can be solved with standard numerical integration methods. Meanwhile, the use of polyhedra provides greater flexibility in defining tight enclosures on the reachable set. These advantages are demonstrated with a few examples, which show that tight bounds can be efficiently computed for general, nonlinear systems. Further, it is demonstrated that the ability to use polyhedra provides a means to meaningfully distinguish between time-varying and constant, but uncertain, inputs."
      }
    ]
  },
  {
    "tid":"215",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Cost Effective Centralized Adaptive Routing for Networks-on-Chip\nContent: As the number of applications and programmable units in CMPs and MPSoCs increases, the Network-on-Chip (NoC) encounters unpredictable, heterogeneous and time dependent traffic loads. This motivates the introduction of adaptive routing mechanisms that balance the NoC's loads and achieve higher throughput compared with traditional oblivious routing schemes. An effective adaptive routing scheme should be based on a global view of the network state. However, most current adaptive routing schemes, following off-chip networks, are based on distributed reactions to local congestion. In this paper we leverage the unique on-chip capabilities and introduce a novel paradigm of NoC centralized adaptive routing. Our scheme continuously monitors the global traffic load in the network and modifies the routing of packets to improve load balancing accordingly. We present a specific design for the case of mesh topology, where XY or YX routes are adaptively selected for each source-destination pair. We show that while our implementation is lightweight and scalable in hardware costs, it outperforms oblivious and distributed adaptive routing schemes in terms of load balancing and average packet delay.\n\nTitle: Efficient link capacity and QoS design for network-on-chip\nContent: This paper addresses the allocation of link capacities in the automated design process of a network-on-chip based system. Communication resource costs are minimized under Quality-of-Service timing constraints.First, we introduce a novel analytical delay model for virtual channeled wormhole networks with non-uniform link capacities that eliminates costly simulations at the inner-loop of the optimization process. Second, we present an efficient capacity allocation algorithm that assigns link capacities such that packet delays requirements for each flow are satisfied. We demonstrate the benefit of capacity allocation for a typical system on chip, where the traffic is heterogeneous and delay requirements may largely vary, in comparison with the standard approach which assumes uniform-capacity links.\n\nTitle: Design Tradeoffs of Long Links in Hierarchical Tiled Networks-on-Chip\nContent: Hierarchical topologies are frequently proposed for large Networks-on-Chip (NoCs). Hierarchical architectures utilize, at the upper levels, long links of the order of the die size. RC delays of long links might reach dozens of clock cycles in advanced technology nodes, if delay reduction techniques (e.g. wire sizing and repeater insertion) are not applied. Some proposals assume that long links can be adjusted to satisfy timing requirements but lack a deep evaluation of the tradeoffs and costs. Other proposals assume that long links must be pipelined, but do not provide a comprehensive justification. In this paper we evaluate the efficiency and the system costs of wire sizing and repeater insertion as methods to reduce link delays in hierarchical NoCs. We present a unified interconnect cost function that accounts for power and wiring overheads of these methods. Then, we quantify the costs of modifying long links in typical hierarchical NoCs for different target clock frequencies and technology nodes. Although long links might undergo aggressive adjustments, we find these overall costs to be low at the system level for many typical cases, taking into account that there are only a few long links in most proposed hierarchical NoC architectures.\n\nTitle: Zooming in on network-on-chip architectures\nContent: The aim of this talk is to expose the theoretical distributed system community to the concept of Network-on-Chip (NoC), an emerging research field within the VLSI realm, in which networking principles play a significant role, and new network architectures are being explored in a new setup as well as new cost and performance models. Researchers should find new challenges in exploring solutions to familiar problems such as network design, routing, and quality-of-service, in unfamiliar settings under new constraints. The unique characteristics of silicon chips require new solutions to these classical problems, and define a new set of NoC specific problems, such as automatic network design process, power and area optimization and specialized system functionalities. We present a new classification of chip architectures into three categories with different requirements from their NoCs. In order to stimulate some research directions, we highlight several research problems arising in these categories such as routing, quality-of-service, flow and congestion control, and resource allocation (e.g., capacity assignment, sharing hot-spots). We provide initial solution directions to example problems.\n\nTitle: QNoC: QoS architecture and design process for network on chip\nContent: We define Quality of Service (QoS) and cost model for communications in Systems on Chip (SoC), and derive related Network on Chip (NoC) architecture and design process. SoC inter-module communication traffic is classified into four classes of service: signaling (for inter-module control signals); real-time (representing delay-constrained bit streams); RD/WR (modeling short data access) and block-transfer (handling large data bursts). Communication traffic of the target SoC is analyzed (by means of analytic calculations and simulations), and QoS requirements (delay and throughput) for each service class are derived. A customized Quality-of-Service NoC (QNoC) architecture is derived by modifying a generic network architecture. The customization process minimizes the network cost (in area and power) while maintaining the required QoS.The generic network is based on a two-dimensional planar mesh and fixed shortest path (X-Y based) multiclass wormhole routing. Once communication requirements of the target SoC are identified, the network is customized as follows: The SoC modules are placed so as to minimize spatial traffic density, unnecessary mesh links and switching nodes are removed, and bandwidth is allocated to the remaining links and switches according to their relative load so that link utilization is balanced. The result is a low cost customized QNoC for the target SoC which guarantees that QoS requirements are met.\n\n## Task\nCompose an abstract for the title 'Cost considerations in network on chip' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Systems on Chip (SoCs) require efficient inter-module interconnection providing for the required communications at a low cost. We analyze the generic cost in area and power of Networks on Chip (NoCs) and alternative interconnect architectures: a shared bus, a segmented bus and a point-to-point interconnect. For each architecture we derive analytical expressions for area, power dissipation and operating frequency as well as asymptotic limits of these functions. The analysis quantifies the intuitive NoC scalability advantages.Next we turn to NoC cost optimization. We explore cost tradeoffs between the number of buffers and the link speed. We use a reference architecture, termed QNoC (Quality-of-Service NoC), which is based on a grid of wormhole switches, shortest path routing and multiple QoS classes. Two traffic scenarios are considered, one dominated by short packets sensitive to queuing delays and the other dominated by large block-transfers. Our simulations show that network cost can be minimized while maintaining quality of service, by trading off buffers with links in the first scenario but not in the second."
      }
    ]
  },
  {
    "tid":"216",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Bayesian fuzzy clustering of colored graphs\nContent: With the increasing availability of interaction data stemming form fields as diverse as systems biology, telecommunication or social sciences, the task of mining and understanding the underlying graph structures becomes more and more important. Here we focus on data with different types of nodes; we subsume this meta information in the color of a node. An important first step is the unsupervised clustering of nodes into communities, which are of the same color and highly connected within but sparsely connected to the rest of the graph. Recently we have proposed a fuzzy extension of this clustering concept, which allows a node to have membership in multiple clusters. The resulting gradient descent algorithm shared many similarities with the multiplicative update rules from nonnegative matrix factorization. Two issues left open were the determination of the number of clusters of each color, as well as the non-defined integration of additional prior information. In this contribution we resolve these issues by reinterpreting the factorization in a Bayesian framework, which allows the ready inclusion of priors. We integrate automatic relevance determination to automatically estimate group sizes. We derive a maximum-a-posteriori estimator, and illustrate the feasibility of the approach on a toy as well as a protein-complex hypergraph, where the resulting fuzzy clusters show significant enrichment of distinct gene ontology categories.\n\nTitle: Model-based branching point detection in single-cell data by K-branches clustering.\nContent: Motivation: The identification of heterogeneities in cell populations by utilizing single-cell technologies such as single-cell RNA-Seq, enables inference of cellular development and lineage trees. Several methods have been proposed for such inference from high-dimensional single-cell data. They typically assign each cell to a branch in a differentiation trajectory. However, they commonly assume specific geometries such as tree-like developmental hierarchies and lack statistically sound methods to decide on the number of branching events. Results: We present K-Branches, a solution to the above problem by locally fitting half-lines to single-cell data, introducing a clustering algorithm similar to K-Means. These halflines are proxies for branches in the differentiation trajectory of cells. We propose a modified version of the GAP statistic for model selection, in order to decide on the number of lines that best describe the data locally. In this manner, we identify the location and number of subgroups of cells that are associated with branching events and full differentiation, respectively. We evaluate the performance of our method on single-cell RNA-Seq data describing the differentiation of myeloid progenitors during hematopoiesis, single-cell qPCR data of mouse blastocyst development, single-cell qPCR data of human myeloid monocytic leukemia and artificial data.\n\nTitle: Sparse Nonnegative Matrix Factorization with Genetic Algorithms for Microarray Analysis\nContent: Nonnegative Matrix Factorization (NMF) has proven to be a useful tool for the analysis of nonnegative multivariate data. Gene expression profiles naturally conform to assumptions about data formats raised by NMF. However, it is known not to lead to unique results concerning the component signals extracted. In this paper we consider an extension of the NMF algorithm which provides unique solutions whenever the underlying component signals are sufficiently sparse. A new sparseness measure is proposed most appropriate to suitably transformed gene expression profiles. The resulting fitness function is discontinuous and exhibits many local minima, hence we use a genetic algorithm for its optimization. The algorithm is applied to toy data to investigate its properties as well as to a microarray data set related to Pseudo-Xanthoma Elasticum (PXE).\n\nTitle: Modularity maximization and tree clustering: Novel ways to determine effective geographic borders\nContent: Territorial subdivisions and geographic borders are essential for understanding phenomena in sociology, political science, history, and economics. They influence the interregional flow of information and cross-border trade and affect the diffusion of innovation and technology. However, most existing administrative borders were determined by a variety of historic and political circumstances along with some degree of arbitrariness. Societies have changed drastically, and it is doubtful that currently existing borders reflect the most logical divisions. Fortunately, at this point in history we are in a position to actually measure some aspects of the geographic structure of society through human mobility. Large-scale transportation systems such as trains and airlines provide data about the number of people traveling between geographic locations, and many promising human mobility proxies are being discovered, such as cell phones, bank notes, and various online social networks. In this chapter we apply two optimization techniques to a human mobility proxy (bank note circulation) to investigate the effective geographic borders that emerge from a direct analysis of human mobility.\n\nTitle: An ISA algorithm with unknown group sizes identifies meaningful clusters in metabolomics data\nContent: Independent Subspace Analysis (ISA) denotes the task of linearly separating multivariate observations into statistically independent multi-dimensional sources, where dependencies only exist within these subspaces but not between them. So far ISA algorithms have mostly been described in the context of known group sizes. Here, we extend a previously proposed ISA algorithm based on joint block diagonalization of 4-th order cumulant matrices to separate subspaces of unknown sizes. Further automated interpretation of the demixed sources then requires a means of recovering the subspace structure within them, and we propose two distinct methods for this. We then apply the method to a novel application field, namely clustering of metabolites, which seems to be well-fit to the ISA model. We are able to successfully identify dependencies between metabolites that could not be recovered using conventional methods.\n\n## Task\nCompose an abstract for the title 'Structuring heterogeneous biological information using fuzzy clustering of k-partite graphs.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"BACKGROUND: Extensive and automated data integration in bioinformatics facilitates the construction of large, complex biological networks. However, the challenge lies in the interpretation of these networks. While most research focuses on the unipartite or bipartite case, we address the more general but common situation of k-partite graphs. These graphs contain k different node types and links are only allowed between nodes of different types. In order to reveal their structural organization and describe the contained information in a more coarse-grained fashion, we ask how to detect clusters within each node type. RESULTS: Since entities in biological networks regularly have more than one function and hence participate in more than one cluster, we developed a k-partite graph partitioning algorithm that allows for overlapping (fuzzy) clusters. It determines for each node a degree of membership to each cluster. Moreover, the algorithm estimates a weighted k-partite graph that connects the extracted clusters. Our method is fast and efficient, mimicking the multiplicative update rules commonly employed in algorithms for non-negative matrix factorization. It facilitates the decomposition of networks on a chosen scale and therefore allows for analysis and interpretation of structures on various resolution levels. Applying our algorithm to a tripartite disease-gene-protein complex network, we were able to structure this graph on a large scale into clusters that are functionally correlated and biologically meaningful. Locally, smaller clusters enabled reclassification or annotation of the clusters' elements. We exemplified this for the transcription factor MECP2. CONCLUSIONS: In order to cope with the overwhelming amount of information available from biomedical literature, we need to tackle the challenge of finding structures in large networks with nodes of multiple types. To this end, we presented a novel fuzzy k-partite graph partitioning algorithm that allows the decomposition of these objects in a comprehensive fashion. We validated our approach both on artificial and real-world data. It is readily applicable to any further problem."
      }
    ]
  },
  {
    "tid":"217",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Reconstructing a 3D Line from a Single Catadioptric Image\nContent: This paper demonstrates that, for axial non-central optical systems, the equation of a 3D line can be estimated using only four points extracted from a single image of the line. This result, which is a direct consequence of the lack of vantage point, follows from a classic result in enumerative geometry: there are exactly two lines in 3-space which intersect four given lines in general position. We present a simple algorithm to reconstruct the equation of a 3D line from four image points. This algorithm is based on computing the Singular Value Decomposition (SVD) of the matrix of Pl\u00fccker coordinates of the four corresponding rays. We evaluate the conditions for which the reconstruction fails, such as when the four rays are nearly coplanar. Preliminary experimental results using a spherical catadioptric camera are presented. We conclude by discussing the limitations imposed by poor calibration and numerical errors on the proposed reconstruction algorithm.\n\nTitle: Dealing with Multiple Requirements in Geometric Arrangements.\nContent: Existing algorithms for building layouts from geometric primitives are typically designed to cope with requirements such as orthogonal alignment, overlap removal, optimal area usage, hierarchical organization, among others. However, most techniques are able to tackle just a few of those requirements simultaneously, impairing their use and flexibility. In this work we propose a novel methodology for building layouts from geometric primitives that concurrently addresses a wider range of requirements. Relying on multidimensional projection and mixed integer optimization, our approach arranges geometric objects in the visual space so as to generate well structured layouts that preserve the semantic relation among objects while still making an efficient use of display area. Moreover, scalability is handled through a hierarchical representation scheme combined with navigation tools. A comprehensive set of quantitative comparisons against existing geometry-based layouts and applications on text, image, and video data set visualization prove the effectiveness of our approach.\n\nTitle: Beyond Silhouettes: Surface Reconstruction Using Multi-Flash Photography\nContent: This paper introduces a novel method for surface reconstruction using the depth discontinuity information captured by a multi-flash camera while the object moves along a known trajectory. Experimental results based on turntable sequences are presented. By observing the visual motion of depth discontinuities, surface points are accurately reconstructed - including many located deep inside concavities. The method extends well-established differential and global shape-from-silhouette surface reconstruction techniques by incorporating the significant additional information encoded in the depth discontinuities. The reconstruction method uses an implicit form of the epipolar parameterization and directly estimates point locations and corresponding surface normals on the surface of the object using a local temporal neighborhood of the depth discontinuities. Outliers, which correspond to the ill-conditioned cases of the reconstruction equations, are easily detected and removed by back-projection. Gaps resulting from curvaturedependent sampling and shallow concavities are filled by fitting an implicit surface to the oriented point cloud's point locations and normal vectors.\n\nTitle: Shield fields: modeling and capturing 3D occluders\nContent: We describe a unified representation of occluders in light transport and photography using shield fields: the 4D attenuation function which acts on any light field incident on an occluder. Our key theoretical result is that shield fields can be used to decouple the effects of occluders and incident illumination. We first describe the properties of shield fields in the frequency-domain and briefly analyze the \"forward\" problem of efficiently computing cast shadows. Afterwards, we apply the shield field signal-processing framework to make several new observations regarding the \"inverse\" problem of reconstructing 3D occluders from cast shadows -- extending previous work on shape-from-silhouette and visual hull methods. From this analysis we develop the first single-camera, single-shot approach to capture visual hulls without requiring moving or programmable illumination. We analyze several competing camera designs, ultimately leading to the development of a new large-format, mask-based light field camera that exploits optimal tiled-broadband codes for light-efficient shield field capture. We conclude by presenting a detailed experimental analysis of shield field capture and 3D occluder reconstruction.\n\nTitle: Constructing Hamiltonian Triangle Strips On Quadrilateral Meshes\nContent: Because of their improved numerical properties, quadrilateral meshes have become a popular representation for finite elements computations and computer animation. In this paper we address the problem of optimally representing quadrilateral meshes as generalized triangle strips (with one swap bit per triangle). This is important because 3D rendering hardware is optimized for rendering triangle meshes transmitted from the CPU to the GPU in the form of triangle strips. We describe simple linear time and space constructive algorithms, where each quadrilateral face is split along one of its two diagonals and the resulting triangles are linked along the original mesh edges. We show that with these algorithms every connected manifold quadrilateral mesh without boundary can be optimally represented as a single Hamiltonian generalized triangle strip cycle in multiple ways, and we discuss simple strategies to tailor the construction for transparent vertex caching. \n\n## Task\nCompose an abstract for the title 'Spherical Catadioptric Arrays: Construction, Multi-View Geometry, and Calibration' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper introduces a novel imaging system composed of an array of spherical mirrors and a single high-resolution digital camera. We describe the mechanical design and construction of a prototype, analyze the geometry of image formation, present a tailored calibration algorithm, and discuss the effect that design decisions had on the calibration routine. This system is presented as a unique platform for the development of efficient multi-view imaging algorithms which exploit the combined properties of camera arrays and non-central projection catadioptric systems. Initial target applications include data acquisition for image-based rendering and 3D scene reconstruction. The main advantages of the proposed system include: a relatively simple calibration procedure, a wide field of view, and a single imaging sensor which eliminates the need for color calibration and guarantees time synchronization."
      }
    ]
  },
  {
    "tid":"218",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: SMTIBEA: a hybrid multi-objective optimization algorithm for configuring large constrained software product lines\nContent: A key challenge to software product line engineering is to explore a huge space of various products and to find optimal or near-optimal solutions that satisfy all predefined constraints and balance multiple often competing objectives. To address this challenge, we propose a hybrid multi-objective optimization algorithm called SMTIBEA that combines the indicator-based evolutionary algorithm (IBEA) with the satisfiability modulo theories (SMT) solving. We evaluated the proposed algorithm on five large, constrained, real-world SPLs. Compared to the state-of-the-art, our approach significantly extends the expressiveness of constraints and simultaneously achieves a comparable performance. Furthermore, we investigate the performance influence of the SMT solving on two evolutionary operators of the IBEA.\n\nTitle: Comparison of exact and approximate multi-objective optimization for software product lines\nContent: Software product lines (SPLs) allow stakeholders to manage product variants in a systematical way and derive variants by selecting features. Finding a desirable variant is often difficult, due to the huge configuration space and usually conflicting objectives (e.g., lower cost and higher performance). This scenario can be characterized as a multi-objective optimization problem applied to SPLs. We address the problem using an exact and an approximate algorithm and compare their accuracy, time consumption, scalability, parameter setting requirements on five case studies with increasing complexity. Our empirical results show that (1) it is feasible to use exact techniques for small SPL multi-objective optimization problems, and (2) approximate methods can be used for large problems but require substantial effort to find the best parameter setting for acceptable approximation which can be ameliorated with known good parameter ranges. Finally, we discuss the tradeoff between accuracy and time consumption when using exact and approximate techniques for SPL multi-objective optimization and guide stakeholders to choose one or the other in practice.\n\nTitle: Visualization and exploration of optimal variants in product line engineering\nContent: The decision-making process in Product Line Engineering (PLE) is often concerned with variant qualities such as cost, battery life, or security. Pareto-optimal variants, with respect to a set of objectives such as minimizing a variant's cost while maximizing battery life and security, are variants in which no single quality can be improved without sacrificing other qualities. We propose a novel method and a tool for visualization and exploration of a multi-dimensional space of optimal variants (i.e., a Pareto front). The visualization method is an integrated, interactive, and synchronized set of complementary views onto a Pareto front specifically designed to support PLE scenarios, including: understanding differences among variants and their positioning with respect to quality dimensions; solving trade-offs; selecting the most desirable variants; and understanding the impact of changes during product line evolution on a variant's qualities. We present an initial experimental evaluation showing that the visualization method is a good basis for supporting these PLE scenarios.\n\nTitle: Modelling and multi-objective optimization of quality attributes in variability-rich software\nContent: Variability-rich software, such as software product lines, offers optional and alternative features to accommodate varying needs of users. Designers of variability-rich software face the challenge of reasoning about the impact of selecting such features on the quality attributes of the resulting software variant. Attributed feature models have been proposed to model such features and their impact on quality attributes, but existing variability modelling languages and tools have limited or no support for such models and the complex multi-objective optimization problem that arises. This paper presents ClaferMoo, a language and tool that addresses these shortcomings. ClaferMoo uses type inheritance to modularize the attribution of features in feature models and allows specifying multiple optimization goals. We evaluate an implementation of the language on a set of attributed feature models from the literature, showing that the optimization infrastructure can handle small-scale feature models with about a dozen features within seconds.\n\nTitle: Software Product Lines with Design Choices: Reasoning about Variability and Design Uncertainty\nContent: When designing changes to a software product line (SPL), developers are faced with uncertainty about deciding among multiple possible SPL designs. Since each SPL design encodes a set of related products, dealing with multiple designs means that developers must reason about sets of sets of products. The additional degree of multiplicity is not well described by existing product line abstractions. In this paper, we propose an approach for dealing with design uncertainty within SPLs using a novel composition of variability modelling with an abstraction for capturing and managing design uncertainty. This allows developers to accurately describe the decisions involved in making changes to an SPL during the design stage and provides them with a framework for SPL design space exploration by analyzing and enforcing SPL properties.\n\n## Task\nCompose an abstract for the title 'Optimizing Alloy for Multi-objective Software Product Line Configuration.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Software product line (SPL) engineering involves the modeling, analysis, and configuration of variability-rich systems. We improve the performance of the multi-objective optimization of SPLs in Alloy by several orders of magnitude with two techniques. First, we rewrite the model to remove binary relations that map to integers, which enables removing most of the integer atoms from the universe. SPL models often require using large bitwidths, hence the number of integer atoms in the universe can be orders of magnitude more than the other atoms. In our approach, the tuples for these integer-valued relations are computed outside the sat solver before returning the solution to the user. Second, we add a checkpointing facility to Kodkod, which allows the multi-objective optimization algorithm to reuse previously computed internal sat solver state, after backtracking. Together these result in orders of magnitude improvement in using Alloy as a multi-objective optimization tool for software product lines."
      }
    ]
  },
  {
    "tid":"219",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Perfect Structure on the Edge of Chaos - Trapdoor Permutations from Indistinguishability Obfuscation.\nContent: We construct trapdoor permutations based on (subexponential) indistinguishability obfuscation and one-way functions, thereby providing the first candidate that is not based on the hardness of factoring. Our construction shows that even highly structured primitives, such as trapdoor permutations, can be potentially based on hardness assumptions with noisy structures such as those used in candidate constructions of indistinguishability obfuscation. It also suggest a possible way to construct trapdoor permutations that resist quantum attacks, and that their hardness may be based on problems outside the complexity class SZK indeed, while factoring-based candidates do not possess such security, future constructions of indistinguishability obfuscation might. As a corollary, we eliminate the need to assume trapdoor permutations and injective one-way function in many recent constructions based on indistinguishability obfuscation.\n\nTitle: Obfuscating Conjunctions Under Entropic Ring Lwe\nContent: We show how to securely obfuscate conjunctions, which are functions f(x(1), . . . , x(n)) = boolean AND(i is an element of I) y(i) where I subset of [n] and each literal y(i) is either just x(i) or (sic)x(i) e.g., f(x(1), . . . , x(n)) = x(1) boolean AND (sic)x(3) boolean AND (sic)x(7) . . . boolean AND x(n-1). Whereas prior work of Brakerski and Rothblum (CRYPTO 2013) showed how to achieve this using a non-standard object called cryptographic multilinear maps, our scheme is based on an \"entropic\" variant of the Ring Learning with Errors (Ring LWE) assumption. As our core tool, we prove that hardness assumptions on the recent multilinear map construction of Gentry, Gorbunov and Halevi (TCC 2015) can be established based on entropic Ring LWE. We view this as a first step towards proving the security of additional multilinear map based constructions, and in particular program obfuscators, under standard assumptions. Our scheme satisfies virtual black box (VBB) security, meaning that the obfuscated program reveals nothing more than black-box access to f as an oracle, at least as long as (essentially) the conjunction is chosen from a distribution having sufficient entropy.\n\nTitle: How to Eat Your Entropy and Have It Too - Optimal Recovery Strategies for Compromised RNGs.\nContent: Random number generators (RNGs) play a crucial role in many cryptographic schemes and protocols, but their security proof usually assumes that their internal state is initialized with truly random seeds and remains secret at all times. However, in many practical situations these are unrealistic assumptions: The seed is often gathered after a reset/reboot from low entropy external events such as the timing of manual key presses, and the state can be compromised at unknown points in time via side channels or penetration attacks. The usual remedy (used by all the major operating systems, including Windows, Linux, FreeBSD, MacOS, iOS, etc.) is to periodically replenish the internal state through an auxiliary input with additional randomness harvested from the environment. However, recovering from such attacks in a provably correct and computationally optimal way had remained an unsolved challenge so far.In this paper we formalize the problem of designing an efficient recovery mechanism from state compromise, by considering it as an online optimization problem. If we knew the timing of the last compromise and the amount of entropy gathered since then, we could stop producing any outputs until the state becomes truly random again. However, our challenge is to recover within a time proportional to this optimal solution even in the hardest (and most realistic) case in which (a) we know nothing about the timing of the last state compromise, and the amount of new entropy injected since then into the state, and (b) any premature production of outputs leads to the total loss of all the added entropy used by the RNG, since the attacker can use brute force to enumerate all the possible low-entropy states. In other words, the challenge is to develop recovery mechanisms which are guaranteed to save the day as quickly as possible after a compromise we are not even aware of. The dilemma that we face is that any entropy used prematurely will be lost, and any entropy which is kept unused will delay the recovery.After developing our formal definitional framework for RNGs with inputs, we show how to construct a nearly optimal RNG which is secure in our model. Our technique is inspired by the design of the Fortuna RNG (which is a heuristic RNG construction that is currently used by Windows and comes without any formal analysis), but we non-trivially adapt it to our much stronger adversarial setting. Along the way, our formal treatment of Fortuna enables us to improve its entropy efficiency by almost a factor of two, and to show that our improved construction is essentially tight, by proving a rigorous lower bound on the possible efficiency of any recovery mechanism in our very general model of the problem.\n\nTitle: Essentially Optimal Robust Secret Sharing with Maximal Corruptions.\nContent: In a t-out-of-nrobust secret sharing scheme, a secret message is shared among n parties who can reconstruct the message by combining their shares. An adversary can adaptively corrupt up\u00ef\u00be\u00bfto t of the parties, get their shares, and modify them arbitrarily. The scheme should satisfy privacy, meaning that the adversary cannot learn anything about the shared message, and robustness, meaning that the adversary cannot cause the reconstruction procedure to output an incorrect message. Such schemes are only possible in the case of an honest majority, and here we focus on unconditional security in the maximal corruption setting where $$n = 2t+1$$ n=2t+1. In this scenario, to share an m-bit message with a reconstruction failure probability of at most $$2^{-k}$$ 2-k, a known lower-bound shows that the share size must be at least $$m + k$$ m+k\u00ef\u00be\u00bfbits. On the other hand, all prior constructions have share size that scales linearly with the number of parties n, and the prior state-of-the-art scheme due to Cevallos et al. EUROCRYPT '12 achieves $$m + \\\\widetilde{O}k + n$$ m+O~k+n. In this work, we construct the first robust secret sharing scheme in the maximal corruption setting with $$n=2t+1$$ n=2t+1, that avoids the linear dependence between share size and the number of parties n. In particular, we get a share size of only $$m + \\\\widetilde{O}k$$ m+O~k\u00ef\u00be\u00bfbits. Our scheme is computationally efficient and relies on approximation algorithms for the minimum graph bisection problem.\n\nTitle: Laconic Function Evaluation and Applications\nContent: We introduce a new cryptographic primitive called laconic function evaluation (LFE). Using LFE, Alice can compress a large circuit f into a small digest. Bob can encrypt some data x under this digest in a way that enables Alice to recover f(x) without learning anything else about Bob's data. For the scheme to be laconic, we require that the size of the digest, the run-time of the encryption algorithm and the size of the ciphertext should all be small, much smaller than the circuit-size of f. We construct an LFE scheme for general circuits under the learning with errors (LWE) assumption, where the above parameters only grow polynomially with the depth but not the size of the circuit. We then use LFE to construct secure 2-party and multi-party computation (2PC, MPC) protocols with novel properties: We construct a 2-round 2PC protocol between Alice and Bob with respective inputs x\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">A</sub>\n, x\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">B</sub>\n in which Alice learns the output f(x\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">A</sub>\n, x\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">B</sub>\n) in the second round. This is the first such protocol which is \u201cBob-optimized\u201d, meaning that Alice does all the work while Bob's computation and the total communication of the protocol are smaller than the size of the circuit f or even Alice's input xA. In contrast, prior solutions based on fully homomorphic encryption are \u201cAlice-optimized\u201d. . We construct an MPC protocol, which allows N parties to securely evaluate a function f(x\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub>\n, ..., x\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">N</sub>\n) over their respective inputs, where the total amount of computation performed by the parties during the protocol execution is smaller than that of evaluating the function itself! Each party has to individually pre-process the circuit f before the protocol starts and post-process the protocol transcript to recover the output after the protocol ends, and the cost of these steps is larger than the circuit size. However, this gives the first MPC where the computation performed by each party during the actual protocol execution, from the time the first protocol message is sent until the last protocol message is received, is smaller than the circuit size.\n\n## Task\nCompose an abstract for the title 'Perfect Structure on the Edge of Chaos.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We construct trapdoor permutations based on sub-exponential indistinguishability obfuscation and one-way functions, thereby providing the first candidate that is not based on the hardness of factoring.Our construction shows that even highly structured primitives, such as trapdoor permutations, can be potentially based on hardness assumptions with noisy structures such as those used in candidate constructions of indistinguishability obfuscation. It also suggest a possible way to construct trapdoor permutations that resist quantum attacks, and that their hardness may be based on problems outside the complexity class $$text{ SZK } $$ i\u00be?-- indeed, while factoring-based candidates do not possess such security, future constructions of indistinguishability obfuscation might.As a corollary, we eliminate the need to assume trapdoor permutations and injective one-way function in many recent constructions based on indistinguishability obfuscation."
      }
    ]
  },
  {
    "tid":"220",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Independent Spanning Trees on Multidimensional Torus Networks\nContent: Two spanning trees rooted at vertex r in a graph G are called independent spanning trees (ISTs) if for each vertex v in G, vner, the paths from vertex v to vertex r in these two trees are internally distinct. If the connectivity of G is k, the IST problem is to construct k ISTs rooted at each vertex. The IST problem has found applications in fault-tolerant broadcasting, but it is still open for general graphs with connectivity greater than four. In this paper, we shall propose a very simple algorithm for solving the IST problem on multidimensional torus networks. In our algorithm, every vertex can determine its parent for a specific independent spanning tree only depending on its own label. Thus, our algorithm can also be implemented in parallel systems or distributed systems very easily.\n\nTitle: Parallel Construction of Independent Spanning Trees on Enhanced Hypercubes\nContent: The use of multiple independent spanning trees (ISTs) for data broadcasting in networks provides a number of advantages, including the increase of fault-tolerance, bandwidth and security. Thus, the designs of multiple ISTs on several classes of networks have been widely investigated. In this paper, we give an algorithm to construct ISTs on enhanced hypercubes Qn;k, which contain folded hypercubes as a subclass. Moreover, we show that these ISTs are near optimal for heights and path lengths. Let D(Qn;k) denote the diameter of Qn;k. If n k is odd or n k 2 f2; ng, we show that all the heights of ISTs are equal to D(Qn;k) + 1, and thus are optimal. Otherwise, we show that each path from a node to the root in a spanning tree has length at most D(Qn;k) + 2. In particular, no more than 2.15% of nodes have the maximum path length. As a by-product, we improve the upper bound of wide diameter (respectively, fault diameter) of Qn;k from these path lengths.\n\nTitle: Parallel construction of optimal independent spanning trees on hypercubes\nContent: The use of multiple independent spanning trees (ISTs) for data broadcasting in networks provides a number of advantages, including the increase of fault-tolerance and bandwidth. Thus, the designs of multiple ISTs on several classes of networks have been widely investigated. Tang et al. [S.-M. Tang, Y.-L. Wang, Y.-H. Leu, Optimal independent spanning trees on hypercubes, Journal of Information Science and Engineering 20 (2004) 143-155] studied the problem of constructing k ISTs on k-dimensional hypercube Q\"k, and provided a recursive algorithm for their construction (i.e., for constructing k ISTs of Q\"k, it needs to build k-1 ISTs of Q\"k\"-\"1 in advance). This kind of construction forbids the possibility that the algorithm could be parallelized. In this paper, based on a simple concept called Hamming distance Latin square, we design a new algorithm for generating k ISTs of Q\"k. The newly proposed algorithm relies on a simple rule and is easy to be parallelized. As a result, we show that the ISTs we constructed are optimal in the sense that both the heights and the average path length of trees are minimized.\n\nTitle: Restricted power domination and fault-tolerant power domination on grids\nContent: The power domination problem is to find a minimum placement of phase measurement units (PMUs) for observing the whole electric power system, which is closely related to the classical domination problem in graphs. For a graph G=(V,E), the power domination number of G is the minimum cardinality of a set S@?V such that PMUs placed on every vertex of S results in all of V being observed. A vertex with a PMU observes itself and all its neighbors, and if an observed vertex with degree d1 has only one unobserved neighbor, then the unobserved neighbor becomes observed. Although the power domination problem has been proved to be NP-complete even when restricted to some special classes of graphs, Dorfling and Henning in [M. Dorfling, M.A. Henning, A note on power domination in grid graphs, Discrete Applied Mathematics 154 (2006) 1023-1027] showed that it is easy to determine the power domination number of an nxm grid. Their proof provides an algorithm for giving a minimum placement of PMUs. In this paper, we consider the situation in which PMUs may only be placed within a restricted subset of V. Then, we present algorithms to solve this restricted type of power domination on grids under the conditions that consecutive rows or columns form a forbidden zone. Moreover, we also deal with the fault-tolerant measurement placement in the designed scheme and provide approximation algorithms when the number of faulty PMUs does not exceed 3.\n\nTitle: Broadcasting secure messages via optimal independent spanning trees in folded hypercubes\nContent: Fault-tolerant broadcasting and secure message distribution are important issues for network applications. It is a common idea to design multiple spanning trees with a specific property in the underlying graph of a network to serve as a broadcasting scheme or a distribution protocol for receiving high levels of fault-tolerance and security. An n-dimensional folded hypercube, denoted by FQ\"n, is a strengthening variation of hypercube by adding additional links between nodes that have the furthest Hamming distance. In, [12], Ho(1990) proposed an algorithm for constructing n+1 edge-disjoint spanning trees each with a height twice the diameter of FQ\"n. Yang et al. (2009), [29] recently proved that Ho's spanning trees are indeed independent, i.e., any two spanning trees have the same root, say r, and for any other node vr, the two different paths from v to r, one path in each tree, are internally node-disjoint. In this paper, we provide another construction scheme to produce n+1 independent spanning trees of FQ\"n, where the height of each tree is equal to the diameter of FQ\"n plus one. As a result, the heights of independent spanning trees constructed in this paper are shown to be optimal.\n\n## Task\nCompose an abstract for the title 'Queue Layouts Of Toroidal Grids' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"A queue layout of a graph G consists of a linear order of its vertices, and a partition of its edges into queues, such that no two edges in the same queue are nested. The queuenumber qn(G) is the minimum number of queues required in a queue layout of G. The Cartesian product of two graphs G(1) = (V-1 E-1) and G(2) = (V-2, E-2), denoted by G(1) x G(2), is the graph with {< v(1), v(2)>: v(1) is an element of V-1 and v(2) is an element of V-2) as its vertex set and an edge (< u(1),u(2)>), < u(1),u(2)>) belongs to G(1) x G(2) if and only if either (u(1), v(1)) E El and u(2) = v(2) or (u(2), v(2)) is an element of E-2 and u(1) = v(1). Let T-k1,(k2,...,kn) denote the ndimensional toroidal grid defined by the Cartesian product of n cycles with varied lengths, i.e. T-k1,(k2,...,kn) = C-k1 x C-k2 x ... x C-kn where C-k1 is a cycle of length k(i) >= 3. If k(1) =k(2) = ... = k(n) = k, the graph is also called the k-ary n-cube and is denoted by Q(n)(k). In this paper, we deal with queue layouts of toroidal grids and show the following bound: qn(T-k1,(k2,...,kn)) <= 2n - 2 if n >= 2 and k(i) >= 3 for all i = 1, 2,...,n. In particular, for n = 2 and k(1), k(2) >= 3, we acquire qn(T-k1,(k2)) = 2. Recently, Pai et al. (Inform. Process. Lett. 110 (2009) pp.50-56) showed that qn(Q(n)(k)) <= 2n - 1 if n >= 1 and k >= 9. Thus, our result improves the bound of qn(Q(n)(k)) when n >= 2 and k >= 9."
      }
    ]
  },
  {
    "tid":"221",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Large Scale Image Annotation via Deep Representation Learning and Tag Embedding Learning\nContent: In this paper, we focus on the issue of large scale image annotation, whereas most existing methods are devised for small datasets. A novel model based on deep representation learning and tag embedding learning is proposed. Specifically, the proposed model learns an unified latent space for image visual features and tag embeddings simultaneously. Furthermore, a metric matrix is introduced to estimate the relevance scores between images and tags. Finally, an objective function modeling triplet relationships (irrelevant tag, image, relevant tag) is proposed with maximum margin pursuit. The proposed model is easy to tackle new images and tags via online learning and has a relatively low test computation complexity. Experimental results on NUS-WIDE dataset demonstrate the effectiveness of the proposed model.\n\nTitle: Image tag-ranking via pairwise supervision based semi-supervised model\nContent: Image tag-ranking, the task to sort tags based on their relevance to the related images, has become a hot topic in the field of multimedia. Most existing methods do not incorporate the tag-ranking order information into the models, which is actually very important to solve the issue of image tag-ranking. In this paper, by taking advantage of such important information, we propose a novel model which uses images with ranked tag lists as its supervision information. In the proposed method, each ranked tag list is decomposed into a number of image-tag pairs, all of which are pooled together for training a scoring function. With this pairwise supervision, the model is able to capture the intrinsic ranking structures. In addition, unsupervised data, namely images with unranked tag lists, is also integrated for digging the binary order: relevant or irrelevant. By leveraging both the pairwise supervision and unsupervised structural information, our model sufficiently exploits the tag relevance to images as well as the ranking structures of tag lists. Extensive experiments are conducted on both image tag-ranking and tag-based image search with three benchmark datasets: SUNAttribute, Labelme and MSRC, demonstrating the effectiveness of the proposed model.\n\nTitle: Image-Text Cross-Modal Retrieval via Modality-Specific Feature Learning\nContent: Cross-modal retrieval extends the ability of search engines to deal with the massive cross-modal data. The goal of image-text cross-modal retrieval is to search images (texts) by using text (image) queries by computing the similarities of images and texts directly. Many existing methods rely on low-level visual features and textual features for cross-modal retrieval, ignoring the characteristics existing in the raw data of different modalities. In this paper, a novel model based on modality-specific feature learning is proposed. Considering the characteristics of different modalities, the model uses two types of convolutional neural networks to map the raw data to the latent space representations for images and texts, respectively. Particularly, the convolution based network used for texts involves word embedding learning, which has been proved effective to extract meaningful textual features for text classification. In the latent space, the mapped features of images and texts form relevant and irrelevant image-text pairs, which are used by the one-vs-more learning scheme. This learning scheme can achieve ranking functionality by allowing for one relevant and more irrelevant pairs. The standard back-propagation technique is employed to update the parameters of two convolutional networks. Extensive cross-modal retrieval experiments are carried out on three challenging datasets that consist of image-document pairs or image-query click-through data from a search engine, and the results firmly demonstrate that the proposed model is much more effective.\n\n\n\nTitle: Semantic Labeling in Very High Resolution Images via a Self-Cascaded Convolutional Neural Network.\nContent: Semantic labeling for very high resolution (VHR) images in urban areas, is of significant importance in a wide range of remote sensing applications. However, many confusing manmade objects and intricate fine-structured objects make it very difficult to obtain both coherent and accurate labeling results. For this challenging task, we propose a novel deep model with convolutional neural networks (CNNs), i.e., an end-to-end self-cascaded network (ScasNet). Specifically, for confusing manmade objects, ScasNet improves the labeling coherence with sequential global-to-local contexts aggregation. Technically, multi-scale contexts are captured on the output of a CNN encoder, and then they are successively aggregated in a self-cascaded manner. Meanwhile, for fine-structured objects, ScasNet boosts the labeling accuracy with a coarse-to-fine refinement strategy. It progressively refines the target objects using the low-level features learned by CNN\u2019s shallow layers. In addition, to correct the latent fitting residual caused by multi-feature fusion inside ScasNet, a dedicated residual correction scheme is proposed. It greatly improves the effectiveness of ScasNet. Extensive experimental results on three public datasets, including two challenging benchmarks, show that ScasNet achieves the state-of-the-art performance.\n\nTitle: Semisupervised dimensionality reduction and classification through virtual label regression.\nContent: Semisupervised dimensionality reduction has been attracting much attention as it not only utilizes both labeled and unlabeled data simultaneously, but also works well in the situation of out-of-sample. This paper proposes an effective approach of semisupervised dimensionality reduction through label propagation and label regression. Different from previous efforts, the new approach propagates the label information from labeled to unlabeled data with a well-designed mechanism of random walks, in which outliers are effectively detected and the obtained virtual labels of unlabeled data can be well encoded in a weighted regression model. These virtual labels are thereafter regressed with a linear model to calculate the projection matrix for dimensionality reduction. By this means, when the manifold or the clustering assumption of data is satisfied, the labels of labeled data can be correctly propagated to the unlabeled data; and thus, the proposed approach utilizes the labeled and the unlabeled data more effectively than previous work. Experimental results are carried out upon several databases, and the advantage of the new approach is well demonstrated.\n\n## Task\nCompose an abstract for the title 'Multi-Label Image Classification via Knowledge Distillation from Weakly-Supervised Detection.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Multi-label image classification is a fundamental but challenging task towards general visual understanding. Existing methods found the region-level cues (e.g., features from RoIs) can facilitate multi-label classification. Nevertheless, such methods usually require laborious object-level annotations (i.e., object labels and bounding boxes) for effective learning of the object-level visual features. In this paper, we propose a novel and efficient deep framework to boost multi-label classification by distilling knowledge from weakly-supervised detection task without bounding box annotations. Specifically, given the image-level annotations, (1) we first develop a weakly-supervised detection (WSD) model, and then (2) construct an end-to-end multi-label image classification framework augmented by a knowledge distillation module that guides the classification model by the WSD model according to the class-level predictions for the whole image and the object-level visual features for object RoIs. The WSD model is the teacher model and the classification model is the student model. After this cross-task knowledge distillation, the performance of the classification model is significantly improved and the efficiency is maintained since the WSD model can be safely discarded in the test phase. Extensive experiments on two large-scale datasets (MS-COCO and NUS-WIDE) show that our framework achieves superior performances over the state-of-the-art methods on both performance and efficiency.\n\n"
      }
    ]
  },
  {
    "tid":"222",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Incomplete Directed Perfect Phylogeny\nContent: Perfect phylogeny is one of the fundamental models for studying evolution. We investigate the following variant of the model: The input is a species-characters matrix. The characters are binary and directed; i.e., a species can only gain characters. The difference from standard perfect phylogeny is that for some species the states of some characters are unknown. The question is whether one can complete the missing states in a way that admits a perfect phylogeny. The problem arises in classical phylogenetic studies, when some states are missing or undetermined. Quite recently, studies that infer phylogenies using inserted repeat elements in DNA gave rise to the same problem. Extant solutions for it take time O(n2m) for n species and m characters. We provide a graph theoretic formulation of the problem as a graph sandwich problem, and give near-optimal $\\tilde{O}(nm)$-time algorithms for the problem. We also study the problem of finding a single, general solution tree, from which any other solution can be obtained by node splitting. We provide an algorithm to construct such a tree, or determine that none exists.\n\nTitle: On the Generality of Phylogenies from Incomplete Directed Characters\nContent: We study a problem that arises in computational biology, when wishing to reconstruct the phylogeny of a set of species. In Incomplete Directed Perfect Phylogeny (IDP), the characters are binary and directed (i.e., species can only gain characters), and the states of some characters are unknown. The goal is to complete the missing states in a way consistent with a perfect phylogenetic tree. This problem arises in classical phylogenetic studies, when some states are missing or undetermined, and in recent phylogenetic studies based on repeat elements in DNA. The problem was recently shown to be polynomial. As different completions induce different trees, it is desirable to find a general solution tree. Such a solution is consistent with the data, and every other consistent solution can be obtained from it by node splitting. Unlike the situation for complete datasets, a general solution may not exist for IDP instances. We provide a polynomial algorithm to find a general solution for an IDP instance, or determine that none exists.\n\nTitle: The incomplete perfect phylogeny haplotype problem.\nContent: The problem of resolving genotypes into haplotypes, under the perfect phylogeny model, has been under intensive study recently. All studies so far handled missing data entries in a heuristic manner. We prove that the perfect phylogeny haplotype problem is NP-complete when some of the data entries are missing, even when the phylogeny is rooted. We define a biologically motivated probabilistic model for genotype generation and for the way missing data occur. Under this model, we provide an algorithm, which takes an expected polynomial time. In tests on simulated data, our algorithm quickly resolves the genotypes under high rates of missing entries.\n\nTitle: Computational Problems in Perfect Phylogeny Haplotyping: Typing without Calling the Allele\nContent: A haplotype is an m-long binary vector. The xor-genotype of two haplotypes is the m-vector of their coordinate-wise xor. We study the following problem: Given a set of xor-genotypes, reconstruct their haplotypes so that the set of resulting haplotypes can be mapped onto a perfect phylogeny tree. The question is motivated by studying population evolution in human genetics, and is a variant of the perfect phylogeny haplotyping problem that has received intensive attention recently. Unlike the latter problem, in which the input is \"full\" genotypes, here we assume less informative input, and so may be more economical to obtain experimentally.Building on ideas of Gusfield, we show how to solve the problem in polynomial time, by a reduction to the graph realization problem. The actual haplotypes are not uniquely determined by that tree they map onto, and the tree itself may or may not be unique. We show that tree uniqueness implies uniquely determined haplotypes, up to inherent degrees of freedom, and give a sufficient condition for the uniqueness. To actually determine the haplotypes given the tree, additional information is necessary. We show that two or three full genotypes suffice to reconstruct all the haplotypes, and present a linear algorithm for identifying those genotypes.\n\nTitle: Tractability of parameterized completion problems on chordal and interval graphs: minimum fill-in and physical mapping\nContent: We study the parameterized complexity of several NP-Hard graph completion problems: The minimum fill-in problem is to decide if a graph can be triangulated by adding at most k edges. We develop an O(k/sup 5/ mn+f(K)) algorithm for the problem on a graph with n vertices and m edges. In particular, this implies that the problem is fixed parameter tractable (FPT). proper interval graph completion problems, motivated by molecular biology, ask for adding edges in order to obtain a proper interval graph, so that a parameter in that graph does not exceed k. We show that the problem is FPT when k is the number of added edges. For the problem where k is the clique size, we give an O(f(k)n/sup k-1/) algorithm, so it is polynomial for fixed k. On the other hand, we prove its hardness in the parameterized hierarchy, so it is probably not FPT. Those results are obtained even when a set of edges which should not be added is given. That set can be given either explicitly or by a proper vertex coloring which the added edges should respect.\n\n## Task\nCompose an abstract for the title 'Incomplete Directed Perfect Phylogeny' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Abstract: Perfect phylogeny is one of the fundamental models for studying evolution. We investigate thefollowing variant of the model: The input is a species-characters matrix. The characters are binary anddirected, i.e., a species can only gain characters. The dierence from standard perfect phylogeny is thatfor some species the states of some characters are unknown. The question is whether one can completethe missing states in a way that admits a perfect phylogeny. The problem arises in classical..."
      }
    ]
  },
  {
    "tid":"223",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Adaptive gbest-guided gravitational search algorithm\nContent: One heuristic evolutionary algorithm recently proposed is the gravitational search algorithm (GSA), inspired by the gravitational forces between masses in nature. This algorithm has demonstrated superior performance among other well-known heuristic algorithms such as particle swarm optimisation and genetic algorithm. However, slow exploitation is a major weakness that might result in degraded performance when dealing with real engineering problems. Due to the cumulative effect of the fitness function on mass in GSA, masses get heavier and heavier over the course of iteration. This causes masses to remain in close proximity and neutralise the gravitational forces of each other in later iterations, preventing them from rapidly exploiting the optimum. In this study, the best mass is archived and utilised to accelerate the exploitation phase, ameliorating this weakness. The proposed method is tested on 25 unconstrained benchmark functions with six different scales provided by CEC 2005. In addition, two classical, constrained, engineering design problems, namely welded beam and tension spring, are also employed to investigate the efficiency of the proposed method in real constrained problems. The results of benchmark and classical engineering problems demonstrate the performance of the proposed method.\n\nTitle: Binary optimization using hybrid particle swarm optimization and gravitational search algorithm\nContent: The PSOGSA is a novel hybrid optimization algorithm, combining strengths of both particle swarm optimization (PSO) and gravitational search algorithm (GSA). It has been proven that this algorithm outperforms both PSO and GSA in terms of improved exploration and exploitation. The original version of this algorithm is well suited for problems with continuous search space. Some problems, however, have binary parameters. This paper proposes a binary version of hybrid PSOGSA called BPSOGSA to solve these kinds of optimization problems. The paper also considers integration of adaptive values to further balance exploration and exploitation of BPSOGSA. In order to evaluate the efficiencies of the proposed binary algorithm, 22 benchmark functions are employed and divided into three groups: unimodal, multimodal, and composite. The experimental results confirm better performance of BPSOGSA compared with binary gravitational search algorithm (BGSA), binary particle swarm optimization (BPSO), and genetic algorithm in terms of avoiding local minima and convergence rate.\n\nTitle: Binary multi-verse optimization algorithm for global optimization and discrete problems\nContent: Multi-verse optimizer is one of the recently proposed nature-inspired algorithms that has proven its efficiency in solving challenging optimization problems. The original version of Multi-verse optimizer is able to solve problems with continuous variables. This paper proposes a binary version of this algorithm to solve problems with discrete variables such as feature selection. The proposed Binary Multi-verse optimizer is equipped with a V-shaped transfer function to covert continuous values to binary, and update the solutions over the course of optimization. A comparative study is conducted to compare Binary Multi-verse optimizer with other binary optimization algorithms such as Binary Bat Algorithm, Binary Particle Swarm Optimization, Binary Dragon Algorithm, and Binary Grey Wolf Optimizer. As case studies, a set of 13 benchmark functions including unimodal and multimodal is employed. In addition, the number of variables of these test functions are changed (5, 10, and 20) to test the proposed algorithm on problems with different number of parameters. The quantitative results show that the proposed algorithm significantly outperforms others on the majority of benchmark functions. Convergence curves qualitatively show that for some functions, proposed algorithm finds the best result at early iterations. To demonstrate the applicability of proposed algorithm, the paper considers solving feature selection and knapsack problems as challenging real-world problems in data mining. Experimental results using seven datasets for feature selection problem show that proposed algorithm tends to provide better accuracy and requires less number of features compared to other algorithms on most of the datasets. For knapsack problem 17 benchmark datasets were used, and the results show that the proposed algorithm achieved higher profit and lower error compared to other algorithms.\n\nTitle: Simultaneous Feature Selection and Support Vector Machine Optimization Using the Grasshopper Optimization Algorithm.\nContent: Support vector machine (SVM) is considered to be one of the most powerful learning algorithms and is used for a wide range of real-world applications. The efficiency of SVM algorithm and its performance mainly depends on the kernel type and its parameters. Furthermore, the feature subset selection that is used to train the SVM model is another important factor that has a major influence on it classification accuracy. The feature subset selection is a very important step in machine learning, specially when dealing with high-dimensional data sets. Most of the previous researches handled these important factors separately. In this paper, we propose a hybrid approach based on the Grasshopper optimisation algorithm (GOA), which is a recent algorithm inspired by the biological behavior shown in swarms of grasshoppers. The goal of the proposed approach is to optimize the parameters of the SVM model, and locate the best features subset simultaneously. Eighteen low- and high-dimensional benchmark data sets are used to evaluate the accuracy of the proposed approach. For verification, the proposed approach is compared with seven well-regarded algorithms. Furthermore, the proposed approach is compared with grid search, which is the most popular technique for tuning SVM parameters. The experimental results show that the proposed approach outperforms all of the other techniques in most of the data sets in terms of classification accuracy, while minimizing the number of selected features.\n\nTitle: Training feedforward neural networks using hybrid particle swarm optimization and gravitational search algorithm.\nContent: The Gravitational Search Algorithm (GSA) is a novel heuristic optimization method based on the law of gravity and mass interactions. It has been proven that this algorithm has good ability to search for the global optimum, but it suffers from slow searching speed in the last iterations. This work proposes a hybrid of Particle Swarm Optimization (PSO) and GSA to resolve the aforementioned problem. In this paper, GSA and PSOGSA are employed as new training methods for Feedforward Neural Networks (FNNs) in order to investigate the efficiencies of these algorithms in reducing the problems of trapping in local minima and the slow convergence rate of current evolutionary learning algorithms. The results are compared with a standard PSO-based learning algorithm for FNNs. The resulting accuracy of FNNs trained with PSO, GSA, and PSOGSA is also investigated. The experimental results show that PSOGSA outperforms both PSO and GSA for training FNNs in terms of converging speed and avoiding local minima. It is also proven that an FNN trained with PSOGSA has better accuracy than one trained with GSA.\n\n## Task\nCompose an abstract for the title 'Chaotic gravitational constants for the gravitational search algorithm.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Display Omitted Chaotic maps have been embedded into Gravitational Search Algorithms (GSA) for the first time.The problem of trapping in local minima in GSA has been improved by the chaotic maps.The convergence rate of GSA has been improved.The statistical test allowed us to judge about the significance of the results.An adaptive normalization is proposed to smoothly transit from the exploration phase to the exploitation phase. In a population-based meta-heuristic, the search process is divided into two main phases: exploration versus exploitation. In the exploration phase, a random behavior is fruitful to explore the search space as extensive as possible. In contrast, a fast exploitation toward the promising regions is the main objective of the latter phase. It is really challenging to find a proper balance between these two phases because of the stochastic nature of population-based meta-heuristic algorithms. The literature shows that chaotic maps are able to improve both phases. This work embeds ten chaotic maps into the gravitational constant (G) of the recently proposed population-based meta-heuristic algorithm called Gravitational Search Algorithm (GSA). Also, an adaptive normalization method is proposed to transit from the exploration phase to the exploitation phase smoothly. As case studies, twelve shifted and biased benchmark functions evaluate the performance of the proposed chaos-based GSA algorithms in terms of exploration and exploitation. A statistical test called Wilcoxon rank-sum is done to judge about the significance of the results as well. The results demonstrate that sinusoidal map is the best map for improving the performance of GSA significantly."
      }
    ]
  },
  {
    "tid":"224",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: FPGA-based broadband current control of a linear motor with Class-G power amplifiers\nContent: In this paper, a broadband current control system using a FPGA and Class-G power amplifiers is realized. The current controllers with general PWM inverter, which is classified as Class-D power amplifier, does not operate in case that bandwidth of current controllers is enlarged due to PWM switching operations. On the contrary, the current control system using Class-G power amplifiers is able to be operated normally, even where the bandwidth is set as 25100 rad/s (\u2243 4kHz). By the numerical simulations and the experiment, validity of the current controllers with Class-G power amplifiers is verified. The broadband current control realized in this paper is useful for servo systems in the field of fast motion control and real-world haptics.\n\nTitle: High-Power-Factor Single-Phase Diode Rectifier Driven by Repetitively Controlled IPM Motor\nContent: This paper proposes a new power factor correction method using an inverter-driven interior permanent magnet (IPM) motor. The proposed system realizes the high power factor of a single-phase diode rectifier by using a three-phase pulsewidth modulation inverter and an IPM motor. The proposed converter consists of a single-phase diode rectifier, a small film capacitor at the dc link (14 \u03bcF and 0.33 J/kVA), a three-phase voltage-source inverter, and an IPM motor. In the proposed system, the inverter has the following two functions: 1) It regulates the velocity of the IPM motor, and 2) it controls the source-side-current waveform. In order to obtain a unity-power-factor operation at the source side, we propose and implement a new control method that regulates the inverter output power. The proposed control method is that the inverter regulates d-q-axis current synchronous with input voltage. An inverter-output-power controller is positioned between a speed controller and a q-axis current controller. The inverter output power is regulated by a proportional-integral and repetitive controller. The maximum power factor obtained by the proposed method is 98.7% at the rated-load conditions. The superior performance of the proposed system is demonstrated by experimental results.\n\nTitle: Wideband Sensorless Force Control Based on Singular Spectrum Analysis and Dither Periodic Component Elimination Kalman Filter\nContent: This paper proposes a new force sensing approach based on the singular spectrum analysis and the dither periodic component elimination Kalman filter to realize a wideband sensorless force control system. The dither periodic component elimination Kalman filter and the disturbance observer are integrated to configure the force sensation with the cancelation of dither signal component and noise in the force information. Moreover, the singular spectrum analysis based velocity estimation, which is the input information of the force sensation, is constructed to improve the force sensing performance and realize the wideband force control system when the pole of the disturbance observer is greatly increased. The proposed method achieves considerably widened force sensing bandwidth. The effectiveness of the proposed method is verified by experimental results.\n\nTitle: Wideband Force Control By Position-Acceleration Integrated Disturbance Observer\nContent: Motion control is widely used in industry applications. One of its key components is the disturbance observer, which estimates a disturbance torque of a motion system and realizes a robust acceleration control. A disturbance observer observes and suppresses the disturbance torque within its bandwidth. Motion systems have started to spread in society and they are required to have the ability to interact with unknown environments. Such a haptic motion requires much wider bandwidth. However, since the conventional disturbance observer attains the acceleration information by the second-order derivative of a position response, the bandwidth is limited due to the derivative noise. To enlarge the bandwidth of a disturbance observer, this paper proposes a position-acceleration integrated disturbance observer (PAIDO). Since an acceleration sensor is implemented in it, the control performance of the PAIDO is superior to the conventional one. In this paper, the PAIDO is applied to force control and the viability of the proposed method is confirmed by fast Fourier transformation analyses. The experimental results show the viability of the proposed method.\n\nTitle: Anti-windup robust controller considering motor dynamics for speed servo system\nContent: A robust servo system is important for improving the performance of motion control systems in several industry applications. In general, a speed servo system has a controller with an integrator, such as a PI controller. When its output variable is saturated by a current and/or voltage limiter, a wind-up phenomenon and an unstable response often occur. We have already proposed an anti-windup algorithm considering voltage saturation for speed servo system, that regulates the current response smoothly and stably. Moreover, we considered the saturation of current and speed for position servo system, and the anti-windup algorithm regulated the speed response stably. However, the speed response has the overshoot, which is caused by current saturation and the speed controller not keeping the response performance. This paper proposes an anti-windup algorithm considering the motor dynamics and current saturation for speed servo system of SPM synchronous motor. The experimental and numerical simulation results confirm that the speed servo system having the proposed algorithm regulates the motor speed smoothly and stably.\n\n## Task\nCompose an abstract for the title 'Motor Current Regulation Based Direct DC-link Current Control of Wide Range Load Condition for IPMSM Drive System Without Passive Component' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In a single-phase to three-phase electrolytic capacitor-less inverter, the inverter must control not only the motor but also the source current because a power factor correction circuit or energy buffer does not exist. To obtain a sinusoidal source current, a control method that modifies the output voltage directly is proposed. However, the motor current control occasionally becomes unstable because the control method modifies the output voltage to control the source current. To control the source current while considering the motor current response, this paper proposes a motor current regulation based direct DC-link current control. The proposed method enhances the stability of the motor current control system by realizing the sinusoidal source current with the motor current reference modification. The effectiveness of the proposed method is experimentally verified."
      }
    ]
  },
  {
    "tid":"225",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: ON THE DESIGN OF SQUARE DIFFERENTIAL MICROPHONE ARRAYS WITH A MULTISTAGE STRUCTURE\nContent: This paper studies the problem of designing square differential microphone arrays (SDMAs). It presents a multistage approach, which first divides an SDMA composed of M-2 microphones into (M - 1)(2) subarrays with each subarray being a 2 x 2 square array formed by four adjacent microphones. Then, differential beamforming is performed with each subarray in the first-stage. The first-stage differential beamformers' outputs are subsequently used as the inputs of the second stage to form (M - 2)(2) subarrays and a second-stage differential beamforming is then performed. Continuing this process till the (M- 1)th stage, we obtain the final output of the SDMA. The SDMA designed in such a multistage structure has two important properties. First, the global weighting matrix is equal to the two dimensional convolution of weighting matrices from the first stage to the last one. Second, the global beampattern is equal to the product of beampatterns from all stages. Consequently, we can combine different kinds of beamformers in different stages and have better control of the performance metrics.\n\nTitle: Optimal design of directivity patterns for endfire linear microphone arrays\nContent: Directivity pattern or beampattern is an important performance measure in all fixed beamformers. Given a microphone array, how to design the beamforming filter so that the resulting directivity pattern is close to the desired one is a critical issue. In this paper, we study the design of such patterns for endfire uniform linear microphone arrays. By considering the frequency-independent Chebyshev pattern as the desired one, we derive an optimal beamforming filter based on the minimization of the mean-squared error (MSE) under the distortionless constraint. It is shown that the proposed beamformer design can generate beampatterns that are very close to the desired ones and, the larger is the number of microphones, the better is the designed beampattern.\n\nTitle: ROBUST STEERABLE DIFFERENTIAL BEAMFORMERS WITH NULL CONSTRAINTS FOR CONCENTRIC CIRCULAR MICROPHONE ARRAYS\nContent: Differential beamformers with concentric circular microphone arrays (CCMAs) are desirable for use in various applications since they can form frequency-invariant spatial responses, have better beam steering flexibility than linear arrays, and suffer less with beampattern irregularity and white noise amplification than circular microphone arrays (CMAs). The methods developed previously for differential beamforming with CCMAs are based on the series expansion. Such methods need to know the analytic form of the target beampattern, which may not be accessible in practice. Furthermore, expansion error may lead to erroneous solution, which can cause noise amplification instead of reduction. In this paper, we extend our recently developed beamforming method for CMAs to the design of differential beamformers with CCMAs, which takes advantage of the symmetric null constraints from the beampattern. Simulations are performed to justify the properties of the proposed approach.\n\nTitle: Theoretical analysis of differential microphone array beamforming and an improved solution\nContent: Differential microphone arrays (DMAs), which are responsive to the differential sound pressure field, have attracted much attention due to their properties of frequency-invariant beampatterns, small apertures, and potential of maximum directivity. Traditionally, DMAs are designed and implemented in a multistage (cascade) way, where a proper time delay is used in each stage to form a beampattern of interest. Recently, it was reported that DMAs can be designed by solving a linear system of equations formed from the information about the nulls of the desired beampattern. This paper deals with the problem of beamforming with linear DMAs. Its major contributions are as follows. 1) By using the spatial Z transform, we present some theoretical analysis of both the traditional cascade and new null-constrained DMA beamforming. It is shown that the cascade and null-constrained DMAs of the same order with the same number of sensors are theoretically identical. 2) We develop a two-stage approach to the study of the robust DMA beamformer, which is based on the principle of maximizing the white noise gain (WNG). The first-stage of this approach is in the structure of the traditional non-robust DMA while the second-stage filter is optimized for improving the WNG. 3) Using the two-stage approach, we show that the robust DMA beamformer may introduce extra nulls in the beampattern at high frequencies; particularly, it introduces M -- N -- 1 extra nulls if the interelement spacing is equal to half of the wavelength, where M and N are the number of sensors and the DMA order, respectively. 4) We develop a method that can solve the extra-null problem while maximizing the WNG in robust DMA beamforming, i.e., a robust solution with a frequency-invariant beampattern.\n\nTitle: COMBINED DIFFERENTIAL BEAMFORMING WITH UNIFORM LINEAR MICROPHONE ARRAYS\nContent: While differential beamformers have been widely used in voice communication and human-machine speech interface systems to enhance speech signals of interest, how to design such beamformers that on the one hand can achieve the highest possible directivity factor (DF) and on the other hand are able to obtain a certain level of white noise gain (WNG), so that they are robust enough to sensors' self noise and array imperfections is still a challenging issue. This paper studies the problem of robust differential beamforming with small-size arrays to achieve a high DF. It presents a method for the design of differential beamformers with uniform linear arrays. We first generate differential pressure signals by applying the recently developed forward spatial difference operator to the outputs of the array with pressure sensors. The pressure microphone observation signals and the differential pressure signals are then put together, and a combined beamformer is subsequently designed, which consists of two sub-beamformers, one operates on the pressure microphone observations and the other on the differential pressure signals. A new class of combined differential beamformers are introduced, which can achieve different levels of compromises between DF and WNG using an adjustable parameter.\n\n## Task\nCompose an abstract for the title 'On the Design of Target Beampatterns for Differential Microphone Arrays' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"\nDifferential microphone arrays DMAs have many interesting properties and have been widely used in acoustic, audio, and speech applications. A critical part of a DMA is the differential beamformer, which is generally designed in two important steps: 1 specifying a target beampattern based on what differential sound pressure field the DMA is expected to respond to and 2 designing the differential beamforming filter so that the resulting beampattern matches the target one. Most efforts in the study of DMAs so far have focused on the second step while choosing one of the limited patterns available in the literature as the target beampattern. Since it governs how the array performs, how to design the target beampattern is an important problem, which this paper addresses. The major contributions of this paper consists of the following four aspects. First, a positive superposition theorem is presented, which shows that the linear combination of effective beampatterns with non-negative coefficients is always an effective beampattern. Second, we propose a general approach to the design of target DMA beampatterns based on the positive superposition theorem. Third, an overview of the classical target beampatterns is provided and discussion is made on how to form effective base patterns. Fourth, we show that the smallest first null of a DMA is $\\pi /2N$ with $N$ being the DMA order, which provides the rule of setting nulls in practice. Finally, with examples, we show that with the use of the alternating-direction-method-of-multipliers algorithm, the proposed approach is able to generate useful DMA target beampatterns.\n"
      }
    ]
  },
  {
    "tid":"226",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Single-channel noise reduction using unified joint diagonalization and optimal filtering.\nContent: In this paper, the important problem of single-channel noise reduction is treated from a new perspective. The problem is posed as a filtering problem based on joint diagonalization of the covariance matrices of the desired and noise signals. More specifically, the eigenvectors from the joint diagonalization corresponding to the least significant eigenvalues are used to form a filter, which effectively estimates the noise when applied to the observed signal. This estimate is then subtracted from the observed signal to form an estimate of the desired signal, i.e., the speech signal. In doing this, we consider two cases, where, respectively, no distortion and distortion are incurred on the desired signal. The former can be achieved when the covariance matrix of the desired signal is rank deficient, which is the case, for example, for voiced speech. In the latter case, the covariance matrix of the desired signal is full rank, as is the case, for example, in unvoiced speech. Here, the amount of distortion incurred is controlled via a simple, integer parameter, and the more distortion allowed, the higher the output signal-to-noise ratio (SNR). Simulations demonstrate the properties of the two solutions. In the distortionless case, the proposed filter achieves only a slightly worse output SNR, compared to the Wiener filter, along with no signal distortion. Moreover, when distortion is allowed, it is possible to achieve higher output SNRs compared to the Wiener filter. Alternatively, when a lower output SNR is accepted, a filter with less signal distortion than the Wiener filter can be constructed.\n\nTitle: A Widely Linear Distortionless Filter for Single-Channel Noise Reduction\nContent: Traditionally in the single-channel noise-reduction problem, speech distortion is inevitable since the desired signal is also filtered while filtering the noise. In fact, the more the noise is reduced, the more the speech distortion is added into the desired signal, as proved in the literature. So, if we require no speech distortion, we either end up with no noise reduction at all or have to use multiple sensors. In this paper, we attempt to apply the widely linear (WL) estimation theory to noise reduction. Unlike the traditional approaches that only filter the short-time Fourier transform (STFT) of the noisy signal, the method developed in this paper applies the noise-reduction filter to both the STFT of the noisy signal and its conjugate. With the constraint of no speech distortion, a WL distortionless filter is derived. We show that this new optimal filter can fully take advantage of the noncircularity property of speech signals to achieve up to 3-dB signal-to-noise-ratio (SNR) improvement without introducing any speech distortion, which can only be obtained with the traditional approaches if two or more microphones are used.\n\nTitle: A Multi-Frame Approach to the Frequency-Domain Single-Channel Noise Reduction Problem\nContent: This paper focuses on the class of single-channel noise reduction methods that are performed in the frequency domain via the short-time Fourier transform (STFT). The simplicity and relative effectiveness of this class of approaches make them the dominant choice in practical systems. Over the past years, many popular algorithms have been proposed. These algorithms, no matter how they are developed, have one feature in common: the solution is eventually formulated as a gain function applied to the STFT of the noisy signal only in the current frame, implying that the interframe correlation is ignored. This assumption is not accurate for speech enhancement since speech is a highly self-correlated signal. In this paper, by taking the interframe correlation into account, a new linear model for speech spectral estimation and some optimal filters are proposed. They include the multi-frame Wiener and minimum variance distortionless response (MVDR) filters. With these filters, both the narrowband and fullband signal-to-noise ratios (SNRs) can be improved. Furthermore, with the MVDR filter, speech distortion at the output can be zero. Simulations present promising results in support of the claimed merits obtained by theoretical analysis.\n\nTitle: Multi-Microphone Noise Reduction Based on Orthogonal Noise Signal Decompositions\nContent: Multi-microphone noise reduction plays an increasing and important role in acoustic communication systems. Existing multichannel noise reduction filters are commonly computed based on a single noise covariance matrix. Recently, an orthogonal noise signal decomposition was proposed that uses a single noise signal as a reference. Using this decomposition, it was possible to reformulate the noise reduction problem and derived a multichannel noise reduction filter that allows a tradeoff between the noise that is coherent and incoherent with respect to the reference signal. In this contribution, we analyze the previously proposed decomposition and propose an orthogonal decomposition that is based on a rank-one projection of all noise signals. The projection is chosen such that the total variance of the coherent noise component is maximized. To further improve the separation between coherent noise and incoherent noise, a rank-$Q$ projection of the observed noise signals is proposed. The decomposed noise covariance matrix is then used to derive a minimum variance distortionless response beamformer that allows a tradeoff between coherent and incoherent noise reduction, and to form a constraint matrix for a linearly constrained minimum variance beamformer. The results of the performance evaluation demonstrate the advantage of the proposed decompositions over the previously proposed decomposition.\n\nTitle: On Optimal Frequency-Domain Multichannel Linear Filtering for Noise Reduction\nContent: Several contributions have been made so far to develop optimal multichannel linear filtering approaches and show their ability to reduce the acoustic noise. However, there has not been a clear unifying theoretical analysis of their performance in terms of both noise reduction and speech distortion. To fill this gap, we analyze the frequency-domain (non-causal) multichannel linear filtering for noise reduction in this paper. For completeness, we consider the noise reduction constrained optimization problem that leads to the parameterized multichannel non-causal Wiener filter (PMWF). Our contribution is fivefold. First, we formally show that the minimum variance distortionless response (MVDR) filter is a particular case of the PMWF by properly formulating the constrained optimization problem of noise reduction. Second, we propose new simplified expressions for the PMWF, the MVDR, and the generalized sidelobe canceller (GSC) that depend on the signals' statistics only. In contrast to earlier works, these expressions are explicitly independent of the channel transfer function ratios. Third, we quantify the theoretical gains and losses in terms of speech distortion and noise reduction when using the PWMF by establishing new simplified closed-form expressions for three performance measures, namely, the signal distortion index, the noise reduction factor (originally proposed in the paper titled ldquoNew insights into the noise reduction Wiener filter,rdquo by J. Chen (IEEE Transactions on Audio, Speech, and Language Processing, Vol. 15, no. 4, pp. 1218-1234, Jul. 2006) to analyze the single channel time-domain Wiener filter), and the output signal-to-noise ratio (SNR). Fourth, we analyze the effects of coherent and incoherent noise in addition to the benefits of utilizing multiple microphones. Fifth, we propose a new proof for the a posteriori SNR improvement achieved by the PMWF. Finally, we provide some simulations results to corroborate the - - findings of this work.\n\n## Task\nCompose an abstract for the title 'Single-channel noise reduction via semi-orthogonal transformations and reduced-rank filtering.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"A general framework is developed that combines semi-orthogonal transformation and reduced-rank filtering for noise reduction.Under this new framework, several optimal reduced-rank filters are derived, including the maximum SNR, the Wiener, the tradeoff, and the MVDR filters.Discussions are also provided on how to derive different semi-orthogonal transformations under four estimation criteria, including minimum correlation, minimum MSE, minimum distortion, and minimum residual noise.Simulations are performed and the results show the properties of the deduced optimal reduced-rank filters. This paper investigates the problem of single-channel noise reduction in the time domain. The objective is to find a lower dimensional filter that can yield a noise reduction performance as close as possible to or even better than that obtained by the full-rank solution. This is achieved in three steps. First, we transform the observation signal vector sequence, through a semi-orthogonal matrix, into a sequence of transformed signal vectors with a reduced dimension. Second, a reduced-rank filter is applied to get an estimate of the clean speech in the transformed domain. Third, the estimate of the clean speech in the time domain is obtained by an inverse semi-orthogonal transformation. The focus of this paper is on the derivation of semi-orthogonal transformations under certain estimation criteria in the first step and the design of the reduced-rank optimal filters that can be used in the second step. We show how noise reduction using the principle of rank reduction can be cast as an optimal filtering problem, and how different semi-orthogonal transformations affect the noise reduction performance. Simulations are performed under various conditions to validate the deduced filters for noise reduction."
      }
    ]
  },
  {
    "tid":"227",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Reinforcement Learning with Reward Shaping and Mixed Resolution Function Approximation\nContent: A crucial trade-off is involved in the design process when function approximation is used in reinforcement learning. Ideally the chosen representation should allow representing as close as possible an approximation of the value function. However, the more expressive the representation the more training data is needed because the space of candidate hypotheses is bigger. A less expressive representation has a smaller hypotheses space and a good candidate can be found faster. The core idea of this paper is the use of a mixed resolution function approximation, that is, the use of a less expressive function approximation to provide useful guidance during learning, and the use of a more expressive function approximation to obtain a final result of high quality. A major question is how to combine the two representations. Two approaches are proposed and evaluated empirically.\n\nTitle: An Empirical Study of Potential-Based Reward Shaping and Advice in Complex, Multi-Agent Systems\nContent: This paper investigates the impact of reward shaping in multi-agent reinforcement learning as a way to incorporate domain knowledge about good strategies. In theory, potential-based reward shaping does not alter the Nash Equilibria of a stochastic game, only the exploration of the shaped agent. We demonstrate empirically the performance of reward shaping in two problem domains within the context of RoboCup KeepAway by designing three reward shaping schemes, encouraging specific behaviour such as keeping a minimum distance from other players on the same team and taking on specific roles. The results illustrate that reward shaping with multiple, simultaneous learning agents can reduce the time needed to learn a suitable policy and can alter the final group performance.\n\nTitle: A comparison of plan-based and abstract MDP reward shaping\nContent: Reward shaping has been shown to significantly improve an agent's performance in reinforcement learning. As attention is shifting away from tabula-rasa approaches many different reward shaping methods have been developed. In this paper, we compare two different methods for reward shaping; plan-based, in which an agent is provided with a plan and extra rewards are given according to the steps of the plan the agent satisfies, and reward shaping via abstract Markov decision process (MDPs), in which an abstract high-level MDP of the environment is solved and the resulting value function is used to shape the agent. The comparison is conducted in terms of total reward, convergence speed and scaling up to more complex environments. Empirical results demonstrate the need to correctly select and set up reward shaping methods according to the needs of the environment the agents are acting in. This leads to the more interesting question, is there a reward shaping method which is universally better than all other approaches regardless of the environment dynamics?\n\nTitle: Theoretical considerations of potential-based reward shaping for multi-agent systems\nContent: Potential-based reward shaping has previously been proven to both be equivalent to Q-table initialisation and guarantee policy invariance in single-agent reinforcement learning. The method has since been used in multi-agent reinforcement learning without consideration of whether the theoretical equivalence and guarantees hold. This paper extends the existing proofs to similar results in multi-agent systems, providing the theoretical background to explain the success of previous empirical studies. Specifically, it is proven that the equivalence to Q-table initialisation remains and the Nash Equilibria of the underlying stochastic game are not modified. Furthermore, we demonstrate empirically that potential-based reward shaping affects exploration and, consequentially, can alter the joint policy converged upon.\n\nTitle: Potential-based reward shaping for finite horizon online POMDP planning\nContent: In this paper, we address the problem of suboptimal behavior during online partially observable Markov decision process (POMDP) planning caused by time constraints on planning. Taking inspiration from the related field of reinforcement learning (RL), our solution is to shape the agent\u2019s reward function in order to lead the agent to large future rewards without having to spend as much time explicitly estimating cumulative future rewards, enabling the agent to save time to improve the breadth planning and build higher quality plans. Specifically, we extend potential-based reward shaping (PBRS) from RL to online POMDP planning. In our extension, information about belief states is added to the function optimized by the agent during planning. This information provides hints of where the agent might find high future rewards beyond its planning horizon, and thus achieve greater cumulative rewards. We develop novel potential functions measuring information useful to agent metareasoning in POMDPs (reflecting on agent knowledge and/or histories of experience with the environment), theoretically prove several important properties and benefits of using PBRS for online POMDP planning, and empirically demonstrate these results in a range of classic benchmark POMDP planning problems.\n\n## Task\nCompose an abstract for the title 'Theoretical and Empirical Analysis of Reward Shaping in Reinforcement Learning' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Reinforcement learning suffers scalability problems due to the state space explosion and the temporal credit assignment problem. Knowledge-based approaches have received a significant attention in the area. Reward shaping is a particular approach to incorporate domain knowledge into reinforcement learning. Theoretical and empirical analysis of this paper reveals important properties of this principle, especially the influence of the reward type, MDP discount factor, and the way of evaluating the potential function on the performance."
      }
    ]
  },
  {
    "tid":"228",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Multipath Routing to Provide Quality of Service in Mobile Ad Hoc Networks\nContent: Because of frequent topology changes and node failures, providing quality of service routing in mobile ad hoc networks becomes a very critical issue. The quality of service can be provided by routing the data along multiple paths. Such selection of multiple paths helps to improve reliability and load balancing, reduce delay introduced due to route rediscovery in presence of path failures. There are basically two issues in such a multipath routing. Firstly, the sender node needs to obtain the exact topology information. Since the nodes are continuously roaming, obtaining the exact topology information is a tough task. Here, we propose an algorithm which constructs highly accurate network topology with minimum overhead. The second issue is that the paths in the path set should offer best reliability and network throughput. This is achieved in two ways 1) by choice of a proper metric which is a function of residual power, traffic load on the node and in the surrounding medium 2) by allowing the reliable links to be shared between different paths.\n\nTitle: A medium access protocol exploiting multiuser-detection in CDMA ad-hoc networks\nContent: A new medium access protocol which exploits the physical layer capability of multiuser detection is proposed to help in improving the throughput/delay performance of ad-hoc networks. When more than one node has packets buffered for a common node in the neighborhood, all such nodes can simultaneously transmit their packets to the common receiver after reserving their surrounding channel. This is achieved in our protocol by extending the (sender-initiated) CSMA/CA collision avoidance framework by the receiver-initiated medium access technique and incorporating the transmission power control. We analyze the improvement in the throughput that can be achieved over the basic sender-initiated collision avoidance protocol in the network. Since the throughput improvement via multi-packet reception is influenced by the network layer activity as well, the performance of our protocol rolls back to that of the basic sender-initiated protocol in case of no coordination from the network layer. For the evaluation of performance of our protocol we simulate ad-hoc networks for different network topologies and traffic configurations. We observe the scheme to be capable in significantly improving the throughput/delay performance of the network.\n\nTitle: Energy Aware Topology Management in Ad Hoc Wireless Networks\nContent: Topology control for ad hoc networks is needed to conserve energy and increase the network life time. In this paper we propose a Residual Energy Adaptive Re-Routing(REAR) scheme in conjunction with a Mixed Integer Linear Programming (MILP) model for prolonging the lifetime of the network. The approach is based on the multi-commodity flow which involves routing several requests from the sources to the sinks in a given network by splitting and re-routing the non-optimal flows over multiple paths. The REAR algorithm ensures uniform consumption of energy across all the nodes in the network thereby increasing the lifetime of the network. This algorithm is compared with earlier works and results observed show that the MILP algorithm in conjunction with the REAR algorithm perform better than the existing algorithms.\n\nTitle: An Efficient Protocol to Study the Effect of Flooding on Energy Consumption in MANETS.\nContent: Mobile Ad-hoc network (MANET) comprises of mobile nodes which can communicate with each other through wireless channel. MANET is different from fixed wired networks, as the nodes are mobile, and they have a limited transmission range. A major issue with ad-hoc networks is energy consumption since node mobility is dependent on battery-operation. The mobility of the nodes introduces a new challenge to find the location of the node. Most of the MANET routing protocols rely on flooding for finding the location of the node. In this paper an attempt is made to study the effects of flooding on the energy consumption of the nodes. The comparison is made on the protocols DSDV, AODV, DSR which rely on flooding, with the Ring routing protocol which does not use flooding for finding the location of the node. The ring protocol introduces a new conceptual design, which does not depend on flooding nor uses location dependent identifiers for locating the node. The ring protocol is well suited for studying the effect of flooding on the energy consumption as this protocol does not use flooding. This paper covers the introduction and working of the protocol, and the results of comparison. The simulation is carded out using ns-2 simulator.\n\nTitle: Multiple Domain Secure Routing for Wireless Sensor Networks.\nContent:   Secure Transmission of data packets in Wireless Sensor Networks is an important area of Research. There is a possibility of an attacker creating security holes in the network. Hence, network security and reliability can be achieved by discovering random multiple paths using multiple domains, and forwarding data packets from the source node to the destination node. We have designed, Multiple Domain Routing with Overlap of Nodes (MDRON) and Multiple Domain Routing Without Overlap of Nodes (MDRWON) algorithms, in which packets follow multiple optimized paths simultaneously. The Special node algorithm searches the node which has maximum power and these nodes are used for transferring the packet from one domain to another domain. Simulation results using MATLAB shows that performance is better than Purely Random Propagation (PRP) and Non Repetitive Random Propagation(NRRP) Algorithms. \n\n## Task\nCompose an abstract for the title 'A Method for Resource and Service Discovery in MANETs' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The paper presents a novel resource and service discovery mechanism for MANETs using Routing Intelligent Mobile Agents(RIMAs). RIMAs periodically collect routing, resource and service availability information and index the same in appropriate RIMA nodes. Every mobile node is close to atleast one RIMA node. The mobile node running an application in need of a resource or a service dispatches a Discovery agent to its nearest RIMA node. The index available at the RIMA node will guide the Discoverer agent to the resource or service node. Analysis of the method reveals the computational complexity to be of the order of square of number of RIMA nodes. Communication overheads are the product of number of generated agents, agent size and number of hops travelled by an agent. The method has been simulated and tested on MANETs of varying sizes from 50 to 800 nodes. Simulation snapshots and results show the proper working and success of the algorithm."
      }
    ]
  },
  {
    "tid":"229",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Fast, memory efficient flow rate estimation using runs\nContent: Per-flow network traffic measurements are needed for effective network traffic management, network performance assessment, and detection of anomalous network events such as incipient denial-of-service (DoS) attacks. Explicit measurement of per-flow traffic statistics is difficult in backbone networks because tracking the possibly hundreds of thousands of flows needs correspondingly large high-speed memories. To reduce the measurement overhead, many previous papers have proposed the use of random sampling and this is also used in commercial routers (Cisco's NetFlow). Our goal is to develop a new scheme that has very low memory requirements and has quick convergence to within a pre-specified accuracy. We achieve this by use of a novel approach based on sampling two-runs to estimate per-flow traffic. (A flow has a two-run when two consecutive samples belong to the same flow). Sampling two-runs automatically biases the samples towards the larger flows thereby making the estimation of these sources more accurate. This biased sampling leads to significantly smaller memory requirement compared to random sampling schemes. The scheme is very simple to implement and performs extremely well.\n\nTitle: ACCEL-RATE: a faster mechanism for memory efficient per-flow traffic estimation\nContent: Per-flow network traffic measurement is an important component of network traffic management, network performance assessment, and detection of anomalous network events such as incipient DoS attacks. In [1], the authors developed a mechanism called RATE where the focus was on developing a memory efficient scheme for estimating per-flow traffic rates to a specified level of accuracy. The time taken by RATE to estimate the per-flow rates is a function of the specified estimation accuracy and this time is acceptable for several applications. However some applications, such as quickly detecting worm related activity or the tracking of transient traffic, demand faster estimation times. The main contribution of this paper is a new scheme called ACCEL-RATE that, for a specified level of accuracy, can achieve orders of magnitude decrease in per-flow rate estimation times. It achieves this by using a hashing scheme to split the incoming traffic into several sub-streams, estimating the per-flow traffic rates in each of the substreams and then relating it back to the original per-flow traffic rates. We show both theoretically and experimentally that the estimation time of ACCEL-RATE is at least one to two orders of magnitude lower than RATE without any significant increase in the memory size.\n\nTitle: Content Based Rate Estimation Using Lazy Membership Testing\nContent: Fast IP flow rate estimation has many potential applications in network management, monitoring, security, and traffic engi- neering. Recently, low cost and memory efficient techniques to accurately estimate flow-rates in real-time have been de- veloped. These techniques rely on flow definitions being con- strained to being subsets of the fields in the packet header mak- ing flow-membership tests relatively inexpensive. In this pa- per, we consider a more general flow-rate estimation problem where flow membership testing is non-trivial and may involve more complex processing such as packet-payload based tests. An example is to estimate the amount of traffic that contains a given set of patterns (e.g., virus or worm signatures). We design new flow estimation techniques to reduce the number of mem- bership tests. These techniques track pairs of arrivals that have the given property of interest and use lazy membership testing to avoid complex property testing unless absolutely necessary. The efficiency of the new schemes is evaluated by both analysis and simulation.\n\nTitle: Fast payload-based flow estimation for traffic monitoring and network security\nContent: Real-time IP flow estimation has many potential applications in network management, monitoring, security, and traffic engineering. Existing techniques typically rely on flow definitions being constrained as subsets of the fields in packet headers. This makes flow-membership tests relatively inexpensive. In this paper, we consider a more general flow estimation problem that needs complex packet-payload based tests for flow-membership. An example is to estimate traffic with common strings in the payload and detect potential virus signatures for early alarm generation. We develop a fast, memory efficient algorithm for solving this problem as a variant of the longest common subsequence problem. This is done via an application of Rabin fingerprinting in combination with Bloom Filters. Both analysis and simulation show the effectiveness of the developed method.\n\nTitle: A Simple Traffic Independent Scheme For Enabling Restoration Oblivious Routing Of Resilient Connections\nContent: Fast restoration is an important feature of both MPLS and optical networks. The main mechanism for achieving fast restoration is by locally routing around failures using pre-setup detour paths. Signaling and routing protocol extensions to implement this local bypass ability are currently being standardized. To make use of this ability, dynamic schemes that jointly route primary paths and all link detours for links used by the primary paths have been previously proposed. These schemes also permit sharing of reserved restoration capacity for achieving efficiency. However, this joint computation places a significantly larger computational load on the network elements than that imposed by the shortest path computation variants typically used for unprotected network connection routing. In this paper, we propose a new scheme that is operationally much simpler, shares capacity used for restoration, and permits the network to route the primary paths in a manner that is oblivious to restoration needs. Restoration of all carried traffic is guaranteed by a new link capacity partitioning scheme that maximizes the working capacity of the network without requiring any knowledge of the traffic that will be imposed on the network. Being traffic independent for a priori link capacity partitioning and being oblivious to restoration needs for on-line network routing makes this scheme operationally simple and desirable in the sense of placing no additional routing load on the constrained computing resources at the network nodes. To compute the link capacity partitions, we develop a fast combinatorial algorithm that uses only iterative shortest path computations, and is a fully polynomial time approximation scheme (FP-TAS), i.e., it achieves a (1 + epsilon)-factor approximation for any epsilon > 0 and runs in time polynomial in the input size and 1/epsilon. The approximation scheme also allows link detour paths to be hop constrained if needed so as to bound restoration latency in optical networks.\n\n## Task\nCompose an abstract for the title 'Runs based traffic estimator (RATE): a simple, memory efficient scheme for per-flow rate estimation' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Per-flow network traffic measurements are needed for effective network traffic management, network performance assessment, and detection of anomalous network events such as incipient DoS attacks. Explicit measurement of per-flow traffic statistics is difficult in backbone networks because tracking the possibly hundreds of thousands of flows needs correspondingly large high-speed memories. To reduce the measurement over- head, many previous papers have proposed the use of random sampling and this is also used in commercial routers (Cisco's NetFlow). Our goal is to develop a new scheme that has very low memory requirements and has quick convergence to within a pre-specified accuracy. We achieve this by use of a novel approach based on sampling two-runs to estimate per-flow traffic. (A flow has a two-run when two consecutive samples belong to the same flow). Sampling two-runs automatically biases the samples towards the larger flows thereby making the estimation of these sources more accurate. This biased sampling leads to significantly smaller memory requirement compared to random sampling schemes. The scheme is very simple to implement and performs extremely well. I. INTRODUCTION In this paper, we address the problem of accurate measure- ment of traffic in a packet network. Measurement of traffic is an important component for traffic management, accounting, detecting DoS attacks, and in traffic engineering (2), (3), (6). The traffic in the network can typically be classified into flows, and measurements are required on a per-flow basis. The definition of flows can be very flexible. Examples are specific application to application traffic characterized by 5-tuples in the IP packet header, all traffic destined toward a a destination network (characterized by a destination address prefix), etc. The standard approach used for measuring traffic is to sample the traffic arriving at the router, keep counts of the traffic arrivals on a per-flow basis and then use this counter to estimate the traffic. This approach has been suggested by the Real-Time Flow Measurement Working Group of the IETF. The main problem with this approach is scalability. If the number of flows are large, then keeping per-flow counts"
      }
    ]
  },
  {
    "tid":"230",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Personalized application enablement by web session analysis and multisource user profiling\nContent: Mastering knowledge of the user profile is one of the technical cornerstones for service providers who handle a large amount of end user service consumption data and are well positioned to dynamically infer user interest domains. This paper presents a holistic approach to service personalization by offering a means to gather a user's consumption data from different multimedia services, to create and track user profiles in real time, and to monetize these profiles through targeted content or other personalized service offers. We describe a multisource profiling engine that deals with both operator-controlled domains and over-the-top (OTT) applications. In particular, to cover the Web domain, we combine the multisource profiling engine with a deep packet inspection (DPI)-based keyword inference engine characterizing users' Web browsing sessions in terms of the most relevant keywords searched. \u00a9 2010 Alcatel-Lucent.\n\nTitle: Optimizing throughput in optical networks: The joint routing and power control problem\nContent: It is well established that physical layer impairments significantly affect the performance of optical networks. The management of these impairments is critical for successful transmission, and may significantly affect network layer routing decisions. Hence, the traditional divide-and-conquer layered approach is sub-optimal, which has led to work on cross-layer techniques for routing in optical networks. Apart from fiber loss, one critical physical layer impairment that limits the capacity of optical networks is fiber nonlinearity. Handling nonlinearity introduces significant complexity to the traditional cross-layer approaches. We formulate and solve a joint routing and power control problem to optimize the system throughput that takes into consideration both fiber loss and nonlinearity. The joint power control and routing problem considered is a nonlinear integer programming problem. By characterizing the feasible solution space of the power control problem, we find a set of universal power settings that transform the complex power control and routing problem into a constrained path routing problem. We then propose an efficient fully polynomial time approximation scheme to solve the constrained path routing problem. Simulation results show that our proposed algorithm significantly improves network throughput and greatly outperforms greedy heuristics by providing a guaranteed performance bound.\n\nTitle: Fast network re-optimization schemes for MPLS and optical networks\nContent: This paper presents algorithms for re-optimizing network routing in connection-oriented networks such as Multi-Protocol Label Switched (MPLS) networks. The objective in re-optimization is to allow the network to carry more traffic without adding capacity. The need for re-optimization arises because of dynamic connection routing where connections, such as bandwidth guaranteed Label Switched Paths (LSPs) in MPLS networks, are routed as they arrive one-by-one to the network. Continual dynamic routing leads to network inefficiencies due to the limited information available for routing and due to simple path selection algorithms often used to satisfy connection set-up time constraints. We present a re-optimization scheme, where the re-optimizer constantly monitors the network to determine if re-optimization will lead to sufficient network efficiency benefits. When sufficient benefits can be obtained, the re-optimizer computes the least cost set of connections which must be re-routed to attain the necessary network efficiency and then computes the routes for the connections to be re-routed. We develop efficient re-optimization algorithms and demonstrate by simulations that several network performance metrics are significantly improved by re-optimization.\n\nTitle: Preconfiguring IP-over-Optical Networks to Handle Router Failures and Unpredictable Traffic\nContent: Abstract\u2014 We consider the realization of traffic-oblivious rout- ing in IP-over-Optical networks where routers are interconnected over a switched optical backbone, also called IP-over-OTN (Optical Transport Network). The traffic-oblivious routing we consider is a scheme where incoming traffic is first distributed in a preset manner,to a set of intermediate nodes. The traffic is then routed from the intermediate nodes to the final destination. This splitting of the routing into two-phases simplifies network configuration significantly [8], [17]. In implementing this scheme, the first and second phase paths are realized at the optical layer with router packet grooming at a single intermediate node only. Studies like [10] indicate that IP routers are 200 times more unreliable than traditional carrier-grade switches and average 1219 minutes of down,time per year. Given this unreliability of routers, we consider how two-phase routing in IP-over-OTN can be made resilient against router node failures. We propose two different schemes for provisioning the optical layer to handle router node failures \u2010 one that is failure node independent and static, and the other that is failure node dependent and dynamic. We develop linear programming,formulations for both schemes and a fast combinatorial algorithm for the second scheme so as to maximize network throughput. In each case, we determine (i) the optimal distribution of traffic to various intermediate routers for both normal (no-failure) and failure conditions, and (ii) provisioning of optical layer circuits to provide the needed inter-router links. We evaluate the performance of the two router failure protection schemes (in terms of throughput) and compare it with that of unprotected routing. For our experiments, we use actual ISP network topologies collected for the Rocketfuel project. I. I NTRODUCTION\n\nTitle: A new method for analyzing feedback-based protocols with applications to engineering Web traffic over the Internet\nContent: Most of the studies of feedback-based flow and congestion control consider only persistent sources which always have data to send. However, with the rapid growth of Internet applications built on TCP/IP such as the World Wide Web and the standardization of traffic management schemes such as Available Bit Rate (ABR) in Asynchronous Transfer Mode (ATM) networks, it is essential to evaluate the performance of feedback-based protocols using traffic models which are specific to dominant applications. This paper presents a method for analysing feedback-based protocols with a Web-user-like input traffic where the source alternates between \"transfer\" periods followed by \"think\" periods. Our key results, which are presented for the TCP protocol, are:(1) The goodputs and the fraction of time that the system has some given number of transferring sources are insensitive to the distributions of transfer (file or page) sizes and think times except through the ratio of their means. Thus, apart from network round-trip times, only the ratio of average transfer sizes and think times of users need be known to size the network for achieving a specific quality of service.(2) The Engset model can be adapted to accurately compute goodputs for TCP and TCP over ATM, with different buffer management schemes. Though only these adaptations are given in the paper, the method based on the Engset model can be applied to analyze other feedback systems, such as ATM ABR, by finding a protocol specific adaptation. Hence, the method we develop is useful not only for analysing TCP using a source model significantly different from the commonly used persistent sources, but also can be useful for analysing other feedback schemes.(3) Comparisons of simulated TCP traffic to measured Ethernet traffic shows qualitatively similar autocorrelation when think times follow a Pareto distribution with infinite variance. Also, the simulated and measured traffic have long range dependence. In this sense our traffic model, which purports to be Web-user-like, also agrees with measured traffic.\n\n## Task\nCompose an abstract for the title 'iOptimize: A software capability for analyzing and optimizing connection-oriented data networks in real time.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper describes a service called iOptimize that analyzes and optimizes service providers' connection-oriented data networks. In these networks, online connection routing is used to set up connections quickly, but the simple path selection scheme and the limited information available for online routing can cause network capacity to be used inefficiently. This may result in the loss or disruption of user data transmissions, leading to a degraded quality of service (QoS) in a network capable of supporting much higher throughput. The objective of iOptimize is to detect and analyze such inefficiencies and to offset them by the occasional rerouting of a selected set of connections, while causing little or no disruption of the existing data services and network operations. By helping service providers support data transport services at the QoS levels desired, iOptimize lets them make the most of their existing network resources and helps them defer capital expenditure. Moreover, by tuning their networks to perform at optimal efficiency, it improves end users' experiences, resulting in lower churn/turnover and reduced maintenance costs. (C) 2005 Lucent Technologies Inc."
      }
    ]
  },
  {
    "tid":"231",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: On the complexity of pure-strategy nash equilibria in congestion and local-effect games\nContent: Congestion games are a fundamental class of noncooperative games possessing pure-strategy Nash equilibria. In the network version, each player wants to route one unit of flow on a path from her origin to her destination at minimum cost, and the cost of using an arc only depends on the total number of players using that arc. A natural extension is to allow for players sending different amounts of flow, which results in so-called weighted congestion games. While examples have been exhibited showing that pure-strategy Nash equilibria need not exist, we prove that it actually is strongly NP-hard to determine whether a given weighted network congestion game has a pure-strategy Nash equilibrium. This is true regardless of whether flow is unsplittable (has to be routed on a single path for each player) or not. A related family of games are local-effect games, where the disutility of a player taking a particular action depends on the number of players taking the same action and on the number of players choosing related actions. We show that the problem of deciding whether a bidirectional local-effect game has a pure-strategy Nash equilibrium is NP-complete, and that the problem of finding a pure-strategy Nash equilibrium in a bidirectional local-effect game with linear local-effect functions (for which the existence of a pure-strategy Nash equilibrium is guaranteed) is PLS-complete. The latter proof uses a tight PLS-reduction, which implies the existence of instances and initial states for which any sequence of selfish improvement steps needs exponential time to reach a pure-strategy Nash equilibrium.\n\nTitle: A geometric approach to the price of anarchy in nonatomic congestion games\nContent: We present a short, geometric proof for the price-of-anarchy results that have recently been established in a series of papers on selfish routing in multicommodity flow networks and on nonatomic congestion games. This novel proof also facilitates two new types of theoretical results: On the one hand, we give pseudo-approximation results that depend on the class of allowable cost functions. On the other hand, we derive stronger bounds on the inefficiency of equilibria for situations in which the equilibrium costs are within reasonable limits of the fixed costs. These tighter bounds help to explain empirical observations in vehicular traffic networks. Our analysis holds in the more general context of nonatomic congestion games, which provide the framework in which we describe this work.\n\nTitle: On the inefficiency of equilibria in congestion games\nContent: We present a short geometric proof for the price of anarchy results that have recently been established in a series of papers on selfish routing in multicommodity flow networks. This novel proof also facilitates two new types of results: On the one hand, we give pseudo-approximation results that depend on the class of allowable cost functions. On the other hand, we derive improved bounds on the inefficiency of Nash equilibria for situations in which the equilibrium travel times are within reasonable limits of the free-flow travel times. These tighter bounds help to explain empirical observations in vehicular traffic networks. Our analysis holds in the more general context of congestion games, which provides the framework in which we describe this work.\n\nTitle: A General Framework for Designing Approximation Schemes for Combinatorial Optimization Problems with Many Objectives Combined into One\nContent: In this paper, we propose a general framework for designing fully polynomial time approximation schemes for combinatorial\n optimization problems, in which more than one objective function are combined into one using any norm. The main idea is to\n exploit the approximate Pareto-optimal frontier for multi-criteria optimization problems. Using this approach, we obtain an\n FPTAS for a novel resource allocation problem, for the problem of scheduling jobs on unrelated parallel machines, and for\n the Santa Claus problem, when the number of agents/machines is fixed, for any norm, including the l\n \u2009\u221e\u2009-norm. Moreover, either FPTAS can be implemented in a manner so that the space requirements are polynomial in all input parameters.\n We also give approximation algorithms and hardness results for the resource allocation problem when the number of agents is\n not fixed.\n \n\nTitle: Selfish Routing in Capacitated Networks\nContent: According to Wardrop's first principle, agents in a congested network choose their routes selfishly, a behavior that is captured by the Nash equilibrium of the underlying noncooperative game. A Nash equilibrium does not optimize any global criterion per se, and so there is no apparent reason why it should be close to a solution of minimal total travel time, i.e., the system optimum. In this paper, we offer positive results on the efficiency of Nash equilibria in traffic networks. In contrast to prior work, we present results for networks with capacities and for latency functions that are nonconvex, nondifferentiable, and even discontinuous.The inclusion of upper bounds on arc flows has early been recognized as an important means to provide a more accurate description of traffic flows. In this more general model, multiple Nash equilibria may exist and an arbitrary equilibrium does not need to be nearly efficient. Nonetheless, our main result shows that the best equilibrium is as efficient as in the model without capacities. Moreover, this holds true for broader classes of travel cost functions than considered hitherto.\n\n## Task\nCompose an abstract for the title 'The complexity of welfare maximization in congestion games' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We investigate issues of complexity related to welfare maximization in congestion games. In particular, we provide a full classification of complexity results for the problem of finding a minimum cost solution to a congestion game, under the model of Rosenthal. We consider both network and general congestion games, and we examine several variants of the problem concerning the structure of the game and the properties of its associated cost functions. Many of these problem variants turn out to be NP-hard, and some are hard to approximate to within any finite factor, unless P = NP. We also identify several versions of the problem that are solvable in polynomial time. \u00a9 2011 Wiley Periodicals, Inc. NETWORKS, 2012 \u00a9 2012 Wiley Periodicals, Inc."
      }
    ]
  },
  {
    "tid":"232",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: User Intention Modeling in Web Applications Using Data Mining\nContent: The problem of inferring a user's intentions in Machine\u2013Human Interaction has been the key research issue for providing personalized experiences and services. In this paper, we propose novel approaches on modeling and inferring user's actions in a computer. Two linguistic features \u2013 keyword and concept features \u2013 are extracted from the semantic context for intention modeling. Concept features are the conceptual generalization of keywords. Association rule mining is used to find the proper concept of corresponding keyword. A modified Na\u00efve Bayes classifier is used in our intention modeling. Experimental results have shown that our proposed approach achieved 84% average accuracy in predicting user's intention, which is close to the precision (92%) of human prediction.\n\nTitle: CubeSVD: a novel approach to personalized Web search\nContent: As the competition of Web search market increases, there is a high demand for personalized Web search to conduct retrieval incorporating Web users' information needs. This paper focuses on utilizing clickthrough data to improve Web search. Since millions of searches are conducted everyday, a search engine accumulates a large volume of clickthrough data, which records who submits queries and which pages he/she clicks on. The clickthrough data is highly sparse and contains different types of objects (user, query and Web page), and the relationships among these objects are also very complicated. By performing analysis on these data, we attempt to discover Web users' interests and the patterns that users locate information. In this paper, a novel approach CubeSVD is proposed to improve Web search. The clickthrough data is represented by a 3-order tensor, on which we perform 3-mode analysis using the higher-order singular value decomposition technique to automatically capture the latent factors that govern the relations among these multi-type objects: users, queries and Web pages. A tensor reconstructed based on the CubeSVD analysis reflects both the observed interactions among these objects and the implicit associations among them. Therefore, Web search activities can be carried out based on CubeSVD analysis. Experimental evaluations using a real-world data set collected from an MSN search engine show that CubeSVD achieves encouraging search results in comparison with some standard methods.\n\nTitle: The good, the bad, and the ugly: uncovering novel research opportunities in social media mining.\nContent: Big data is ubiquitous and can only become bigger, which challenges traditional data mining and machine learning methods. Social media is a new source of data that is significantly different from conventional ones. Social media data are mostly user-generated, and are big, linked, and heterogeneous. We present the good, the bad and the ugly associated with the multi-faceted social media data and exemplify the importance of some original problems with real-world examples. We discuss bias in social media data, evaluation dilemma, data reduction, inferring invisible information, and big-data paradox. We illuminate new opportunities of developing novel algorithms and tools for data science. In our endeavor of employing the good to tame the bad with the help of the ugly, we deepen the understanding of ever growing and continuously evolving data and create innovative solutions with interdisciplinary and collaborative research of data science.\n\nTitle: Toward multidatabase mining: identifying relevant databases\nContent: Various tools and systems for knowledge discovery and data mining have been developed and are available for applications. However, when there are many databases, an immediate question is where one should start mining. It is not true that data mining is better the more databases there are. It is only true when the databases involved are relevant to the task at hand. By breaking away from the conventional data mining assumption that many databases should be joined into one, we argue that the first step for multidatabase mining is to identify databases that are most relevant to an application; without doing so, the mining process can be lengthy, aimless, and ineffective. A measure of relevance is thus proposed for mining tasks with the objective of finding patterns or regularities of certain attributes. An efficient algorithm for identifying relevant databases is described. Experiments are conducted to verify the measure's performance and to exemplify its application\n\nTitle: Query Selection Techniques for Efficient Crawling of Structured Web Sources\nContent: The high quality, structured data from Web structured sources is invaluable for many applications. Hidden Web databases are not directly crawlable by Web search engines and are only accessible through Web query forms or via Web service interfaces. Recent research efforts have been focusing on understanding these Web query forms. A critical but still largely unresolved question is: how to efficiently acquire the structured information inside Web databases through iteratively issuing meaningful queries? In this paper we focus on the central issue of enabling efficient Web database crawling through query selection, i.e. how to select good queries to rapidly harvest data records from Web databases. We model each structured Web database as a distinct attribute-value graph. Under this theoretical framework, the database crawling problem is transformed into a graph traversal one that follows \"relational\" links. We show that finding an optimal query selection plan is equivalent to finding a Minimum Weighted Dominating Set of the corresponding database graph, a well-known NP-Complete problem. We propose a suite of query selection techniques aiming at optimizing the query harvest rate. Extensive experimental evaluations over real Web sources and simulations over controlled database servers validate the effectiveness of our techniques and provide insights for future efforts in this\n\n## Task\nCompose an abstract for the title 'Mining \"Hidden phrase\" definitions from the web' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Keyword searching is the most common form of document search on the Web. Many Web publishers manually annotate the META tags and titles of their pages with frequently queried phrases in order to improve their placement and ranking. A \"hidden phrase\" is defined as a phrase that occurs in the META tag of a Web page but not in its body. In this paper we present an algorithm that mines the definitions of hidden phrases from the Web documents. Phrase definitions allow (i) publishers to find relevant phrases with high query frequency, and, (ii) search engines to test if the content of the body of a document matches the phrases. We use co-occurrence clustering and association rule mining algorithms to learn phrase definitions from high-dimensional data sets. We also provide experimental results."
      }
    ]
  },
  {
    "tid":"233",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Sensor modeling, probabilistic hypothesis generation, and robust localization for object recognition\nContent: In an effort to make object recognition efficient and accurate enough for real applications, we have developed three probabilistic techniques\u9a74sensor modeling, probabilistic hypothesis generation, and robust localization\u9a74which form the basis of a promising paradigm for object recognition. Our techniques effectively exploit prior knowledge to reduce the number of hypotheses that must be tested during recognition. Our recognition approach utilizes statistical constraints on the matches between image and model features. These statistical constraints are computed using a model of the entire sensing process\u9a74resulting in more realistic and tighter constraints on matches. The candidate hypotheses are pruned by probabilistic constraint satisfaction to select likely matches based on the image evidence and prior statistical constraints. The resulting hypotheses are ordered most-likely first for verification, thus minimizing unnecessary verifications. The reliability of the verification decision is significantly increased by the use of a robust localization algorithm. Our localization algorithm reliably locates objects despite partial occlusion and significant errors in initial location estimates. We have implemented these techniques in a system that recognizes polyhedral objects in range images. Our results demonstrate accurate recognition while greatly limiting the number of verifications.\n\nTitle: Model based recognition of specular objects using sensor models.\nContent: The authors present a model-based object recognition system for specular objects. Objects with specular surfaces present a problem for computer vision. Simulating object appearances by using the sensor model, and the object model allows us to predict specular features, and to analyze the detectability and reliability of each feature. The system generates a set of aspects of the object. By precompiling the aspects with the feature detectability and the feature reliability, the system prepares adaptable matching templates. At the runtime, an input image is first classified into a few candidate aspects. A deformable template matching finds the best match among them. This method is applicable to multiple objects simply by changing object and sensor models. Experimental results using two kinds of objects and sensors are presented: a TV image of a shiny object and a synthetic aperture radar (SAR) image of an airplane. The results show the flexibility of the proposed model based approach\n\nTitle: A Method to Obtain Sensing Requirements in Robotic Assemblies by Geometric Reasoning\nContent: This paper presents a method for determining sensing requirements for robotic assemblies from a geometrical analysis of critical contact-state transitions produced among mating parts during the execution of nominal assembly plans. The goal is to support the reduction of real-life uncertainty through the recognition of assembly tasks that require force and visual feedback operations. The assembly tasks are decomposed into assembly skill primitives based on transitions described on a taxonomy of contact relations. Force feedback operations are described as a set of force compliance skills which are systematically associated to the assembly skill primitives. To determine the visual feedback operations and the type of visual information needed, a backward propagation process of geometrical constraints is used. This process defines new visual feedback requirements for the tasks from the discovery of direct, and indirect, insertion and contact dependencies among the mating parts. A computational implementation of the method was developed and validated with test cases containing assembly tasks including all the combinations of sensing requirements. The program behave as expected in every case.\n\nTitle: Modeling sensors: toward automatic generation of object recognition program\nContent: One of the most important and systematic methods of building model-based vision systems is that of generating object recognition programs automatically from given geometric models. Automatic generation of object recognition programs requires several key components to be developed: object models to describe the geometric and photometric properties of the object to be recognized, sensor models to predict object appearances from the object model under a given sensor, strategy generation using the predicted appearances to produce a recognition strategy, and program generation converting the recognition strategy into an executable code. This paper concentrates on sensor modeling and its relationship to strategy generation, because we regard it as the bottleneck to automatic generation of object recognition programs. We consider two aspects of sensor characteristics: sensor detectability and sensor reliability. Sensor detectability specifies what kinds of featuers can be detected and under what conditions the features are detected; sensor reliability is a confidence level for the detected features. We define a configuration space to represent sensor characteristics. Then, we propose a representation method for sensor detectability and reliability in the configuration space. Finally, we investigate how to use the proposed sensor model in automatic generation of object recognition programs.\n\nTitle: The climbing sensor: 3-D modeling of a narrow and vertically stalky space by using spatio-temporal range image\nContent: In this paper, we propose a novel type of 3D scanning system named 'climbing sensor'. This system has been designed for scanning narrow and vertically stalky spaces, which are hard or extremely inefficient to scan by commercial laser range scanners due to their dimensions and limitation of FOVs. The climbing sensor equips a platform with two line scanners on a lift, and they scan through the whole target while the lift moves downwards along a ladder. One scanner is for scanning the target, which scans horizontally as the lift moves vertically, and the other scanner is for localizing the platform, which scans vertically. By using spatio-temporal range image acquired from the vertical scanning, we can accurately calculate the speed of the moving platform, with which a correct 3D model can be constructed from horizontal scans. We applied this scanning system to the Bayon Temple in Cambodia as a part of our digital archiving project of cultural assets. The scanning results proved that the system gives a sufficiently accurate 3D model and the effectiveness of our proposed system and speed estimating process.\n\n## Task\nCompose an abstract for the title 'Modeling sensor detectability with the VANTAGE geometric/sensor modeler' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"A G-source, an abstract sensor component representing either a light source or a TV camera, is defined. G-source illumination conditions are investigated to describe the condition under which each G-source illuminates (or observes) surface regions with respect to its illumination/observation directions. Sensor detectability is defined as a function that takes object boundaries and produces a set of points on the object from which the sensor can obtain useful data. It is shown that sensor detectability can be represented by G-source illumination conditions and/or operations between them. The way in which G-sources are combined is described by a sensor-composition tree in which leaf nodes represent G-source illumination conditions and branch nodes represent set operations. Applications of the sensor composition tree to object boundaries are investigated and a set of detectable points is obtained using the VANTAGE geometric modeler. Several 2D structures are proposed to represent these detected points given by VANTAGE. A brief overview of how to use these capabilities in building model-based vision systems is given"
      }
    ]
  },
  {
    "tid":"234",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Heart rate responses to synthesized affective spoken words\nContent: The present study investigated the effects of brief synthesized spoken words with emotional content on the ratings of emotions and heart rate responses. Twenty participants' heart rate functioning was measured while they listened to a set of emotionally negative, neutral, and positive words produced by speech synthesizers. At the end of the experiment, ratings of emotional experiences were also collected. The results showed that the ratings of the words were in accordance with their valence. Heart rate deceleration was significantly the strongest and most prolonged to the negative stimuli. The findings are the first suggesting that brief spoken emotionally toned words evoke a similar heart rate response pattern found earlier for more sustained emotional stimuli.\n\nTitle: Ballistocardiographic Responses to Dynamic Facial Displays of Emotion While Sitting on the EMFi Chair\nContent: The aim of the present paper was to study heart rate changes during a video stimulation depicting two actors (male and female) producing dynamic facial expressions of happiness, sadness, and a neutral expression. We measured ballistocardiographic emotion-related heart rate responses with an unobtrusive measurement device called the EMFi chair. Ratings of subjective responses to the video stimuli were also collected. The results showed that the video stimuli evoked significantly different ratings of emotional valence and arousal. Heart rate decelerated in response to all stimuli and the deceleration was the strongest during negative stimulation. Furthermore, stimuli from the male actor evoked significantly larger arousal ratings and heart rate responses than the stimuli from the female actor. The results also showed differential responding between female and male participants. The present results support the hypothesis that heart rate decelerates in response to films depicting dynamic negative facial expressions. The present results also support the idea that the EMFi chair can be used to perceive emotional responses from people while they are interacting with technology.\n\nTitle: The effects of emotionally worded synthesized speech on the ratings of emotions and voice quality\nContent: The present research investigated how the verbal content of synthetic messages affects participants' emotional responses and the ratings of voice quality. 28 participants listened to emotionally worded sentences produced by a monotonous and a prosodic tone of voice while the activity of corrugator supercilii facial muscle was measured. Ratings of emotions and voice quality were also collected. The results showed that the ratings of emotions were significantly affected by the emotional contents of the sentences. The prosodic tone of voice evoked more emotion-relevant ratings of arousal than the monotonous voice. Corrugator responses did not seem to reflect emotional reactions. Interestingly, the quality of the same voice was rated higher when the content of the sentences was positive as compared to the neutral and negative sentences. Thus, the emotional content of the spoken messages can be used to regulate users' emotions and to evoke positive feelings about the voices.\n\nTitle: Pupillary responses to emotionally provocative stimuli\nContent: This paper investigated in two experiments pupillary responses to emotionally provocative sound stimuli. In experiment one, 30 subjects' pupillary responses were measured while listening to 10 negatively and 10 positively arousing sounds, and 10 emotionally neutral sounds. In addition, the subjects rated their subjective experiences to these stimuli. The results showed that the pupil size was significantly larger after highly arousing positive and negative stimuli than after neutral stimuli with medium arousal.In experiment two, the contents of the stimuli were more controlled than in experiment one. 22 subjects' pupillary responses were measured while listening to four negatively and four positively arousing sounds, and four emotionally neutral sounds. The results showed that the pupil size was significantly larger during negative highly arousing stimuli than during moderately arousing positive stimuli. The pupil size was also significantly larger after highly arousing negative stimuli than after moderately arousing neutral and positive stimuli.The results of the two experiments suggest that pupil size discriminates during and after different kinds of emotional stimuli. Thus, the measurement of pupil size variation my be a potentially useful computer input signal, for example, for affective computing.\n\nTitle: Emotions and heart rate while sitting on a chair\nContent: New methods for unobtrusive monitoring of computer users' emotion psychophysiology are very much needed in human-computer interaction research. The present aim was to study heart rate changes during emotionally provocative stimulation. Six-second long auditory, visual, and audiovisual emotionally negative, neutral, and positive stimuli were presented to 24 participants. Heart rate responses were measured with a regular office chair embedded with electromechanical film (the EMFi chair) and with traditional earlobe photoplethysmography (PPG). Ratings of the stimuli were also collected. The results showed that the two heart rate measurements were significantly correlated, r = 0.99. In line with other studies the results showed that, in general, heart rate decelerated in response to emotional stimulation and it decelerated the most in response to negative stimuli as compared with responses to positive and neutral stimuli. Especially, emotional stimulation caused significant changes in heart rate at the 6th second from the stimulus onset. We suggest that the EMFi chair could be used in human-computer interaction for unobtrusive measurement of the user's emotional reactions.\n\n## Task\nCompose an abstract for the title 'Music, Heart Rate, and Emotions in the Context of Stimulating Technologies' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The present aim was to explore heart rate responses when stimulating participants with technology primarily aimed at the rehabilitation of older adults. Heart rate responses were measured from 31 participants while they listened to emotionally provoking negative, neutral, and positive musical clips. Ratings of emotional experiences were also collected. The results showed that heart rate responses to negative musical stimuli differed significantly from responses to neutral stimuli. The use of emotion-related physiological responses evoked by stimulating devices offers a possibility to enhance, for example, emotionally stimulating or otherwise therapeutic sessions."
      }
    ]
  },
  {
    "tid":"235",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Progressive Iso-Surface Extraction from Hierarchical 3D Meshes\nContent: A multiresolution data decompositionoffers a fundamentalframework supporting compression, progressive trans- mission, and level-of-detail (LOD) control for large two or three dimensional data sets discretized on complex meshes. In this paper we extend a previously presented algorithm for 3D mesh reduction for volume data based on multilevel finite element approximations in two ways. First, we present efficient data structures which allow to incrementally construct approximations of the volume data at lower or higher resolutions at interactive rates. An abstract description of the mesh hierarchy in terms of a coarse base mesh and a set of integer records offers a high compression potential which is essential for an efficient storage and a progressive network transmission. Based on this mesh hierarchy we then develop a new progressive iso-surface extraction algorithm. For a given iso-value, the corresponding iso-surface can be computed at different levels of resolution. Changing to a higher or coarser resolution will update the surface only in those regions where the volume data is being refined or coarsened. Our approach allows to interactively visualize very large scalar fields like medical data sets, whereas the conventional algorithms would have required at least an order of magnitude more resources.\n\nTitle: Isosurface extraction techniques for Web-based volume visualization\nContent: The reconstruction of isosurfaces from scalar volume data has positioned itself as a fundamental visualization technique in many different applications. But the dramatically increasing size of volumetric data sets often prohibits the handling of these models on affordable low-end single processor architectures. Distributed client-server systems integrating high-bandwidth transmission channels and Web-based visualization tools are one alternative to attack this particular problem, but therefore new approaches to reduce the load of numerical processing and the number of generated primitives are required. In this paper we outline different scenarios for distributed isosurface reconstruction from large-scale volumetric data sets. We demonstrate how to directly generate stripped surface representations and we introduce adaptive and hierarchical concepts to minimize the number of vertices that have to be reconstructed, transmitted and rendered. Furthermore, we propose a novel computation scheme, which allows the user to flexibly exploit locally available resources. The proposed algorithms have been merged together in order to build a platform-independent Web-based application. Extensive use of VRML and Java OpenGL-bindings allows for the exploration of large-scale volume data quite efficiently.\n\nTitle: Generation Of Mesh Variants Via Volumetrical Representation And Subsequent Mesh Optimisation\nContent: Having reliable finite element (FE) meshes is one of the basics of reliable FE simulations. As development times i.e. in the car industry are expected to decrease, engineers need to edit and optimise FE meshes without access to the underlying CAD geometry. If meshes are not only locally effected by the editing operation, simple mesh optimisations such as mesh relaxation or local remeshing are not sufficient to make the mesh suitable for numerical simulation again and global remeshing is needed. To avoid the traditionally used time-consuming remeshing strategy, we developed a tool to remesh an FE surface model - taking into account the needs for good FE meshes - via volumes. We first voxelise the surface and then generate a new quad mesh via isosurface extraction and subsequent mesh optimisation. This method provides the opportunity to directly couple editing operations on the volumetrical representation with the remeshing procedure.\n\nTitle: Hardware-Accelerated Reconstruction of Polygonal Isosurface Representations on Unstructured Grids\nContent: Volume visualization using isosurface extraction is a well-researched topic. Recent research demonstrated that even for unstructured grids peak performances of millions of tetrahedra per second can be achieved by exploiting the parallel processing capabilities of modern GPUs. In this paper we present a novel hardware-accelerated solution that further improves the extraction performance. In contrary to existing approaches, our technique explicitly extractsthe isosurface geometry in a fragment program by rendering only a single screen-sized quadrilateral. The extracted geometry is directly written to an on-board graphics memory object allowing for direct rendering without further bus transfers. Additionally, the geometry can be manipulated by shader programs or read back to the application for further processing. Examples and application scenarios are given that can benefit from our approach.\n\nTitle: Real-time exploration of regular volume data by adaptive reconstruction of isosurfaces\nContent: Recent advances in the technology of 3D sensors and in the perfor- mance of numerical simulations result in the generation of volume data at ever growing size. In order to allow real-time exploration of even the highest resolution data sets, adaptive techniques benefit- ing from the hierarchical nature of multi-resolution representations have gained special attention. In this paper we propose an adap- tive approach to the fast reconstruction of iso-surfaces from regular volume data at arbitrary levels of detail. The algorithm has been de- signed to enable real-time navigation through complex structures while providing user-adjustable resolution levels. Since adaptive on-the-fly reconstruction and rendering is performed from a hier- archical octree representation of the volume data, the method does not depend on pre-processing with respect to a specific iso-value thus allowing the user to interactively browse through the set of all possible iso-surfaces. Special attention is paid to the fixing of cracks in the surface where the adaptive reconstruction level changes and to the efficient estimation of the iso-surface' s curvature.\n\n## Task\nCompose an abstract for the title 'Mesh Generation from Layered Depth Images Using Isosurface Raycasting.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper presents an approach for the fast generation of meshes from Layered Depth Images (LDI), a representation that is independent of the underlying data structure and widely used in image-based rendering. LDIs can be quickly generated from high-quality, yet computationally expensive isosurface raycasters that are available for a wide range of different types of data. We propose a fast technique to extract meshes from one or several LDIs which can then be rendered for fast, yet high-quality analysis with comparatively low hardware requirements. To further improve quality, we also investigate mesh geometry merging and adaptive refinement, both for triangle and quad meshes. Quality and performance are evaluated using simulation data and analytic functions."
      }
    ]
  },
  {
    "tid":"236",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Prioritizing JUnit Test Cases: An Empirical Assessment and Cost-Benefits Analysis\nContent: Test case prioritization provides a way to run test cases with the highest priority earliest. Numerous empirical studies have shown that prioritization can improve a test suite's rate of fault detection, but the extent to which these results generalize is an open question because the studies have all focused on a single procedural language, C, and a few specific types of test suites. In particular, Java and the JUnit testing framework are being used extensively to build software systems in practice, and the effectiveness of prioritization techniques on Java systems tested under JUnit has not been investigated. We have therefore designed and performed a controlled experiment examining whether test case prioritization can be effective on Java programs tested under JUnit, and comparing the results to those achieved in earlier studies. Our analyses show that test case prioritization can significantly improve the rate of fault detection of JUnit test suites, but also reveal differences with respect to previous studies that can be related to the language and testing paradigm. To investigate the practical implications of these results, we present a set of cost-benefits models for test case prioritization, and show how the effectiveness differences observed can result in savings in practice, but vary substantially with the cost factors associated with particular testing processes.\n\nTitle: Test Case Prioritization: A Family of Empirical Studies\nContent: To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One potential goal of such prioritization is to increase a test suite's rate of fault detection. Previous work reported results of studies that showed that prioritization techniques can significantly improve rate of fault detection. Those studies, however, raised several additional questions: 1) Can prioritization techniques be effective when targeted at specific modified versions; 2) what trade-offs exist between fine granularity and coarse granularity prioritization techniques; 3) can the incorporation of measures of fault proneness into prioritization techniques improve their effectiveness? To address these questions, we have performed several new studies in which we empirically compared prioritization techniques using both controlled experiments and case studies. The results of these studies show that each of the prioritization techniques considered can improve the rate of fault detection of test suites overall. Fine-granularity techniques typically outperformed coarse-granularity techniques, but only by a relatively small margin overall; in other words, the relative imprecision in coarse-granularity analysis did not dramatically reduce coarse-granularity techniques' ability to improve rate of fault detection. Incorporation of fault-proneness techniques produced relatively small improvements over other techniques in terms of rate of fault detection, a result which ran contrary to our expectations. Our studies also show that the relative effectiveness of various techniques can vary significantly across target programs. Furthermore, our analysis shows that whether the effectiveness differences observed will result in savings in practice varies substantially with the cost factors associated with particular testing processes. Further work to understand the sources of this variance and to incorporate such understanding into prioritization techniques and the choice of techniques would be beneficial.\n\nTitle: Test Case Prioritization: An Empirical Study\nContent: Test case prioritization techniques schedule test cases for execution in an order that attempts to maximize some objective function. A variety of objective functions are applicable; one such function involves rate of fault detection --- a measure of how quickly faults are detected within the testing process. An improved rate of fault detection during regression testing can provide faster feedback on a system under regression test and let debuggers begin their work earlier than might otherwise be possible. In this paper, we describe several techniques for prioritizing test cases and report our empirical results measuring the effectiveness of these techniques for improving rate of fault detection. The results provide insights into the tradeoffs among various techniques for test case prioritization.\n\nTitle: On the Use of Mutation Faults in Empirical Assessments of Test Case Prioritization Techniques\nContent: Regression testing is an important activity in the software life cycle, but it can also be very expensive. To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One potential goal of test case prioritization techniques is to increase a test suite's rate of fault detection (how quickly, in a run of its test cases, that test suite can detect faults). Previous work has shown that prioritization can improve a test suite's rate of fault detection, but the assessment of prioritization techniques has been limited primarily to hand-seeded faults, largely due to the belief that such faults are more realistic than automatically generated (mutation) faults. A recent empirical study, however, suggests that mutation faults can be representative of real faults and that the use of hand-seeded faults can be problematic for the validity of empirical results focusing on fault detection. We have therefore designed and performed two controlled experiments assessing the ability of prioritization techniques to improve the rate of fault detection of test case prioritization techniques, measured relative to mutation faults. Our results show that prioritization can be effective relative to the faults considered, and they expose ways in which that effectiveness can vary with characteristics of faults and test suites. More importantly, a comparison of our results with those collected using hand-seeded faults reveals several implications for researchers performing empirical studies of test case prioritization techniques in particular and testing techniques in general\n\nTitle: Incorporating varying test costs and fault severities into test case prioritization\nContent: Test case prioritization techniques schedule test cases for regression testing in an order that increases their ability to meet some performance goal. One performance goal, rate of fault detection, measures how quickly faults are detected within the testing process. In previous work we provided a metric, APFD, for measuring rate of fault detection, and techniques for prioritizing test cases to improve APFD, and reported the results of experiments using those techniques. This metric and these techniques, however, applied only in cases in which test costs and fault severity are uniform. In this paper, we present a new metric for assessing the rate of fault detection of prioritized test cases, that incorporates varying test case and fault costs. We present the results of a case study illustrating the application of the metric. This study raises several practical questions that might arise in applying test case prioritization; we discuss how practitioners could go about answering these questions.\n\n## Task\nCompose an abstract for the title 'Bridging the gap between the total and additional test-case prioritization strategies' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In recent years, researchers have intensively investigated various topics in test-case prioritization, which aims to re-order test cases to increase the rate of fault detection during regression testing. The total and additional prioritization strategies, which prioritize based on total numbers of elements covered per test, and numbers of additional (not-yet-covered) elements covered per test, are two widely-adopted generic strategies used for such prioritization. This paper proposes a basic model and an extended model that unify the total strategy and the additional strategy. Our models yield a spectrum of generic strategies ranging between the total and additional strategies, depending on a parameter referred to as the p value. We also propose four heuristics to obtain differentiated p values for different methods under test. We performed an empirical study on 19 versions of four Java programs to explore our results. Our results demonstrate that wide ranges of strategies in our basic and extended models with uniform p values can significantly outperform both the total and additional strategies. In addition, our results also demonstrate that using differentiated p values for both the basic and extended models with method coverage can even outperform the additional strategy using statement coverage."
      }
    ]
  },
  {
    "tid":"237",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: TinyDB: an acquisitional query processing system for sensor networks\nContent: We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices.\n\nTitle: Approximating sensor network queries using in-network summaries\nContent: In this work we present new in-network techniques for communication efficient approximate query processing in wireless sensor nets. We use a model-based approach that constructs and maintains a spanning tree within the network, rooted at the base station. The tree maintains compressed summary information for each link that is used to ldquostub outrdquo traversal during query processing. Our work is based on a formal model of the in-network tree construction task framed as an optimization problem.We demonstrate hardness results for that problem, and develop efficient approximation algorithms for subtasks that are too expensive to compute exactly. We also propose efficient heuristics to accommodate a wider set of workloads, and empirically evaluate their performance and sensitivity to model changes.\n\nTitle: Model-based approximate querying in sensor networks\nContent: Declarative queries are proving to be an attractive paradigm for inter- acting with networks of wireless sensors. The metaphor that \"the sensornet is a database\" is problematic, however, because sensors do not exhaustively represent the data in the real world. In order to map the raw sensor readings onto physical reality, a model of that reality is required to complement the readings. In this ar- ticle, we enrich interactive sensor querying with statistical modeling techniques. We demonstrate that such models can help provide answers that are both more meaningful, and, by introducing approximations with probabilistic confidences, significantly more efficient to compute in both time and energy. Utilizing the com- bination of a model and live data acquisition raises the challenging optimization problem of selecting the best sensor readings to acquire, balancing the increase in the confidence of our answer against the communication and data acquisition costs in the network. We describe an exponential time algorithm for finding the optimal solution to this optimization problem, and a polynomial-time heuristic for identi- fying solutions that perform well in practice. We evaluate our approach on several real-world sensor-network data sets, taking into account the real measured data and communication quality, demonstrating that our model-based approach pro- vides a high-fidelity representation of the real phenomena and leads to significant performance gains versus traditional data acquisition techniques.\n\nTitle: Multiresolution Cube Estimators for Sensor Network Aggregate Queries\nContent:   In this work we present in-network techniques to improve the efficiency of spatial aggregate queries. Such queries are very common in a sensornet setting, demanding more targeted techniques for their handling. Our approach constructs and maintains multi-resolution cube hierarchies inside the network, which can be constructed in a distributed fashion. In case of failures, recovery can also be performed with in-network decisions. In this paper we demonstrate how in-network cube hierarchies can be used to summarize sensor data, and how they can be exploited to improve the efficiency of spatial aggregate queries. We show that query plans over our cube summaries can be computed in polynomial time, and we present a PTIME algorithm that selects the minimum number of data requests that can compute the answer to a spatial query. We further extend our algorithm to handle optimization over multiple queries, which can also be done in polynomial time. We discuss enriching cube hierarchies with extra summary information, and present an algorithm for distributed cube construction. Finally we investigate node and area failures, and algorithms to recover query results. \n\nTitle: Model-Driven Data Acquisition in Sensor Networks\nContent: Declarative queries are proving to be an attractive paradigm for in- teracting with networks of wireless sensors. The metaphor that \"the sensornet is a database\" is problematic, however, because sensors do not exhaustively represent the data in the real world. In order to map the raw sensor readings onto physical reality, a model of that reality is required to complement the readings. In this paper, we enrich interactive sensor querying with statistical modeling tech- niques. We demonstrate that such models can help provide answers that are both more meaningful, and, by introducing approximations with probabilistic confidences, significantly more efficient to com- pute in both time and energy. Utilizing the combination of a model and live data acquisition raises the challenging optimization prob- lem of selecting the best sensor readings to acquire, balancing the increase in the confidence of our answer against the communication and data acquisition costs in the network. We describe an expo- nential time algorithm for finding the optimal solution to this op- timization problem, and a polynomial-time heuristic for identifying solutions that perform well in practice. We evaluate our approach on several real-world sensor-network data sets, taking into account the real measured data and communication quality, demonstrating that our model-based approach provides a high-fidelity representation of the real phenomena and leads to significant performance gains ver- sus traditional data acquisition techniques.\n\n## Task\nCompose an abstract for the title 'The design of an acquisitional query processor for sensor networks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices."
      }
    ]
  },
  {
    "tid":"238",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Information Lower Bounds via Self-reducibility.\nContent: We use self-reduction methods to prove strong information lower bounds on two of the most studied functions in the communication complexity literature: Gap Hamming Distance (GHD) and Inner Product (IP). In our first result we affirm the conjecture that the information cost of GHD is linear even under the uniform distribution, which strengthens the \u03a9(n) bound recently shown by Kerenidis et al. (2012), and answers an open problem from Chakrabarti et al. (2012). In our second result we prove that the information cost of IPn is arbitrarily close to the trivial upper bound n as the permitted error tends to zero, again strengthening the \u03a9(n) lower bound recently proved by Braverman and Weinstein (Electronic Colloquium on Computational Complexity (ECCC) 18, 164 2011). Our proofs demonstrate that self-reducibility makes the connection between information complexity and communication complexity lower bounds a two-way connection. Whereas numerous results in the past (Chakrabarti et al. 2001; Bar-Yossef et al. J. Comput. Syst. Sci. 68(4), 702---732 2004; Barak et al. 2010) used information complexity techniques to derive new communication complexity lower bounds, we explore a generic way in which communication complexity lower bounds imply information complexity lower bounds in a black-box manner.\n\nTitle: A discrepancy lower bound for information complexity\nContent: This paper provides the first general technique for proving information lower bounds on two-party unbounded-rounds communication problems. We show that the discrepancy lower bound, which applies to randomized communication complexity, also applies to information complexity. More precisely, if the discrepancy of a two-party function f with respect to a distribution $$\\\\mu $$\u03bc is $$Disc_\\\\mu f$$Disc\u03bcf, then any two party randomized protocol computing f must reveal at least $$\\\\Omega (\\\\log (1/Disc_\\\\mu f))$$\u03a9(log(1/Disc\u03bcf)) bits of information to the participants. As a corollary, we obtain that any two-party protocol for computing a random function on $$\\\\{0,1\\\\}^n\\\\times \\\\{0,1\\\\}^n$${0,1}n\u00d7{0,1}n must reveal $$\\\\Omega (n)$$\u03a9(n) bits of information to the participants. In addition, we prove that the discrepancy of the Greater-Than function is $$\\\\Omega (1/\\\\sqrt{n})$$\u03a9(1/n), which provides an alternative proof to the recent proof of Viola (Proceedings of the twenty-fourth annual ACM-SIAM symposium on discrete algorithms, SODA 2013, New Orleans, LA, USA, 6---8 Jan 2013, pp 632---651, 2013) of the $$\\\\Omega (\\\\log n)$$\u03a9(logn) lower bound on the communication complexity of this well-studied function and, combined with our main result, proves the tight $$\\\\Omega (\\\\log n)$$\u03a9(logn) lower bound on its information complexity. The proof of our main result develops a new simulation procedure that may be of an independent interest. In a followup breakthrough work of Kerenidis et al. (53rd annual IEEE symposium on foundations of computer science, FOCS 2012, New Brunswick, NJ, USA, 20---23 Oct 2012, pp 500---509, 2012), our simulation procedure served as a building block towards a proof that almost all known lower bound techniques for communication complexity (and not just discrepancy) apply to information complexity as well.\n\nTitle: Near-optimal bounds on bounded-round quantum communication complexity of disjointness.\nContent: We prove a near optimal round-communication trade off for the two-party quantum communication complexity of disjointness. For protocols with r rounds, we prove a lower bound of &#x03A9;(n/r) on the communication required for computing disjointness of input size n, which is optimal up to logarithmic factors. The previous best lower bound was &#x03A9;(n/r2) due to Jain, Radha krishnan and Sen. Along the way, we develop several tools for quantum information complexity, one of which is a lower bound for quantum information complexity in terms of the generalized discrepancy method. As a corollary, we get that the quantum communication complexity of any boolean function f is at most 2O(QIC(f)), where QIC(f) is the prior-free quantum information complexity of f (with error 1/3).\n\nTitle: A Rounds vs. Communication Tradeoff for Multi-Party Set Disjointness\nContent: In the set disjointess problem, we have k players, each with a private input X\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i</sup>\n \u2286 [n], and the goal is for the players to determine whether or not their sets have a global intersection. The players communicate over a shared blackboard, and we charge them for each bit that they write on the board. We study the trade-off between the number of interaction rounds we allow the players, and the total number of bits they must send to solve set disjointness. We show that if R rounds of interaction are allowed, the communication cost is \u03a9\u0303(nk\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1/R</sup>\n/R\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">4</sup>\n), which is nearly tight. We also leverage our proof to show that wellfare maximization with unit demand bidders cannot be solved efficiently in a small number of rounds: here, we have k players bidding on n items, and the goal is to find a matching between items and player that bid on them which approximately maximizes the total number of items assigned. It was previously shown by Alon et. al. that \u03a9(log log k) rounds of interaction are required to find an assignment which achieves a constant approximation to the maximum-wellfare assignment, even if each player is allowed to write n\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\u03f5(R)</sup>\n bits on the board in each round, where \u03f5(R) = exp(-R). We improve this lower bound to \u03a9(log k/log log k), which is known to be tight up to a log log k factor.\n\nTitle: Direct Sums in Randomized Communication Complexity\nContent: We prove a direct sum theorem for randomized communication complexity. Ignoring loga- rithmic factors, our results show that: \u2022 Computing n copies of a function requires p n times the communication. \u2022 For average case complexity, given any distribution \u00b5 on inputs, computing n copies of the function on n independent inputs sampled according to \u00b5 requires p n times the commu- nication for computing one copy. \u2022 If \u00b5 is a product distribution, computing n copies on n independent inputs sampled ac- cording to \u00b5 requires n times the communication. We also study the complexity of computing the parity of n evaluations of f, and obtain results analogous to those above. Our results are obtained by designing new compression schemes that can compress the com- munication in interactive processes that do not reveal too much information about their inputs. This generalizes the notion of traditional compression, which can be viewed as compressing protocols that involve only one way communication.\n\n## Task\nCompose an abstract for the title 'Communication Lower Bounds for Statistical Estimation Problems via a Distributed Data Processing Inequality.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We study the tradeoff between the statistical error and communication cost of distributed statistical estimation problems in high dimensions. In the distributed sparse Gaussian mean estimation problem, each of the m machines receives n data points from a d-dimensional Gaussian distribution with unknown mean \u03b8 which is promised to be k-sparse. The machines communicate by message passing and aim to estimate the mean \u03b8. We provide a tight (up to logarithmic factors) tradeoff between the estimation error and the number of bits communicated between the machines. This directly leads to a lower bound for the distributed sparse linear regression problem: to achieve the statistical minimax error, the total communication is at least \u03a9(min{n,d}m), where n is the number of observations that each machine receives and d is the ambient dimension. These lower results improve upon Shamir (NIPS'14) and Steinhardt-Duchi (COLT'15) by allowing multi-round iterative communication model. We also give the first optimal simultaneous protocol in the dense case for mean estimation. As our main technique, we prove a distributed data processing inequality, as a generalization of usual data processing inequalities, which might be of independent interest and useful for other problems."
      }
    ]
  },
  {
    "tid":"239",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A survey on ear biometrics\nContent: Recognizing people by their ear has recently received significant attention in the literature. Several reasons account for this trend: first, ear recognition does not suffer from some problems associated with other non-contact biometrics, such as face recognition; second, it is the most promising candidate for combination with the face in the context of multi-pose face recognition; and third, the ear can be used for human recognition in surveillance videos where the face may be occluded completely or in part. Further, the ear appears to degrade little with age. Even though current ear detection and recognition systems have reached a certain level of maturity, their success is limited to controlled indoor conditions. In addition to variation in illumination, other open research problems include hair occlusion, earprint forensics, ear symmetry, ear classification, and ear individuality. This article provides a detailed survey of research conducted in ear detection and recognition. It provides an up-to-date review of the existing literature revealing the current state-of-art for not only those who are working in this area but also for those who might exploit this new approach. Furthermore, it offers insights into some unsolved ear recognition problems as well as ear databases available for researchers.\n\nTitle: On the use of multispectral conjunctival vasculature as a soft biometric\nContent: Ocular biometrics has made significant progress over the past decade primarily due to advances in iris recognition. Initial research in the field of iris recognition focused on the acquisition and processing of frontal irides which may require considerable subject cooperation. However, when the iris is off-angle with respect to the acquisition device, the sclera (the white part of the eye) is exposed. The sclera is covered by a thin transparent layer called conjunctiva. Both the episclera and conjunctiva contain blood vessels that are observable from the outside. In this work, these blood vessels are referred to as conjunctival vasculature. Iris patterns are better observed in the near infrared spectrum while conjunctival vasculature is better seen in the visible spectrum. Therefore, multispectral (i.e., color-infrared) images of the eye are acquired to allow for the combination of the iris biometric with the conjunctival vasculature. The paper focuses on conjunctival vasculature enhancement, registration and matching. Initial results are promising and suggest the need for further investigation of this biometric in a bimodal configuration with iris.\n\nTitle: Biometric Sensor Interoperability: A Case Study in Fingerprints\nContent: The problem of biometric sensor interoperability has received limited attention in the literature. Most biometric systems operate under the assumption that the data (viz., images) to be compared are obtained using the same sensor and, hence, are restricted in their ability to match or compare biometric data originating from different sensors. Although progress has been made in the development of common data exchange formats to facilitate the exchange of feature sets between vendors, very little effort has been invested in the actual development of algorithms and techniques to match these feature sets. In the Fingerprint Verification Competition (FVC 2002), for example, the evaluation protocol only matched images originating from the same sensor although fingerprint data from 3 different commercial sensors was available. This is an indication of the difficulty in accommodating sensor interoperability in biometric systems. In this paper we discuss this problem and present a case study involving two different fingerprint sensors.\n\nTitle: Biometric classifier update using online learning: A case study in near infrared face verification\nContent: The performance of a large scale biometric system may deteriorate over time as new individuals are continually enrolled. To maintain an acceptable level of performance, the classifier has to be re-trained offline in batch mode using both existing and new data. The process of re-training can be computationally expensive and time consuming. This paper presents a new biometric classifier update algorithm that incrementally re-trains the classifier using online learning and progressively establishes a decision hyperplane for improved classification. The proposed algorithm incorporates soft labels and granular computing in the formulation of a 2 \u03bd -Online Granular Soft Support Vector Machine (SVM) to re-train the classifier using only the new data. Granular computing makes it adaptive to local and global variations in data distribution, while soft labels provide resilience to noise. Each time data is acquired, new support vectors that are linearly independent are added and existing support vectors that do not improve the classifier performance are removed. This constrains the size of the support vectors and significantly reduces the training time without compromising the classification accuracy. The efficacy of the proposed online learning strategy is validated in a near infrared face verification application involving different covariates. The results obtained on a heterogeneous near infrared face database of 328 subjects show that in all experiments using different feature extraction and classification algorithms the proposed online 2 \u03bd -Granular Soft Support Vector Machine learning approach is 2\u20133 times faster while achieving a high level of accuracy similar to offline training using all data.\n\nTitle: Human Recognition Using Biometrics: An Overview\nContent: Establishing identity is becoming critical in our vastly interconnected society. Questions such as \"Is she really who she claims to be?\", \"Is this person authorized to use this facility?\", or \"Is he in the watchlist posted by the government? \" are routinely being posed in a variety of scenarios ranging from issuing a drivers license to gaining entry into a country. The need for reliable user authentication techniques has increased in the wake of heightened concerns about security and rapid advancements in networking, communication, and mobility Biometrics, described as the science of recognizing an individual based on his or her physical or behavioral traits, is beginning to gain acceptance as a legitimate method for determining an individuals identity. Biometric systems have now been deployed in various commercial, civilian, and forensic applications as a means of establishing identity. This paper presents an overview of biometrics and discusses some of the pertinent terminology necessary to understand this technology. The importance of information fusion in the context of biometrics is also highlighted. It is becoming increasingly apparent that multibiometric systems will play a significant role in dispensing the role of identity management in the 21(st) century.\n\n## Task\nCompose an abstract for the title 'Towards understanding the symmetry of human ears: A biometric perspective' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, an analysis of the symmetry of human ears is presented. Such an analysis is essential in order to understand the possibility of matching the left and right ears of an individual, or to reconstruct portions of the ear that may be occluded in a surveillance video. Ear symmetry is assessed geometrically using symmetry operators and Iannarelli's measurements, where the contribution of individual ear regions to the overall symmetry of the ear is studied. Next, to assess the ear symmetry (or asymmetry) from a biome\u0301trie recognition system perspective, several experiments were conducted on the WVU Ear Database. Our experiments suggest the existence of some degree of symmetry in the human ears that can perhaps be systematically exploited in the design of commercial ear recognition systems. At the same time, the degree of asymmetry it offers may be used in designing effective fusion schemes that combine the face information with the two ears."
      }
    ]
  },
  {
    "tid":"240",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Intrusion as (anti)social communication: characterization and detection\nContent: A reasonable definition of intrusion is: entering a community to which one does not belong. This suggests that in a network, intrusion attempts may be detected by looking for communication that does not respect community boundaries. In this paper, we examine the utility of this concept for identifying malicious network sources. In particular, our goal is to explore whether this concept allows a core-network operator using flow data to augment signature-based systems located at network edges. We show that simple measures of communities can be defined for flow data that allow a remarkably effective level of intrusion detection simply by looking for flows that do not respect those communities. We validate our approach using labeled intrusion attempt data collected at a large number of edge networks. Our results suggest that community-based methods can offer an important additional dimension for intrusion detection systems.\n\nTitle: Diagnosing network-wide traffic anomalies\nContent: Anomalies are unusual and significant changes in a network's traffic levels, which can often span multiple links. Diagnosing anomalies is critical for both network operators and end users. It is a difficult problem because one must extract and interpret anomalous patterns from large amounts of high-dimensional, noisy data.In this paper we propose a general method to diagnose anomalies. This method is based on a separation of the high-dimensional space occupied by a set of network traffic measurements into disjoint subspaces corresponding to normal and anomalous network conditions. We show that this separation can be performed effectively by Principal Component Analysis.Using only simple traffic measurements from links, we study volume anomalies and show that the method can: (1) accurately detect when a volume anomaly is occurring; (2) correctly identify the underlying origin-destination (OD) flow which is the source of the anomaly; and (3) accurately estimate the amount of traffic involved in the anomalous OD flow.We evaluate the method's ability to diagnose (i.e., detect, identify, and quantify) both existing and synthetically injected volume anomalies in real traffic from two backbone networks. Our method consistently diagnoses the largest volume anomalies, and does so with a very low false alarm rate.\n\nTitle: Characterization of network-wide anomalies in traffic flows\nContent: Detecting and understanding anomalies in IP networks is an open and ill-defined problem. Toward this end, we have recently proposed the subspace method for anomaly diagnosis. In this paper we present the first large-scale exploration of the power of the subspace method when applied to flow traffic. An important aspect of this approach is that it fuses information from flow measurements taken throughout a network. We apply the subspace method to three different types of sampled flow traffic in a large academic network: multivariate timeseries of byte counts, packet counts, and IP-flow counts. We show that each traffic type brings into focus a different set of anomalies via the subspace method. We illustrate and classify the set of anomalies detected. We find that almost all of the anomalies detected represent events of interest to network operators. Furthermore, the anomalies span a remarkably wide spectrum of event types, including denial of service attacks (single-source and distributed), flash crowds, port scanning, downstream traffic engineering, high-rate flows, worm propagation, and network outage.\n\nTitle: Detecting Unusually-Routed ASes: Methods and Applications.\nContent: The routes used in the Internet's interdomain routing system are a rich information source that could be exploited to answer a wide range of questions. However, analyzing routes is difficult, because the fundamental object of study is a set of paths. In this paper we present new analysis tools -- metrics and methods -- for analyzing AS paths, and apply them to study interdomain routing in the Internet over a recent 13-year period. Our goal is to develop a quantitative understanding of changes in Internet routing at the micro level (of individual ASes) as well as at the macro level (of the set of all ASes). To that end we equip an existing metri (Routing State Distance) with a new set of tools for identifying and characterizing unusually-routed ASes. At the micro level, we use our tools to identify clusters of ASes that have the most unusual routing at each time (interestingly, such clusters often correspond to sets of jointly-owned ASes). We also show that analysis of individual ASes can expose busis and engineering strategies of the organizations owning the ASes. These strategies are often related to content delivery or service replication. At the macro level, we show that ASes with the most unusual routing define discernible and interpretable phases of the Internet's evolution. Furthermore, we show that our tools can be used to provide a quantitative measure of the \\\"flattening\\\" of the Internet.\n\nTitle: Mining anomalies using traffic feature distributions\nContent: The increasing practicality of large-scale flow capture makes it possible to conceive of traffic analysis methods that detect and identify a large and diverse set of anomalies. However the challenge of effectively analyzing this massive data source for anomaly diagnosis is as yet unmet. We argue that the distributions of packet features (IP addresses and ports) observed in flow traces reveals both the presence and the structure of a wide range of anomalies. Using entropy as a summarization tool, we show that the analysis of feature distributions leads to significant advances on two fronts: (1) it enables highly sensitive detection of a wide range of anomalies, augmenting detections by volume-based methods, and (2) it enables automatic classification of anomalies via unsupervised learning. We show that using feature distributions, anomalies naturally fall into distinct and meaningful clusters. These clusters can be used to automatically classify anomalies and to uncover new anomaly types. We validate our claims on data from two backbone networks (Abilene and Geant) and conclude that feature distributions show promise as a key element of a fairly general network anomaly diagnosis framework.\n\n## Task\nCompose an abstract for the title 'Towards detecting anomalous user behavior in online social networks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Users increasingly rely on crowdsourced information, such as reviews on Yelp and Amazon, and liked posts and ads on Facebook. This has led to a market for blackhat promotion techniques via fake (e.g., Sybil) and compromised accounts, and collusion networks. Existing approaches to detect such behavior relies mostly on supervised (or semisupervised) learning over known (or hypothesized) attacks. They are unable to detect attacks missed by the operator while labeling, or when the attacker changes strategy. We propose using unsupervised anomaly detection techniques over user behavior to distinguish potentially bad behavior from normal behavior. We present a technique based on Principal Component Analysis (PCA) that models the behavior of normal users accurately and identifies significant deviations from it as anomalous. We experimentally validate that normal user behavior (e.g., categories of Facebook pages liked by a user, rate of like activity, etc.) is contained within a low-dimensional subspace amenable to the PCA technique. We demonstrate the practicality and effectiveness of our approach using extensive ground-truth data from Facebook: we successfully detect diverse attacker strategies--fake, compromised, and colluding Facebook identities--with no a priori labeling while maintaining low false-positive rates. Finally, we apply our approach to detect click-spam in Facebook ads and find that a surprisingly large fraction of clicks are from anomalous users."
      }
    ]
  },
  {
    "tid":"241",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Reference idempotency analysis: a framework for optimizing speculative execution\nContent: Recent proposals for multithreaded architectures allow threads with unknown dependences to execute speculatively in parallel. These architectures use hardware speculative storage to buffer uncertain data, track data dependences and roll back incorrect executions. Because all memory references access the speculative storage, current proposals implement this storage using small memory structures for fast access. The limited capacity of the speculative storage causes considerable performance loss due to speculative storage overflow whenever a thread's speculative state exceeds the storage capacity. Larger threads exacerbate the overflow problem but are preferable to smaller threads, as larger threads uncover more parallelism. In this paper, we discover a new program property called memory reference idempotency. Idempotent references need not be tracked in the speculative storage, and instead can directly access non-speculative storage (i.e., the conventional memory hierarchy). Thus, we reduce the demand for speculative storage space. We define a formal framework for reference idempotency and present a novel compiler-assisted speculative execution model. We prove the necessary and sufficient conditions for reference idempotency using our model. We present a compiler algorithm to label idempotent memory references for the hardware. Experimental results show that for our benchmarks, over 60% of the references in non-parallelizable program sections are idempotent.\n\nTitle: Min-cut program decomposition for thread-level speculation\nContent: With billion-transistor chips on the horizon, single-chip multiprocessors (CMPs) are likely to become commodity components. Speculative CMPs use hardware to enforce dependence, allowing the compiler to improve performance by speculating on ambiguous dependences without absolute guarantees of independence. The compiler is responsible for decomposing a sequential program into speculatively parallel threads, while considering multiple performance overheads related to data dependence, load imbalance, and thread prediction. Although the decomposition problem lends itself to a min-cut-based approach, the overheads depend on the thread size, requiring the edge weights to be changed as the algorithm progresses. The changing weights make our approach different from graph-theoretic solutions to the general problem of task scheduling. One recent work uses a set of heuristics, each targeting a specific overhead in isolation, and gives precedence to thread prediction, without comparing the performance of the threads resulting from each heuristic. By contrast, our method uses a sequence of balanced min-cuts that give equal consideration to all the overheads, and adjusts the edge weights after every cut. This method achieves an (geometric) average speedup of 74% for floating-point programs and 23% for integer programs on a four-processor chip, improving on the 52% and 13% achieved by the previous heuristics.\n\nTitle: Exploiting reference idempotency to reduce speculative storage overflow\nContent: Recent proposals for multithreaded architectures employ speculative execution to allow threads with unknown dependences to execute speculatively in parallel. The architectures use hardware speculative storage to buffer speculative data, track data dependences and correct incorrect executions through roll-backs. Because all memory references access the speculative storage, current proposals implement speculative storage using small memory structures to achieve fast access. The limited capacity of the speculative storage causes considerable performance loss due to speculative storage overflow whenever a thread's speculative state exceeds the speculative storage capacity. Larger threads exacerbate the overflow problem but are preferable to smaller threads, as larger threads uncover more parallelism.In this article, we discover a new program property called memory reference idempotency. Idempotent references are guaranteed to be eventually corrected, though the references may be temporarily incorrect in the process of speculation. Therefore, idempotent references, even from nonparallelizable program sections, need not be tracked in the speculative storage, and instead can directly access nonspeculative storage (i.e., conventional memory hierarchy). Thus, we reduce the demand for speculative storage space in large threads. We define a formal framework for reference idempotency and present a novel compiler-assisted speculative execution model. We prove the necessary and sufficient conditions for reference idempotency using our model. We present a compiler algorithm to label idempotent memory references for the hardware. Experimental results show that for our benchmarks, over 60&percnt; of the references in nonparallelizable program sections are idempotent.\n\nTitle: Speculative thread decomposition through empirical optimization\nContent: Chip multiprocessors (CMPs), or multi-core processors, have become a common way of reducing chip complexity and power consumption while maintaining high performance. Speculative CMPs use hardware to enforce dependence, allowing a parallelizing compiler to generate multithreaded code without needing to prove independence. In these systems, a sequential program is decomposed into threads to be executed in parallel; dependent threads cause performance degradation, but do not affect correctness. Thread decomposition attempts to reduce the run-time overheads of data dependence, thread misprediction, and load imbalance. Because these overheads depend on the runtimes of the threads that are being created by the decomposition, reducing the overheads while creating the threads is a circular problem. Static compile-time decomposition handles this problem by estimating the run times of the candidate threads, but is limited by the estimates' inaccuracy. Dynamic execution-time decomposition in hardware has better run-time information, but is limited by the decomposition hardware's complexity and run-time overhead. We propose a third approach where a compiler instruments a profile run of the application to search through candidate threads and pick the best threads as the profile run executes. The resultant decomposition is compiled into the application so that a production run of the application has no instrumentation and does not incurany decomposition overhead. We avoid static decomposition's estimation accuracy problem by using actual profile-run execution times to pick threads, and we avoid dynamic decomposition's overhead by performing the decomposition at profile time. Because we allow candidate threads to span arbitrary sections of the application's call graph and loop nests, an exhaustive search of the decomposition space is prohibitive, even in profile runs. To address this issue, we make the key observation that the run-time overhead of a thread depends, to the first order, only on threads that overlap with the thread inexecution (e.g., in a four-core CMP, a given thread can overlap with at most three preceding and three following threads). This observation implies that a given thread affects only a few other threads, allowing pruning of the space. Using a CMP simulator, we achieve an average speedup of 3.51 on four cores for five of the SPEC CFP2000 benchmarks, which compares favorably to recent static techniques. We also discuss experiments with CINT2000.\n\nTitle: Optimizing Replication, Communication, and Capacity Allocation in CMPs\nContent: Chip multiprocessors (CMPs) substantially increase capacity pressure on the on-chip memory hierarchy while requiring fast access. Neither private nor shared caches can provide both large capacity and fast access in CMPs. We observe that compared to symmetric multiprocessors (SMPs), CMPs change the latency-capacity tradeoff in two significant ways. We propose three novel ideas to exploit the changes: (1) Though placing copies close to requestors allows fast access for read-only sharing, the copies also reduce the already-limited on-chip capacity in CMPs. We propose controlled replication to reduce capacity pressure by not making extra copies in some cases, and obtaining the data from an existing on-chip copy. This option is not suitable for SMPs because obtaining data from another processor is expensive and capacity is not limited to on-chip storage. (2) Unlike SMPs, CMPs allow fast on-chip communication between processors for read-write sharing. Instead of incurring slow access to read-write shared data through coherence misses as do SMPs, we propose in-situ communication to provide fast access without making copies or incurring coherence misses. (3) Accessing neighbors\u00fd caches is not as expensive in CMPs as it is in SMPs. We propose capacity stealing in which private data that exceeds a core\u00fds capacity is placed in a neighboring cache with less capacity demand. To incorporate our ideas, we use a hybrid of private, per-processor tag arrays and a shared data array. Because the shared data array is slow, we employ non-uniform access and distance associativity from previous proposals to hold frequently-accessed data in regions close to the requestor. We extend the previously-proposed Non-uniform access with Replacement And Placement usIng Distance associativity (NuRAPID) to CMPs, and call our cache CMP-NuRAPID. Our results show that for a 4-core CMP with 8 MB cache, CMP-NuRAPID improves performance by 13% over a shared cache and 8% over private caches for three commercial multithreaded workloads.\n\n## Task\nCompose an abstract for the title 'Is SC + ILP = RC?' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Sequential consistency (SC) is the simplest programming interface for shared-memory systems but imposes program order among all memory operations, possibly precluding high performance implementations. Release consistency (RC), however, enables the highest performance implementations but puts the burden on the programmer to specify which memory operations need to be atomic and in program order. This paper shows, for the first time, that SC implementations can perform as well as RC implementations if the hardware provides enough support for speculation. Both SC and RC implementations rely on reordering and overlapping memory operations for high performance. To enforce order when necessary, an RC implementation uses software guarantees, whereas an SC implementation relies on hardware speculation. Our SC implementation, called SC++, closes the performance gap because: (1) the hardware allows not just loads, as some current SC implementations do, but also stores to bypass each other speculatively to hide remote latencies, (2) the hardware provides large speculative state for not just processor, as previously proposed, but also memory to allow out-of-order memory operations, (3) the support for hardware speculation does not add excessive overheads to processor pipeline critical paths, and (4) well-behaved applications incur infrequent rollbacks of speculative execution. Using simulation, we show that SC++ achieves an RC implementation's performance in all the six applications we studied.\n\n"
      }
    ]
  },
  {
    "tid":"242",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Condor: Better Topologies Through Declarative Design\nContent: The design space for large, multipath datacenter networks is large and complex, and no one design fits all purposes. Network architects must trade off many criteria to design cost-effective, reliable, and maintainable networks, and typically cannot explore much of the design space. We present Condor, our approach to enabling a rapid, efficient design cycle. Condor allows architects to express their requirements as constraints via a Topology Description Language (TDL), rather than having to directly specify network structures. Condor then uses constraint-based synthesis to rapidly generate candidate topologies, which can be analyzed against multiple criteria. We show that TDL supports concise descriptions of topologies such as fat-trees, BCube, and DCell; that we can generate known and novel variants of fat-trees with simple changes to a TDL file; and that we can synthesize large topologies in tens of seconds. We also show that Condor supports the daunting task of designing multi-phase network expansions that can be carried out on live networks.\n\n\n\nTitle: Cicada: introducing predictive guarantees for cloud networks\nContent: In cloud-computing systems, network-bandwidth guarantees have been shown to improve predictability of application performance and cost [1, 28]. Most previous work on cloud-bandwidth guarantees has assumed that cloud tenants know what bandwidth guarantees they want [1, 17]. However, as we show in this work, application bandwidth demands can be complex and time-varying, and many tenants might lack sufficient information to request a guarantee that is well-matched to their needs, which can lead to over-provisioning (and thus reduced cost-efficiency) or under-provisioning (and thus poor user experience). We analyze traffic traces gathered over six months from an HP Cloud Services datacenter, finding that application bandwidth consumption is both time-varying and spatially inhomogeneous. This variability makes it hard to predict requirements. To solve this problem, we develop a prediction algorithm usable by a cloud provider to suggest an appropriate bandwidth guarantee to a tenant. With tenant VM placement using these predictive guarantees, we find that the inter-rack network utilization in certain datacenter topologies can be more than doubled.\n\nTitle: Corybantic: towards the modular composition of SDN control programs\nContent: Software-Defined Networking (SDN) promises to enable vigorous innovation, through separation of the control plane from the data plane, and to enable novel forms of network management, through a controller that uses a global view to make globally-valid decisions. The design of SDN controllers creates novel challenges; much previous work has focused on making them scalable, reliable, and efficient. However, prior work has ignored the problem that multiple controller functions may be competing for resources (e.g., link bandwidth or switch table slots). Our Corybantic design supports modular composition of independent controller modules, which manage different aspects of the network while competing for resources. Each module tries to optimize one or more objective functions; we address the challenge of how to coordinate between these modules to maximize the overall value delivered by the controllers' decisions, while still achieving modularity.\n\nTitle: DevoFlow: scaling flow management for high-performance networks\nContent: OpenFlow is a great concept, but its original design imposes excessive overheads. It can simplify network and traffic management in enterprise and data center environments, because it enables flow-level control over Ethernet switching and provides global visibility of the flows in the network. However, such fine-grained control and visibility comes with costs: the switch-implementation costs of involving the switch's control-plane too often and the distributed-system costs of involving the OpenFlow controller too frequently, both on flow setups and especially for statistics-gathering. In this paper, we analyze these overheads, and show that OpenFlow's current design cannot meet the needs of high-performance networks. We design and evaluate DevoFlow, a modification of the OpenFlow model which gently breaks the coupling between control and global visibility, in a way that maintains a useful amount of visibility without imposing unnecessary costs. We evaluate DevoFlow through simulations, and find that it can load-balance data center traffic as well as fine-grained solutions, without as much overhead: DevoFlow uses 10--53 times fewer flow table entries at an average switch, and uses 10--42 times fewer control messages.\n\nTitle: NetLord: a scalable multi-tenant network architecture for virtualized datacenters\nContent: Providers of \"Infrastructure-as-a-Service\" need datacenter networks that support multi-tenancy, scale, and ease of operation, at low cost. Most existing network architectures cannot meet all of these needs simultaneously. In this paper we present NetLord, a novel multi-tenant network architecture. NetLord provides tenants with simple and flexible network abstractions, by fully and efficiently virtualizing the address space at both L2 and L3. NetLord can exploit inexpensive commodity equipment to scale the network to several thousands of tenants and millions of virtual machines. NetLord requires only a small amount of offline, one-time configuration. We implemented NetLord on a testbed, and demonstrated its scalability, while achieving order-of-magnitude goodput improvements over previous approaches.\n\n## Task\nCompose an abstract for the title 'Taming the flying cable monster: a topology design and optimization framework for data-center networks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Data-center network designers now have many choices for high-bandwidth, multi-path network topologies. Some of these topologies also allow the designer considerable freedom to set parameters (for example, the number of ports on a switch, link bandwidths, or switch-to-switch wiring patterns) at design time. This freedom of choice, however, requires the designer to balance among bandwidth, latency, reliability, parts cost, and other real-world details. Designers need help in exploring this design space, and especially in finding optimal network designs. We describe the specific challenges that designers face, and we present Perseus, a framework for quickly guiding a network designer to a small set of candidate designs. We identify several optimization problems that can be accommodated within this framework, and present solution algorithms for many of these problems."
      }
    ]
  },
  {
    "tid":"243",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Multi-modal learning for affective content analysis in movies\nContent: Affective content analysis is an important research topic in video content analysis, and has extensive applications in many fields. However, it is a challenging task to design a computational model for predicting emotions induced by videos, since the elicited emotions can be considered relatively subjective. Intuitively, several features of different modalities can depict the elicited emotions, but the correlation and influence of these features are still not well studied. To address this issue, we propose a multi-modal learning framework, which classifies affective contents in the valence-arousal space. In particular, we utilize the features extracted by the methods of motion keypoint trajectory and convolutional neural networks to depict the visual modality of elicited emotions, and extract a global audio feature by the openSMILE toolkit to describe the audio modality. Then, the linear support vector machine and support vector regression are employed to learn the affective models. By comparing these three features with five baseline features, we discover that the three features are significant for describing affective content. Experimental results also demonstrate that the three features complement each other. Moreover, the proposed framework obtains the state-of-the-art results on two challenging datasets of video affective content analysis.\n\nTitle: Human Action Recognition With Trajectory Based Covariance Descriptor In Unconstrained Videos\nContent: Human action recognition from realistic videos plays a key role in multimedia event detection and understanding. In this paper, a novel Trajectory Based Covariance (TBC) descriptor is proposed, which is formulated along the dense trajectories. To map the descriptor matrix to vector space and trim out the redundancy of data, the TBC descriptor matrix is projected to Euclidean space by the Logarithm Principal Components Analysis (LogPCA). Our method is tested on the challenging Hollywood2 and TV Human Interaction datasets. Experimental results show that the proposed TBC descriptor outperforms three baseline descriptors (i.e., histogram of oriented gradient, histogram of optical flow and motion boundary histogram), and our method achieves better recognition performances than a number of state-of-the-art approaches.\n\nTitle: Real-Time Action Recognition With Enhanced Motion Vector Cnns\nContent: The deep two-stream architecture [23] exhibited excellent performance on video based action recognition. The most computationally expensive step in this approach comes from the calculation of optical flow which prevents it to be real-time. This paper accelerates this architecture by replacing optical flow with motion vector which can be obtained directly from compressed videos without extra calculation. However, motion vector lacks fine structures, and contains noisy and inaccurate motion patterns, leading to the evident degradation of recognition performance. Our key insight for relieving this problem is that optical flow and motion vector are inherent correlated. Transferring the knowledge learned with optical flow CNN to motion vector CNN can significantly boost the performance of the latter. Specifically, we introduce three strategies for this, initialization transfer, supervision transfer and their combination. Experimental results show that our method achieves comparable recognition performance to the state-of-the-art, while our method can process 390.7 frames per second, which is 27 times faster than the original two-stream method.\n\nTitle: Motion keypoint trajectory and covariance descriptor for human action recognition.\nContent: Human action recognition from videos is a challenging task in computer vision. In recent years, histogram-based descriptors that are calculated along dense trajectories have shown promising results for human action recognition, but they usually ignore motion information of the tracking points, and the relationship between different motion variables is not well utilized. To address this issue, we propose a motion keypoint trajectory (MKT) approach and a trajectory-based covariance (TBC) descriptor, which is calculated along the motion keypoint trajectories. The proposed MKT approach tracks motion keypoints at multiple spatial scales and employs an optical flow rectification algorithm to reduce the influence of camera motions and thus achieves better performance than the improved dense trajectory (IDT) approach well known in the literature. In particular, MKT is faster than IDT, because MKT does not need to use human detection and extracts fewer trajectories than IDT. Furthermore, the TBC descriptor outperforms the classical histogram-based descriptors, such as the Histogram of Oriented Gradient, Histogram of Optical Flow and Motion Boundary Histogram. Experimental results on three challenging datasets (i.e., Olympic Sports, HMDB51 and UCF50) demonstrate that our approach is able to achieve better recognition performances than a number of state-of-the-art approaches.\n\nTitle: Richer Semantic Visual and Language Representation for Video Captioning.\nContent: Translating and summarizing a video into natural language is an interesting and challenging visual task. In this work, a novel framework is built to generate sentences for videos with more coherence and semantics. A long short term memory (LSTM) network with an improved factored way is first developed, which takes inspiration of the conventional factored way and a common practice of presenting multi-modal features at the first time in LSTM for video captioning. An LSTM network with the combination of improved factored and un-factored ways is exploited, and a voting strategy is employed to predict the words. Then, the residual is used to enhance the gradient signals which is learned from residual network (ResNet), and a deeper LSTM network is constructed. Furthermore, several convolutional neural network (CNN) features from deep models with different architectures are fused to catch more comprehensive and complementary visual information. Experiments are conducted on the MSR-VTT2016 and MSR-VTT2017 grand challenge datasets to demonstrate the effectiveness of each presented techniques as well as the superiority compared to other state-of-the-art methods.\n\n\n\n## Task\nCompose an abstract for the title 'The MediaEval 2015 Affective Impact of Movies Task.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper provides a description of the MediaEval 2015 \\Aective Impact of Movies Task\", which is running for the fth year, previously under the name \\Violent Scenes Detection\". In this year\u2019s task, participants are expected to create systems that automatically detect video content that depicts violence, or predict the aective impact that video content will have on viewers. Here we provide insights on the use case, task challenges, data set and ground truth, task run requirements and evaluation metrics."
      }
    ]
  },
  {
    "tid":"244",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Low-Complexity Encoder Framework For Window-Level Rate Control Optimization\nContent: There exists a tradeoff between visual quality smoothness and buffer constraint in rate control. Thus, a window model and a window-level rate control algorithm were proposed to handle such a tradeoff recently. However, the extra computational complexity and encoding delay were introduced due to a pre-analysis process in the window-level rate control. In this paper, the window model is first improved for a more convenient usage in practical encodings. Second, a new low-complexity encoder framework is proposed based on an open-loop motion estimation (ME). Third, a new memory-efficient algorithm is proposed to eliminate the drift error of open-loop ME. Fourth, the window-level rate control algorithm is improved to adapt to the proposed encoder framework. The experimental results show that the visual quality smoothness is much improved by the proposed algorithm against the traditional one of H.264/AVC. In addition, the improvement of the window-level rate control is better than the previous one in terms of both bit control accuracy and visual quality smoothness. Besides, the proposed encoder could be used to any other rate control algorithms with pre-analysis to reduce computational complexity.\n\nTitle: Optimal entropy-constrained non-uniform scalar quantizer design for low bit-rate pixel domain DVC\nContent: In this paper, an optimal entropy-constrained non-uniform scalar quantizer is proposed for the pixel domain DVC. The uniform quantizer is efficient for the hybrid video coding since the residual signals conforming to a single-variance Laplacian distribution. However, the uniform quantizer is not optimal for pixel domain distributed video coding (DVC). This is because the uniform quantizer is not adaptive to the joint distribution of the source and the SI, especially for low level quantization. The signal distribution of pixel domain DVC conforms to the mixture model with multi-variance. The optimal non-uniform quantizer is designed according to the joint distribution, the error between the source and the SI can be decreased. As a result, the bit rate can be saved and the video quality won't sacrifice too much. Accordingly, a better R-D trade-off can be achieved. First, the quantization level is fixed and the optimal RD trade-off is achieved by using a Lagrangian function J(Q). The rate and distortion components is designed based on P(Y|Q). The conditional probability density function of SI Y depend on quantization partitions Q, P(Y|Q), is approximated by a Guassian mixture model at encocder. Since the SI can not be accessed at encoder, an estimation of P(Y|Q) based on the distribution of the source is proposed. Next, J(Q) is optimized by an iterative Lloyd-Max algorithm with a novel quantization partition updating algorithm. To guarantee the convergence of J(Q), the monotonicity of the interval in which the endpoints of the quantizer lie must be satisfied. Then, a quantizer partition updating algorithm which considers the extreme points of the histogram of the source is proposed. Consequently, the entropy-constrained optimal non-uniform quantization partitions are derived and a better RD trade-off is achieved by applying them. Experiment results show that the proposed scheme can improve the performance by 0.5 dB averagely compared to the uniform scalar quantization.\n\nTitle: Error Resilient Dual Frame Motion Compensation with Uneven Quality Protection\nContent: Summary form only given. In this paper, an error resilient JU-DFMC is proposed for video transmission over error-prone channels. In the proposed error resilient JU-DFMC, a new error resilient prediction structure of DFMC is firstly presented. The LQF can adaptively select reference frame according to different packet loss rate. Then the MB information is divided into two partition header information (A) and texture coefficients (B). Based on the partition, an end-to-end distortion model is applied for macroblock (MB) level mode decision. Finally a frame level rate distortion cost scheme is proposed to determine how many times the header information will be transmitted in a high quality frame (HQF). The HQF (LTR) is given more protection. The experimental results show that the proposed method can achieve better performance than the previous DFMC schemes. In the future, how to determine LQF header transmission times will be further exploited.\n\nTitle: High efficient context-based variable length coding with parallel orientation\nContent: Entropy coding is one of the most important techniques in video codec. Two main criteria to assess an entropy coder are coding efficiency and friendly realization characteristic. In the recent development of the H.264/AVC standard, a sophisticated entropy coder, named Context-based Adaptive Variable Length Coding (CAVLC), has been invented, which supplies higher coding efficiency than other VLC-based entropy coders in previous video coding standards. But due to its algorithm\u2019s inherit traits CAVLC must be executed in a sequential manner, which results in relative low throughput rate. In this paper, a new well-designed context-based VLC entropy coder for transform coefficients is presented. It exploits coefficient block\u2019s inner context information to obtain high coding efficiency, and at the same time context models for successive coding elements are designed to be dependent-free so that the coder is more apt to be parallel. Experimental results show that the proposed entropy coder can exhibit the same coding efficiency as CAVLC. Therefore, a new high performance entropy coder with characteristics of parallel orientation and high coding efficiency is supplied.\n\nTitle: An Enhanced Robust Entropy Coder for Video Codecs Based on Context-Adaptive Reversible VLC\nContent: This paper proposes an enhanced RVLC coder, context-adaptive reversible variable length coder (CRVLC), for DCT coefficients by using the techniques of data sub-partitioning and context modeling. The data sub-partitioning means that the data part of DCT coefficients is split into several small sub-partitions. As each sub-partition can be reversibly decoded by RVLC, more data as well as higher error resilience can be obtained. The context modeling exploits the correlation of DCT coefficients for further compression. This modeling defines the contexts by hierarchical-dependent information. The information is also available in the backward decoding, so that it supports the reversible decoding. And with it the data outputted by CRVLC can be naturally placed into multiple sub-partitions.\n\n## Task\nCompose an abstract for the title 'Error-resistance and Low-complexity Integer Inverse Discrete Cosine Transform' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, a low-complexity multiplication-free integer inverse discrete cosine transform (IDCT) based on data flow structure of improved AAN's fast IDCT is proposed for error drifting resistance of decoder. Two algorithms are used in this integer IDCT improvement. One is common factor extraction which extracts the complicated common factors from transform kernel to scale; the other is two-stage scale which splits a more than 16-bit scale into two less than 16-bit scales. With the two algorithms, high-accuracy integer IDCT is implemented in lower complexity. The experimental results show that the proposed transform exceeds the requirements of IEEE1180-1990 significantly (about 10 times). The results of the proposed IDCT implemented into MPEG-2 and MPEG-4 decoders instead of original 64-bit floating-point IDCT also show that it reduces the error drifting of decoders efficiently."
      }
    ]
  },
  {
    "tid":"245",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Heterogeneous Metric Learning with Content-Based Regularization for Software Artifact Retrieval\nContent: The problem of software artifact retrieval has the goal to effectively locate software artifacts, such as a piece of source code, in a large code repository. This problem has been traditionally addressed through the textual query. In other words, information retrieval techniques will be exploited based on the textual similarity between queries and textual representation of software artifacts, which is generated by collecting words from comments, identifiers, and descriptions of programs. However, in addition to these semantic information, there are rich information embedded in source codes themselves. These source codes, if analyzed properly, can be a rich source for enhancing the efforts of software artifact retrieval. To this end, in this paper, we develop a feature extraction method on source codes. Specifically, this method can capture both the inherent information in the source codes and the semantic information hidden in the comments, descriptions, and identifiers of the source codes. Moreover, we design a heterogeneous metric learning approach, which allows to integrate code features and text features into the same latent semantic space. This, in turn, can help to measure the artifact similarity by exploiting the joint power of both code and text features. Finally, extensive experiments on real-world data show that the proposed method can help to improve the performances of software artifact retrieval with a significant margin.\n\nTitle: Efficient Discovery of Confounders in Large Data Sets\nContent: Given a large transaction database, association analysis is concerned with efficiently finding strongly related objects. Unlike traditional associate analysis, where relationships among variables are searched at a global level, we examine confounding factors at a local level. Indeed, many real-world phenomena are localized to specific regions and times. These relationships may not be visible when the entire data set is analyzed. Specially, confounding effects that change the direction of correlation is the most significant. Along this line, we propose to efficiently find confounding effects attributable to local associations. Specifically, we derive an upper bound by a necessary condition of confounders, which can help us prune the search space and efficiently identify confounders. Experimental results show that the proposed CONFOUND algorithm can effectively identify confounders and the computational performance is an order of magnitude faster than benchmark methods.\n\nTitle: Remix in 3D Printing: What your Sources say About You\nContent: Concurrently with the recent, rapid adoption of 3D printing technologies, online sharing of 3D-printable designs is growing equally rapidly, even though it has received far less attention. We study remix relationships on Thingiverse, the dominant online repository and social network for 3D printing. We collected data of designs published over five years, and we find that remix ties exhibit both homophily and inverse-homophily across numerous key metrics, which is stronger compared to other kinds of social and content links. This may have implications on graph prediction tasks, as well as on the design of 3D-printable content repositories.\n\nTitle: SAIL: summation-based incremental learning for information-theoretic clustering\nContent: Information-theoretic clustering aims to exploit information theoretic measures as the clustering criteria. A common practice on this topic is so-called INFO-K-means, which performs K-means clustering with the KL-divergence as the proximity function. While expert efforts on INFO-K-means have shown promising results, a remaining challenge is to deal with high-dimensional sparse data. Indeed, it is possible that the centroids contain many zero-value features for high-dimensional sparse data. This leads to infinite KL-divergence values, which create a dilemma in assigning objects to the centroids during the iteration process of K-means. To meet this dilemma, in this paper, we propose a Summation-based Incremental Learning (SAIL) method for INFO-K-means clustering. Specifically, by using an equivalent objective function, SAIL replaces the computation of the KL-divergence by the computation of the Shannon entropy. This can avoid the zero-value dilemma caused by the use of the KL-divergence. Our experimental results on various real-world document data sets have shown that, with SAIL as a booster, the clustering performance of K-means can be significantly improved. Also, SAIL leads to quick convergence and a robust clustering performance on high-dimensional sparse data.\n\nTitle: Mining maximal hyperclique pattern: A hybrid search strategy\nContent: A hyperclique pattern is a new type of association pattern that contains items which are highly affiliated with each other. Specifically, the presence of an item in one transaction strongly implies the presence of every other item that belongs to the same hyperclique pattern. In this paper, we present an algorithm for mining maximal hyperclique patterns, which specifies a more compact representation of hyperclique patterns and are desirable for many applications, such as pattern-based clustering. Our algorithm exploits key advantages of both the Depth First Search (DFS) strategy and the Breadth First Search (BFS) strategy. Indeed, we adapt the equivalence pruning method, one of the most efficient pruning methods of the DFS strategy, into the process of the BFS strategy. Our experimental results show that the performance of our algorithm can be orders of magnitude faster than standard maximal frequent pattern mining algorithms, particularly at low levels of support.\n\n## Task\nCompose an abstract for the title 'REMIX: Automated Exploration for Interactive Outlier Detection.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Outlier detection is the identification of points in a dataset that do not conform to the norm. Outlier detection is highly sensitive to the choice of the detection algorithm and the feature subspace used by the algorithm. Extracting domain-relevant insights from outliers needs systematic exploration of these choices since diverse outlier sets could lead to complementary insights. This challenge is especially acute in an interactive setting, where the choices must be explored in a time-constrained manner. In this work, we present REMIX, the first system to address the problem of outlier detection in an interactive setting. REMIX uses a novel mixed integer programming (MIP) formulation for automatically selecting and executing a diverse set of outlier detectors within a time limit. This formulation incorporates multiple aspects such as (i) an upper limit on the total execution time of detectors (ii) diversity in the space of algorithms and features, and (iii) meta-learning for evaluating the cost and utility of detectors. REMIX provides two distinct ways for the analyst to consume its results: (i) a partitioning of the detectors explored by REMIX into perspectives through low-rank non-negative matrix factorization; each perspective can be easily visualized as an intuitive heatmap of experiments versus outliers, and (ii) an ensembled set of outliers which combines outlier scores from all detectors. We demonstrate the benefits of REMIX through extensive empirical validation on real-world data."
      }
    ]
  },
  {
    "tid":"246",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: AMPol-Q: adaptive middleware policy to support qos\nContent: There are many problems hindering the design and development of Service-Oriented Architectures (SOAs), which can dynamically discover and compose multiple services so that the quality of the composite service is measured by its End-to-End (E2E) quality, rather than that of individual services in isolation. The diversity and complexity of QoS constraints further limit the wide-scale adoption of QoS-aware SOA. We propose extensions to current OWL-S service description mechanisms to describe QoS information of all the candidate services. Our middleware based solution, AMPol-Q, enables clients to discover, select, compose, and monitor services that fulfil E2E QoS constraints. Our implementation and case studies demonstrate how AMPol-Q can accomplish these goals for web services that implement messaging.\n\nTitle: PolicyMorph: interactive policy transformations for a logical attribute-based access control framework\nContent: Constraint systems provide techniques for automatically analyzing the conformance of low-level access control policies to high-level business rules formalized as logical constraints. However, there are likely to be priorities for solutions that are not easy to encode formally, so administrator input is often important. This paper introduces PolicyMorph, a constraint system that supports interactive development and maintenance of access control policies that respect both formalized and un-formalized business rules and priorities. We provide a mathematical description of the system and an architecture for implementing it. We constructed a prototype that is validated using a case study in which constraints are imposed on a building automation system that controls door locks. PolicyMorph advances the state-of-the-art in constraint systems by suggesting predictable policy model modifications that will resolve specific constraint violations and then allowing policy administrators to select the appropriate modifications using knowledge that is not formally encoded in the constraint system.\n\nTitle: Using Attribute-Based Access Control to Enable Attribute-Based Messaging\nContent: Attribute Based Messaging (ABM) enables message senders to dynamically create a list of recipients based on their attributes as inferred from an enterprise database. Such targeted messaging can reduce unnecessary communications and enhance privacy, but faces challenges in access control. In this paper we explore an approach to ABM based on deriving access control information from the same attribute database exploited by the addressing scheme. We show how to address three key challenges. First, we demonstrate a manageable access control system based on attributes. Second we show how this can be used with existing messaging systems to provide a practical deployment strategy. Third, we show that such a system can be efficient enough to support ABM for mid-size enterprises. Our implementation can dispatch ABM messages approved by XACML review for an enterprise of at least 60,000 users with only seconds of latency.\n\nTitle: MyABDAC: compiling XACML policies for attribute-based database access control\nContent: Attribute-based Access Control (ABAC) based on XACML can substantially improve the security and management of access rights on databases. However, existing implementations rely on high-level policy interpretation and are not as efficient as mechanisms natively supported by commodity databases. In this paper we explore advantages and challenges arising from compiling XACML policies for database access into Access Control Lists (ACLs) natively supported by the database. The main contributions are an architecture and algorithms for efficiently addressing incremental changes in attributes that could trigger changes to the ACLs. We consider this in a context of reflective database access control where attributes used in access decisions are stored in the database itself. Our implementation and experiments demonstrate a significant improvement in access decision times compared to the best available optimizations for general XACML access engines.\n\nTitle: Defeasible security policy composition for web services\nContent: The ability to automatically compose security policies created by multiple organizations is fundamental to the development of scalable security systems. The diversity of policies leads to conflicts and the need to resolve priorities between rules. In this paper we explore the concept of defeasible policy composition, wherein policies are represented in defeasible logic and composition is based on rules for non-monotonic inference. This enables policy writers to assert rules tentatively; when policies are composed the policy with the firmest position takes precedence. In addition, the structure of our policies allows for composition to occur using a single operator; this allows for entirely automated composition. We argue that this provides a practical system that can be understood by typical policy writers, analyzed rigorously by theoreticians, and efficiently automated by computers. We aim to partially validate these claims here with a formulation of defeasible policy composition for web services, an emerging foundation for B2B commerce on the World Wide Web.\n\n## Task\nCompose an abstract for the title 'AMPol: Adaptive Messaging Policy' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Interoperability in a large-scale distributed system is challenged by by the diversity of node policies. We introduce AMPol (Adaptive Messaging Policy), a serviceoriented architecture that facilitates policy-aware, end-toend adaptive messaging between multiple parties. AMPol provides services for expressing non-functional QoS policies, finding them, and carrying out system extensions to adapt to them. We implement this approach with a web service middleware that allows parties to use policies for features like attachments, payment, encryption, and signatures. Our implementation demonstrates how AMPol can enhance the function of email messaging by enabling automatic deployment and use of features like cycle exhaustion puzzles, reverse Turing tests and identity based encryption without the need for global deployment or changes to the baseline messaging system."
      }
    ]
  },
  {
    "tid":"247",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A New Channel Assignment Scheme for Interference-Aware Routing in Vehicular Networks\nContent: Mobile computing and vehicular communications are becoming a very important paradigm for wireless communications, mainly because of their ability to adapt to different mobile applications. In this paper, we propose a new scheme for reducing the interference level during mobile transmissions in the VehiculAr inter-NETworking (VANET) environment, taking the advantage of the multi-channel nature of IEEE802.11p standard. In order to relieve the effects of the co-channel interference perceived by mobile nodes, transmission channels are switched on a basis of a periodical Signal-to-Interference Ratio (SIR) evaluation. The attention is focused on the routing level of VANET and we propose an interference aware routing scheme for multi-radio vehicular networks, wherein each node is equipped with a multi-channel radio interface. A new metric is also proposed, based on the maximization of the average SIR level of the connection between source and destination. Our solution has been integrated with the AODV routing protocol to design an enhanced Signal-to-Interference-Ratio-AODV (SIR-AODV). NS-2 has been used for implementing and testing the proposed idea, and significant performance enhancements were obtained, in terms of throughput, packet delivery and, obviously, interference.\n\nTitle: A distributed admission control system for MANET environments supporting multipath routing protocols\nContent: Multipath routing protocols have proved to be able to enhance the performance of MANET in terms of reliability, load balancing, multimedia streaming, security, etc. However, deploying a QoS framework on top of such routing protocols is a complex task, requiring an appropriate QoS strategy to be developed and deployed. In this paper we propose an admission control strategy that can operate both over single and multipath routing protocols. The results achieved through simulation show that the proposed QoS framework can perfectly coexist with multipath routing protocols, achieving significant improvements on the overall network performance, especially from the point of view of demanding applications such as real time video and voice applications.\n\nTitle: On the Interaction Between IEEE 802.11e and Routing Protocols in Mobile Ad-Hoc Networks\nContent: The upcoming IEEE 802.11e standard was developed to offer QoS capabilities to WLAN, offering significative im- provements to multimedia traffic. MANETs will also bene- fit from this new technology since the most widely deployed and used wireless interfaces are IEEE 802.11 based. In this paper we expose results relative to the interaction of reac- tive routing protocols for MANETs and the IEEE 802.11e technology. We find that very substantial improvements in terms of throughput and normalized routing overhead are achieved due to increased routing responsiveness. We also detail the relation between the behavior experienced in each case and the internal mechanisms of the routing protocol being used, offering a holistic view of the phenomena.\n\nTitle: A Map-based Sensor data Delivery Protocol for vehicular networks\nContent: Nowadays our vehicles incorporate multiple sensors that collect information not only about speed or fuel consumption, but also about weather (rain sensors), road state (ESP sensors), etc. Achieving a distributed traffic monitoring system that combines all this information gathered by different vehicles is a challenging task. In particular, we consider that vehicular ad-hoc networks (VANETs) are particularly suitable for this endeavor. VANETs present some characteristics such as a high mobility, a variable node density or frequent radio obstacles that are prone to generate network partitions. Under these conditions, a complete multi-hop path between the sender and the receiver of a message is rarely available. Previous studies have proposed using DTN protocols to address these problems. In this paper we propose a collecting sensor data routing protocol, called Map-based Sensor-data Delivery Protocol (MSDP), which combines information about the road map and the nodes' future routes to improve data delivery. Through simulations based on accurate mobility and propagation models, and using the well-known Epidemic protocol as reference, we demonstrate that our proposal is able to significantly reduce channel usage, while maintaining or even improving the packet delivery ratio.\n\nTitle: An Adaptive Anycasting Solution for Crowd Sensing in Vehicular Environments\nContent: Vehicular networks can be seen as the new key enablers of the future networked society. Vehicles traveling can act as mobile sensors and collect a variety of information that can be used to enable various new services like environment monitoring, traffic management, urban surveillance, and so on. In this paper we present AVE (\u201dadaptive Anycasting solution for Vehicular Environments\u201d), a message delivery protocol which combines geographical and topological information to dynamically adapt its behavior to network conditions. We focus on V2I connectivity for cloud services, where the vehicles send the sensed information as individual and independent messages to a cloud service in the Internet. This scenario requires the access to any available close-by Road Side Unit (RSU), thus making anycasting the ideal delivery mechanism. Simulations results show that the hybrid and adaptive approach of AVE is able to improve network performance. For example, regarding delivery ratio, AVE outperforms DYMO by 10% in sparse scenarios and outperforms DTN techniques by 10% in dense scenarios.\n\n## Task\nCompose an abstract for the title 'A Location-Aware Waypoint-Based Routing Protocol for Airborne DTNs in Search and Rescue Scenarios.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose GeoSaW, a delay-tolerant routing protocol for Airborne Networks in Search and Rescue scenarios. The protocol exploits the geographical information of UAVs to make appropriate message forwarding decisions. More precisely, the information about the future UAV's motion path is exploited to select the best UAV carrying the message towards the destination. Simulation results show that the proposed solution outperforms the classic DTN routing protocols in terms of several performance metrics."
      }
    ]
  },
  {
    "tid":"248",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Practical Framework for Privacy-Preserving Data Analytics\nContent: The availability of an increasing amount of user generated data is transformative to our society. We enjoy the benefits of analyzing big data for public interest, such as disease outbreak detection and traffic control, as well as for commercial interests, such as smart grid and product recommendation. However, the large collection of user generated data contains unique patterns and can be used to re-identify individuals, which has been exemplified by the AOL search log release incident. In this paper, we propose a practical framework for data analytics, while providing differential privacy guarantees to individual data contributors. Our framework generates differentially private aggregates which can be used to perform data mining and recommendation tasks. To alleviate the high perturbation errors introduced by the differential privacy mechanism, we present two methods with different sampling techniques to draw a subset of individual data for analysis. Empirical studies with real-world data sets show that our solutions enable accurate data analytics on a small fraction of the input data, reducing user privacy risk and data storage requirement without compromising the analysis results.\n\nTitle: Differentially Private User Data Perturbation with Multi-level Privacy Controls.\nContent: Service providers typically collect user data for profiling users in order to provide high-quality services, yet this brings up user privacy concerns. One hand, service providers oftentimes need to analyze multiple user data attributes that usually have different privacy concern levels. On the other hand, users often pose different trusts towards different service providers based on their reputation. However, it is unrealistic to repeatedly ask users to specify privacy levels for each data attribute towards each service provider. To solve this problem, we develop the first lightweight and provably framework that not only guarantees differential privacy on both service provider and different data attributes but also allows configurable utility functions based on service needs. Using various large-scale real-world datasets, our solution helps to significantly improve the utility up to 5 times with negligible computational overhead, especially towards numerous low reputed service providers in practice.\n\nTitle: Privacy-Preserving Personalized Recommendation: An Instance-Based Approach via Differential Privacy\nContent: Recommender systems become increasingly popular and widely applied nowadays. The release of users' private data is required to provide users accurate recommendations, yet this has been shown to put users at risk. Unfortunately, existing privacy-preserving methods are either developed under trusted server settings with impractical private recommender systems or lack of strong privacy guarantees. In this paper, we develop the first lightweight and provably private solution for personalized recommendation, under untrusted server settings. In this novel setting, users' private data is obfuscated before leaving their private devices, giving users greater control on their data and service providers less responsibility on privacy protections. More importantly, our approach enables the existing recommender systems (with no changes needed) to directly use perturbed data, rendering our solution very desirable in practice. We develop our data perturbation approach on differential privacy, the state-of-the-art privacy model with lightweight computation and strong but provable privacy guarantees. In order to achieve useful and feasible perturbations, we first design a novel relaxed admissible mechanism enabling the injection of flexible instance-based noises. Using this novel mechanism, our data perturbation approach, incorporating the noise calibration and learning techniques, obtains perturbed user data with both theoretical privacy and utility guarantees. Our empirical evaluation on large-scale real-world datasets not only shows its high recommendation accuracy but also illustrates the negligible computational overhead on both personal computers and smart phones. As such, we are able to meet two contradictory goals, privacy preservation and recommendation accuracy. This practical technology helps to gain user adoption with strong privacy protection and benefit companies with high-quality personalized services on perturbed user data.\n\nTitle: Private Analysis of Infinite Data Streams via Retroactive Grouping\nContent: With the rapid advances in hardware technology, data streams are being generated daily in large volumes, enabling a wide range of real-time analytical tasks. Yet data streams from many sources are inherently sensitive, and thus providing continuous privacy protection in data streams has been a growing demand. In this paper, we consider the problem of private analysis of infinite data streams under differential privacy. We propose a novel data stream sanitization framework that periodically releases histograms summarizing the event distributions over sliding windows to support diverse data analysis tasks. Our framework consists of two modules, a sampling-based change monitoring module and a continuous histogram publication module. The monitoring module features an adaptive Bernoulli sampling process to accurately track the evolution of a data stream. We for the first time conduct error analysis of sampling under differential privacy, which allows to select the best sampling rate. The publication module features three different publishing strategies, including a novel technique called retroactive grouping to enjoy reduced noise. We provide theoretical analysis of the utility, privacy and complexity of our framework. Extensive experiments over real datasets demonstrate that our solution substantially outperforms the state-of-the-art competitors.\n\nTitle: An analytical solution for consent management in patient privacy preservation\nContent: With the growing awareness and enforcement of patient rights, patients are empowered with increasing control on their medical information. In many situations, laws and regulation rules require the acquisition of patients' consent before one can access the patients' health data. However, in practice, patients oftentimes have difficulties determining whether they should permit or deny a certain access request. In this article, we propose an analytical approach to assist patients in the consent management of their medical information. Our consent management system employs a statistical learning method that evaluates the benefits and risks associated with access requests, so as to make personalized recommendation on consent decisions. Multiple factors are considered in the assessment process, including the importance of the request, the sensitivity of the requested information, and correlation information. We have implemented a prototype of our solution and performed evaluation with large-scale medical records.\n\n## Task\nCompose an abstract for the title 'An Information-Theoretic Approach to Individual Sequential Data Sanitization.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Fine-grained, personal data has been largely, continuously generated nowadays, such as location check-ins, web histories, physical activities, etc. Those data sequences are typically shared with untrusted parties for data analysis and promotional services. However, the individually-generated sequential data contains behavior patterns and may disclose sensitive information if not properly sanitized. Furthermore, the utility of the released sequence can be adversely affected by sanitization techniques. In this paper, we study the problem of individual sequence data sanitization with minimum utility loss, given user-specified sensitive patterns. We propose a privacy notion based on information theory and sanitize sequence data via generalization. We show the optimization problem is hard and develop two efficient heuristic solutions. Extensive experimental evaluations are conducted on real-world datasets and the results demonstrate the efficiency and effectiveness of our solutions.\n\n"
      }
    ]
  },
  {
    "tid":"249",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Discrimination of springs with vision, proprioception, and artificial skin stretch cues\nContent: During upper-limb prosthesis use, proprioception is not available so visual cues are used to identify the location of the artificial limb. We investigate the efficacy of a skin stretch device for artificially relaying proprioception during a spring discrimination task, with the goal of enabling the task to be achieved in the absence of vision. In this study, intact users perceive the location of a virtual prosthetic limb using each of four sensory conditions: Vision, Proprioception, Skin Stretch, and Skin Stretch with Vision. For the conditions with skin stretch, a haptic device stretches the forearm skin by an amount proportional to the angular rotation of a virtual prosthetic limb. Sensory condition was not found to significantly influence task performance, exploration methods, or perceived usefulness. We conclude that, in the absence of vision, artificial skin stretch could be used by prosthesis wearers to obtain position/motion information and identify the behavior of a spring.\n\nTitle: Perception of Springs With Visual and Proprioceptive Motion Cues: Implications for Prosthetics.\nContent: Manipulating objects with an upper limb prosthesis requires significantly more visual attention than doing the same task with an intact limb. Prior work and comments from individuals lacking proprioception indicate that conveying prosthesis motion through a nonvisual sensory channel would reduce and possibly remove the need to watch the prosthesis. To motivate the design of suitable sensory substitution devices, this study investigates the difference between seeing a virtual prosthetic limb move and feeling one's real limb move. Fifteen intact subjects controlled a virtual prosthetic finger in a one- degree-of-freedom rotational spring discrimination task. A custom haptic device was used to measure both real finger position and applied finger force, and the resulting prosthetic finger movement was displayed visually (on a computer screen) and/or proprioceptively (by allowing the subject's real finger to move). Spring discrimination performance was tested for three experimental sensory conditions-visual motion, proprioceptive motion, and visual and proprioceptive motion-using the method of constant stimuli, with a reference stiffness of 290 N/m. During each trial, subjects sequentially pressed the right index finger on a pair of hard-surfaced virtual springs and decided which was stiffer. No significant performance differences were found between the three experimental sensory conditions, but subjects perceived proprioceptive motion to be significantly more useful than visual motion. These results imply that relaying proprioceptive information through a nonvisual channel could reduce visual attention during prosthesis control while maintaining task performance, thus improving the upper limb prosthesis experience.\n\nTitle: Sensory augmentation of stiffness using fingerpad skin stretch\nContent: When interacting with everyday objects, we experience kinesthetic force feedback as well as various forms of cutaneous tactile feed-back. Skin stretch is part of the cutaneous tactile experience that is caused by friction between the skin and the grasped object. Interacting with stiffer objects causes larger force, and results in a larger amount of skin stretch. Therefore, we hypothesize that adding artificial fingerpad skin stretch to kinesthetic force feedback will increase users' perception of stiffness. A tactile display called the Skin Stretch Stylus was designed to augment kinesthetic force feedback with skin stretch feedback. The change in users' stiffness perception due to the addition of skin stretch feedback is quantified through a two-alternative forced-choice paradigm, method of constant stimuli experiment. In this experiment, subjects compared the stiffness of virtual springs with kinesthetic force feedback augmented with skin stretch feedback versus virtual springs with only kinesthetic force feedback. Results show that the addition of skin stretch causes a significant increase in the perception of stiffness, and this effect increases with higher amount of applied skin stretch. These results indicate that skin stretch feedback could be used to augment perceived stiffness in situations where it is not possible to increase force feedback gains. Such scenarios include teleoperation systems where force feedback gains must remain low to ensure stability, and haptic devices with limited actuator force.\n\nTitle: Quantifying the Value of Visual and Haptic Position Feedback During Force-Based Motion Control\nContent: Controlling the motion of a prosthetic upper limb without visual feedback is extremely difficult because the wearer does not know the prosthesis' configuration. This paper describes an experiment designed to determine the relative importance of visual and haptic position feedback during targeted force-based motion by non-amputee human subjects as an analogy to prosthetic use. Subjects control the angle of a virtual proxy through an admittance relationship by generating torque at the MCP joint of the right index finger. During successive repetitions of a target acquisition task, the proxy's state is selectively conveyed to the user through graphical display, finger motion, and tactile stimulation. Performance metrics for each feedback condition will provide insights on the role of haptic position feedback and may help guide the development of future upper-limb prostheses.\n\nTitle: Identifying the role of proprioception in upper-limb prosthesis control: Studies on targeted motion\nContent: Proprioception plays a crucial role in enabling humans to move purposively and interact with their physical surroundings. Current technology in upper-limb prostheses, while beginning to incorporate some haptic feedback, does not provide amputees with proprioceptive information about the state of the limb. Thus, the wearer must visually monitor the limb, which is often inconvenient or even impossible for some tasks. This work seeks to quantify the potential benefits of incorporating proprioceptive motion feedback into upper-limb prosthesis designs. We apply a noninvasive method for controlling the availability of proprioceptive motion feedback in unimpaired individuals in a human subject study to compare the benefits of visual and proprioceptive motion feedback in targeted motion tasks. Combined results of the current study and our previous study using a different task indicate that the addition of proprioceptive motion feedback improves targeting accuracy under nonsighted conditions and, for some tasks, under sighted conditions as well. This work motivates the development of methods for providing artificial proprioceptive feedback to a prosthesis wearer.\n\n## Task\nCompose an abstract for the title 'Stiffness discrimination with visual and proprioceptive cues' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This study compares the Weber fraction for human perception of stiffness among three conditions: vision, proprioceptive motion feedback, and their combination. To make comparisons between these feedback conditions, a novel haptic device was designed that senses the spring behavior through encoder and force measurements, and implements a controller to render linear virtual springs so that the stimuli displayed haptically could be compared with their visual counterparts. The custom-designed, torque-controlled haptic interface non-invasively controls the availability of proprioceptive motion feedback in unimpaired individuals using a virtual environment. When proprioception is available, the user feels an MCP joint rotation that is proportional to his or her finger force. When proprioception is not available, the actual finger is not allowed to move, but a virtual finger displayed graphically moves in proportion to the user's applied force. Visual feedback is provided and removed by turning on and off this graphical display. Weber fractions were generated from an experiment in which users examined pairs of springs and attempted to identify the spring with higher stiffness. To account for slight trial-to-trial variations in the relationship between force and position in the proprioceptive feedback conditions, our analysis uses measurements of the actual rendered stiffness, rather than the commanded stiffness. Results for 10 users give average Weber fractions of 0.056 for vision, 0.036 for proprioception, and 0.039 for their combination, indicating that proprioception is important for stiffness perception for this experimental setup. The long-term goal of this research is to motivate and develop methods for proprioception feedback to wearers of dexterous upper-limb prostheses."
      }
    ]
  },
  {
    "tid":"250",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Peeling the flow: a sketch-based interface to generate stream surfaces.\nContent: We present a user-centric approach for stream surface generation. Given a set of densely traced streamlines over the flow field, we design a sketch-based interface that allows users to draw simple strokes directly on top of the streamline visualization result. Based on the 2D stroke, we identify a 3D seeding curve and generate a stream surface that captures the flow pattern of streamlines at the outermost layer. Then, we remove the streamlines whose patterns are covered by the stream surface. Repeating this process, users can peel the flow by replacing the streamlines with customized surfaces layer by layer. Our sketch-based interface leverages an intuitive painting metaphor which most users are familiar with. We present results using multiple data sets to show the effectiveness of our approach, and discuss the limitations and future directions.\n\nTitle: A Graph-Based Interface for VisualAnalytics of 3D Streamlines and Pathlines\nContent: Visual exploration of large and complex 3D steady and unsteady flow fields is critically important in many areas of science and engineering. In this paper, we introduce FlowGraph, a novel compound graph representation that organizes field line clusters and spatiotemporal regions hierarchically for occlusion-free and controllable visual exploration. It works with any seeding strategy as long as the domain is well covered and important flow features are captured. By transforming a flow field to a graph representation, we enable observation and exploration of the relationships among field line clusters, spatiotemporal regions and their interconnection in the transformed space. FlowGraph not only provides a visual mapping that abstracts field line clusters and spatiotemporal regions in various levels of detail, but also serves as a navigation tool that guides flow field exploration and understanding. Through brushing and linking in conjunction with the standard field line view, we demonstrate the effectiveness of FlowGraph with several visual exploration and comparison tasks that cannot be well accomplished using the field line view alone. We also perform an empirical expert evaluation to confirm the usefulness of this graph-based technique.\n\nTitle: Parallel hierarchical visualization of large time-varying 3D vector fields\nContent: We present the design of a scalable parallel pathline construction method for visualizing large time-varying 3D vector fields. A 4D (i.e., time and the 3D spatial domain) representation of the vector field is introduced to make a time-accurate depiction of the flow field. This representation also allows us to obtain pathlines through streamline tracing in the 4D space. Furthermore, a hierarchical representation of the 4D vector field, constructed by clustering the 4D field, makes possible interactive visualization of the flow field at different levels of abstraction. Based on this hierarchical representation, a data partitioning scheme is designed to achieve high parallel efficiency. We demonstrate the performance of parallel pathline visualization using data sets obtained from terascale flow simulations. This new capability will enable scientists to study their time-varying vector fields at the resolution and interactivity previously unavailable to them.\n\nTitle: A Vocabulary Approach to Partial Streamline Matching and Exploratory Flow Visualization\nContent: Measuring the similarity of integral curves is fundamental to many important flow data analysis and visualization tasks such as feature detection, pattern querying, streamline clustering, and hierarchical exploration. In this paper, we introduce FlowString, a novel vocabulary approach that extracts shape invariant features from streamlines and utilizes a string-based method for exploratory streamline analysis and visualization. Our solution first resamples streamlines by considering their local feature scales. We then classify resampled points along streamlines based on the shape similarity around their local neighborhoods. We encode each streamline into a string of well-selected shape characters, from which we construct meaningful words for querying and retrieval. A unique feature of our approach is that it captures intrinsic streamline similarity that is invariant under translation, rotation and scaling. We design an intuitive interface and user interactions to support flexible querying, allowing exact and approximate searches for partial streamline matching. Users can perform queries at either the character level or the word level, and define their own characters or words conveniently for customized search. We demonstrate the effectiveness of FlowString with several flow field data sets of different sizes and characteristics. We also extend FlowString to handle multiple data sets and perform an empirical expert evaluation to confirm the usefulness of this approach.\n\nTitle: FlowGraph: A compound hierarchical graph for flow field exploration\nContent: Visual exploration of large and complex 3D flow fields is critically important for understanding many aero- and hydro-dynamical systems that dominate various physical and natural phenomena in the world. In this paper, we introduce the FlowGraph, a novel compound graph representation that organizes streamline clusters and spatial regions hierarchically for occlusion-free and controllable visual exploration. Our approach works with any seeding strategies as long as the domain is well covered and important flow features are captured. By transforming a flow field to a graph representation, we enable observation and exploration of the relationships among streamline clusters, spatial regions and their interconnection in the transformed space. The FlowGraph not only provides a visual mapping that abstracts streamline clusters and spatial regions in various levels of detail, but also serves as a navigation tool that guides flow field exploration and understanding. Through brushing and linking in conjunction with the spatial streamline view, we demonstrate the effectiveness of FlowGraph with several visual exploration and comparison tasks that can not be well accomplished using the streamline view alone. As occlusion and clutter are almost ubiquitous in 3D flows, the FlowGraph represents a promising direction for enhancing our ability to understand large and complex flow field data.\n\n## Task\nCompose an abstract for the title 'A Sketch-Based Interface For Classifying And Visualizing Vector Fields' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In flow visualization, field lines are often used to convey both global and local structure and movement of the flow. One challenge is to find and classify the representative field lines. Most existing solutions follow an automatic approach that generates field lines characterizing the flow and arranges these lines into a single picture. In our work, we advocate a user-centric approach to exploring 3D vector fields. Our method allows the user to sketch 2D curves for pattern matching in 2D and field lines clustering in 3D. Specifically, a 3D field line whose view-dependent 2D projection is most similar to the user drawing will be identified and utilized to extract all similar 3D field lines. Furthermore, we employ an automatic clustering method to generate field-line templates for the user to locate sub-fields of interest. This semi-automatic process leverages the user's knowledge about the flow field through intuitive user interaction, resulting in a promising alternative to existing flow visualization solutions. With our sketch-based interface, the user can effectively dissect the flow field and make more structured visualization for analysis or presentation."
      }
    ]
  },
  {
    "tid":"251",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Estimating specular normals from spherical Stokes reflectance fields\nContent: Despite being at the focal point of intense research in both computer graphics as well as in computer vision, accurately reproducing the shape and appearance of real-world scenes remains a challenging problem, especially under uncontrolled conditions. One cue that has been used to separate diffuse and specular reflectance is polarization. Recent work in computer graphics has explored polarization of incident illumination in conjunction with spherical gradient illumination to infer high quality diffuse-specular separation of both albedo as well as photometric normal information [Ma et al. 2007]. Ghosh et al. [2010] improved upon this by removing the view-dependence of the polarization scheme of Ma et al. by analyzing the Stokes reflectance field under incident circularly polarized spherical gradient illumination, and recover more detailed specular reflectance information including index of refraction as well as specular roughness.\n\nTitle: Acquiring reflectance and shape from continuous spherical harmonic illumination\nContent: We present a novel technique for acquiring the geometry and spatially-varying reflectance properties of 3D objects by observing them under continuous spherical harmonic illumination conditions. The technique is general enough to characterize either entirely specular or entirely diffuse materials, or any varying combination across the surface of the object. We employ a novel computational illumination setup consisting of a rotating arc of controllable LEDs which sweep out programmable spheres of incident illumination during 1-second exposures. We illuminate the object with a succession of spherical harmonic illumination conditions, as well as photographed environmental lighting for validation. From the response of the object to the harmonics, we can separate diffuse and specular reflections, estimate world-space diffuse and specular normals, and compute anisotropic roughness parameters for each view of the object. We then use the maps of both diffuse and specular reflectance to form correspondences in a multiview stereo algorithm, which allows even highly specular surfaces to be corresponded across views. The algorithm yields a complete 3D model and a set of merged reflectance maps. We use this technique to digitize the shape and reflectance of a variety of objects difficult to acquire with other techniques and present validation renderings which match well to photographs in similar lighting.\n\nTitle: Estimating specular roughness and anisotropy from second order spherical gradient illumination\nContent: This paper presents a novel method for estimating specular roughness and tangent vectors, per surface point, from polarized second order spherical gradient illumination patterns. We demonstrate that for isotropic BRDFs, only three second order spherical gradients are sufficient to robustly estimate spatially varying specular roughness. For anisotropic BRDFs, an additional two measurements yield specular roughness and tangent vectors per surface point. We verify our approach with different illumination configurations which project both discrete and continuous fields of gradient illumination. Our technique provides a direct estimate of the per-pixel specular roughness and thus does not require off-line numerical optimization that is typical for the measure-and-fit approach to classical BRDF modeling.\n\nTitle: Practical Acquisition and Rendering of Diffraction Effects in Surface Reflectance\nContent: We propose two novel contributions for measurement-based rendering of diffraction effects in surface reflectance of planar homogeneous diffractive materials. As a general solution for commonly manufactured materials, we propose a practical data-driven rendering technique and a measurement approach to efficiently render complex diffraction effects in real time. Our measurement step simply involves photographing a planar diffractive sample illuminated with an LED flash. Here, we directly record the resultant diffraction pattern on the sample surface due to a narrow-band point source illumination. Furthermore, we propose an efficient rendering method that exploits the measurement in conjunction with the Huygens-Fresnel principle to fit relevant diffraction parameters based on a first-order approximation. Our proposed data-driven rendering method requires the precomputation of a single diffraction look-up table for accurate spectral rendering of complex diffraction effects. Second, for sharp specular samples, we propose a novel method for practical measurement of the underlying diffraction grating using out-of-focus \u201cbokeh\u201d photography of the specular highlight. We demonstrate how the measured bokeh can be employed as a height field to drive a diffraction shader based on a first-order approximation for efficient real-time rendering. Finally, we also drive analytic solutions for a few special cases of diffraction from our measurements and demonstrate realistic rendering results under complex light sources and environments.\n\nTitle: Data-driven diffuse-specular separation of spherical gradient illumination\nContent: Separation of the diffuse and specular components of observed reflectance has been an active area of research in computer graphics and vision, with major applications in reflectance modeling and scene analysis. Traditionally, researchers have investigated diffusespecular separation under point or directional illumination conditions while employing polarization and, in the case of dielectric materials, color space analysis techniques. Recently, Ma et al. [2007] introduced a technique for estimating high quality diffuse and specular normals and albedo maps (see Fig. 1, (a) & (d)) of a specular object using polarized spherical gradient illumination. However, the employed polarization technique imposes view-point restriction, and results in insufficient light levels for performance capture with high speed acquisition. Hence, in this work, we look into an alternate diffuse-specular separation technique for spherical gradients based on a data-driven reflectance model. Traditional separation techniques based on color space analysis focus on removing specular reflections from the observation for scene analysis [Mallick et al. 2005]. In contrast, we focus on obtaining high quality estimates of both the diffuse and the specular reflectance components.\n\n## Task\nCompose an abstract for the title 'Estimating surface normals from spherical stokes reflectance fields' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper we introduce a novel technique for estimating surface normals from the four Stokes polarization parameters of specularly reflected light under a single spherical incident lighting condition that is either unpolarized or circularly polarized. We illustrate the practicality of our technique by estimating surface normals under uncontrolled outdoor illumination from just four observations from a fixed viewpoint."
      }
    ]
  },
  {
    "tid":"252",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Common due-window problem: Polynomial algorithms for a given processing sequence\nContent: The paper considers the Common Due-Window (CDW) problem where a single machine processes a certain number of jobs against a common due-window. Each job possesses different processing times but different and asymmetric earliness and tardiness penalties. The objective of the problem is to find the processing sequence of jobs, their completion times and the position of the given due-window to minimize the total penalty incurred due to tardiness and earliness of the jobs. This work presents exact polynomial algorithms for optimizing a given job sequence for a single machine with the run-time complexity of O(n2), where n is the number of jobs. We also provide an O(n) algorithm for optimizing the CDW with unit processing times. The algorithms take a sequence consisting of all the jobs (Ji, i = 1, 2, ..., n) as input and return the optimal completion times, which offers the minimum possible total penalty for the sequence. Furthermore, we implement our polynomial algorithms in conjunction with Simulated Annealing (SA) to obtain the best processing sequence. We compare our results with that of Biskup and Feldmann for different due-window lengths.\n\nTitle: Two-stage ensemble memetic algorithm: Function optimization and digital IIR filter design\nContent: The research on optimal design of infinite-impulse response (IIR) filters based on optimization techniques has gained much attention in recent years. However, due to the limited performance of the applied optimization techniques, the orders of the filters, which can be obtained, are very low in the previous research. Memetic algorithms (MAs) are widely recognized to have better convergence capability than their conventional counterparts. However, the universality of the MAs, e.g. the ability of solving diverse kinds of digital IIR filter designs, is still limited. In this paper, we design a Two-Stage ensemble Memetic Algorithm (TSMA) framework to more appropriately synthesize the strengths of the evolutionary global search and local search techniques. In the first optimization stage, a competition is held among the candidate local search techniques. Its major idea is to choose the best local search technique and to obtain good initial state. Inheriting the good information of the first stage, the second optimization stage is to implement effective adaptive MA to pursue high-quality solution. The experimental studies presented in this paper contain three aspects: (1) the benefits of the TSMA framework are experimentally investigated by comparing TSMA with its sub-optimizers and recent effective evolutionary algorithms (EAs) on 26 test functions; then (2) TSMA is compared with 4 MAs on the CEC05 functions to comprehensively show the advantages of TSMA; and (3) the TSMA and 6 state-of-the-art algorithms are applied to design high-order digital infinite-impulse response (IIR) filters. The experimental results definitely demonstrate the excellent effectiveness, efficiency and reliability of TSMA on both function optimization and digital IIR filter design tasks.\n\nTitle: Template Design Using Extremal Optimization with Multiple Search Operators\nContent: The template design problem is a constrained optimization problem originated from the printing industry. It involves printing several variations of a design onto one or more stencil sheets, where the aims are to minimize the number of stencils as well as the overproduction of prints of a particular design. Over the years, exact solution methods have been used to solve the problem. These methods could be useful for small to moderate-sized problem instances. However, when the problem instances are huge, the search space may easily grow too large for the systematic approaches. To date, no meta-heuristic or soft computing techniques have been used for this problem. In this paper, we propose the use of Extremal Optimization (EO) with multiple search operators for solving the template design problem. Different combinations of the search operators are tested via extensive numerical experiments. The results show that EO is indeed a feasible approach for template design optimization. The hybridization of EO with a deterministic local search has proven to be particularly effective.\n\nTitle: A New Memetic Algorithm With Fitness Approximation for the Defect-Tolerant Logic Mapping in Crossbar-Based Nanoarchitectures\nContent: The defect-tolerant logic mapping (DTLM), which has been proved to be an NP-complete combinatorial search problem, is a key step for logic implementation in emerging crossbar-based nano-architectures. However, no practically satisfactory solution has been suggested for the DTLM until now. In this paper, the problem of DTLM is first modeled as a combinatorial optimization problem through the introduction of maximum-bipartite-matching. Then, a new memetic algorithm with fitness approximation (MA/FA) is proposed to solve the optimization problem efficiently. In MA/FA, a new greedy reassignment local search operator, capable of utilizing the domain knowledge and information from problem instances, is designed to help the algorithm find optimal logic mapping with consumption of relatively lower computational resources. A fitness approximation method is adopted to reduce the time consumption of fitness evaluation dramatically. In addition, a hybrid fitness evaluation strategy that combines the exact and approximated fitness evaluation methods is presented to balance the accuracy and time efficiency of fitness evaluation. The effectiveness and efficiency of the proposed methods are testified and evaluated on a large set of benchmark instances of various scales, and the advantage of MA/FA on keeping good balance between effectiveness and efficiency is also observed.\n\nTitle: Difficult features of combinatorial optimization problems and the tunable w-model benchmark problem for simulating them.\nContent: The first event of the Black-Box Discrete Optimization Benchmarking (BB-DOB) workshop series aims to establish a set of example problems for benchmarking black-box optimization algorithms for discrete or combinatorial domains. In this paper, we 1) discuss important features that should be embodied by these benchmark functions and 2) present the W-Model problem which exhibits them. The W-Model follows a layered approach, where each layer can either be omitted or introduce a different characteristic feature such as neutrality via redundancy, ruggedness and deceptiveness, epistasis, and multi-objectivity, in a tunable way. The model problem is defined over bit string representations, which allows for extracting some of its layers and stacking them on top of existing problems that use this representation, such as OneMax, the Maximum Satisfiability or the Set Covering tasks, and the NK landscape. The ruggedness and deceptiveness layer can be stacked on top of any problem with integer-valued objectives. We put the W-Model into the context of related model problems targeting ruggedness, neutrality, and epistasis. We then present the results of a series of experiments to further substantiate the utility of the W-Model and to give an idea about suitable configurations of it that could be included in the BB-DOB benchmark suite.\n\n\n\n## Task\nCompose an abstract for the title 'Tackling Common Due Window Problem with a Two-Layered Approach.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This work presents a polynomial algorithm to optimize any given job sequence for the Common Due-Window (CDW) Problem. The CDW problem comprises of scheduling and sequencing a set of jobs against a due-window to minimize the total weighted earliness/tardiness penalty. This due-window is defined by the left and right common due-dates. Jobs that finish before (after) the left (right) due-date are termed as early (tardy) jobs. We present an exact polynomial algorithm for optimally scheduling a given fixed job sequence for a single machine with the runtime complexity of O(n), where n is the number of jobs. The linear algorithm and a heuristic based on the V-shaped property are then incorporated with a modified Simulated Annealing (SA) algorithm to obtain the optimal/near-optimal solutions. We carry out computational experiments to demonstrate the utility of our approach over the benchmark instances and previous work on this problem."
      }
    ]
  },
  {
    "tid":"253",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Empirical analysis of evolutionary algorithms with immigrants schemes for dynamic optimization\nContent: In recent years, there has been a growing interest in studying evolutionary algorithms (EAs) for dynamic optimization problems (DOPs). Among approaches developed for EAs to deal with DOPs, immigrants schemes have been proven to be beneficial. Immigrants schemes for EAs on DOPs aim at maintaining the diversity of the population throughout the run via introducing new individuals into the current population. In this paper, we carefully examine the mechanism of generating immigrants, which is the most important issue among immigrants schemes for EAs in dynamic environments. We divide existing immigrants schemes into two types, namely the direct immigrants scheme and the indirect immigrants scheme, according to the way in which immigrants are generated. Then experiments are conducted to understand the difference in the behaviors of different types of immigrants schemes and to compare their performance in dynamic environments. Furthermore, a new immigrants scheme is proposed to combine the merits of two types of immigrants schemes. The experimental results show that the interactions between the two types of schemes reveal positive effect in improving the performance of EAs in dynamic environments.\n\nTitle: An Adaptive Framework to Tune the Coordinate Systems in Evolutionary Algorithms.\nContent: The performance of many nature-inspired optimization algorithms (NIOAs) depends strongly on their implemented coordinate system. However, the commonly used coordinate system is fixed and not well suited for different function landscapes, NIOAs thus might not search efficiently. To overcome this shortcoming, in this paper we propose a framework, named ACoS, to adaptively tune the coordinate systems in NIOAs. In ACoS, an Eigen coordinate system is established by making use of the cumulative population distribution information, which can be obtained based on a covariance matrix adaptation strategy and an additional archiving mechanism. Since the population distribution information can reflect the features of the function landscape to some extent, NIOAs in the Eigen coordinate system have the capability to identify the modality of the function landscape. In addition, the Eigen coordinate system is coupled with the original coordinate system, and they are selected according to a probability vector. The probability vector aims to determine the selection ratio of each coordinate system for each individual, and is adaptively updated based on the collected information from the offspring. ACoS has been applied to two of the most popular paradigms of NIOAs, i.e., particle swarm optimization and differential evolution, for solving 30 test functions with 30D and 50D at the 2014 IEEE Congress on Evolutionary Computation. The experimental studies demonstrate its effectiveness.\n\nTitle: A new self-adaptation scheme for differential evolution.\nContent: The performance of Differential Evolution (DE) largely depends on the choice of trial vector generation strategy and the values of its control parameters. In the past years, quite a few DE variants have been developed to adaptively adjust the strategy and control parameters during the search process. However, these variants may not perform satisfactorily when coping with computationally expensive problems (CEPs) for which a satisfying solution needs to be obtained with very limited fitness evaluations (FEs). In this paper, we demonstrate that not only can surrogate models be used to approximate the fitness function, they can also provide a good alternative method to adapt the strategy and control parameters of DE, and thus propose a framework called DE with Surrogate-assisted Self-Adaptation (DESSA). DESSA generates multiple trial vectors using different trial vector generation strategies and parameter settings, and then employs a surrogate model to identify the potentially best trial vector to undergo real fitness evaluation. As each trial vector corresponds to a unique combination of strategy and parameter setting, the surrogate model acts like a strategy/parameter setting selector that aims to identify the most suitable strategy and parameter setting for each target vector. Since DESSA can be easily combined with different DE variants, three concrete DE variants, namely DESSA-CoDE, DESSA-SaDE, and DESSA-CoDE*, are proposed. Comprehensive empirical studies demonstrate that DESSA can lead to superior performance over the compared adaptive DE variants. More importantly, it is shown that DESSA has the potential of accommodating more search strategies, which may lead to novel DE variants with even more competitive performance.\n\nTitle: An adaptive coevolutionary Differential Evolution algorithm for large-scale optimization\nContent: In this paper, we propose a new algorithm, named JACC-G, for large scale optimization problems. The motivation is to improve our previous work on grouping and adaptive weighting based cooperative coevolution algorithm, DECC-G [1], which uses random grouping strategy to divide the objective vector into subcomponents, and solve each of them in a cyclical fashion. The adaptive weighting mechanism is used to adjust all the subcomponents together at the end of each cycle. In the new JACC-G algorithm: (1) A most recent and efficient Differential Evolution (DE) variant, JADE [2], is employed as the subcomponent optimizer to seek for a better performance; (2) The adaptive weighting is time-consuming and expected to work only in the first few cycles, so a detection module is added to prevent applying it arbitrarily; (3) JADE is also used to optimize the weight vector in adaptive weighting process instead of using a basic DE in previous DECC-G. The efficacy of the proposed JACC-G algorithm is evaluated on two sets of widely used benchmark functions up to 1000 dimensions.\n\nTitle: An Adaptive Framework to Tune the Coordinate Systems in Nature-Inspired Optimization Algorithms.\nContent: The performance of many nature-inspired optimization algorithms (NIOAs) depends strongly on their implemented coordinate system. However, the commonly used coordinate system is fixed and not well suited for different function landscapes, NIOAs thus might not search efficiently. To overcome this shortcoming, in this paper we propose a framework, named ACoS, to adaptively tune the coordinate systems in NIOAs. In ACoS, an Eigen coordinate system is established by making use of the cumulative population distribution information, which can be obtained based on a covariance matrix adaptation strategy and an additional archiving mechanism. Since the population distribution information can reflect the features of the function landscape to some extent, NIOAs in the Eigen coordinate system have the capability to identify the modality of the function landscape. In addition, the Eigen coordinate system is coupled with the original coordinate system, and they are selected according to a probability vector. The probability vector aims to determine the selection ratio of each coordinate system for each individual, and is adaptively updated based on the collected information from the offspring. ACoS has been applied to two of the most popular paradigms of NIOAs, i.e., particle swarm optimization and differential evolution, for solving 30 test functions with 30D and 50D at the 2014 IEEE Congress on Evolutionary Computation. The experimental studies demonstrate its effectiveness.\n\n## Task\nCompose an abstract for the title 'Immigrant schemes for evolutionary algorithms in dynamic environments: Adapting the replacement rate' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"One approach for evolutionary algorithms (EAs) to address dynamic optimization problems (DOPs) is to maintain diversity of the population via introducing immigrants. So far all immigrant schemes developed for EAs have used fixed replacement rates. This paper examines the impact of the replacement rate on the performance of EAs with immigrant schemes in dynamic environments, and proposes a self-adaptive mechanism for EAs with immigrant schemes to address DOPs. Our experimental study showed that the new approach could avoid the tedious work of fine-tuning the parameter and outperformed other immigrant schemes using a fixed replacement rate with traditionally suggested values in most cases."
      }
    ]
  },
  {
    "tid":"254",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: 2D meets 3D: a human-centered interface for visual data exploration\nContent: There is still a controversial debate on the usefulness of 3D user interfaces. Most of the time, 2D metaphors are force-fitted to establish a rich set of functionality. With the aim of shifting the 20-year-old 2D WIMP (windows, icons, menus, and pointing device) paradigm towards a more natural and intuitive 3D user interface, we have developed a hybrid 2D + 3D prototype targeted at immersive environments. Our complementary display and interaction environment combines visual design techniques with mixed-mode interaction to support typical tasks of information workers to the greatest potential. To achieve this, we match visualization and interaction metaphors by the principle of dimensional congruence. This results in a sophisticated and more intuitive user interface. An ongoing evaluation gives encouraging feedback and shows that even non-expert users can efficiently work with the system.\n\nTitle: Giga-scale multiresolution volume rendering on distributed display clusters\nContent: Visualizing the enormous level of detail comprised in many of today's data sets is a challenging task and demands special processing techniques as well as a presentation on appropriate display devices. Desktop computers and laptops are often not suited for this task because data sets are simply too large and the limited screen size of these devices prevents users from perceiving the entire data set and severely restricts collaboration. Large high-resolution displays that combine the images of multiple smaller devices to form one large display area have proven to be an adequate solution to the ever-growing quantity of available data. The displays offer enough screen real estate to visualize such data sets entirely and facilitate collaboration, since multiple users are able to perceive the information at the same time. For an interactive visualization, the CPUs on the cluster driving the GPUs can be used to split up the computation of a scene into different areas, where each area is computed by a different rendering node. In this paper we focus on volumetric data sets and introduce a dynamic subdivision scheme incorporating multi-resolution wavelet representation to visualize data sets with several gigabytes of voxel data interactively on distributed rendering clusters. The approach makes efficient use of the resources available on modern graphics cards which mainly limit the amount of data that can be visualized. The implementation was successfully tested on a tiled display comprised of 25 compute nodes driving 50 LCD panels.\n\nTitle: The Reflection Layer Extension to the Stereoscopic Highlight Technique for Node-Link Diagrams: An Empirical Study.\nContent: The stereoscopic highlighting technique is a new emerging technique that supports using the depth cue in 3D devices to encode some attributes in general node-link diagrams. In this paper, we extend the original stereoscopic highlighting technique by adding an extra reflection layer at the bottom of the graph to help users detecting the highlighted nodes faster and more accurate. We measure the accuracy of this extension in the technique by evaluating the users' ability of reading the variation of nodes depth values using the proposed reflection layer. We carried out a controlled user-study. The results show that using the depth cue in stereoscopic devices is readable for medium data sizes with at most three or four different layers without the reflection layer, while they were able to read more complicated configurations through the support of the reflection layer with higher accuracy.\n\nTitle: Multivariate Networks: A Novel Edge Visualization Approach for Graph-based Visual Analysis Tasks.\nContent: Providing insight in complex networks or graphs with multivariate data is one of the main challenges for visual analysis today. Much work has been done for visualizing information on nodes, but the space in between has mostly not been used yet. We present the current progress of our approach for using this free space to visualize additional information. We developed two techniques called Partially filled Bars and Bars of Varying Height. Both techniques enable presenting multiple attribute values on the edges of a network simultaneously. We briefly discuss first use cases for such interfaces as well as advantages and disadvantages of both techniques. For proof the concept, a preliminary evaluation has been performed. The results show, that both techniques are promising for many use cases.\n\nTitle: Dimensional congruence for interactive visual data mining and knowledge discovery\nContent: Many authors in the field of 3D human computer interaction have described the advantages of 3D user interfaces: Intuitive metaphors from daily life, immersive workspaces, virtual environments that closely resemble natural environments, and the usefulness of the third dimension as an additional visualization scale. Nevertheless, there are still few or no successful applications for complex information spaces that make use of it in a natural human-centered manner. In this paper, we propose a novel hybrid user interface for knowledge workers to support document spaces and corresponding visualizations. We propose a combined 2D + 3D interface to support both visualization approaches and interaction metaphors to their maximum potential. Interaction is matched due to the principle of dimensional congruence and a thorough investigation of previous approaches and problems is given. Finally, a user study to evaluate the effectiveness and usability of the proposed system is presented.\n\n## Task\nCompose an abstract for the title 'ExpanD: a stereoscopic expanding technique for compound graphs' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this work, we present a new technique, called ExpanD, for expanding compound nodes in stereoscopic platforms. In this technique, stereoscopic depth is used to encode the structural relationships in compound graphs using the containment representation, while explicit edges are used to represent the adjacency relations between the graph nodes. Further, stereoscopic depth is used to show the parent-child relations in such a manner that the children level is rendered in a plane closer to the viewer than the parent node. The technique provides a novel interactive operation for expanding or contracting nodes in order to align graph nodes in the 3D space with minimum occlusion. Different visual cues are used together to encode other data aspects, e.g., color to encode node status, shape to encode node type, and stereoscopic depth to encode hierarchical relations between nodes."
      }
    ]
  },
  {
    "tid":"255",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Consistent Viewing And Interaction For Multiple Users In Projection-Based Vr Systems\nContent: In projection-based Virtual Reality(VR) systems, typically only one headtracked user views stereo images rendered from the correct view position. For other users, who are presented a distorted image, moving with the first user's head motion, it is difficult to correctly view and interact with 3D objects in the virtual environment. In close-range VR systems, such as the Virtual Workbench, distortion effects are especially large because objects are within close range and users are relatively Jar apart. On these systems, multi-user collaboration proves to be difficult. In this paper we analyze the problem and describe a novel, easy to implement method to prevent and reduce image distortion and its negative effects on close-range interaction task performance. First, our method combines a shared camera model and view distortion compensation. It minimizes the overall distortion for each user while important user personal objects such as interaction cursors, rays and controls remain distortion-free. Second, our method retains co-location for interaction techniques to make interaction more consistent. We performed a user experiment on our Virtual Workbench to analyze user performance under distorted view conditions with and without the use of our method. Our findings demonstrate the negative impact of view distortion on task performance and the positive effect our method introduces. This indicates that our method can enhance the multi-user collaboration experience on close-range, projection-based VR systems.\n\nTitle: Interactive visualization of multi-field medical data using linked physical and feature-space views\nContent: Multi-field datasets contain multiple parameters defined over the same spatio-temporal domain. In medicine, such multi-field data is being used more often every day, and there is an urgent need for exploratory visualization approaches that are able to deal effectively with the data-analysis. In this paper, we present a highly interactive, coordinated view-based visualization approach that has been developed especially for dealing with multi-field medical data. It can show any number of views of the physical domain and also of the abstract high-dimensional feature space. The approach has been optimized for interactive use with very large datasets. It is based on intuitive interaction techniques, and integrates analysis techniques from pattern classification to guide the exploration process. We will give some details about the implementation, and we demonstrate the utility of our approach with two real medical use cases.\n\nTitle: Interactive out-of-core isosurface visualisation in time-varying data sets\nContent: We present a combination of techniques for interactive out-of-core visualisation of isosurfaces from large time-dependent data sets. We make use of an index tree, computed in a pre-processing stage, which effectively captures temporal coherence in the data set. This tree data structure enables fast extraction of all isovalue-spanning cells from any time step and for any isovalue. For very large time-dependent data sets, such as those resulting from CFD simulations, this data structure can easily become too large to fit in main memory. Therefore, we have adapted the generation of the data structure, as well as the data structure itself for out-of-core application. During generation, the data set is spatially divided into several regions, each resulting in a separate tree. For visualisation, the application uses all these trees simultaneously, but will use only part of each of the trees. Only a user-specified time window will be kept in main memory and other parts of the tree will be read and released on demand. Finally, to avoid time-consuming triangulation and surface reconstruction, we have used a hardware-assisted direct point rendering algorithm for displaying the isosurfaces. These combined techniques allow interactive exploration and visualisation of very large time-varying data sets on a normal PC.\n\nTitle: Hybrid interfaces in VEs: intent and interaction\nContent: Hybrid user interfaces (UIs) integrate well-known 2D user interface elements into the 3D virtual environment, and provide a familiar and portable interface across a variety of VR systems. However, their usability is often reduced by accuracy and speed, caused by inaccuracies in tracking and a lack of constraints and feedback. To ease these difficulties often large widgets and bulky interface elements must be used, which, at the same time, limit the size of the 3D workspace and restrict the space where other supplemental 2D information can be displayed. In this paper, we present two developments addressing this problem: supportive user interaction and a new implementation of a hybrid interface. First, we describe a small set of tightly integrated 2D windows we developed with the goal of providing increased flexibility in the UI and reducing UI clutter. Next we present extensions to our supportive selection technique, IntenSelect. To better cope with a variety of VR and UI tasks, we extended the selection assistance technique to include direct selection, spring-based manipulation, and specialized snapping behavior. Finally, we relate how the effective integration of these two developments eases some of the UI restrictions and produces a more comfortable VR experience.\n\nTitle: Flexible Abstraction Layers For Vr Application Development\nContent: The development of domain-specific Virtual Reality applications is often a slow and laborious process. The integration of the domain-specific functionality in an interactive Virtual Environment requires close collaboration between domain expert and VR developer, as well as the integration of domain-specific data and software in a VR application. The software environment needs to support the entire development process and software life cycle, from the early stages of iterative, rapid prototyping to a final end-user application. In this paper, we propose the use of flexible abstraction layers in the form of a dynamic scripting language, which act as the glue between VR system components and external software libraries and applications. First, we discuss the motivation and potential of our approach, after which we overview related approaches. Then, we describe the integration of a Python interpreter in our VR toolkit. The potential of our integration approach is demonstrated by rapid prototyping features, the flexible extension of core functionality and the integration of an external toolkit. We conclude with an overview of implications our approach has for the future development of new framework features and application integration.\n\n## Task\nCompose an abstract for the title 'Towards intuitive exploration tools for data visualization in VR' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper we present a basic set of intuitive exploration tools for the data visualization in a Virtual Environment on the Responsive Workbench. First, we introduce the Plexipad, a transparent acrylic panel which allows two-handed interaction in combination with a stylus. After a description of various interaction scenarios with these two devices, we present a basic set of interaction tools, which support the user in the process of exploring volumetric datasets. Besides the interaction tools for navigation and selection we present tools that are closely coupled with probing tools. These interactive probing tools are used as input for complex visualization tools and for performing virtual measurements. We illustrate the use of our tools in two applications from different research areas which use volumetric and particle data."
      }
    ]
  },
  {
    "tid":"256",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Phase estimation for fast action recognition and trajectory generation in human-robot collaboration\nContent: AbstractThis paper proposes a method to achieve fast and fluid human-robot interaction by estimating the progress of the movement of the human. The method allows the progress, also referred to as the phase of the movement, to be estimated even when observations of the human are partial and occluded; a problem typically found when using motion capture systems in cluttered environments. By leveraging on the framework of Interaction Probabilistic Movement Primitives, phase estimation makes it possible to classify the human action, and to generate a corresponding robot trajectory before the human finishes his/her movement. The method is therefore suited for semi-autonomous robots acting as assistants and coworkers. Since observations may be sparse, our method is based on computing the probability of different phase candidates to find the phase that best aligns the Interaction Probabilistic Movement Primitives with the current observations. The method is fundamentally different from approaches based on Dynamic Time Warping that must rely on a consistent stream of measurements at runtime. The resulting framework can achieve phase estimation, action recognition and robot trajectory coordination using a single probabilistic representation. We evaluated the method using a seven-degree-of-freedom lightweight robot arm equipped with a five-finger hand in single and multi-task collaborative experiments. We compare the accuracy achieved by phase estimation with our previous method based on dynamic time warping.\n\nTitle: Active learning using mean shift optimization for robot grasping\nContent: When children learn to grasp a new object, they often know several possible grasping points from observing a parent's demonstration and subsequently learn better grasps by trial and error. From a machine learning point of view, this process is an active learning approach. In this paper, we present a new robot learning framework for reproducing this ability in robot grasping. For doing so, we chose a straightforward approach: first, the robot observes a few good grasps by demonstration and learns a value function for these grasps using Gaussian process regression. Subsequently, it chooses grasps which are optimal with respect to this value function using a mean-shift optimization approach, and tries them out on the real system. Upon every completed trial, the value function is updated, and in the following trials it is more likely to choose even better grasping points. This method exhibits fast learning due to the data-efficiency of the Gaussian process regression framework and the fact that the mean-shift method provides maxima of this cost function. Experiments were repeatedly carried out successfully on a real robot system. After less than sixty trials, our system has adapted its grasping policy to consistently exhibit successful grasps.\n\nTitle: Optimizing Execution of Dynamic Goal-Directed Robot Movements with Learning Control.\nContent: Highly dynamic tasks that require large accelerations and precise tracking usually rely on accurate models and/or high gain feedback. While kinematic optimization allows for efficient representation and online generation of hitting trajectories, learning to track such dynamic movements with inaccurate models remains an open problem. In particular, stability issues surrounding the learning performance, in the iteration domain, can prevent the successful implementation of model based learning approaches. To achieve accurate tracking for such tasks in a stable and efficient way, we propose a new adaptive Iterative Learning Control (ILC) algorithm that is implemented efficiently using a recursive approach. Moreover, covariance estimates of model matrices are used to exercise caution during learning. We evaluate the performance of the proposed approach in extensive simulations and in our robotic table tennis platform, where we show how the striking performance of two seven degree of freedom anthropomorphic robot arms can be optimized. Our implementation on the table tennis platform compares favorably with high-gain PD-control, model-free ILC (simple PD feedback type) and model-based ILC without cautious adaptation.\n\nTitle: Online optimal trajectory generation for robot table tennis.\nContent: In highly dynamic tasks that involve moving targets, planning is necessary to figure out when, where and how to intercept the target. In robotic table tennis in particular, motion planning can be very challenging due to time constraints, dimension of the search space and joint limits. Conventional planning algorithms often rely on a fixed virtual hitting plane to construct robot striking trajectories. These algorithms, however, generate restrictive strokes and can result in unnatural strategies when compared with human playing. In this paper, we introduce a new trajectory generation framework for robotic table tennis that does not involve a fixed hitting plane. A free-time optimal control approach is used to derive two different trajectory optimizers. The resulting two algorithms, Focused Player and Defensive Player, encode two different play-styles. We evaluate their performance in simulation and in our robot table tennis platform with a high speed cable-driven seven DOF robot arm. The algorithms return the balls with a higher probability to the opponent\u2019s court when compared with a virtual hitting plane based method. Moreover, both can be run online and the trajectories can be corrected with new ball observations.\n\nTitle: Online kernel-based learning for task-space tracking robot control.\nContent: Task-space control of redundant robot systems based on analytical models is known to be susceptive to modeling errors. Data-driven model learning methods may present an interesting alternative approach. However, learning models for task-space tracking control from sampled data is an ill-posed problem. In particular, the same input data point can yield many different output values, which can form a nonconvex solution space. Because the problem is ill-posed, models cannot be learned from such data using common regression methods. While learning of task-space control mappings is globally ill-posed, it has been shown in recent work that it is locally a well-defined problem. In this paper, we use this insight to formulate a local kernel-based learning approach for online model learning for task-space tracking control. We propose a parametrization for the local model, which makes an application in task-space tracking control of redundant robots possible. The model parametrization further allows us to apply the kernel-trick and, therefore, enables a formulation within the kernel learning framework. In our evaluations, we show the ability of the method for online model learning for task-space tracking control of redundant robots.\n\n## Task\nCompose an abstract for the title 'Trajectory planning for optimal robot catching in real-time' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Many real-world tasks require fast planning of highly dynamic movements for their execution in real-time. The success often hinges on quickly finding one of the few plans that can achieve the task at all. A further challenge is to quickly find a plan which optimizes a desired cost. In this paper, we will discuss this problem in the context of catching small flying targets efficiently. This can be formulated as a non-linear optimization problem where the desired trajectory is encoded by an adequate parametric representation. The optimizer generates an energy-optimal trajectory by efficiently using the robot kinematic redundancy while taking into account maximal joint motion, collision avoidance and local minima. To enable the resulting method to work in real-time, examples of the global planner are generalized using nearest neighbour approaches, Support Vector Machines and Gaussian process regression, which are compared in this context. Evaluations indicate that the presented method is highly efficient in complex tasks such as ball-catching."
      }
    ]
  },
  {
    "tid":"257",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Energy frugal tags in reprogrammable I-caches for application-specific embedded processors\nContent: Presents a software-directed customization methodology for minimizing the energy dissipation in the instruction cache (I-cache), one of the most power-consuming microarchitectural components of high-end embedded processors. We target particularly the instruction cache tag operations and show how an exceedingly small number of tag bits, if any, are needed to compute the miss/hit behavior for the most frequently executed application loops, thus minimizing the energy needed to perform the tag reads and comparisons. The proposed methodology exploits the fact that the code layout structure of the program loops can be identified after compile and link, and that it typically resides in a very confined memory location, for which very few bits from the effective address can be utilized as a tag. Subsequently, we present an efficient, programmable implementation to apply the suggested energy minimization technique. The experimental results show a significant decrease in energy dissipation for a set of real-life applications\n\nTitle: Application-aware adaptive cache architecture for power-sensitive mobile processors\nContent: Today, mobile smartphones are expected to be able to run the same complex, algorithm-heavy, memory-intensive applications that were originally designed and coded for general-purpose processors. All the while, it is also expected that these mobile processors be power-conscientious as well as of minimal area impact. These devices pose unique usage demands of ultra-portability but also demand an always-on, continuous data access paradigm. As a result, this dichotomy of continuous execution versus long battery life poses a difficult challenge. This article explores a novel approach to mitigating mobile processor power consumption while abating any significant degradation in execution speed. The concept relies on efficiently leveraging both compile-time and runtime application memory behavior to intelligently target adjustments in the cache to significantly reduce overall processor power, taking into account both the dynamic and leakage power footprint of the cache subsystem. The simulation results show a significant reduction in power consumption of approximately 13&percnt; to 29&percnt;, while only incurring a nominal increase in execution time and area.\n\nTitle: Energy-efficient physically tagged caches for embedded processors with virtual memory\nContent: In this paper we present a low-power tag organization for physically tagged caches in embedded processors with virtual memory support. An exceedingly small subset of tag bits is identified for each application hot-spot so that only these tag bits are used for cache access with no performance sacrifice as they provide complete address resolution. The minimal subset of physical tag bits, i.e. the compressed tag, is dynamically updated following the changes in the physical address space of the application. Special support from the operating system (OS) is introduced in order to maintain the compressed tag during program execution. The compressed tag is updated by the OS to match the current set of physical memory pages allocated to the application. We have proposed efficient algorithms that are incorporated within the memory allocator and the dynamic linker in order to achieve dynamic update of the compressed tags in the cases where the mapping between virtual and physical addresses is modified; such cases include memory allocation/deallocation and swapping physical pages on the secondary memory storage. The only hardware support needed within the l/D-caches is the support for disabling bitlines of the tag arrays. An extensive set of experimental results demonstrates the efficacy of the proposed approach.\n\nTitle: Dynamic tag reduction for low-power caches in embedded systems with virtual memory\nContent: This paper presents a low-power tag organization for physically tagged caches in embedded processors with virtual memory support. An exceedingly small subset of tag bits is identified for each application hot-spot so that only these tag bits are used for cache access with no performance sacrifice as they provide complete address resolution. The minimal subset of physical tag bits is dynamically updated following the changes in the physical address space of the application. Operating system support is introduced in order to maintain the reduced tags during program execution. Efficient algorithms are incorporated within the memory allocator and the dynamic linker in order to achieve dynamic update of the reduced tags. The only hardware support needed within the I/D-caches is the support for disabling bitlines of the tag arrays. An extensive set of experimental results demonstrates the efficacy of the proposed approach.\n\nTitle: Register allocation for embedded systems to simultaneously reduce energy and temperature on registers\nContent: Energy and thermal issues are two important concerns for embedded system design. Diminished energy dissipation leads to a longer battery life, while reduced temperature hotspots decelerate the physical failure mechanisms. The instruction fetch logic associated with register access has a significant contribution towards the total energy consumption. Meanwhile, the register file has also been previously shown to exhibit the highest temperature compared to the rest of the components in an embedded processor. Therefore, the optimization of energy and the resolution of the thermal issue for register accesses are of great significance. In this article, register allocation techniques are studied to simultaneously reduce energy consumption and heat buildup on register accesses for embedded systems. Contrary to prevailing intuition, we observe that optimizing energy and optimizing temperature on register accesses conflict with each other. We introduce a rotator hardware in the instruction decoder to facilitate a balanced solution for the two conflicting objectives. Algorithms for register allocation and refinement are proposed based on the access patterns and the effects of the rotator. Experimental results show that the proposed algorithms obtain notable improvements of energy and peak temperature for embedded applications.\n\n## Task\nCompose an abstract for the title 'Performance and energy efficient cache migrationapproach for thermal management in embedded systems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper we propose an approach for performance and power aware warm start for the data cache during core level migration events that originate from overheating. We utilize the concept of reuse in the references to eliminate unnecessary information from being migrated. Furthermore, we exploit the temperature predictability to trigger the cache migration slightly before the actual thermal limit to allow sufficient time for the extraction of the reuse information and transfer of it to the destination cache while the execution is resuming normally. The suggested hardware not only is cost efficient but is also programmable so as to maintain flexibility in targeting application particularities. The experimental results we provide confirm the applicability of this approach."
      }
    ]
  },
  {
    "tid":"258",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: SafeDrive: safe and recoverable extensions using language-based techniques\nContent: We present SafeDrive, a system for detecting and recovering from type safety violations in software extensions. SafeDrive has low overhead and requires minimal changes to existing source code. To achieve this result, SafeDrive uses a novel type system that provides fine-grained isolation for existing extensions written in C. In addition, SafeDrive tracks invariants using simple wrappers for the host system API and restores them when recovering from a violation. This approach achieves fine-grained memory error detection and recovery with few code changes and at a significantly lower performance cost than existing solutions based on hardware-enforced domains, such as Nooks, L4, and Xen, or software-enforced domains, such as SFI. The principles used in SafeDrive can be applied to any large system with loadable, error-prone extension modules. In this paper we describe our experience using SafeDrive for protection and recovery of a variety of Linux device drivers. In order to apply SafeDrive to these device drivers, we had to change less than 4% of the source code. SafeDrive recovered from all 44 crashes due to injected faults in a network card driver. In experiments with 6 different drivers, we observed increases in kernel CPU utilization of 4-23% with no noticeable degradation in end-to-end performance.\n\nTitle: Capriccio: scalable threads for internet services\nContent: This paper presents Capriccio, a scalable thread package for use with high-concurrency servers. While recent work has advocated event-based systems, we believe that thread-based systems can provide a simpler programming model that achieves equivalent or superior performance.By implementing Capriccio as a user-level thread package, we have decoupled the thread package implementation from the underlying operating system. As a result, we can take advantage of cooperative threading, new asynchronous I/O mechanisms, and compiler support. Using this approach, we are able to provide three key features: (1) scalability to 100,000 threads, (2) efficient stack management, and (3) resource-aware scheduling.We introduce linked stack management, which minimizes the amount of wasted stack space by providing safe, small, and non-contiguous stacks that can grow or shrink at run time. A compiler analysis makes our stack implementation efficient and sound. We also present resource-aware scheduling, which allows thread scheduling and admission control to adapt to the system's current resource usage. This technique uses a blocking graph that is automatically derived from the application to describe the flow of control between blocking points in a cooperative thread package. We have applied our techniques to the Apache 2.0.44 web server, demonstrating that we can achieve high performance and scalability despite using a simple threaded programming model.\n\nTitle: A Software Framework for Hard Real-Time Distributed Embedded Systems\nContent: The paper presents a framework for distributed embedded applications that can be used to engineer open, and the same time, predictable embedded systems. Applications are composed from components (actors), which communicate transparently by exchanging labeled messages (signals) over a real-time network. The signals are exchanged at precisely specified time instants, in accordance with the Distributed Timed Multitasking (DTM) model of computation, resulting in the elimination of task and transaction I/O jitter. DTM is supported by an operational environment, which has been integrated with application components in an implementation model specifying explicitly the composition of software nodes allocated to physical network nodes. The framework is characterized by complete separation of computation and communication, whereby communication is delegated to the timed-multitasking operational environment. This has resulted in a simplified application model in which actors have been reduced to actor tasks composed of prefabricated components, such as state machine and action function blocks.\n\nTitle: Simulink analysis of component-based embedded applications\nContent: The widespread use of embedded systems requires the creation of industrial software technology, which will make it possible to engineer systems that are correct by construction. That can be achieved through the use of validated (trusted) components, verification of design models and automatic configuration of applications from validated design models. These guidelines have been instrumental for developing COMDES - a component-based framework for real-time embedded control systems. In this framework, an application is conceived as a network of distributed embedded actors that communicate with one another by means of labeled messages (signals), whereby I/O signals are exchanged with the controlled plant at precisely specified time instants, resulting in the elimination of I/O jitter. The paper presents an analysis method that can be used to validate COMDES design models using the Simulink environment. It is based on a semantics-preserving transformation of a COMDES design model into a Simulink analysis model, which preserves both the functional and timing behaviour of the original design model. The discussion is illustrated with an industrial case study -- a Medical Ventilator Control System, which has been used to validate the developed design and analysis methods.\n\nTitle: A multi-dimensional trust evaluation model for large-scale P2P computing\nContent: In large-scale peer-to-peer (P2P) computing, trust management technology has become a key requirement for network security. However, trust is one of the most complex concepts in network communities. It involves many factors, such as, assumptions, expectations, behaviors, risks, and so on. As a result, trustworthiness has multi-dimensional properties. In this paper, an innovative trust model is proposed for large-scale P2P computing, in which multiple factors are incorporated to reflect the complexity of trust. More importantly, the properties (weights) of these factors are dynamically assigned by weighted moving average and ordered weighted averaging (WMA-OWA) combination algorithms. This model surpasses the limitations of existing approaches, wherein weights are assigned subjectively. The simulation results show that, compared with the existing approaches, the proposed model provides greater accuracy and a more detailed analysis in trust evaluation.\n\n## Task\nCompose an abstract for the title 'Beyond bug-finding: sound program analysis for Linux' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"It is time for us to focus on sound analyses for our critical systems software--that is, we must focus on analyses that ensure the absence of defects of particular known types, rather than best-effort bug-finding tools. This paper presents three sample analyses for Linux that are aimed at eliminating bugs relating to type safety, deallocation, and blocking. These analyses rely on lightweight programmer annotations and run-time checks in order to make them practical and scalable. Sound analyses of this sort can check a wide variety of properties and will ultimately yield more reliable code than bug-finding alone."
      }
    ]
  },
  {
    "tid":"259",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Using the multi-living agent concept to investigate complex information systems.\nContent: In this paper, we propose the multi-living agent (MLA) concept based on the of analysis the characteristics of complex information\n systems, especially those systems that require multi-functional operations under strict restraint strong countermeasures (SRSC)\n environment. First, we investigate the representation of the livelihood of the system under the SRSC conditions, and obtain\n the basic dynamical presentation of the MLA from the profile of the system\u2019s function. Next, we propose the concept of the\n living self-organization based on the self-organization profile of the system, and derive a Two-Set model of the living self-organization\n mechanism. Further, based on the above results, we present a basic construction model of MLA-based information systems in\n the field of information security and countermeasures. A three-level negotiation-coordination mechanism is also derived. Finally,\n we present two practical examples to show how to use the MLA concept to analyze and study real complex information systems.\n In our opinion, the proposal of the MLA will bridge the gap between the research of the application and the basic application\n level of science. It provides the principle research methods and the supporting theories for the construction and analysis\n of complex information systems in the information security and countermeasures field.\n\nTitle: A Novel Multi-carrier Order Division Multi-access Communication System Based on TDCS with Fractional Fourier Transform Scheme\nContent: In this paper, we propose a novel fractional Fourier transform (FrFT) based multi-carrier order division multi-access communication system, in which each user is uniquely identified by an FrFT order. Transform domain communication system (TDCS) with FrFT scheme is also proposed to synthesize the wide-band baseband waveforms in all FrFT domains with different users' FrFT orders, which enhances the interference avoidance capability of this system under most of interference. Therefore, multiple independent data streams can be transmitted by using FrFT---OFDM in the same time and different FrFT domains. However, chirp bases, as the new kind of carriers with different modulated rates, are merely mutually approximately orthogonal. There is a problem of energy leakage between multiple chirp carriers, which possibly causes the multiple chirp carriers inter-shielding to influence the FrFT---OFDM demodulation performance. An efficient allocation algorithm of multiple chirp carriers by presetting the carrier parameters is proposed to solve this problem. Based on MC-CDMA, a variable bit rate system structure is proposed for TDCS with FrFT scheme under different channel environments. In order to simplify the process of modulation and demodulation of TDCS with FrFT scheme, a whole new cyclic shift key modulation mode in FrFT domain is also proposed. Both theories and simulations confirm strictly the validity of the proposed system.\n\nTitle: A Network Security Analysis Method Using Vulnerability Correlation\nContent: Recently in-depth analysis of network security vulnerability must consider attacker exploits not just in isolation, but also in combination. The general approach to this problem is to compute attack graphs using a variety of graph-based algorithms. However, such methods generally suffer the exponential state space problem. Therefore, this paper brings forward two conceptions of vulnerability correlation matrix and vulnerability correlation graph (VCG). An algorithm based on vulnerability correlation matrix was proposed to generate VCGs. An example was given to illustrate the application and effect of the algorithm in network security analysis. Deep analysis proves that VCGs have polynomial complexity of the number of network vulnerabilities and scale well for large networks. Moreover, the example shows that VCGs are a good help to and convenient for network security management.\n\nTitle: Detection and parameter estimation of multicomponent LFM signal based on the fractional fourier transform\nContent: This paper presents a new method for the detection and parameter estimation of multicomponent LFM signals based on the fractional Fourier transform. For the optimization in the fractional Fourier domain, an algorithm based on Quasi-Newton method is proposed which consists of two steps of searching, leading to a reduction in computation without loss of accuracy. And for multicomponent signals, we further propose a signal separation technique in the fractional Fourier domain which can effectively suppress the interferences on the detection of the weak components brought by the stronger components. The statistical analysis of the estimate errors is also performed which perfects the method theoretically, and finally, simulation results are provided to show the validity of our method.\n\nTitle: An improved self-organizing CPN-based fuzzy system with adaptive back-propagation algorithm\nContent: This paper describes an improved self-organizing CPN-based (Counter-Propagation Network) fuzzy system. Two self-organizing algorithms IUSOCPN and ISSOCPN, being unsupervised and supervised respectively, are introduced. The idea is to construct the neural-fuzzy system with a two-phase hybrid learning algorithm, which utilizes a CPN-based nearest-neighbor clustering scheme for both structure learning and initial parameters setting, and a gradient descent method with adaptive learning rate for fine tuning the parameters. The obtained network can be used in the same way as a CPN to model and control dynamic systems, while it has a faster learning speed than the original back-propagation algorithm. The comparative results on the examples suggest that the method is fairly efficient in terms of simple structure, fast learning speed, and relatively high modeling accuracy.\n\n## Task\nCompose an abstract for the title 'Research on distributed intrusion detection system based on multi-living agent.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a novel distributed intrusion detection system based on multi-living agent inspired by the research\n method of multi-living agent complex information system for improving the security of distributed intrusion detection system\n using mobile agent. The previous two-layer distributed intrusion detection system has been expanded into a three-layer structure,\n the new layer of which can monitor the livelihood of each detection agent and utilize the negotiation-coordination mechanism\n to adjust the working way of each agent for protecting the security of system. In this paper, we propose a quantitative analysis\n method of agent livelihood in distributed intrusion detection system and further present the design method of living agent.\n Finally, through the analysis of system working flow and the detection effect of distributed denial of service attack, we\n have shown that the security and stability of the novel distributed intrusion detection system is effectively enhanced and\n the agent can work normally in strict restraint strong countermeasure environment."
      }
    ]
  },
  {
    "tid":"260",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Interactive Segmentation by Image Foresting Transform on Superpixel Graphs.\nContent: There are many scenarios in which user interaction is essential for effective image segmentation. In this paper, we present a new interactive segmentation method based on the Image Foresting Transform (IFT). The method over segments the input image, creates a graph based on these segments (super pixels), receives markers (labels) drawn by the user on some super pixels and organizes a competition to label every pixel in the image. Our method has several interesting properties: it is effective, efficient, capable of segmenting multiple objects in almost linear time on the number of super pixels, readily extendable through previously published techniques, and benefits from domain-specific feature extraction. We also present a comparison with another technique based on the IFT, which can be seen as its pixel-based counterpart. Another contribution of this paper is the description of automatic (robot) users. Given a ground truth image, these robots simulate interactive segmentation by trained and untrained users, reducing the costs and biases involved in comparing segmentation techniques.\n\nTitle: How to fix any 3D segmentation interactively via Image Foresting Transform and its use in MRI brain segmentation\nContent: In medical imaging, there are many approaches for automatic segmentation. However, none of these methods provide any effective solution to correct segmentation interactively, which becomes a necessity in the case of poorly defined structures. Manual segmentation can not be an alternative given that it might be unfeasible in many cases. On the other hand, how to complete a poor automatic segmentation in an interactive tool is an issue, since the automatic approach and the tool may have been designed with different optimization criteria. We propose solutions to this problem using the framework of the \u201cImage Foresting Transform\u201d (IFT), with evaluation in the context of the segmentation of MR-T1 brain structures. The results indicate that effective semi-automatic correction is possible using just a few markers.\n\nTitle: Differential and relaxed image foresting transform for graph-cut segmentation of multiple 3D objects.\nContent: Graph-cut algorithms have been extensively investigated for interactive binary segmentation, when the simultaneous delineation of multiple objects can save considerable user's time. We present an algorithm (named DRIFT) for 3D multiple object segmentation based on seed voxels and Differential Image Foresting Transforms (DIFTs) with relaxation. DRIFT stands behind efficient implementations of some stateof- the-art methods. The user can add/remove markers (seed voxels) along a sequence of executions of the DRIFT algorithm to improve segmentation. Its first execution takes linear time with the image's size, while the subsequent executions for corrections take sublinear time in practice. At each execution, DRIFT first runs the DIFT algorithm, then it applies diffusion filtering to smooth boundaries between objects (and background) and, finally, it corrects possible objects' disconnection occurrences with respect to their seeds. We evaluate DRIFT in 3D CT-images of the thorax for segmenting the arterial system, esophagus, left pleural cavity, right pleural cavity, trachea and bronchi, and the venous system.\n\nTitle: Fuzzy-connected 3D image segmentation at interactive speeds\nContent: Image segmentation techniques using fuzzy connectedness principles have shown their effectiveness in segmenting a variety of objects in several large applications in recent years. However, one problem with these algorithms has been their excessive computational requirements. In an attempt to substantially speed them up, in the present paper, we study systematically a host of 18 'optimal' graph search algorithms. Extensive testing of these algorithms on a variety of 3D medical images taken from large ongoing applications demonstrates that a 20-1000-fold improvement over current speeds is achievable with a combination of algorithms and fast modern PCs. Utilizing efficient algorithms and careful selection of implementations can speed up the computation of fuzzy connectedness values by a factor of 16-29 (on the same hard-ware), as compared to the implementation previously used in our applications utilizing fuzzy object segmentation. The optimality of an algorithm depends on the input data as well as on the choice of the fuzzy affinity relation. The running time is reduced considerably (by a factor up to 34 for brain MR and even more for bone CT), when the algorithms make use of pre-determined thresholds for the fuzzy objects. The reliable recognition (assisted by human operators) and the accurate, efficient, and sophisticated delineation (automatically performed by the computer) can be effectively incorporated into a single interactive process. If images having intensities with tissue-specific meaning (such as CT or standardized MR images) are utilized, most of the parameters for the segmentation method can be fixed once for all, all intermediate data (feature and fuzzy affinity values for the whole scene) can be computed before the user interaction is needed and the user can be provided with more information at the time of interaction.\n\nTitle: Multiscale skeletons by image foresting transform and its application to neuromorphometry\nContent: The image foresting transform (IFT) reduces optimal image partition problems based on seed pixels to a shortest-path forest problem in a graph, whose solution can be obtained in linear time. Such a strategy has allowed a unified and efficient approach to the design of image processing operators, such as edge tracking, region growing, watershed transforms, distance transforms, and connected filters. This paper presents a fast and simple method based on the IFT to compute multiscale skeletons and shape reconstructions without border shifting. The method also generates one-pixel-wide connected skeletons and the skeleton by influence zones, simultaneously, for objects of arbitrary topologies. The results of the work are illustrated with respect to skeleton quality, execution time, and its application to neuromorphometry.\n\n## Task\nCompose an abstract for the title 'Interactive volume segmentation with differential image foresting transforms.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The absence of object information very often asks for considerable human assistance in medical image segmentation. Many interactive two-dimensional and three-dimensional (3-D) segmentation methods have been proposed, but their response time to user&#39;s actions should be considerably reduced to make them viable from the practical point of view. We circumvent this problem in the framework of the image..."
      }
    ]
  },
  {
    "tid":"261",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Inflatable Icons: Diffusion-Based Interactive Extrusion of 2D Images into 3D Models\nContent: There are many applications, such as rapid prototyping, simulations and presentations, where non-professional computer end-users could benefit from the ability to create simple 3D models. Existing tools are geared towards the creation of production quality 3D models by professional users with sufficient background, time and motivation to overcome steep learning curves. Inflatable Icons combine diffusion-based image extrusion with a number of interactively controllable parameters to allow end-users to efficiently design 3D objects. Early user testing has indicated that it takes end-users, even kids, just minutes to draw icons and then use the Inflatable Icons approach to create 3D models which they can use to build video games. \n\nTitle: Using scalable game design to promote 3D fluency: Assessing the AgentCubes incremental 3D end-user development framework\nContent: With the IT crisis reaching alarming levels, it is more important than ever to attract K-12 students to computer science. 3D game development can be an enticing way to achieve that, but building 3D games is far from trivial. Students need to achieve a degree of 3D fluency in modeling, animation and programming to be able to create compelling 3D content. The combination of innovative end-user development tools and standards-based curriculum promoting IT fluency by shifting the pedagogical focus from programming to design, can address motivational aspects without sacrificing principled educational goals. The AgentCubes 3D game-authoring environment raises the ceiling of end-user development without raising the threshold. Our formal user study shows that with Incremental 3D, the gradual approach to transition from 2D to 3D authoring, middle school students can build sophisticated 3D games including 3D models, animations and programming.\n\nTitle: AgentCubes: Raising the Ceiling of End-User Development in Education through Incremental 3D\nContent: Now that we have end-user programming environments capable of empowering kids with no programming background to build games in a matter of hours, a new quest for raising the ceiling of enduser development is emerging. Environments not only focusing on programming, but also including rich media such as 3D, could work as compelling tools for introducing information technology at the K-12 level. The new challenge is raising the ceiling without raising the threshold. Based on our experience with AgentSheets, a tool used worldwide for computational science and game design applications, we created a new authoring tool called AgentCubes. This article discusses the notion of Incremental 3D as a design approach for media-rich end-user development with low threshold and high ceiling in education.\n\nTitle: Tactile Programming: A Unified Manipulation Paradigm Supporting Program Comprehension, Composition and Sharing\nContent: Although visual programming techniques have been used to lower the threshold of programming for end users, they are not sufficient for creating end user programming environments that are both easy to use and powerful. To achieve this, an environment must support the definition of programs that are not just static representations of behavior, but are instead dynamic collections of program objects which can be applied in a number of contexts rather than just a program editor. We describe an approach to end user programming called tactile programming which extends visual techniques with a unified program manipulation paradigm that makes programs easy to comprehend, compose and, most importantly, share over the World Wide Web. Tactile programming's inherent ability to support the social context in which programming takes place along with its ability to ease program comprehension and composition is what differentiates this approach from others. In the context of the Agentsheets programming substrate, we have created an instance of a tactile programming environment called Visual AgenTalk which is used to create interactive simulations.\n\nTitle: AgentCubes: Incremental 3D end-user development\nContent: 3D game development can be an enticing way to attract K-12 students to computer science, but designing and programming 3D games is far from trivial. Students need to achieve a certain level of 3D fluency in modeling, animation, and programming to be able to create compelling 3D content. The combination of innovative end-user development tools and standards-based curriculum that promotes IT fluency by shifting the pedagogical focus from programming to design, can address motivational aspects without sacrificing principled educational goals. The AgentCubes 3D game-authoring environment raises the ceiling of end-user development without raising the threshold. Our formal user study shows that with Incremental 3D, the gradual approach to transition from 2D to 3D authoring, middle school students can build sophisticated 3D games including 3D models, animations, and programming.\n\n## Task\nCompose an abstract for the title 'Agent warp engine: formula based shape warping for networked applications' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Computer visualization and networking have advanced dramatically. 3D hardware acceleration has reached the point where even low-power handheld computers can render and animate complex 3D graphics efficiently. Unfortunately, end-user computing does not yet provide the necessary tools and conceptual frameworks to let end-users access these technologies and build their own networked interactive 2D and 3D applications such as rich visualizations, animations and simulations. The Agent Warp Engine (AWE) is a formula-based shape-warping framework that combines end-user visualization and end-user networking. AWE is a spreadsheet-inspired framework based on Web sharable variables. To build visualizations, users define these variables, relate them through equations and connect them to 2D and 3D shapes. In addition to basic shape control such as rotation, size, and location, AWE enables the creation of rich shape warping visualizations. We motivate the AWE approach with the Mr. Vetro human physiology simulation supporting collaborative learning through networked handheld computers."
      }
    ]
  },
  {
    "tid":"262",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Gaze-Contingent Motor Channelling and Haptic Constraints for Minimally Invasive Robotic Surgery\nContent: The use of master-slave surgical robots for Minimally Invasive Surgery (MIS) has created a physical separation between the surgeon and the patient. Reconnecting the essential visuomotor sensory feedback is important for the safe practice of robotic assisted MIS procedures. This paper introduces a novel gaze contingent framework with real-time haptic feedback by transforming visual sensory information into physical constraints that can interact with the motor sensory channel. We demonstrate how motor tracking of deforming tissue can be made more effective and accurate through the concept of gaze-contingent motor channelling. The method also uses 3D eye gaze to dynamically prescribe and update safety boundaries during robotic assisted MIS without prior knowledge of the soft-tissue morphology. Initial validation results on both simulated and robotic assisted phantom procedures demonstrate the potential clinical value of the technique.\n\nTitle: Gaze-Contingent Motor Channelling, haptic constraints and associated cognitive demand for robotic MIS\nContent: The success of MIS is coupled with an increasing demand on surgeons\u2019 manual dexterity and visuomotor coordination due to the complexity of instrument manipulations. The use of master\u2013slave surgical robots has avoided many of the drawbacks of MIS, but at the same time, has increased the physical separation between the surgeon and the patient. Tissue deformation combined with restricted workspace and visibility of an already cluttered environment can raise critical issues related to surgical precision and safety. Reconnecting the essential visuomotor sensory feedback is important for the safe practice of robot-assisted MIS procedures. This paper introduces a novel gaze-contingent framework for real-time haptic feedback and virtual fixtures by transforming visual sensory information into physical constraints that can interact with the motor sensory channel. We demonstrate how motor tracking of deforming tissue can be made more effective and accurate through the concept of Gaze-Contingent Motor Channelling. The method is also extended to 3D by introducing the concept of Gaze-Contingent Haptic Constraints where eye gaze is used to dynamically prescribe and update safety boundaries during robot-assisted MIS without prior knowledge of the soft-tissue morphology. Initial validation results on both simulated and robot assisted phantom procedures demonstrate the potential clinical value of the technique. In order to assess the associated cognitive demand of the proposed concepts, functional Near-Infrared Spectroscopy is used and preliminary results are discussed.\n\nTitle: Dense surface reconstruction for enhanced navigation in MIS.\nContent: Recent introduction of dynamic view expansion has led to the development of computer vision methods for minimally invasive surgery to artificially expand the intra-operative field-of-view of the laparoscope. This provides improved awareness of the surrounding anatomical structures and minimises the effect of disorientation during surgical navigation. It permits the augmentation of live laparoscope images with information from previously captured views. Current approaches, however, can only represent the tissue geometry as planar surfaces or sparse 3D models, thus introducing noticeable visual artefacts in the final rendering results. This paper proposes a high-fidelity tissue geometry mapping by combining a sparse SLAM map with semi-dense surface reconstruction. The method is validated on phantom data with known ground truth, as well as in-vivo data captured during a robotic assisted MIS procedure. The derived results have shown that the method is able to effectively increase the coverage of the expanded surgical view without compromising mapping accuracy.\n\nTitle: Vision-based deformation recovery for intraoperative force estimation of tool-tissue interaction for neurosurgery.\nContent: In microsurgery, accurate recovery of the deformation of the surgical environment is important for mitigating the risk of inadvertent tissue damage and avoiding instrument maneuvers that may cause injury. The analysis of intraoperative microscopic data can allow the estimation of tissue deformation and provide to the surgeon useful feedback on the instrument forces exerted on the tissue. In practice, vision-based recovery of tissue deformation during tool-tissue interaction can be challenging due to tissue elasticity and unpredictable motion.The aim of this work is to propose an approach for deformation recovery based on quasi-dense 3D stereo reconstruction. The proposed framework incorporates a new stereo correspondence method for estimating the underlying 3D structure. Probabilistic tracking and surface mapping are used to estimate 3D point correspondences across time and recover localized tissue deformations in the surgical site.We demonstrate the application of this method to estimating forces exerted on tissue surfaces. A clinically relevant experimental setup was used to validate the proposed framework on phantom data. The quantitative and qualitative performance evaluation results show that the proposed 3D stereo reconstruction and deformation recovery methods achieve submillimeter accuracy. The force-displacement model also provides accurate estimates of the exerted forces.A novel approach for tissue deformation recovery has been proposed based on reliable quasi-dense stereo correspondences. The proposed framework does not rely on additional equipment, allowing seamless integration with the existing surgical workflow. The performance evaluation analysis shows the potential clinical value of the technique.\n\nTitle: Visual Force Feedback for Hand-Held Microsurgical Instruments\nContent: Microsurgery is technically challenging, demanding both rigorous precision under the operating microscope and great care when handling tissue. Applying excessive force can result in irreversible tissue injury, but sufficient force must be exerted to carry out manoeuvres in an efficient manner. Technological advances in hand-held instruments have allowed the integration of force sensing capabilities into surgical tools, resulting in the possibility of force feedback during an operation. This paper presents a novel method of graduated online visual force-feedback for hand-held microsurgical instruments. Unlike existing visual force-feedback techniques, the force information is integrated into the surgical scene by highlighting the area around the point of contact while preserving salient anatomical features. We demonstrate that the proposed technique can be integrated seamlessly with image guidance techniques. Critical anatomy beyond the exposed tissue surface is revealed using an augmented reality overlay when the user is exerting large forces within their proximity. The force information is further used to improve the quality of the augmented reality by displacing the overlay based on the forces exerted. Detailed user studies were performed to assess the efficacy of the proposed method.\n\n## Task\nCompose an abstract for the title 'Assessment of navigation cues with proximal force sensing during endovascular catheterization.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Despite increased use of robotic catheter navigation systems for endovascular intervention procedures, current master-slave platforms have not yet taken into account dexterous manipulation skill used in traditional catheterization procedures. Information on tool forces applied by operators is often limited. A novel force/torque sensor is developed in this paper to obtain behavioural data across different experience levels and identify underlying factors that affect overall operator performance. The miniature device can be attached to any part of the proximal end of the catheter, together with a position sensor attached to the catheter tip, for relating tool forces to catheter dynamics and overall performance. The results show clear differences in manipulation skills between experience groups, thus providing insights into different patterns and range of forces applied during routine endovascular procedures. They also provide important design specifications for ergonomically optimized catheter manipulation platforms with added haptic feedback while maintaining natural skills of the operators."
      }
    ]
  },
  {
    "tid":"263",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: An Application of Latent Topic Document Analysis to Large-Scale Proteomics Databases\nContent: Since the advent of public data repositories for proteomics data, readily accessible results from high-throughput experiments have been accumulating steadily. Several large-scale projects in particular have contributed substantially to the amount of identifications available to the community. Despite the considerable body of information amassed, very few successful analysis have been performed and published on this data, levelling o the ultimate value of these projects far below their potential. In order to illustrate that these repositories should be considered sources of detailed knowledge instead of data graveyards, we here present a novel way of analyzing the information contained in proteomics experiments with a 'latent semantic analysis'. We apply this information retrieval approach to the peptide identification data contributed by the Plasma Proteome Project. Interestingly, this analysis is able to overcome the fundamental diculties of analyzing such divergent and heterogeneous data emerging from large scale proteomics studies employing a vast spec- trum of dierent sample treatment and mass-spectrometry technologies. Moreover, it yields several concrete recommendations for optimizing pro- teomics project planning as well as the choice of technologies used in the experiments. It is clear from these results that the analysis of large bodies of publicly available proteomics data holds great promise and is currently underexploited.\n\nTitle: Pride: A Public Repository Of Protein And Peptide Identifications For The Proteomics Community\nContent: PRIDE, the 'PRoteomics IDEntifications database' (http://www.ebi.ac.uk/pride) is a database of protein and peptide identifications that have been described in the scientific literature. These identifications will typically be from specific species, tissues and subcellular locations, perhaps under specific disease conditions. Any post-translational modifications that have been identified on individual peptides can be described. These identifications may be annotated with supporting mass spectra. At the time of writing, PRIDE includes the full set of identifications as submitted by individual laboratories participating in the HUPO Plasma Proteome Project and a profile of the human platelet proteome submitted by the University of Ghent in Belgium. By late 2005 PRIDE is expected to contain the identifications and spectra generated by the HUPO Brain Proteome Project. Proteomics laboratories are encouraged to submit their identifications and spectra to PRIDE to support their manuscript submissions to proteomics journals. Data can be submitted in PRIDE XML format if identifications are included or mzData format if the submitter is depositing mass spectra without identifications. PRIDE is a web application, so submission, searching and data retrieval can all be performed using an internet browser. PRIDE can be searched by experiment accession number, protein accession number, literature reference and sample parameters including species, tissue, sub-cellular location and disease state. Data can be retrieved as machine-readable PRIDE or mzData XML ( the latter for mass spectra without identifications), or as human-readable HTML.\n\nTitle: Critical amino acid residues in proteins: a BioMart integration of Reactome protein annotations with PRIDE mass spectrometry data and COSMIC somatic mutations.\nContent: The reversible phosphorylation of serine, threonine and tyrosine hydroxyl groups is an especially prominent form of post-translational modification (PTM) of proteins. It plays critical roles in the regulation of diverse processes, and mutations that directly or indirectly affect these phosphorylation events have been associated with many cancers and other pathologies. Here, we describe the development of a new BioMart tool that gathers data from three different biological resources to provide the user with an integrated view of phosphorylation events associated with a human protein of interest, the complexes of which the protein (modified or not) is a part, the reactions in which the protein and its complexes participate and the somatic mutations that might be expected to perturb those functions. The three resources used are the Reactome, PRIDE and COSMIC databases. The Reactome knowledgebase contains annotations of phosphorylated human proteins linked to the reactions in which they are phosphorylated and dephosphorylated, to the complexes of which they are parts and to the reactions in which the phosphorylated proteins participate as substrates, catalysts and regulators. The PRIDE database holds extensive mass spectrometry data from which protein phosphorylation patterns can be inferred, and the COSMIC database holds records of somatic mutations found in human cancer cells. This tool supports both flexible, user-specified queries and standard ('canned') queries to retrieve frequently used combinations of data for user-specified proteins and reactions. We demonstrate using the Wnt signaling pathway and the human c-SRC protein how the tool can be used to place somatic mutation data into a functional perspective by changing critical residues involved in pathway modulation, and where available, check for mass spectrometry evidence in PRIDE supporting identification of the critical residue. Database URL: http://www.reactome.org/cgi-bin/mart\n\nTitle: InteroPORC: automated inference of highly conserved protein interaction networks.\nContent: Motivation: Proteinprotein interaction networks provide insights into the relationships between the proteins of an organism thereby contributing to a better understanding of cellular processes. Nevertheless, large-scale interaction networks are available for only a few model organisms. Thus, interologs are useful for a systematic transfer of protein interaction networks between organisms. However, no standard tool is available so far for that purpose. Results: In this study, we present an automated prediction tool developed for all sequenced genomes available in Integr8. We also have developed a second method to predict proteinprotein interactions in the widely used cyanobacterium Synechocystis. Using these methods, we have constructed a new network of 8783 inferred interactions for Synechocystis.\n\nTitle: The Proteomics Identifications (Pride) Database And Associated Tools: Status In 2013\nContent: The PRoteomics IDEntifications (PRIDE, http://www.ebi.ac.uk/pride) database at the European Bioinformatics Institute is one of the most prominent data repositories of mass spectrometry (MS)-based proteomics data. Here, we summarize recent developments in the PRIDE database and related tools. First, we provide up-to-date statistics in data content, splitting the figures by groups of organisms and species, including peptide and protein identifications, and post-translational modifications. We then describe the tools that are part of the PRIDE submission pipeline, especially the recently developed PRIDE Converter 2 (new submission tool) and PRIDE Inspector (visualization and analysis tool). We also give an update about the integration of PRIDE with other MS proteomics resources in the context of the ProteomeXchange consortium. Finally, we briefly review the quality control efforts that are ongoing at present and outline our future plans.\n\n## Task\nCompose an abstract for the title 'Hydra: a scalable proteomic search engine which utilizes the Hadoop distributed computing framework.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"For shotgun mass spectrometry based proteomics the most computationally expensive step is in matching the spectra against an increasingly large database of sequences and their post-translational modifications with known masses. Each mass spectrometer can generate data at an astonishingly high rate, and the scope of what is searched for is continually increasing. Therefore solutions for improving our ability to perform these searches are needed.We present a sequence database search engine that is specifically designed to run efficiently on the Hadoop MapReduce distributed computing framework. The search engine implements the K-score algorithm, generating comparable output for the same input files as the original implementation. The scalability of the system is shown, and the architecture required for the development of such distributed processing is discussed.The software is scalable in its ability to handle a large peptide database, numerous modifications and large numbers of spectra. Performance scales with the number of processors in the cluster, allowing throughput to expand with the available resources."
      }
    ]
  },
  {
    "tid":"264",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Unknown solution length problems with no asymptotically optimal run time.\nContent: We revisit the problem of optimizing a fitness function of unknown dimension; that is, we face a function defined over bit-strings of large length N, but only n \u226a N of them have an influence on the fitness. Neither the position of these relevant bits nor their number is known. In previous work, variants of the (1 + 1) evolutionary algorithm (EA) have been developed that solve, for arbitrary s \u2208 \u2115, such OneMax and LeadingOnes instances, simultaneously for all n \u2208 \u2115, in expected time O(n(log(n))2 log log(n) ... log(s\u22121)(n)(log(s)(n))1+\u03b5) and O(n2 log(n) log log(n) ... log(s\u22121)(n)(log(s)(n))1+\u03b5), respectively; that is, in almost the same time as if n and the relevant bit positions were known. In this work, we prove the first, almost matching, lower bounds for this setting. For LeadingOnes, we show that, for every s \u2208 \u2115, the (1 + 1) EA with any mutation operator treating zeros and ones equally has an expected run time of \u03c9(n2 log(n) log log(n) ... log(s)(n)) when facing problem size n. Aiming at closing the small remaining gap, we realize that, quite surprisingly, there is no asymptotically best performance. For any algorithm solving, for all n, all instances of size n in expected time at most T(n), there is an algorithm doing the same in time T'(n) with T' = o(T). For OneMax we show results of similar flavor.\n\nTitle: Unbiased black-box complexities of jump functions: how to cross large plateaus\nContent: We analyze the unbiased black-box complexity of jump functions with large jump sizes. Among other results, we show that when the jump size is (1/2 - epsilon)n, that is, only a small constant fraction of the fitness values is visible, then the unbiased black-box complexities for arities 3 and higher are of the same order as those for the simple OneMax function. Even for the extreme jump function, in which all but the two fitness values n/2 and n are blanked out, polynomial-time mutation-based (i.e., unary unbiased) black-box optimization algorithms exist. This is quite surprising given that for the extreme jump function almost the whole search space (all but a Theta(n-1/2) fraction) is a plateau of constant fitness. To prove these results, we introduce new tools for the analysis of unbiased black-box complexities, for example, selecting the new parent individual not by comparing the fitnesses of the competing search points, but also by taking into account the (empirical) expected fitnesses of their offspring.\n\nTitle: Memory-limited non-U-shaped learning with solved open problems\nContent: In empirical cognitive science, for human learning, a semantic or behavioral U-shape occurs when a learner first learns, then unlearns, and, finally, relearns, some target concept. Within the formal framework of Inductive Inference, for learning from positive data, previous results have shown, for example, that such U-shapes are unnecessary for explanatory learning, but are necessary for behaviorally correct and non-trivial vacillatory learning. Herein we also distinguish between semantic and syntactic U-shapes. We answer a number of open questions in the prior literature as well as provide new results regarding syntactic U-shapes. Importantly for cognitive science, we see more of a previously noticed pattern that, for parameterized learning criteria, beyond very few initial parameter values, U-shapes are necessary for full learning power. We analyze the necessity of U-shapes in two memory-limited settings. The first setting is Bounded Memory State (BMS) learning, where a learner has an explicitly-bounded state memory, and otherwise only knows its current datum. We show that there are classes learnable with three (or more) memory states that are not learnable non-U-shapedly with any finite number of memory states. This result is surprising, since, for learning with one or two memory states, U-shapes are known to be unnecessary. This solves an open question from the literature. The second setting is that of Memoryless Feedback (MLF) learning, where a learner may ask a bounded number of questions about what data has been seen so far, and otherwise only knows its current datum. We show that there is a class learnable memorylessly with a single feedback query such that this class is not learnable non-U-shapedly memorylessly with any finite number of feedback queries. We employ self-learning classes together with the Operator Recursion Theorem for many of our results, but we also introduce two new techniques for obtaining results. The first is for transferring inclusion results from one setting to another. The main part of the second is the Hybrid Operator Recursion Theorem, which enables us to separate some learning criteria featuring complexity-bounded learners, employing self-learning classes. Both techniques are not specific to U-shaped learning, but applicable for a wide range of settings.\n\nTitle: More effective crossover operators for the all-pairs shortest path problem\nContent: The all-pairs shortest path problem is the first non-artificial problem for which it was shown that adding crossover can significantly speed up a mutation-only evolutionary algorithm. Recently, the analysis of this algorithm was refined and it was shown to have an expected optimization time (w. r. t. the number of fitness evaluations) of @Q(n^3^.^2^5(logn)^0^.^2^5). In contrast to this simple algorithm, evolutionary algorithms used in practice usually employ refined recombination strategies in order to avoid the creation of infeasible offspring. We study extensions of the basic algorithm by two such concepts which are central in recombination, namely repair mechanisms and parent selection. We show that repairing infeasible offspring leads to an improved expected optimization time of O(n^3^.^2(logn)^0^.^2). As a second part of our study we prove that choosing parents that guarantee feasible offspring results in an optimization time of O(n^3logn). Both results show that already simple adjustments of the recombination operator can asymptotically improve the runtime of evolutionary algorithms.\n\nTitle: Solutions to open questions for non-u-shaped learning with memory limitations\nContent: A U-shape occurs when a learner first learns, then unlearns, and, finally, relearns, some target concept. Within the framework of Inductive Inference, previous results have shown, for example, that U-shapes are unnecessary for explanatory learning, but are necessary for behaviorally correct learning. This paper solves the following two problems regarding non-U-shaped learning posed in the prior literature. First, it is shown that there are classes learnable with three memory states that are not learnable non-U-shapedly with any finite number of memory states. This result is surprising, as for learning with one or two memory states, U-shapes are known to be unnecessary. Secondly, it is shown that there is a class learnable memorylessly with a single feedback query such that this class is not learnable non-U-shapedly memorylessly with any finite number of feedback queries. This result is complemented by the result that any class of infinite languages learnable memorylessly with finitely many feedback queries is so learnable without U-shapes - in fact, all classes of infinite languages learnable with complete memory are learnable memorylessly with finitely many feedback queries and without U-shapes. On the other hand, we show that there is a class of infinite languages learnable memorylessly with a single feedback query, which is not learnable without U-shapes by any particular bounded number of feedback queries.\n\n## Task\nCompose an abstract for the title 'Solving Problems with Unknown Solution Length at (Almost) No Extra Cost' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Most research in the theory of evolutionary computation assumes that the problem at hand has a fixed problem size. This assumption does not always apply to real-world optimization challenges, where the length of an optimal solution may be unknown a priori. Following up on previous work of Cathabard, Lehre, and Yao [FOGA 2011] we analyze variants of the (1+1) evolutionary algorithm for problems with unknown solution length. For their setting, in which the solution length is sampled from a geometric distribution, we provide mutation rates that yield an expected optimization time that is of the same order as that of the (1+1) EA knowing the solution length. We then show that almost the same run times can be achieved even if no a priori information on the solution length is available. Finally, we provide mutation rates suitable for settings in which neither the solution length nor the positions of the relevant bits are known. Again we obtain almost optimal run times for the OneMax and LeadingOnes test functions, thus solving an open problem from Cathabard et al."
      }
    ]
  },
  {
    "tid":"265",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Real-time spatiotemporal stereo matching using the dual-cross-bilateral grid\nContent: We introduce a real-time stereo matching technique based on a reformulation of Yoon and Kweon's adaptive support weights algorithm [1]. Our implementation uses the bilateral grid to achieve a speedup of 200\u00d7 compared to a straightforward full-kernel GPU implementation, making it the fastest technique on the Middlebury website. We introduce a colour component into our greyscale approach to recover precision and increase discriminability. Using our implementation, we speed up spatialdepth superresolution 100\u00d7. We further present a spatiotemporal stereo matching approach based on our technique that incorporates temporal evidence in real time (14 fps). Our technique visibly reduces flickering and outperforms per-frame approaches in the presence of image noise. We have created five synthetic stereo videos, with ground truth disparity maps, to quantitatively evaluate depth estimation from stereo video. Source code and datasets are available on our project website.\n\nTitle: Extracting layers and analyzing their specular properties using epipolar-plane-image analysis\nContent: Despite progress in stereo reconstruction and structure from motion, 3D scene reconstruction from multiple images still faces many difficulties, especially in dealing with occlusions, partial visibility, textureless regions, and specular reflections. Moreover, the problem of recovering a spatially dense 3D representation from many views has not been adequately treated. This document addresses the problems of achieving a dense reconstruction from a sequence of images and analyzing and removing specular highlights. The first part describes an approach for automatically decomposing the scene into a set of spatio-temporal layers (namely EPI-tubes) by analyzing the epipolar plane image (EPI) volume. The key to our approach is to directly exploit the high degree of regularity found in the EPI volume. In contrast to past work on EPI volumes that focused on a sparse set of feature tracks, we develop a complete and dense segmentation of the EPI volume. Two different algorithms are presented to segment the input EPI volume into its component EPI tubes. The second part describes a mathematical characterization of specular reflections within the EPI framework and proposes a novel technique for decomposing a static scene into its diffuse (Lambertian) and specular components. Furthermore, a taxonomy of specularities based on their photometric properties is presented as a guide for designing further separation techniques. The validity of our approach is demonstrated on a number of sequences of complex scenes with large amounts of occlusions and specularity. In particular, we demonstrate object removal and insertion, depth map estimation, and detection and removal of specular highlights.\n\nTitle: Efficient Dense Stereo with Occlusions for New View-Synthesis by Four-State Dynamic Programming\nContent: A new algorithm is proposed for efficient stereo and novel view synthesis. Given the video streams acquired by two synchronized cameras the proposed algorithm synthesises images from a virtual camera in arbitrary position near the physical cameras. The new technique is based on an improved, dynamic-programming, stereo algorithm for efficient novel view generation. The two main contributions of this paper are: (i) a new four state matching graph for dense stereo dynamic programming, that supports accurate occlusion labelling; (ii) a compact geometric derivation for novel view synthesis by direct projection of the minimum cost surface. Furthermore, the paper presents an algorithm for the temporal maintenance of a background model to enhance the rendering of occlusions and reduce temporal artefacts (flicker); and a cost aggregation algorithm that acts directly in the three-dimensional matching cost space.The proposed algorithm has been designed to work with input images with large disparity range, a common practical situation. The enhanced occlusion handling capabilities of the new dynamic programming algorithm are evaluated against those of the most powerful state-of-the-art dynamic programming and graph-cut techniques. Four-state DP is also evaluated against the disparity-based Middlebury error metrics and its performance found to be amongst the best of the efficient algorithms. A number of examples demonstrate the robustness of four-state DP to artefacts in stereo video streams. This includes demonstrations of cyclopean view synthesis in extended conversational sequences, synthesis from a freely translating virtual camera and, finally, basic 3D scene editing.\n\nTitle: Learning priors for calibrating families of stereo cameras\nContent: Online camera recalibration is necessary for long-term deploymentof computervision systems. Existing algorithms assume that the source of recalibration information is a set of features in a general 3D scene; and that enough features are observed that the calibration problem is well- constrained. However, these assumptions are frequently in- valid outside the laboratory. Real-world scenes often lack texture, contain repeated texture, or are mostly planar, mak- ing calibration difficult or impossible. In this paper we consider the calibration of families of stereo cameras, where each camera is assumed to have pa- rameters drawn from a common but unknown prior dis- tribution. We show how estimation of this prior using a small-number of offline-calibrated cameras (e.g. from the same production line) allows online calibration of addi- tional cameras using a small number of point correspon- dences; and that using the estimated prior significantly in- creases the accuracy and robustness of stereo camera cali- bration.\n\nTitle: Dense stereo using pivoted dynamic programming\nContent: This paper describes an improvement to the dynamic programming approach for dense stereo. Traditionally dense stereo algorithms proceed independently for each pair of epipolar lines, and then a further step is used to smooth the estimated disparities between the epipolar lines. This typically results in a streaky disparity map along depth discontinuities. In order to overcome this problem the information from corner and edge matching algorithms are exploited. Indeed we present a unified dynamic programming/statistical framework that allows the incorporation of any partial knowledge about disparities, such as matched features and known surfaces within the scene. The result is a fully automatic dense stereo system with a faster run time and greater accuracy than the standard dynamic programming method.\n\n## Task\nCompose an abstract for the title 'Duality, Rigidity and Planar Parallax' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We investigate the geometry of two views of seven points,four of which are coplanar, and the geometry of three views of six points,four of which are coplanar. We prove that the two are dual, and that the fundamental geometric constraints in each case are encapsulated by aplanar homology. The work unifies a number of previously diverse resultsrelated to planar parallax, duality and planar homologies.In addition, we make a number of practical contributions, including formulae for computing ... "
      }
    ]
  },
  {
    "tid":"266",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Quantifying the effects of objective space dimension in evolutionary multiobjective optimization\nContent: The scalability of EMO algorithms is an issue of significant concern for both algorithm developers and users. A key aspect of the issue is scalability to objective space dimension, other things being equal. Here, we make some observations about the efficiency of search in discrete spaces as a function of the number of objectives, considering both uncorrelated and correlated objective values. Efficiency is expressed in terms of a cardinality-based (scaling-independent) performance indicator. Considering random sampling of the search space, we measure, empirically, the fraction of the true PF covered after p iterations, as the number of objectives grows, and for different correlations. A general analytical expression for the expected performance of random search is derived, and is shown to agree with the empirical results. We postulate that for even moderately large numbers of objectives, random search will be competitive with an EMO algorithm and show that this is the case empirically: on a function where each objective is relatively easy for an EA to optimize (an NK-landscape with K=2), random search compares favourably to a well-known EMO algorithm when objective space dimension is ten, for a range of inter-objective correlation values. The analytical methods presented here may be useful for benchmarking of other EMO algorithms.\n\nTitle: Reducing Local Optima in Single-Objective Problems by Multi-objectivization\nContent: One common characterization of how simple hill-climbing optimization methods can fail is that they become trapped in local optima - a state where no small modification of the current best solution will produce a solution that is better. This measure of 'better' depends on the performance of the solution with respect to the single objective being optimized. In contrast, multi-objective optimization (MOO) involves the simultaneous optimization of a number of objectives. Accordingly, the multi-objective notion of 'better' permits consideration of solutions that may be superior in one objective but not in another. Intuitively, we may say that this gives a hill-climber in multi-objective space more freedom to explore and less likelihood of becoming trapped. In this paper, we investigate this intuition by comparing the performance of simple hill-climber-style algorithms on single-objective problems and multi-objective versions of those same problems. Using an abstract building-block problem we illustrate how 'multi-objectivizing' a single-objective optimization (SOO) problem can remove local optima. Then we investigate small instances of the travelling salesman problem where additional objectives are defined using arbitrary sub-tours. Results indicate that multi-objectivization can reduce local optima and facilitate improved optimization in some cases. These results enlighten our intuitions about the nature of search in multi-objective optimization and sources of difficulty in single-objective optimization.\n\nTitle: Artefacts and biases affecting the evaluation of scoring functions on decoy sets for protein structure prediction.\nContent: Decoy datasets, consisting of a solved protein structure and numerous alternative native-like structures, are in common use for the evaluation of scoring functions in protein structure prediction. Several pitfalls with the use of these datasets have been identified in the literature, as well as useful guidelines for generating more effective decoy datasets. We contribute to this ongoing discussion an empirical assessment of several decoy datasets commonly used in experimental studies.We find that artefacts and sampling issues in the large majority of these data make it trivial to discriminate the native structure. This underlines that evaluation based on the rank/z-score of the native is a weak test of scoring function performance. Moreover, sampling biases present in the way decoy sets are generated or used can strongly affect other types of evaluation measures such as the correlation between score and root mean squared deviation (RMSD) to the native. We demonstrate how, depending on type of bias and evaluation context, sampling biases may lead to both over- or under-estimation of the quality of scoring terms, functions or methods.Links to the software and data used in this study are available at http://dbkgroup.org/handl/decoy_sets.\n\nTitle: Multiobjectivization by Decomposition of Scalar Cost Functions\nContent: The term `multiobjectivization' refers to the casting of a single-objec-tive optimization problem as a multiobjective one, a transformation that can be achieved by the addition of supplementary objectives or by the decomposition of the original objective function. In this paper, we analyze how multiobjectivization by decompositionchanges the fitness landscape of a given problem and affects search. We find that decomposition has only one possible effect: to introduce plateaus of incomparable solutions. Consequently, multiobjective hillclimbers using no archive `see' a smaller (or at most equal) number of local optima on a transformed problem compared to hillclimbers on the original problem. When archived multiobjective hillclimbers are considered this effect may partly be reversed. Running time analyses conducted on four example functions demonstrate the (positive and negative) influence that both the multiobjectivization itself, and the use vs. non-use of an archive, can have on the performance of simple hillclimbers. In each case an exponential/polynomial divide is revealed.\n\nTitle: Generating, Maintaining, and Exploiting Diversity in a Memetic Algorithm for Protein Structure Prediction.\nContent: Computational approaches to de novo protein tertiary structure prediction, including those based on the preeminent \\\"fragment-assembly\\\" technique, have failed to scale up fully to larger proteins on the order of 100 residues and above. A number of limiting factors are thought to contribute to the scaling problem over and above the simple combinatorial explosion, but the key ones relate to the lack of exploration of properly diverse protein folds, and to an acute form of \\\"deception\\\" in the energy function, whereby low-energy conformations do not reliably equate with native structures. In this article, solutions to both of these problems are investigated through a multistage memetic algorithm incorporating the successful Rosetta method as a local search routine. We found that specialised genetic operators significantly add to structural diversity and that this translates well to reaching low energies. The use of a generalised stochastic ranking procedure for selection enables the memetic algorithm to handle and traverse deep energy wells that can be considered deceptive, which further adds to the ability of the algorithm to obtain a much-improved diversity of folds. The results should translate to a tangible improvement in the performance of protein structure prediction algorithms in blind experiments such as CASP, and potentially to a further step towards the more challenging problem of predicting the three-dimensional shape of large proteins.\n\n## Task\nCompose an abstract for the title 'Investigations into the Effect of Multiobjectivization in Protein Structure Prediction' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Physics-based potential energy functions used in protein structure prediction are composed of several energy terms combined in a weighted sum. `Multiobjectivization' -- splitting up the energy function into its components and optimizing the components as a vector using multiobjective methods -- may have beneficial effects for tackling these difficult problems. In this paper we investigate the hypotheses that multiobjectivization can (i) reduce the number of local optima in the landscapes, as seen by hillclimbers, and (ii) equalize the influence of different energy components that range over vastly different energy scales and hence usually swamp each other's search gradients. The investigations use models of two real molecules, the alanine dipeptide and Metenkephalin under the Amber99 energy function, and consider hillclimbers with a range of mutation step sizes. Our findings support the hypotheses and also indicate that multiobjectivization is competitive with alternative methods of escaping local optima."
      }
    ]
  },
  {
    "tid":"267",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Auction Equilibrium Strategies for Task Allocation in Uncertain Environments\nContent: In this paper we address a model of self interested information agents competing to perform tasks. The agents are situated in an uncertain environment while different tasks dynamically arrive from a central manager. The agents differ in their capabilities to perform a task under different world states. Previous models concerning cooperative agents aiming for a joint goal are not applicable in such environments, since self interested agents have a motivation to deviate from the joint allocation strategy, in order to increase their own benefits. Given the allocation protocol set by the central manager, a stable solution, is a set of strategies, derived from an equilibrium where no agent can benefit from changing its strategy given the other agents' strategies. Specifically we focus on a protocol in which, upon arrival of a new task, the central manager starts a reverse auction among the agents, and the agent who bids the lowest cost wins. We introduce the model, formulate its equations and suggest equilibrium strategies for the agents. By identifying specific characteristics of the equilibria, we manage to suggest an efficient algorithm for enhancing the agents' calculation of the equilibrium strategies. A comparison with the central allocation mechanism, and the effect of environmental settings on the perceived equilibrium are given using several sample environments.\n\nTitle: Effective Information Value Calculation for Interruption Management in Multi-Agent Scheduling\nContent: This paper addresses the problem of deciding effec- tively whether to interrupt a teammate who may have information that is valuable for solving a collaborative scheduling problem. Two characteristics of multi-agent scheduling complicate the determination of the value of the teammate's information, and hence whether it exceeds the costs of an interruption. First, in many scheduling contexts, task and scheduling knowledge re- side in a scheduler module which is external to the agent, and the agent must query that module to estimate the value to the solution of knowing a specific piece of information. Second, the agent does not know the spe- cific information its teammate has, resulting in the need for it to repeatedly query the scheduler. Choosing the right sequence of queries to the scheduler may enable the agent to make an interruption decision sooner, thus saving query time and computational load for both the agent and the external system. This paper defines two new sequencing heuristics which enhance the efficiency of the querying process. It also introduces three met- rics for measuring the efficiency of a query sequence. It presents extensive simulation-based evidence that the new heuristics significantly outperform previously pro- posed methods for determining the value of information a teammate has.\n\nTitle: Solving the auction-based task allocation problem in an open environment\nContent: In this paper we analyze the process of allocating tasks to self-interested agents in uncertain changing open environments. The allocator in our model is responsible for the performance of dynamically arriving tasks using a second price reverse auction as the allocation protocol. Since the agents are self-interested (i.e. each agent attempts to maximize its own revenue), previous models concerning cooperative agents aiming for a joint goal are not applicable. Thus the main challenge is to identify a set of equilibrium strategies - a stable solution where no agent can benefit from changing its strategy given the other agents' strategies - for any specific environmental settings. We formulate the model and discuss the difficulty in extracting the agents' equilibrium strategies directly from the model's equations. Consequently we propose an efficient algorithm to accurately approximate the agents' equilibrium strategies. A comparative illustration through simulation of the system performance in a closed and open environments is given, emphasizing the advantage of the allocator operating in the latter environment, reaching results close to those obtained by a central enforceable allocation.\n\nTitle: Time-Variant Distributed Agent Matching Applications\nContent: The process of pair partnership formation is an important infrastructure for many plausible MAS applications. Each agent evaluates potential partner agents, where each potential match yields a different utility. Commonly, the utility associated with a given agent partner in such two-sided search processes may change over time. This change in the agent\u00fds future attractiveness to potential partners significantly increases the complexity of the agent\u00fds decision making process regarding the set of agents it is willing to partner with. In this paper we analyze the special dynamics and present equilibrium characteristics of such a model. The agents can gain a utility derived from the partner agent\u00fds type. However, as an agent has an incentive to extend its search for a better type partner, the benefit that can be offered to potential partners reduces as the search proceeds. We introduce a two-sided model which takes into consideration a continuous decrease in the agent\u00fds type and formulate the appropriate equilibrium equations. The suggested equilibrium analysis yields an algorithm for the calculation of the agents\u00fd equilibrium strategy. Special emphasis is placed on the scenario where an agent\u00fds attractiveness is influenced by an additional dimension other than just time. Simulation results are presented to illustrate the findings.\n\nTitle: Efficiency and fairness in team search with self-interested agents\nContent: We consider team-work settings where individual agents incur costs on behalf of the team. In such settings it is frequently the custom to reimburse agents for the costs they incur (at least in part) in order to promote fairness. We show, however, that when agents are self-interested such reimbursement can result in degradation in efficiency - at times severe degradation. We thus study the relationship between efficiency and fairness in such settings, distinguishing between ex-ante and ex-post fairness. First, we analyze reimbursement policies that reimburse solely based on purchase receipts (as is customary), and show that with such policies the degradation in both efficiency and fairness can be unbounded. We thus introduce two other families of reimbursement policies. The first family guarantees optimal efficiency and ex-ante fairness, but not ex-post fairness. The second family improves (at times) on ex-post fairness, but at the expense of efficiency, thus providing a tradeoff between the two.\n\n## Task\nCompose an abstract for the title 'Equilibrium Strategies for Task Allocation in Dynamic Multi-Agent Systems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We address a model of self interested agents competing to perform tasks. The agents are situated in an uncertain environment while dif-ferent tasks dynamically arrive from a central manager. The agents differ in their capabilities to perform a task under different world states. The tasks are allocated according to a pre-defined protocol set by a central manager such as a government, a municipality, a com-pany. The central manager lacks the required resources to perform the tasks by itself. The protocol defines the rules for selecting a per-former for a task and the appropriate payment to this performer. The goal of the central manager is to maximize his expected utility, de-fined as a function of the number of tasks being performed and the total payment. Previous models concerning cooperative agents aim-ing for a joint goal are not applicable in such environments, since self interested agents have a motivation to deviate from the joint alloca-tion strategy, in order to increase their own benefits. Given the allo-cation protocol set by the central manager, a stable solution is a set of strategies derived from an equilibrium where no agent can benefit from changing its strategy given the other agents'strategies. Hence, a major challenge in task allocation process for self-interested agents in such environments is to identify the agents'equilibrium strategies, for a given protocol and environmental settings. We suggest a methodology for calculating the agents'equilibrium strategies. Specifically, we focus on a protocol in which, upon arrival of a new task, the central manager starts a Vickrey auction, and the agent who bids the lowest cost wins. In our domains, the number of competing agents is relatively small, and their overall capabilities can be estimated with some probability. Thus, an agent's strategy must consider the long term strategies of the other agents in the en-vironment as part of its own bid strategy determination. An example of such an application can be found in an environment where self interested servers, with different configurations and changing loads, compete for the execution of jobs arriving from an external source. The servers set their strategies on-the-fly, according to their current information of the world sate, and their evaluation of their competing agents'capabilities. The main difference from e-commerce domains is that in e-commerce the rate of new agents entering the environment is relatively high so it is unfeasible to consider the modulation of competitors'future strategies when setting an agent's bid (e. g. [2]). The concept of task allocation in a competitive environment is dis-cussed in several works (e. g., [1, 3]). The main focus of these works is on the commitments and the communication problems that emerge "
      }
    ]
  },
  {
    "tid":"268",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A decentralised public key infrastructure for customer-to-customer e-commerce.\nContent: The success story of eBay has shown the demand for customer-to-customer (C2C) electronic commerce. eBay is a centralised infrastructure with all its scalability problems (network bandwidth, server load, availability, etc.). In this paper we argue that C2C e-commerce is an application domain that maps naturally onto the emerging field of peer-to-peer (P2P) systems simply by its underlying interaction model of customers, i.e., peers. This offers the opportunity to take P2P systems beyond mere file sharing systems into interesting new application domains. The long-term goal would be to design a fully functional decentralised system which resembles eBay without eBay's dedicated, centralised infrastructure. Since security (authenticity, non-repudiation, trust, etc.) is key to any e-commerce infrastructure, our envisioned P2P e-commerce platform has to address these security issues adequately. As the first step in this direction we present an approach for a completely decentralised P2P public key infrastructure (PKI) which can serve as the basis for higher-level security service. We base it on a statistical approach and present an analytical model to quantify its behaviour and properties and to provide probabilistic guarantees. To justify our claims we provide a first-order analysis and discuss the PKI's resilience against various known threats and attack scenarios.\n\nTitle: Efficient, Self-Contained Handling of Identity in Peer-to-Peer Systems\nContent: Identification is an essential building block for many services in distributed information systems. The quality and purpose of identification may differ, but the basic underlying problem is always to bind a set of attributes to an identifier in a unique and deterministic way. Name/directory services, such as DNS, X.500, or UDDI, are a well-established concept to address this problem in distributed information systems. However, none of these services addresses the specific requirements of peer-to-peer systems with respect to dynamism, decentralization, and maintenance. We propose the implementation of directories using a structured peer-to-peer overlay network and apply this approach to support self-contained maintenance of routing tables with dynamic IP addresses in structured P2P systems. Thus, we can keep routing tables intact without affecting the organization of the overlay networks, making it logically independent of the underlying network infrastructure. Even though the directory is self-referential, since it uses its own service to maintain itself, we show that it is robust due to a self-healing capability. For security, we apply a combination of PGP-like public key distribution and a quorum-based query scheme. We describe the algorithm as implemented in the P-Grid P2P lookup system (http://www.p-grid.org/) and give a detailed analysis and simulation results demonstrating the efficiency and robustness of our approach.\n\nTitle: Towards p2p-based semantic web service discovery with qos support\nContent: The growing number of web services advocates distributed discovery infrastructures which are semantics-enabled and support quality of service (QoS). In this paper, we introduce a novel approach for semantic discovery of web services in P2P-based registries taking into account QoS characteristics. We distribute (semantic) service advertisements among available registries such that it is possible to quickly identify the repositories containing the best probable matching services. Additionally, we represent the information relevant for the discovery process using Bloom filters and pre-computed matching information such that search efforts are minimized when querying for services with a certain functional/QoS profile. Query results can be ranked and users can provide feedbacks on the actual QoS provided by a service. To evaluate the credibility of these user reports when predicting service quality, we include a robust trust and reputation management mechanism.\n\nTitle: On P2P collaboration infrastructures\nContent: BSCW and Groove (G. Neworks, 2005) have become the two de-facto standards for collaboration over the Internet. They offer a wealth of functionalities, support a variety of possible collaboration styles, and are applied frequently in research and business projects. While being useful and powerful for standard scenarios, they suffer from some problematic aspects if support for distributed ad-hoc and mobile collaboration is needed. Both are centralized systems which require the setup and maintenance of a server. Setting up such an infrastructure for short-term and ad-hoc collaboration of mobile users is not feasible since it requires the infrastructure to be in place and configured a priori, which is costly in terms of hardware, software, and time. Thus, they are not very adequate for flexible, short term collaborations, a genuine building block of many light-weight distributed mobile collaboration scenarios. In this paper, we discuss how P2P approaches could be applied to remedy these shortcomings and to what extent existing technology can already be applied \"out of the box\".\n\nTitle: The Essence of P2P: A Reference Architecture for Overlay Networks\nContent: The success of the P2P idea has created a huge diversity of approaches, among which overlay networks, for example, Gnutella, Kazaa, Chord, Pastry, Tapestry, P-Grid, or DKS, have received specific attention from both developers and researchers. A wide variety of algorithms, data structures, and architectures have been proposed. The terminologies and abstractions used, however, have become quite inconsistent since the P2P paradigm has attracted people from many different communities, e.g., networking, databases, distributed systems, graph theory, complexity theory, biology, etc. In this paper we propose a reference model for overlay networks which is capable of modeling different approaches in this domain in a generic manner. It is intended to allow researchers and users to assess the properties of concrete systems, to establish a common vocabulary for scientific discussion, to facilitate the qualitative comparison of the systems, and to serve as the basis for defining a standardized API to make overlay networks interoperable.\n\n## Task\nCompose an abstract for the title 'Beyond \"Web Of Trust\": Enabling P2p E-Commerce*' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The huge success of eBay has proven the demand for customer-to-customer (C2C) electronic commerce. eBay is a centralized infrastructure with all its scalability problems (network bandwidth, server load, availability, etc.). In this paper we argue that C2C e-commerce is an application domain that maps naturally onto the emergent field of P2P systems simply by its underlying interaction model of customers, i.e., peers. This offers the opportunity to take P2P systems beyond mere file sharing systems into interesting new application domains. The long-term goal would be to design a fully, functional decentralized system which resembles eBay without eBay's dedicated, centralized infrastructure. Since security (authenticity, non-repudiation, trust, etc.) is key to any e-commerce infrastructure, our envisioned P2P e-commerce platform has to address this adequately. As the first step in this direction we present an approach for a completely decentralized P2P public key infrastructure (PKI) which can serve as the basis for higher-level security service. In contrast to other systems in this area, such as PGP which uses a \"web of trust\" concept, we use a statistical approach which allows us to provide an analytical model with provable guarantees, and quantify the behavior and specific properties of the PKI. To justify our claims we provide a first-order analysis and discuss its resilience against various known threats and attack scenarios. In support of our belief that C2C E-commerce is one of the potential killer applications of the emerging structured P2P systems, we provide a layered model for P2P E-commerce, demonstrating the dependencies of various security related issues that can be built on top of a decentralized PKI."
      }
    ]
  },
  {
    "tid":"269",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Design issues and an empirical study in mobility oriented service development\nContent: The usability of mobile services is degraded while moving, since users cannot pay enough attention to the services in such dynamic and complicated mobile environments. Mobile and wearable services should be appropriately designed to handle human factor issues, otherwise fine user experiences cannot be offered. In this paper, we discuss design issues in mobile service development from a human factor point of view, and propose a design framework to overcome the challenges. Our approach provides two interaction modes (i.e. simple interaction mode and normal interaction mode) to mobile services, in order to allow a user to keep using the service without frequent interruptions while moving. Also, a service's events are simplified in order to support several modalities so that a user can be notified in the most suitable way according to the situation. To evaluate the feasibility of our approach in field experiments, we developed a pedestrian navigation service as a part of the framework. The results showed that the simple interaction mode successfully decreased a user's cognitive load to the service. Also, future directions for further improvements are discussed based on comments and feedbacks.\n\nTitle: Designing Human Behavior Through Social Influence in Mobile Crowdsourcing with Micro-communities.\nContent: This paper proposes a new mobile social media infrastructure for motivating collective people to participate in flourishing our society. For motivating them, designing human behavior through social influence within communities is one of the most important issues to make their lifestyle better, but it is not easy to promote the entire collective people's activities towards achieving a common goal. Our proposal is to use a layered approach where an entire community consists of many micro-communities; if each independent community is encouraged to contribute to its society, the entire community will be finally motivated to achieve a flourishing society. Our approach adopts a virtual currency and a crowdfunding concept to encourage members in a micro-community; we analyze the effect of social influence on human behavior from the experiment-based analysis. The analysis shows that the proposed approach might work well within a community of well- known people. We finally suggest a possible solution to overcome potential limitations as a future direction.\n\nTitle: Mobile Image Search via Local Crowd: A User Study\nContent: In this paper we present a on-field study for evaluating a crowd sourcing mobile social search application. With the help of the local crowd via social medias, this application assists foreign visitors in Japan by answering their image-based questions at hand in a timely fashion. We ran a controlled field experiment for 6 weeks with 55 participants. We found that the mobile crowd sourcing model demonstrated a reliable performance on response speed and response quantity: half of the requests were answered within 10 minutes, 75% of requests were answered within 30 minutes, and on average every request had 4.2 answers. Especially in the afternoon, evening and night, nearly 88% requests were answered in average approximately 10 minutes, with more than 4 answers per request. In terms of participation motivation, we found the top active crowd workers were more driven by intrinsic motivations rather than any of the extrinsic incentives (gamification incentives and social incentives) we designed.\n\nTitle: Towards mobility oriented interaction design: experiments in pedestrian navigation on mobile devices\nContent: Current mobile interaction is not well designed with considering mobility. Usability of a mobile service is degraded while on the move, since users can not pay enough attention to the service in such a dynamic and complicated mobile context. In this paper, we propose the mobile service design framework, which improves the mobility by decreasing the user's cognitive load. Our approach provides two interaction modes (i.e. simple interaction mode and normal interaction mode) to mobile services so that the user can retrieve important information with less attention. Moreover, the service's events are simplified to support several modalities, and thus the user can be notified in the most suitable way according to the situation. In order to evaluate the feasibility of our approach through field experiments, we have developed a pedestrian navigation service as a part of the framework. The results showed that the simple interaction mode successfully decreased the user's attention to the service. Also, future directions for further improvements are discussed based on feedbacks from subjective comments.\n\nTitle: An approach for constructing mobile applications using service proxies\nContent: In this paper, we propose software architecture using a notion of service proxies, that is a new framework for constructing applications in mobile computing environment. Our framework especially takes into account applications accessing services on Internet such as WWW browsers and MBONE conference tools. In our framework, an application is partitioned into two pieces, one piece runs on a mobile computer, and another piece runs on a stationary computer, where they are connected by wireless networks that may be replaced on the fly. The piece on a stationary computer is called a service proxy. The service proxy filters or caches data from servers before transmitting the data to the piece on a mobile computer. These two pieces are constructed by composing small objects whose composition can be dynamically reconfigured by adding or removing replaceable devices. The paper also presents a toolkit using our framework for building mobile applications easily.\n\n## Task\nCompose an abstract for the title 'Design Strategies for Building Mobile Crowdsourcing Services' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, we present an overview of three community-based mobile crowd sourcing services that we have developed as case studies. We then extract four lessons learned from our experiences to show that motivating people is an important factor in designing mobile crowd sourcing service. The extracted lessons are essential to successful design strategies for developing future crowd sourcing services."
      }
    ]
  },
  {
    "tid":"270",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Logging every footstep: quantile summaries for the entire history\nContent: Quantiles are a crucial type of order statistics in databases. Extensive research has been focused on maintaining a space-efficient structure for approximate quantile computation as the underlying dataset is updated. The existing solutions, however, are designed to support only the current, most-updated, snapshot of the dataset. Queries on the past versions of the data cannot be answered. This paper studies the problem of historical quantile search. The objective is to enable \u03b5-approximate quantile retrieval on any snapshot of the dataset in history. The problem is very important in analyzing the evolution of a distribution, monitoring the quality of services, query optimization in temporal databases, and so on. We present the first formal results in the literature. First, we prove a novel theoretical lower bound on the space cost of supporting \u03b5-approximate historical quantile queries. The bound reveals the fundamental difference between answering quantile queries about the past and those about the present time. Second, we propose a structure for finding \u03b5-approximate historical quantiles, and show that it consumes more space than the lower bound by only a square-logarithmic factor. Extensive experiments demonstrate that in practice our technique performs much better than predicted by theory. In particular, the quantiles it returns are remarkably more accurate than the theoretical precision guarantee.\n\nTitle: Randomized Synopses for Query Assurance on Data Streams\nContent: The overwhelming flow of information in many data stream applications forces many companies to outsource to a third-party the deployment of a Data Stream Management System (DSMS) for performing desired computations. Remote computations intrinsically raise issues of trust, making query execution assurance on data streams a problem with practical implications. Consider a client observing the same data stream as a remote server (e.g., network traffic), that registers a continuous query on the server's DSMS, and receives answers upon request. The client needs to verify the integrity of the results using significantly fewer resources than evaluating the query locally. Towards that goal, we propose a probabilistic algorithm for selection and aggregate/group-by queries, that uses constant space irrespective of the result-set size, has low update cost, and arbitrarily small probability of failure. We generalize this algorithm to allow some tolerance on the number of errors permitted (irrespective of error magnitude), and also discuss the hardness of permitting arbitrary errors of small magnitude. We also perform an empirical evaluation using live network traffic.\n\nTitle: Proof-infused streams: enabling authentication of sliding window queries on streams\nContent: As computer systems are essential components of many critical commercial services, the need for secure online transactions is now becoming evident. The demand for such applications, as the market grows, exceeds the capacity of individual businesses to provide fast and reliable services, making outsourcing technologies a key player in alleviating issues of scale. Consider a stock broker that needs to provide a real-time stock trading monitoring service to clients. Since the cost of multicasting this information to a large audience might become prohibitive, the broker could outsource the stock feed to third-party providers, who are in turn responsible for forwarding the appropriate sub-feed to clients. Evidently, in critical applications the integrity of the third-party should not be taken for granted. In this work we study a variety of authentication algorithms for selection and aggregation queries over sliding windows. Our algorithms enable the end-users to prove that the results provided by the third-party are correct, i.e., equal to the results that would have been computed by the original provider. Our solutions are based on Merkle hash trees over a forest of space partitioning data structures, and try to leverage key features, like update, query, signing, and authentication costs. We present detailed theoretical analysis for our solutions and empirically evaluate the proposed techniques.\n\nTitle: Dynamic authenticated index structures for outsourced databases\nContent: In outsourced database (ODB)systems the database owner publishes its data through a number of remote servers, with the goal of enabling clients at the edge of the network to access and query the data more efficiently. As servers might be untrusted or can be compromised, query authentication becomes an essential component of ODB systems. Existing solutions for this problem concentrate mostly on static scenarios and are based on idealistic properties for certain cryptographic primitives. In this work, first we define a variety of essential and practical cost metrics associated with ODB systems. Then, we analytically evaluate a number of different approaches, in search for a solution that best leverages all metrics. Most importantly, we look at solutions that can handle dynamic scenarios, where owners periodically update the data residing at the servers. Finally, we discuss query freshness, a new dimension in data authentication that has not been explored before. A comprehensive experimental evaluation of the proposed and existing approaches is used to validate the analytical models and verify our claims. Our findings exhibit that the proposed solutions improve performance substantially over existing approaches, both for static and dynamic environments.\n\nTitle: Efficient Threshold Monitoring for Distributed Probabilistic Data\nContent: In distributed data management, a primary concern is monitoring the distributed data and generating an alarm when a user specified constraint is violated. A particular useful instance is the threshold based constraint, which is commonly known as the distributed threshold monitoring problem [4], [16], [19], [29]. This work extends this useful and fundamental study to distributed probabilistic data that emerge in a lot of applications, where uncertainty naturally exists when massive amounts of data are produced at multiple sources in distributed, networked locations. Examples include distributed observing stations, large sensor fields, geographically separate scientific institutes/units and many more. When dealing with probabilistic data, there are two thresholds involved, the score and the probability thresholds. One must monitor both simultaneously, as such, techniques developed for deterministic data are no longer directly applicable. This work presents a comprehensive study to this problem. Our algorithms have significantly outperformed the baseline method in terms of both the communication cost (number of messages and bytes) and the running time, as shown by an extensive experimental evaluation using several, real large datasets.\n\n## Task\nCompose an abstract for the title 'Persistent Bloom Filter: Membership Testing for the Entire History.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Membership testing is the problem of testing whether an element is in a set of elements. Performing the test exactly is expensive space-wise, requiring the storage of all elements in a set. In many applications, an approximate testing that can be done quickly using small space is often desired. Bloom filter (BF) was designed and has witnessed great success across numerous application domains. But there is no compact structure that supports set membership testing for temporal queries, e.g., has person A visited a web server between 9:30am and 9:40am? And has the same person visited the web server again between 9:45am and 9:50am? It is possible to support such \"temporal membership testing\" using a BF, but we will show that this is fairly expensive. To that end, this paper designs persistent bloom filter (PBF), a novel data structure for temporal membership testing with compact space.\n\n"
      }
    ]
  },
  {
    "tid":"271",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Fast Reduced Kernel Extreme Learning Machine.\nContent: In this paper, we present a fast and accurate kernel-based supervised algorithm referred to as the Reduced Kernel Extreme Learning Machine (RKELM). In contrast to the work on Support Vector Machine (SVM) or Least Square SVM (LS-SVM), which identifies the support vectors or weight vectors iteratively, the proposed RKELM randomly selects a subset of the available data samples as support vectors (or mapping samples). By avoiding the iterative steps of SVM, significant cost savings in the training process can be readily attained, especially on Big datasets. RKELM is established based on the rigorous proof of universal learning involving reduced kernel-based SLFN. In particular, we prove that RKELM can approximate any nonlinear functions accurately under the condition of support vectors sufficiency. Experimental results on a wide variety of real world small instance size and large instance size applications in the context of binary classification, multi-class problem and regression are then reported to show that RKELM can perform at competitive level of generalized performance as the SVM/LS-SVM at only a fraction of the computational effort incurred.\n\nTitle: Extreme Learning Machine For Multi-Categories Classification Applications\nContent: In the paper, the multi-class pattern classification using extreme learning machine (ELM) is studied. The study is based on either a series of ELM binary classifiers or a single ELM classifier. When using binary ELM classifiers, the multi-class problem is decomposed into two-class problem using the one-against-all (OAA) and one-against-one (OAO) schemes, which are named as ELM-OAA and ELM-OAO respectively for brevity. In a single ELM classifier, the multi-class problem is implemented with an architecture of multi-output nodes which is equal to the number of pattern classes. Their performance is evaluated using some multi-class benchmark problems and simulation results show that ELM-OAA and ELM-OAO requires fewer hidden nodes than the single ELM classifier. In addition ELM-OAO usually has similar or less computation burden than the single ELM classifier when the pattern class labels is not larger than 10.\n\nTitle: A fast pruned-extreme learning machine for classification problem\nContent: Extreme learning machine (ELM) represents one of the recent successful approaches in machine learning, particularly for performing pattern classification. One key strength of ELM is the significantly low computational time required for training new classifiers since the weights of the hidden and output nodes are randomly chosen and analytically determined, respectively. In this paper, we address the architectural design of the ELM classifier network, since too few/many hidden nodes employed would lead to underfitting/overfitting issues in pattern classification. In particular, we describe the proposed pruned-ELM (P-ELM) algorithm as a systematic and automated approach for designing ELM classifier network. P-ELM uses statistical methods to measure the relevance of hidden nodes. Beginning from an initial large number of hidden nodes, irrelevant nodes are then pruned by considering their relevance to the class labels. As a result, the architectural design of ELM network classifier can be automated. Empirical study of P-ELM on several commonly used classification benchmark problems and with diverse forms of hidden node functions show that the proposed approach leads to compact network classifiers that generate fast response and robust prediction accuracy on unseen data, comparing with traditional ELM and other popular machine learning approaches.\n\nTitle: Adaptive Subgradient Methods for Online AUC Maximization.\nContent: Learning for maximizing AUC performance is an important research problem in Machine Learning and Artificial Intelligence. Unlike traditional batch learning methods for maximizing AUC which often suffer from poor scalability, recent years have witnessed some emerging studies that attempt to maximize AUC by single-pass online learning approaches. Despite their encouraging results reported, the existing online AUC maximization algorithms often adopt simple online gradient descent approaches that fail to exploit the geometrical knowledge of the data observed during the online learning process, and thus could suffer from relatively larger regret. To address the above limitation, in this work, we explore a novel algorithm of Adaptive Online AUC Maximization (AdaOAM) which employs an adaptive gradient method that exploits the knowledge of historical gradients to perform more informative online learning. The new adaptive updating strategy of the AdaOAM is less sensitive to the parameter settings and maintains the same time complexity as previous non-adaptive counterparts. Additionally, we extend the algorithm to handle high-dimensional sparse data (SAdaOAM) and address sparsity in the solution by performing lazy gradient updating. We analyze the theoretical bounds and evaluate their empirical performance on various types of data sets. The encouraging empirical results obtained clearly highlighted the effectiveness and efficiency of the proposed algorithms.\n\nTitle: Autonomous Deep Learning: Incremental Learning of Denoising Autoencoder for Evolving Data Streams.\nContent: The generative learning phase of Autoencoder (AE) and its successor Denosing Autoencoder (DAE) enhances the flexibility of data stream method in exploiting unlabelled samples. Nonetheless, the feasibility of DAE for data stream analytic deserves in-depth study because it characterizes a fixed network capacity which cannot adapt to rapidly changing environments. An automated construction of a denoising autoeconder, namely deep evolving denoising autoencoder (DEVDAN), is proposed in this paper. DEVDAN features an open structure both in the generative phase and in the discriminative phase where input features can be automatically added and discarded on the fly. A network significance (NS) method is formulated in this paper and is derived from the bias-variance concept. This method is capable of estimating the statistical contribution of the network structure and its hidden units which precursors an ideal state to add or prune input features. Furthermore, DEVDAN is free of the problem- specific threshold and works fully in the single-pass learning fashion. The efficacy of DEVDAN is numerically validated using nine non-stationary data stream problems simulated under the prequential test-then-train protocol where DEVDAN is capable of delivering an improvement of classification accuracy to recently published online learning works while having flexibility in the automatic extraction of robust input features and in adapting to rapidly changing environments.\n\n## Task\nCompose an abstract for the title 'Online sequential reduced kernel extreme learning machine.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, we present an Online Sequential Reduced Kernel Extreme Learning Machine (OS-RKELM). In OS-RKELM, only a small part of the instances in the original training samples is employed for training the kernel neurons, while the output weights are attained analytically. Similar to the Online Sequential Extreme Learning Machine (OS-ELM), OS-RKELM learns data samples in a chunk-by-chunk or one-by-one mode and does not require an archival of the data sample once it has been learned. OS-RKELM also contains few control parameters, thus avoiding the need for cumbersome fine-tuning of the algorithm. OS-RKELM supports a widespread types of kernels as hidden neurons and is capable of addressing the singular problem that arises when the initial training samples are smaller than the neuron size. A comprehensive performance evaluation of the OS-RKELM against other state-of-the-art sequential learning algorithms, including OS-ELM, Large-scale Active Support Vector Machine (LASVM) and Budgeted Stochastic Gradient Descent Support Vector Machine (BSGD) using popular time series, regression and classification benchmarks have been conducted. Experimental results obtained indicate that the proposed OS-RKELM showcases improved prediction accuracy and efficiency over the OS-ELM, LASVM and BSGD in many cases."
      }
    ]
  },
  {
    "tid":"272",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Dynamic tunneling for network-based distributed mobility management coexisting with PMIPv6.\nContent: The world is entering a post-PC era in which more real-time entertainment traffic is delivered to mobile devices than to PCs. The unprecedented expansion of mobile Internet traffic has resulted in the development of distributed mobility management (DMM) at the IETF. In this paper, we introduce a dynamic tunneling scheme for network-based DMM, in which mobility anchors are distributed at an access network level while tunnels for data packets are dynamically configured taking into account of both session and mobility status. We illustrate the proposed scheme's initial attachment and handover procedures step-by-step in an environment where network-based DMM is coexisted with Proxy Mobile IPv6. To justify the effectiveness of the proposed scheme, we develop an analytical cost model that is used to compare the performance of the proposed scheme with existing ones in terms of signaling and packet delivery costs. Our analysis shows that the proposed scheme optimizes packet delivery paths and thus provides a cost reduction during the packet delivery compared to the existing schemes. We also describe possible performance extensions to the proposed scheme. \u00a9 2013 IEEE.\n\nTitle: Distributed IP mobility management from the perspective of the IETF: motivations, requirements, approaches, comparison, and challenges\nContent: The recent explosion of mobile Internet traffic is accelerating the evolution of mobile network architectures toward flat architectures with all IP backhaul support. IP mobility support protocols are thus required to be adopted in such mobile network architectures. Accordingly, the IETF Distributed Mobility Management (DMM) working group was recently chartered to address such architectural changes with rapidly increasing mobile Internet usages. Unlike existing IP mobility support protocols, DMM protocols being considered at the IETF aim at distributing mobile Internet traffic in an optimal way while not relying on centrally deployed mobility anchors. In this article, we first discuss motivations and requirements of DMM. We then present two different possible approaches: host-based and network-based DMM approaches. Furthermore, comprehensive comparison results are provided. Finally, we discuss challenges in developments of DMM protocols.\n\nTitle: Host-based distributed mobility management support protocol for IPv6 mobile networks\nContent: The recent explosion of mobile Internet traffic has resulted in an evolution of mobile network architectures. Mobile network architectures are being flattened to cope with the recent traffic growth. Existing IP mobility support protocols relying on centralized mobility anchors are thus required to be adopted in the evolution of mobile network architectures. In this paper, a new IP mobility support protocol is presented: a new host-based distributed mobility management support protocol designed for the current evolution of mobile network architectures. Mobility anchors are distributed at an access network level and a mobile node is served by a close-by mobility anchor. While the mobile node changes its point of attachment to the Internet, it keeps previous IP address(es) for continuous communications established with the previous IP address(es). The detailed protocol operation is explained with discussion for operational considerations. The performance of the proposed protocol is compared with Mobile IPv6 in terms of handover latency and throughput.\n\nTitle: Host-based distributed mobility management: Example of traffic offloading\nContent: Recent technical breakthroughs in wireless communications have facilitated significantly increased data rates and reduced end-to-end latency in cellular networks. Higher throughputs however lead to increase traffic loads on cellular networks and require mechanisms to mitigate such traffic loads on the cellular networks. In this paper, we introduce a host-based distributed mobility management protocol supporting selective traffic offloading for the evolution of the 3GPP core network architecture. Unlike the current approach of 3GPP, traffic offloading is achieved based on IP flow mobility in a distributed manner. The offloading point is thus also distributed. We show how the proposed host-based distributed mobility management protocol alleviates the high traffic loads in the evolution of the 3GPP core network architecture.\n\nTitle: A Smart Management Framework for Multihomed Mobile Nodes & Mobile Routers.\nContent: The ceaseless evolution of wireless communications is taking the shape of the so called 4G networks: Fundamentally different radio technologies coexist and cooperate to offer continuous connectivity to applications. Mobile devices are now equipped to take advantage of access media abundance to improve the user experience. However, traditional management techniques were quickly obsoleted by the spatial and temporal context variations, especially during mobility. Therefore, novel management frameworks need to be designed to accompany this evolution.The following paper describes SmartMob6 (Smart Management Framework for Multihomed Mobile Nodes & Mobile Routers): a context-aware policy-based management solution combined with a combinatorial optimization algorithm to maximize the satisfaction of the applications while respecting the stakeholders policy rules. Case studies highlight the benefits of such a system over more conventional management techniques.\n\n## Task\nCompose an abstract for the title 'A Distributed Dynamic Mobility Management Scheme Designed for Flat IP Architectures' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Current mobility management approaches are highly centralized and hierarchical. They make use of centralized anchors located in core networks, responsible for forwarding traffic to/from mobile nodes' location. Emerging networks standards developed for new broadband cellular radio interfaces are use flat architecture where control functions are deployed in network edges such as evolved base stations. In order to better fit to such distributed logic while optimizing handover efficiency and minimizing traffic encapsulation, we propose a new scheme designed for mobility management in flat architectures. Our scheme dynamically anchors mobile nodes' traffic in access nodes, depending on their location when sessions are set up. A first estimation shows that we can obtain low handover delays, promising for future adoption and development of our distributed and dynamic approach."
      }
    ]
  },
  {
    "tid":"273",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A framework for truthful online auctions in cloud computing with heterogeneous user demands\nContent: The paradigm of cloud computing has spontaneously prompted a wide interest in market-based resource allocation mechanisms by which a cloud provider aims at efficiently allocating cloud resources among potential users. Among these mechanisms, auction-style pricing policies, as they can effectively reflect the underlying trends in demand and supply for the computing resources, have attracted a research interest recently. This paper conducts the first work on a framework for truthful online cloud auctions where users with heterogeneous demands could come and leave on the fly. Our framework desirably supports a variety of design requirements, including (1) dynamic design for timely reflecting fluctuation of supply-demand relations, (2) joint design for supporting the heterogeneous user demands, and (3) truthful design for discouraging bidders from cheating behaviors. Concretely speaking, we first design a novel bidding language, wherein users' heterogeneous demands are generalized to regulated and consistent forms. Besides, building on top of our bidding language we propose COCA, an incentive-Compatible (truthful) Online Cloud Auction mechanism based on two proposed guidelines. Our theoretical analysis shows that the worst-case performance of COCA can be well-bounded. Further, in simulations the performance of COCA is seen to be comparable to the well-known off-line Vickrey-Clarke-Groves (VCG) mechanism [11].\n\nTitle: Truthful Online Auction For Cloud Instance Subletting\nContent: Despite that IaaS users are busy scaling up/out their cloud instances to meet the ever-increasing demands, the dynamics of their demands, as well as the coarse-grained billing options offered by leading cloud providers, have led to substantial instance underutilization in both temporal and spatial domains. This paper theoretically examines an instance subletting service, where underutilized instances are leased to others within user-specified periods. Serving as a secondary market that complements the existing instance market of IaaS providers, we specifically identify the theoretical challenges in instance subletting services, and design an online auction mechanism to make allocation and pricing decisions for the instances to be sublet. Our mechanism guarantees truthfulness and individual rationality with the best possible competitive ratio. Extensive trace-driven simulations show that our proposed mechanism achieves significant performance gains in both cost and social welfare.\n\nTitle: Design, Modeling, And Analysis Of Online Combinatorial Double Auction For Mobile Cloud Computing Markets\nContent: With the burgeoning of cloud service companies, cloud computing is becoming an efficient means of providing computing resources. Amazon EC2, Rackspace, Google App, and Microsoft Azure are attracting more and more users over the Internet these years. However, in mobile cloud computing (MCC), traditional cloud pricing models can no longer support the above popular applications, because user behaviors are dynamic and time sensitive. As an MCC application is the combination of communication services (eg, wireless access services) and computation services (eg, cloud services), it lacks new auctions for capturing the feature of MCC markets based on the communication and computation cooperation (3C). In this paper, we design an efficient double-sided combinatorial auction model in the context of 3C-based MCC to mitigate this problem. We first propose the framework of online combinatorial double auctions to model mobile cloud computing market. On this base, we give four principles of design requirements, which can make the scheme more efficient and practical, and then we design a new winner determination algorithm that shows how the auction mechanism decides commodity allocation and transaction prices. At last, we conduct a series of experiments to deep analyze the property of our mechanism. The experiment results indicate that the proposed online auction mechanism obtains comparable allocation efficiency to the social optimal solution.\n\nTitle: Continuous Double Auction For Cloud Market: Pricing And Bidding Analysis\nContent: Cloud computing has recently attracted a substantial amount of attention from both industry and academia. Its growing demand gives normal users an opportunity to sell their local resources to the cloud market, which introduces new challenges for the existing coarse-grained pricing models.In this paper, we examine the potential of applying continuous double auction framework to handle these heterogeneous cloud resources. First, we establish an e-auction platform, on which cloud service providers and users can trade computing and storage resources online. Then we formulate a continuous double auction model for cloud market and further develop a novel belief-based hybrid bidding strategy (BH-strategy) for cloud players to ensure their profit maximization. At last, we conduct three simulation scenarios to compare the performance between BH-strategy and other dominating bidding strategies, and plenty of simulation results show that our BH-strategy outperforms others in all the scenarios on user surpluses by 20% or above. Besides, the BH-strategy can obtain a 16% higher efficiency in 1/3 the amount of time of other strategies.\n\nTitle: Collaborative Caching in Wireless Video Streaming Through Resource Auctions.\nContent: Recent advances in wireless communications and mobile networking have dramatically increased the popularity of multimedia services for mobile users, with wireless video streaming at their fingertips. To facilitate efficient acquisition of video content, proxy caching has been widely used by wireless service providers (WSPs), which typically deploy cache servers at mobile switching centers (MSCs). However, capacity provisioning of cache servers is challenging, given the dynamic user demands and the limited cache server resources. With increased densities of wireless service deployment, it is increasingly common that mobile users are covered by more than one WSP within an area. This brings opportunities of a collaborative caching paradigm among the cache servers deployed at different MSCs. In this paper, we explore the benefits of collaborative caching in wireless streaming services, addressing both challenges of incentives and truthfulness of selfish WSPs. We propose a collaborative mechanism that maximizes the social welfare in the context of Vickrey-Clarke-Groves (VCG) auctions, in which cache servers cooperate in the trading of their resources in a self-enforcing manner. Experimental results demonstrate that superior performance can be achieved with respect to the quality of video streaming.\n\n## Task\nCompose an abstract for the title 'Truthful Online Auction Toward Maximized Instance Utilization in the Cloud.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Although infrastructure as a service (IaaS) users are busy scaling up/out their cloud instances to meet the ever-increasing demands, the dynamics of their demands, as well as the coarse-grained billing options offered by leading cloud providers, have led to substantial instance underutilization in both temporal and spatial domains. This paper systematically examines an instance subletting service,..."
      }
    ]
  },
  {
    "tid":"274",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: An Experimental Study of Router Buffer Sizing for Mixed TCP and Real-Time Traffic\nContent: Recent research results on Internet router buffer sizing suggest that when TCP traffic is well-paced, Internet routers need as few as 20-30 packet buffers to realise near- maximum link throughputs, independent of link capacities and number of TCP flows. However, these studies have ignored non- TCP traffic, on the grounds that TCP traffic predominates in the Internet. In this paper we evaluate this assumption via practical experiments on a long-haul Australian network. Specifically, for different volumes of long range dependent real- time streaming video traffic in the network, we quantify end-to- end TCP throughput and real-time traffic loss as router buffer sizes at bottleneck and non-bottleneck links vary. Our results indicate that even in the presence of 5-15% bursty real-time traffic, TCP requires larger router buffers to achieve a given fraction of its saturation throughput. However, larger buffers lead to increased losses for real-time traffic. This suggests that TCP and non-TCP traffic can negatively impact each other, and their performance trade-offs need to be considered when sizing router buffers.\n\nTitle: Routers With Very Small Buffers: Anomalous Loss Performance for Mixed Real-Time and TCP Traffic\nContent: The past few years have seen researchers debate the size of buffers required at core Internet routers. Much of this debate has focused on TCP throughput, and recent arguments supported by theory and experimentation suggest that few tens of packets of buffering suffice at bottleneck routers for TCP traffic to realise acceptable link utilisation. This paper introduces a small fraction of real-time (i.e. open-loop) traffic into the mix, and discovers an anomalous behaviour: In this specific regime of very small buffers, losses for real- time traffic do not fall monotonically with buffer size, but instead exhibit a region where larger buffers cause higher losses. Our contributions pertaining to this phenomenon are threefold: First, we demonstrate this anomalous loss performance for real- time traffic via extensive simulations including real video traces. Second, we provide qualitative explanations for the anomaly and develop a simple analytical model that reveals the dynamics of buffer sharing between TCP and real-time traffic leading to this behaviour. Third, we show how various factors such as traffic characteristics and link rates impact the severity of this anomaly. Our study particularly informs all-optical packet router designs (envisaged to have buffer sizes in the few tens of packets) and network service providers who operate their buffer sizes in this regime, of the negative impact investment in larger buffers can have on the quality of service performance.\n\nTitle: Anomalous loss performance for mixed real-time and TCP traffic in routers with very small buffers\nContent: In the past few years there has been vigorous debate regarding the size of buffers required at core Internet routers. Recent arguments supported by theory and experimentation show that under certain conditions, core router buffer sizes of a few tens of packets suffice for realizing acceptable end-to-end TCP throughputs. This is a significant step toward the realization of optical packet switched (OPS) networks, which are inherently limited in their ability to buffer optical signals. However, prior studies have largely ignored the presence of real-time traffic, which is increasing in importance as a source of revenue for Internet service providers. In this paper, we study the interaction that happens between real-time (open-loop) and TCP (closed-loop) traffic when they multiplex at buffers of very small size (few tens of packets) and make a significant discovery--namely that in a specific range of buffer size, real-time traffic losses increase as buffer size becomes larger. Our contributions pertaining to this anomalous behavior are threefold. First, we exhibit this anomalous loss performance for real-time traffic via extensive simulations using synthetic traffic and real video traces. Second, we develop quantitative models that reveal the dynamics of buffer sharing between real-time and TCP traffic that lead to this behavior. Third, we show how various factors such as the nature of real-time traffic, mixture of long-lived and short-lived TCP flows, and packet sizes impact the severity of the anomaly. Our study is the first to consider interactions between real-time and TCP traffic in very small (potentially all-optical) buffers and informs router manufacturers and network operators of the factors to consider when dimensioning such small buffer sizes for desired performance balance between real-time and TCP traffic.\n\nTitle: Comparing edge and host traffic pacing in small buffer networks\nContent: packet switching speeds scale to Terabits-per second and beyond, power considerations are increasingly forcing core router manufacturers to adopt all-optical and hybrid opto-electronic single-chip switching solutions. Such routers will have small buffers, typically in the range of a few tens of Kilobytes, causing potentially increased packet loss, with adverse impact on end-to-end TCP performance. We recently proposed and analysed the benefits of pacing traffic at the network edge for open-loop real-time traffic in a small buffer network. However, no detailed study of the efficacy of edge pacing on closed-loop TCP performance has been undertaken for such a network.In this paper, we consider two pacing methods - TCP pacing at the end-hosts, and traffic pacing by the network edge - in the context of small buffer networks, and undertake a comprehensive comparison. Our contributions are threefold: First, we show via extensive simulations that under most scenarios (considering bottleneck and non-bottleneck core links, low-speed and high-speed access links, long- and short-lived TCP flows, and different variants of TCP) edge pacing performs as well or better than host pacing in terms of link utilisation (TCP throughputs) and average per-flow goodputs. Second, we provide analytical insight into the setting of the edge pacing delay parameter, showing how the efficacy of pacing relates to bottleneck buffer size. Third, we demonstrate the benefits of pacing in practical scenarios multiplexing both TCP and real-time traffic, and discuss incremental deployment of pacing, highlighting that unlike host pacing that requires adoption by a critical mass of users, edge pacing can be deployed relatively easily under service provider control to facilitate rapid migration to core networks with small buffers.\n\nTitle: Packet Pacing in Small Buffer Optical Packet Switched Networks\nContent: In the absence of a cost-effective technology for storing optical signals, emerging optical packet switched (OPS) networks are expected to have severely limited buffering capa- bility. This paper investigates the resulting impact on end-to-end loss and throughput, and proposes that the optical edge switches \"pace\" packets into the OPS core to improve performance without adversely affecting end-to-end delays. In this context, our contributions are three-fold. We first evaluate the impact of short buffers on the performance of real-time and TCP traffic. This helps us identify short-time-scale burstiness as the major contributor to performance degradation, so we propose that the optical edge switches pace the transmission of packets into the OPS core while respecting their delay-constraints. Our second contribution develops algorithms of poly-logarithmic complexity that can perform optimal real-time pacing of high data rate traffic. Lastly, we show via simulations of a realistic network carrying real-time traffic that pacing can significantly reduce losses at the expense of a bounded increase in end-to-end delay. The loss-delay trade-off mechanism provided by pacing can help achieve desired OPS network performance.\n\n## Task\nCompose an abstract for the title 'Sharing small optical buffers between real-time and TCP traffic' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Buffers in emerging optical packet routers are expensive resources, and it is expected that they would be able to store at most a few tens of KiloBytes of data in the optical domain. When TCP and real-time (UDP) traffic multiplex at an optical router with such small buffers (less than 50 KB), we recently showed that UDP packet loss can increase with increasing buffer size. This anomalous loss behaviour can negatively impact the investment made in larger buffers and degrade quality of service. In this paper we explore if this anomalous behaviour can be alleviated by dedicating (i.e., pre-allocating) buffers to UDP traffic. Our contributions within this context are two fold. First, we show using extensive simulations that there would seem to be a critical buffer size above which UDP benefits with dedicated buffers that protect it from the aggressive nature of TCP. However, for smaller buffers that are below this critical value, UDP can benefit by time-sharing the buffers with TCP. Second, we develop a simple linear model that quantitatively captures the combined utility of TCP and real-time traffic for shared and dedicated buffers, and propose a method to optimise the buffer split ratio with the objective of maximising the overall network utility. Our study equips designers of optical packet switched networks with quantitative tools to tune their buffer allocation strategies subject to various system parameters such as the ratio of traffic mix and relative weights associated with TCP and UDP traffic."
      }
    ]
  },
  {
    "tid":"275",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Crossmodal Matching of Speakers Using Lip and Voice Features in Temporally Non-overlapping Audio and Video Streams\nContent: Person identification using audio (speech) and visual (facial appearance, static or dynamic) modalities, either independently or jointly, is a thoroughly investigated problem in pattern recognition. In this work, we explore a novel task : person identification in a cross-modal scenario, i.e., matching the speaker in an audio recording to the same speaker in a video recording, where the two recordings have been made during different sessions, using speaker specific information which is common to both the audio and video modalities. Several recent psychological studies have shown how humans can indeed perform this task with an accuracy significantly higher than chance. Here we propose two systems which can solve this task comparably well, using purely pattern recognition techniques. We hypothesize that such systems could be put to practical use in multimodal biometric and surveillance systems.\n\nTitle: Introducing crossmodal biometrics: Person identification from distinct audio & visual streams\nContent: Person identification using audio or visual biometrics is a well-studied problem in pattern recognition. In this scenario, both training and testing are done on the same modalities. However, there can be situations where this condition is not valid, i.e. training and testing has to be done on different modalities. This could arise, for example, in covert surveillance. Is there any person specific information common to both the audio and visual (video-only) modalities which could be exploited to identify a person in such a constrained situation? In this work, we investigate this question in a principled way and propose a framework which can perform this task consistently better than chance, suggesting that such crossmodal biome\u0301trie information exists.\n\nTitle: Visual processing-inspired fern-audio features for noise-robust speaker verification\nContent: In this paper, we consider the problem of speaker verification as a two-class object detection problem in computer vision, where the object instances are 1-D short-time spectral vectors obtained from the speech signal. More precisely, we investigate the general problem of speaker verification in the presence of additive white Gaussian noise, which we consider as analogous to visual object detection under varying illumination conditions. Inspired by their recent success in illumination-robust object detection, we apply a certain class of binary-valued pixel-pair based features called Ferns for noise-robust speaker verification. Intensive experiments on a benchmark database according to a standard evaluation protocol have shown the advantage of the proposed features in the presence of moderate to extremely high amounts of additive noise.\n\nTitle: The 2013 Speaker Recognition Evaluation In Mobile Environment\nContent: This paper evaluates the performance of the twelve primary systems submitted to the evaluation on speaker verification in the context of a mobile environment using the MOBIO database. The mobile environment provides a challenging and realistic test-bed for current state-of-the-art speaker verification techniques. Results in terms of equal error rate (EER), half total error rate (HTER) and detection error trade-off (DET) confirm that the best performing systems are based on total variability modeling, and are the fusion of several sub-systems. Nevertheless, the good old UBM-GMM based systems are still competitive. The results also show that the use of additional data for training as well as gender-dependent features can be helpful.\n\nTitle: Cross-Pollination of Normalization Techniques From Speaker to Face Authentication Using Gaussian Mixture Models\nContent: This paper applies score and feature normalization techniques to parts-based Gaussian mixture model (GMM) face authentication. In particular, we propose to utilize techniques that are well established in state-of-the-art speaker authentication, and apply them to the face authentication task. For score normalization, T-, Z- and ZT-norm techniques are evaluated. For feature normalization, we propose a generalization of feature warping to 2D images, which is applied to discrete cosine transform (DCT) features prior to modeling. Evaluation is performed on a range of challenging databases relevant to forensics and security, including surveillance and access control scenarios. The normalization techniques are shown to generalize well to the face authentication task, resulting in relative improvements in half total error rate (HTER) of between 17% and 62%.\n\n## Task\nCompose an abstract for the title 'Audio-Visual processing in meetings: seven questions and current AMI answers' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The project Augmented Multi-party Interaction (AMI) is concerned with the development of meeting browsers and remote meeting assistants for instrumented meeting rooms \u2013 and the required component technologies R&D themes: group dynamics, audio, visual, and multimodal processing, content abstraction, and human-computer interaction. The audio-visual processing workpackage within AMI addresses the automatic recognition from audio, video, and combined audio-video streams, that have been recorded during meetings. In this article we describe the progress that has been made in the first two years of the project. We show how the large problem of audio-visual processing in meetings can be split into seven questions, like \u201cWho is acting during the meeting?\u201d. We then show which algorithms and methods have been developed and evaluated for the automatic answering of these questions."
      }
    ]
  },
  {
    "tid":"276",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Vision-based finger detection, tracking, and event identification techniques for multi-touch sensing and display systems.\nContent: This study presents efficient vision-based finger detection, tracking, and event identification techniques and a low-cost hardware framework for multi-touch sensing and display applications. The proposed approach uses a fast bright-blob segmentation process based on automatic multilevel histogram thresholding to extract the pixels of touch blobs obtained from scattered infrared lights captured by a video camera. The advantage of this automatic multilevel thresholding approach is its robustness and adaptability when dealing with various ambient lighting conditions and spurious infrared noises. To extract the connected components of these touch blobs, a connected-component analysis procedure is applied to the bright pixels acquired by the previous stage. After extracting the touch blobs from each of the captured image frames, a blob tracking and event recognition process analyzes the spatial and temporal information of these touch blobs from consecutive frames to determine the possible touch events and actions performed by users. This process also refines the detection results and corrects for errors and occlusions caused by noise and errors during the blob extraction process. The proposed blob tracking and touch event recognition process includes two phases. First, the phase of blob tracking associates the motion correspondence of blobs in succeeding frames by analyzing their spatial and temporal features. The touch event recognition process can identify meaningful touch events based on the motion information of touch blobs, such as finger moving, rotating, pressing, hovering, and clicking actions. Experimental results demonstrate that the proposed vision-based finger detection, tracking, and event identification system is feasible and effective for multi-touch sensing applications in various operational environments and conditions.\n\nTitle: A Component-Based, Reconfigurable Mobile Agent System for Context-Aware Computing\nContent: In the near future, people can use various kinds of portable terminal devices, such as PDAs or smart phones, to access the information on Internet at any time, in any place, for supporting their daily life. Personal mobile agents can help those people to search, to filter, to analyze, or to translate the large amount of Internet information. Since the context of a user may vary frequently, the way a personal mobile agent process the Internet information must adapt to the context change. However, we think that the current mainstream mobile agent platforms, such as the IBM Aglet and the IKV++ Grasshopper, are not good enough for developing context-aware applications. Hence, we propose a Component-based, Reconfigurable Mobile Agent (CRMA) system, which is designed for context-aware computing. It offers downloadable user-interface modules on terminal devices for application to adapt the characteristics of various terminal devices. In addition, on agent side, CRMA provides a reconfigurable agent-programming framework, which uses component-based pipelines to perform context-aware data-processing operations. This not only simplifies the programming task for dynamically reconfiguring an agent's behavior to adapt the context change, but also improves the reusability of agent software. In the end of this paper, we will propose a context-aware meta-search service to demonstrate how to use CRMA to develop a context-aware application.\n\nTitle: An Efficient Component-Based Framework for Intelligent Home-Care System Design with Video and Physiological Monitoring Machineries\nContent: This study proposes a customized and reusable component-based design framework based on the UML modeling process for intelligent home healthcare systems. All the proposed functional components are reusable, replaceable, and extensible for the system developers to implement customized home healthcare systems for different demands of patients and caregivers on healthcare monitoring aspects. The prototype design of the intelligent healthcare system based on these proposed components can provide the following features: 1). the system can monitor and record the videos of rehabilitation situations and actions of the patient by multiple CCD cameras, and the monitoring videos at different times can be accordingly stored in the archive. 2). the system can record the patient's physiological data records and the corresponding treatment plan, and these records can be stored in a XML archiving database for caregivers' review. 3). during the times for the patient to take medicine or other healing activities listed on the given treatment plan, the system can automatically alarm the patient and record the patient's treatment situations. 4). The patient's caregivers and family members can ubiquitously monitor the videos and physiological records of the patient's rehabilitation situations via the handheld mobile devices via the internet or wireless communication networks. 5). The caregivers and patients can setup the alarm machinery for the patients' physiological warning states, and once the patients' physiological states suddenly deteriorate, the module will immediately alarm the caregivers by sending notification messages to their remote mobile devices or web browsers.\n\nTitle: A Web-based, Offline-able, and Personalized Runtime Environment for executing applications on mobile devices\nContent: An increasing number of people use cell phones daily. Users are not only capable of making phone calls, but can also install applications on their mobile phones. When creating mobile applications, developers usually encounter the cross-platform incompatibility problem (for example, iPhone applications cannot be executed on the Android platform). Moreover, because mobile Web browsers have increasingly supported more and more Web-related standards, Web applications are more possible to be executed on different platforms than mobile applications. However, the problem of Web application is that it cannot be executed in offline mode. This study proposes a Web-based platform for executing applications on mobile devices. This platform provides several services for developers such as offline service, content adaptation service, and synchronization service. With the help of the proposed platform, application developers can develop and publish offline Web applications easily with simplified external Web content and synchronization capability.\n\nTitle: Real-time eye tracking and event identification techniques for smart TV applications\nContent: Due to the progress of computer technology, the detecting accuracy and speed of pattern recognition are promoted. Additionally, according to research advances in embedded systems, the applications of digital cameras, such as internet cameras, smart phones and smart TV, are widely used in nowadays. Therefore, we propose a set of real-time human-eye recognition and tracking systems with human-computer interaction mechanism. It can be adopted to help the smart TV users or any device which have camera and computing power to improve their using experience. The proposed system is demonstrated on OMAP4430 for embedded applications, and experimental results show that the proposed architecture is capable of effective and real-time eye position tracking. The proposed system can also detect the status of eye blink. Even for the case of uneven lighting, the proposed system can successfully recognize the human eyes with high accuracy.\n\n## Task\nCompose an abstract for the title 'Developing Ubiquitous Multi-touch Sensing and Displaying Systems with Vision-Based Finger Detection and Event Identification Techniques' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This study presents efficient vision-based finger detection, tracking, and event identification techniques, as well as a low-cost hardware framework for multi-touch sensing and display applications. A fast bright-blob segmentation process based on automatic multilevel histogram thresholding is performed to extract pixels of touch blobs from the captured image sequences obtained from the scattered infrared lights by the video camera. Given the touch blobs extracted from each of the captured frames, a blob tracking and event recognition process is then conducted to analyze the spatial and temporal information of these touch blobs from consecutive frames and determine the possible touch events issued by users. This process also refines the detection results and corrects for errors and occlusions caused by noise and errors during the blob extraction processes. Our proposed blob tracking and touch event recognition process includes two phases. First, the phase of blob tracking associates the motion correspondence of blobs in succeeding frames by analyzing their spatial and temporal features. Then the phase of touch event recognition process can identify meaningful touch events activated by users from the motion information of touch blobs. Experimental results demonstrate that the proposed vision-based finger detection, tracking, and event identification system is feasible and effective for multi-touch sensing applications in various operational environments and conditions."
      }
    ]
  },
  {
    "tid":"277",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: txt 4 l8r: lowering the burden for diary studies under mobile conditions\nContent: We present and evaluate a new technique for performing diary studies under mobile or active conditions. Diary studies play an important role as a means for ecologically valid participant data capture. Unfortunately, when participants are asked to capture data while mobile or active, they are often unwilling or unable to invest time in thorough, reflective entries. Ultimately, this leads to lowered entry quality and quantity. The technique presented here suggests the capture of only small snippets of information in the field. These snippets then serve as prompts for participants when completing full diary entries at a convenient time. We describe how this system automates collection of snippets via text (SMS), picture (MMS) and voicemail messages and later presents these snippets for full entry elicitation. We then present results from a preliminary evaluation of this technique.\n\nTitle: Structure and messaging techniques for online peer learning systems that increase stickiness\nContent: When students work with peers, they learn more actively, build richer knowledge structures, and connect material to their lives. However, not every peer learning experience online sees successful adoption. This paper articulates and addresses three adoption challenges for global-scale peer learning. First, peer interactions struggle to bootstrap critical mass. However, class incentives can signal importance and spur initial usage. Second, online classes have limited peer visibility and awareness, so students often feel alone even when surrounded by peers. We find that highlighting interdependence and strengthening norms can mitigate this issue. Third, teachers can readily access \\\"big\\\" aggregate data but not \\\"thick\\\" contextual data that helps build intuitions, so software should guide teachers' scaffolding of peer interactions. We illustrate these challenges through studying 8,500 students' usage of two peer learning platforms, Talkabout and PeerStudio. This paper measures efficacy through sign-up and participation rates and the structure and duration of student interactions.\n\nTitle: Power to the peers: authority of source effects for a voice-based agricultural information service in rural India\nContent: Online communities enable people to easily connect and share knowledge across geographies. Mobile phones can enable billions of new users in emerging countries to participate in these online communities. In India, where social hierarchy is important, users may over-value institutionally-recognized authorities relative to peer-sourced content. We tested this hypothesis through a controlled experiment of source authority effects on a voice-based agricultural information service for farmers in Gujarat, India. 305 farmers were sent seven agricultural tips via automated phone calls over a two-week period. The same seven tips were each voice-recorded by two university scientists and two peer farmers. Participants received a preview of the tip from a randomly assigned source via the automated call, and played the remainder of the tip by calling a dedicated phone number. Participants called the follow-up number significantly more often when the tip preview was recorded by a peer than a scientist. On the other hand, in interviews conducted both before and after the experiment, a majority of farmers maintained that they preferred receiving information from scientists. This stated preference may have been expressing the more socially acceptable response. We interpret our experimental results as a demonstration of the demand for peer-based agricultural information dissemination. We conclude with design implications for peer-to-peer information services for rural communities in India.\n\nTitle: Coordinating tasks on the commons: designing for personal goals, expertise and serendipity\nContent: How is work created, assigned, and completed on large-scale, crowd-powered systems like Wikipedia? And what design principles might enable these federated online systems to be more effective? This paper reports on a qualitative study of work and task practices on Wikipedia. Despite the availability of tag-based community-wide task assignment mechanisms, informants reported that self-directed goals, within-topic expertise, and fortuitous discovery are more frequently used than community-tagged tasks. We examine how Wikipedia editors organize their actions and the actions of other participants, and what implications this has for understanding, and building tools for, crowd-powered systems, or any web site where the main force of production comes from a crowd of online participants. From these observations and insights, we developed WikiTasks, a tool that integrates with Wikipedia and supports both grassroots creation of site-wide tasks and self-selection of personal tasks, accepted from this larger pool of community tasks.\n\nTitle: Bridging physical and electronic media for distributed design collaboration\nContent: Research on distributed collaboration has predominantly focused on shared electronic media. We have found, as other researchers have, that users often have good reason to want to work with physical media. Yet they would still like to collaborate with each other. A fundamental tension exists in the design of systems to support remote collaboration when the interaction primitives are physical: physical objects live in one place. We have designed and implemented a remote collaboration system where users can still use physical objects. We introduce an interaction paradigm where objects that are physical in one space are electronic in the other space, and vice versa. Our distributed system is designed for two groups, with multiple users at each end. Our tangible approach is the first system to enable simultaneous, multi-input across locations. We have implemented this system as an extension to the Designers' Outpost[5].\n\n## Task\nCompose an abstract for the title 'An asymmetric communications platform for knowledge sharing with low-end mobile phones' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We present Awaaz.De (\"give voice\"), a social platform for communities to access and share knowledge using low-end mobile phones. Awaaz.De features a configurable mobile voice application organized into asynchronous voice mes-sage boards. For poor, remote and marginal communities, the voice-touchtone interface addresses the constraints of low literacy, language diversity, and affordability of only basic mobile devices. Voice content also presents a low barrier to content authoring, encouraging otherwise disconnected communities to actively participate in knowledge exchange. Awaaz.De includes a web-based administration interface for Internet-connected community managers to moderate, annotate, categorize, route, and narrow-cast voice messages. In this paper we describe the platform's design, implementation, and future directions."
      }
    ]
  },
  {
    "tid":"278",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: MV-MAX: improving wireless infrastructure access for multi-vehicular communication\nContent: When a roadside 802.11-based wireless access point is shared by more than one vehicle, the vehicle with the lowest transmission rate reduces the effective transmission rate of all other vehicles. This performance anomaly [9] degrades both individual and overall throughput in such multi-vehicular environments. Observing that every vehicle eventually receives good performance when it is near the access point, we propose MV-MAX (Multi-Vehicular Maximum), a medium access protocol that opportunistically grants wireless access to vehicles with the maximum transmission rate. Mathematical analysis and trace-driven simulations based on real data show that MV-MAX not only improves overall system throughput, compared to 802.11, by a factor of almost 4, but also improves on the previously proposed time-fairness scheme [20, 22, 15] by a factor of more than 2. Moreover, despite being less fair than 802.11, almost every vehicle benefits by using MV-MAX over the more equitable 802.11 access mechanism. Finally, we show that our results are consistent across different data sets.\n\nTitle: Low-cost communication for rural internet kiosks using mechanical backhaul\nContent: Rural kiosks in developing countries provide a variety of services such as birth, marriage, and death certificates, electricity bill collection, land records, email services, and consulting on medical and agricultural problems. Fundamental to a kiosk's operation is its connection to the Internet. Network connectivity today is primarily provided by dialup telephone, although Very Small Aperture Terminals (VSAT) or long-distance wireless links are also being deployed. These solutions tend to be both expensive and failure prone. Instead, we propose the use of buses and cars as \"mechanical backhaul\" devices to carry data to and from a village and an internet gateway. Building on the pioneering lead of Daknet [15], and extending the Delay Tolerant Networking Research Group architecture [24], we describe a comprehensive solution, encompassing naming, addressing, forwarding, routing, identity management, application support, and security. We believe that this architecture not only meets the top-level goals of low cost and robustness, but also exposes fundamental architectural principles necessary for any such design. We also describe our experiences in implementing a prototype of this architecture.\n\nTitle: A Successive Refinement Approach To Wireless Infrastructure Network Deployment\nContent: There has been a recent proliferation in wireless infrastructure network deployments. In a typical deployment, an installer uses either a one-time site survey or rules of thumb to place wireless access points and allocate them channels and power levels. Because the access point location problem is inherently complex and one that requires tradeoffs among competing requirements, these approaches can result in either dead spots or significant unintended interference among wireless access points. This degrades network performance for end clients, with throughput reduction factors of 4x found in field measurements [1]. In this paper, we take a first step towards improving client performance by coordinating choices of channels and power levels at wireless access points using a successive refinement approach. Our contributions are two-fold: First, we develop a mathematical model that crisply defines the solution space and identifies the characteristics of an optimal channel and power-level configuration. Second, we present heuristics that, under some simplifying assumptions, yield near-optimal configurations. We use Monte Carlo simulations to evaluate the performance of our heuristics. We find that the choice of heuristics for transmit power control impacts performance more than the channel allocation strategy, especially at high densities. Also, surprisingly, randomly assigning channels to access points appears to be an effective strategy at higher deployment densities. Taken together, we believe that this study paves the way to designing rapidly deployable real-world infrastructure networks that also have good performance.\n\nTitle: Vehicular opportunistic communication under the microscope\nContent: We consider the problem of providing vehicular Internet access using roadside 802.11 access points. We build on previous work in this area [18, 8, 5, 11] with an extensive experimental analysis of protocol operation at a level of detail not previously explored. We report on data gathered with four capture devices from nearly 50 experimental runs conducted with vehicles on a rural highway. Our three primary contributions are: (1) We experimentally demonstrate that, on average, current protocols only achieve 50% of the overall throughput possible in this scenario. In particular, even with a streamlined connection setup procedure that does not use DHCP, high losses early in a vehicular connection are responsible for the loss of nearly 25% of overall throughput, 15% of the time. (2) We quantify the effects of ten problems caused by the mechanics of existing protocols that are responsible for this throughput loss; and (3) We recommend best practices for using vehicular opportunistic connections. Moreover, we show that overall throughput could be significantly improved if environmental information was made available to the 802.11 MAC and to TCP. The central messagein this paper is that wireless conditions in the vicinity of a roadside access point are predictable, and by exploiting this information, vehicular opportunistic access can be greatly improved.\n\nTitle: Design principles for opportunistic communication in constrained computing environments\nContent: Constrained computing environments, such as smartphones and embedded wireless devices, are becoming increasingly prevalent. Driven by the need to minimize power usage, these devices are characterized by their low-power CPUs, limited memory, slow yet vast amounts of persistent storage, and one or more wireless network interfaces. As the dominant form of future computing, understanding and adapting to the trade offs that exist between computing and communication resources and energy consumption will become increasingly important. In this paper we consider the effect of a constrained computing environment on opportunistic communication. Drawing from our experiences with two existing mobile systems, we detail the constraints that inhibit opportunistic communication. We also show how these constraints can be satisfied using a set of design principles for systems that depend on opportunistic communication.\n\n## Task\nCompose an abstract for the title 'OmniVoice: a mobile voice solution for small-scale enterprises' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We consider the problem of providing mobility support for Voice-over-IP (VoIP) traffic in small-scale enterprises. There is considerable interest in providing on-the-go support for VoIP through the use of WiFi-enabled smart phones. However, existing solutions either do not support client mobility or require client modifications, making them difficult to deploy in practice. In this paper, we present OmniVoice, an 802.11 compliant solution that supports mobility for VoIP traffic without any client modifications. To effectively support such traffic, OmniVoice eliminates client handoff delays and manages interference from non-VoIP background traffic. It achieves this by using (a) a single-channel WLAN design and (b) a lightweight central controller for scheduling non-interfering AP-to-client transmissions and dynamically associating clients. The controller minimizes interference with the help of an interference map that models potential exposed and hidden terminal conflicts in the WLAN, while allowing for the serialization of transmissions that would otherwise compete for medium access. We have implemented and extensively evaluated OmniVoice on a 40 node wireless testbed. OmniVoice meets the QoS requirements for VoIP in all operating scenarios and is unaffected by interference from non-VoIP traffic. In particular, OmniVoice provides 100% greater throughput and 130% greater uninterrupted connectivity time to mobile VoIP users as compared to an off-the-shelf 802.11 solution."
      }
    ]
  },
  {
    "tid":"279",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Causal Behavioural Profiles - Efficient Computation, Applications, and Evaluation\nContent: Analysis of behavioural consistency is an important aspect of software engineering. In process and service management, consistency verification of behavioural models has manifold applications. For instance, a business process model used as system specification and a corresponding workflow model used as implementation have to be consistent. Another example would be the analysis to what degree a process log of executed business operations is consistent with the corresponding normative process model. Typically, existing notions of behaviour equivalence, such as bisimulation and trace equivalence, are applied as consistency notions. Still, these notions are exponential in computation and yield a Boolean result. In many cases, however, a quantification of behavioural deviation is needed along with concepts to isolate the source of deviation. In this article, we propose causal behavioural profiles as the basis for a consistency notion. These profiles capture essential behavioural information, such as order, exclusiveness, and causality between pairs of activities of a process model. Consistency based on these profiles is weaker than trace equivalence, but can be computed efficiently for a broad class of models. In this article, we introduce techniques for the computation of causal behavioural profiles using structural decomposition techniques for sound free-choice workflow systems if unstructured net fragments are acyclic or can be traced back to S- or T-nets. We also elaborate on the findings of applying our technique to three industry model collections.\n\nTitle: Efficient Consistency Measurement Based on Behavioral Profiles of Process Models\nContent: Engineering of process-driven business applications can be supported by process modeling efforts in order to bridge the gap between business requirements and system specifications. However, diverging purposes of business process modeling initiatives have led to significant problems in aligning related models at different abstract levels and different perspectives. Checking the consistency of such corresponding models is a major challenge for process modeling theory and practice. In this paper, we take the inappropriateness of existing strict notions of behavioral equivalence as a starting point. Our contribution is a concept called behavioral profile that captures the essential behavioral constraints of a process model. We show that these profiles can be computed efficiently, i.e., in cubic time for sound free-choice Petri nets w.r.t. their number of places and transitions. We use behavioral profiles for the definition of a formal notion of consistency which is less sensitive to model projections than common criteria of behavioral equivalence and allows for quantifying deviation in a metric way. The derivation of behavioral profiles and the calculation of a degree of consistency have been implemented to demonstrate the applicability of our approach. We also report the findings from checking consistency between partially overlapping models of the SAP reference model.\n\nTitle: Change Propagation in Process Models Using Behavioural Profiles\nContent: Business process change is at the very core of business process management, which aims at enabling flexible adaptation to changing business needs. However, the wide variety of drivers for business process modeling initiatives, reaching from business evolution to process enactment, results in multiple models that overlap in content due to serving different purposes. That, in turn, imposes serious challenges for the propagation of changes between these process models. Given a change in one model, this paper introduces an approach to determine a change region in another model by exploiting the behavioral profile of corresponding activities. It, therefore, supports the process of change propagation and eases the synchronization of process models significantly. As a major contribution, our approach can handle changes in pairs of models, even if they are not defined in terms of a hierarchical refinement.\n\nTitle: Generalised computation of behavioural profiles based on petri-net unfoldings\nContent: Behavioural profiles have been proposed as a concept to judge on the behavioural consistency of process models that depict different perspectives of a process. These profiles describe the observable relations between the activities of a process model. Consistency criteria based on behavioural profiles are less sensitive to model projections than common equivalence criteria, such as trace equivalence. Existing algorithms derive those profiles for unlabelled sound free-choice workflow nets efficiently. In this paper, we generalise the computation of behavioural profiles by relaxing the aforementioned assumptions. First, we introduce an algorithm that derives behavioural profiles from the complete prefix unfolding of a bounded Petri net. Hence, it is applicable in a more general case. Second, we lift the concept to the level of labelled Petri nets. We also elaborate on findings of applying our approach to a collection of industry models.\n\nTitle: On Application of Structural Decomposition for Process Model Abstraction\nContent: Real world business process models may consist of hundreds of elements and have sophisticated structure. Although there are tasks where such models are valu- able and appreciated, in general complexity has a negative influence on model compre- hension and analysis. Thus, means for managing the complexity of process models are needed. One approach is abstraction of business process models\u2014creation of a process model which preserves the main features of the initial elaborate process model, but leaves out insignificant details. In this paper we study the structural aspects of process model abstraction and introduce an abstraction approach based on process structure trees (PST). The developed approach assures that the abstracted process model pre- serves the ordering constraints of the initial model. It surpasses pattern-based process model abstraction approaches, allowing to handle graph-structured process models of arbitrary structure. We also provide an evaluation of the proposed approach.\n\n## Task\nCompose an abstract for the title 'Efficient computation of causal behavioural profiles using structural decomposition' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Identification of behavioural contradictions is an important aspect of software engineering, in particular for checking the consistency between a business process model used as system specification and a corresponding workflow model used as implementation. In this paper, we propose causal behavioural profiles as the basis for a consistency notion, which capture essential behavioural information, such as order, exclusiveness, and causality between pairs of activities. Existing notions of behavioural equivalence, such as bisimulation and trace equivalence, might also be applied as consistency notions. Still, they are exponential in computation. Our novel concept of causal behavioural profiles provides a weaker behavioural consistency notion that can be computed efficiently using structural decomposition techniques for sound free-choice workflow systems if unstructured net fragments are acyclic or can be traced back to S- or T-nets."
      }
    ]
  },
  {
    "tid":"280",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Sensor-Mission Assignment in Constrained Environments\nContent: When a sensor network is deployed in the field it is typically required to support multiple simultaneous missions, which may start and finish at different times. Schemes that match sensor resources to mission demands thus become necessary. In this paper, we consider new sensor-assignment problems motivated by frugality, i.e., the conservation of resources, for both static and dynamic settings. In the most general setting, the problems we study are NP-hard even to approximate, and so we focus on heuristic algorithms that perform well in practice. In the static setting, we propose a greedy centralized solution and a more sophisticated solution that uses the Generalized Assignment Problem model and can be implemented in a distributed fashion. In what we call the dynamic setting, missions arrive over time and have different durations. For this setting, we give heuristic algorithms in which available sensors propose to nearby missions as they arrive. We find that the overall performance can be significantly improved if available sensors sometimes refuse to offer utility to missions they could help, making this decision based on the value of the mission, the sensor's remaining energy, and (if known) the remaining target lifetime of the network. Finally, we evaluate our solutions through simulations.\n\nTitle: Demo: A distributed architecture for heterogeneous multi sensor-task allocation\nContent: To complement our paper \u201cA Distributed Architecture For Heterogeneous Multi-Sensor Task Allocation\u201d published in DCOSS 2011 conference proceedings, we will demonstrate the current implementation of our architecture for Multi-Sensor Task Allocation (MSTA) shown in Figure 1. This consists of a prototype interface for task submission, and a simulated sensor network serving other mobile users on the field competing for the same sensing resources. The prototype interface is implemented on two different mobile devices (iPhone & iPad1), dynamically interacting with a heterogeneous sensor network simulated in Java on top of the REPAST Symphony2 discrete time simulation environment. A video of the demo is available at http://www.youtube.com/watch?v=QzrpKRhGFRU.\n\nTitle: Sensor Selection for Heterogeneous Coverage Measures\nContent: We consider sensor selection to optimize multiple conditions. Specifically, we model the sensor network as a graph, in which weighted edges indicate the ability of one node to predict the data of another. Each node is associated with several data types, so there are links for each data type. The objective is to maximize the coverage of all data types. This is applicable to such problems as monitoring air quality in cities and coal mines using several indicators of quality. We first define the maximization criteria, and then how to modify the model and existing algorithms to solve the problem. We demonstrate the importance of the problem and the quality of our methodology on synthetic and realistic scenarios.\n\nTitle: An Ontology-Centric Approach to Sensor-Mission Assignment\nContent: Sensor-mission assignment involves the allocation of sensor and other information-providing resources to missions in order to cover the information needs of the individual tasks in each mission. This is an important problem in the intelligence, surveillance, and reconnaissance (ISR) domain, where sensors are typically over-subscribed, and task requirements change dynamically. This paper approaches the sensor-mission assignment problem from a Semantic Web perspective: the core of the approach is a set of ontologies describing mission tasks, sensors, and deployment platforms. Semantic reasoning is used to recommend collections of types of sensors and platforms that are known to be \"fit-for-purpose\" for a particular task, during the mission planning process. These recommended solutions are used to constrain a search for available instances of sensors and platforms that can be allocated at mission execution-time to the relevant tasks. An interface to the physical sensor environment allows the instances to be configured to operate as a coherent whole and deliver the necessary data to users. Feedback loops exist throughout, allowing re-planning of the sensor-task fitness, reallocation of instances, and reconfiguration of the sensor network.\n\nTitle: Dynamic sensor selection for path coverage.\nContent: We study the problem of dynamically selecting sensors to maximize the number of steps covered on the path of a mobile target. The sensors are selected from those already deployed in a network as the target moves. The total number of sensors selected and rounds of selection are limited according to budget constraints. The variables are which sensors to select, when, and how. The best settings depend on how much is known about the target's path, such as its start and end locations and mobility pattern, which is updated as the target moves. We study how coverage is affected by these parameters, and how much such dynamic selection strategies can improve coverage over sensors selected in advance.\n\n## Task\nCompose an abstract for the title 'Sensor Allocation in Diverse Environments' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"\n Sensor coverage varies with location due to factors such as weather, terrain, and obstacles. If a field can be partitioned\n into zones of homogeneous sensing areas, then coverage by a random deployment of sensors can be optimized by controlling the\n number of sensors deployed in each zone. We derive expressions to directly calculate the optimal sensor partition in runtime\n asymptotically equal to the number of zones. We further derive expressions to determine the minimum sensor count required\n to achieve a specific coverage threshold. We bound the maximum increase in coverage over a strategy oblivious to differences\n in sensing areas, which our results show is no greater than 13% for a field with two zones. While the analytical solutions\n assume that each zone is covered independently, we allow sensors to affect neighboring zones in simulations. Nevertheless,\n the simulations support the optimality of our solution.\n \n "
      }
    ]
  },
  {
    "tid":"281",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Honey Encryption: Encryption beyond the Brute-Force Barrier\nContent: Honey encryption (HE) addresses the challenge of encrypting messages using keys that are vulnerable to guessing attacks, such as the passwords selected by ordinary users. HE creates a ciphertext that, when decrypted with an incorrect key or password, yields a valid-looking but bogus message. So, attackers can't tell when decryption has been successful. Counterintuitively, HE enables the encryption of a message using a weak password such that even a strong attacker--one with unlimited computing power--can't decrypt the message with certainty. You can use HE to encrypt the list of passwords in a password manager, credentials used in SSH (Secure Shell), and so on. HE fuses the creative use of honey objects and decoys in system security with the rigor and principled application imparted by cryptography.\n\nTitle: Honey Encryption Beyond Message Recovery Security.\nContent: Juels and Ristenpart introduced honey encryption HE and showed how to achieve message recovery security even in the face of attacks that can exhaustively try all likely keys. This is important in contexts like password-based encryption where keys are very low entropy, and HE schemes based on the JR construction were subsequently proposed for use in password management systems and even long-term protection of genetic data. But message recovery security is in this setting, like previous ones, a relatively weak property, and in particular does not prohibit an attacker from learning partial information about plaintexts or from usefully mauling ciphertexts. We show that one can build HE schemes that can hide partial information about plaintexts and that prevent mauling even in the face of exhaustive brute force attacks. To do so, we introduce target-distribution semantic-security and target-distribution non-malleability security notions. We prove that a slight variant of the JR HE construction can meet them. The proofs require new balls-and-bins type analyses significantly different from those used in prior work. Finally, we provide a formal proof of the folklore result that an unbounded adversary which obtains a limited number of encryptions of known plaintexts can always succeed at message recovery.\n\nTitle: Hedged Public-Key Encryption: How to Protect against Bad Randomness\nContent: Public-key encryption schemes rely for their IND-CPA security on per-message fresh randomness. In practice, randomness may be of poor quality for a variety of reasons, leading to failure of the schemes. Expecting the systems to improve is unrealistic. What we show in this paper is that we can, instead, improve the cryptography to offset the lack of possible randomness. We provide public-key encryption schemes that achieve IND-CPA security when the randomness they use is of high quality, but, when the latter is not the case, rather than breaking completely, they achieve a weaker but still useful notion of security that we call IND-CDA. This hedged public-key encryption provides the best possible security guarantees in the face of bad randomness. We provide simple RO-based ways to make in-practice IND-CPA schemes hedge secure with minimal software changes. We also provide non-RO model schemes relying on lossy trapdoor functions (LTDFs) and techniques from deterministic encryption. They achieve adaptive security by establishing and exploiting the anonymity of LTDFs which we believe is of independent interest.\n\nTitle: Cracking-Resistant Password Vaults Using Natural Language Encoders\nContent: Password vaults are increasingly popular applications that store multiple passwords encrypted under a single master password that the user memorizes. A password vault can greatly reduce the burden on a user of remembering passwords, but introduces a single point of failure. An attacker that obtains a user's encrypted vault can mount offline brute-force attacks and, if successful, compromise all of the passwords in the vault. In this paper, we investigate the construction of encrypted vaults that resist such offline cracking attacks and force attackers instead to mount online attacks. Our contributions are as follows. We present an attack and supporting analysis showing that a previous design for cracking-resistant vaults -- the only one of which we are aware -- actually degrades security relative to conventional password-based approaches. We then introduce a new type of secure encoding scheme that we call a natural language encoder (NLE). An NLE permits the construction of vaults which, when decrypted with the wrong master password, produce plausible-looking decoy passwords. We show how to build NLEs using existing tools from natural language processing, such as n-gram models and probabilistic context-free grammars, and evaluate their ability to generate plausible decoys. Finally, we present, implement, and evaluate a full, NLE-based cracking-resistant vault system called NoCrack.\n\nTitle: Breaking Web Applications Built On Top of Encrypted Data.\nContent: We develop a systematic approach for analyzing client-server applications that aim to hide sensitive user data from untrusted servers. We then apply it to Mylar, a framework that uses multi-key searchable encryption (MKSE) to build Web applications on top of encrypted data. We demonstrate that (1) the Popa-Zeldovich model for MKSE does not imply security against either passive or active attacks; (2) Mylar-based Web applications reveal users' data and queries to passive and active adversarial servers; and (3) Mylar is generically insecure against active attacks due to system design flaws. Our results show that the problem of securing client-server applications against actively malicious servers is challenging and still unsolved. We conclude with general lessons for the designers of systems that rely on property-preserving or searchable encryption to protect data from untrusted servers.\n\n## Task\nCompose an abstract for the title 'Honey Encryption: Security Beyond the Brute-Force Bound.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We introduce honey encryption (HE), a simple, general approach to encrypting messages using low min-entropy keys such as passwords. HE is designed to produce a ciphertext which, when decrypted with any of a number of incorrect keys, yields plausible-looking but bogus plaintexts called honey messages. A key benefit of HE is that it provides security in cases where too little entropy is available to withstand brute-force attacks that try every key; in this sense, HE provides security beyond conventional brute-force bounds. HE can also provide a hedge against partial disclosure of high min-entropy keys. HE significantly improves security in a number of practical settings. To showcase this improvement, we build concrete HE schemes for password-based encryption of RSA secret keys and credit card numbers. The key challenges are development of appropriate instances of a new type of randomized message encoding scheme called a distribution-transforming encoder (DTE), and analyses of the expected maximum loading of bins in various kinds of balls-and-bins games."
      }
    ]
  },
  {
    "tid":"282",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Estimating Signals With Finite Rate of Innovation From Noisy Samples: A Stochastic Algorithm\nContent: As an example of the recently introduced concept of rate of innovation, signals that are linear combinations of a finite number of Diracs per unit time can be acquired by linear filtering followed by uniform sampling. However, in reality, samples are rarely noiseless. In this paper, we introduce a novel stochastic algorithm to reconstruct a signal with finite rate of innovation from its noisy samples. Even though variants of this problem have been approached previously, satisfactory solutions are only available for certain classes of sampling kernels, for example, kernels that satisfy the Strang-Fix condition. In this paper, we consider the infinite-support Gaussian kernel, which does not satisfy the Strang-Fix condition. Other classes of kernels can be employed. Our algorithm is based on Gibbs sampling, a Markov chain Monte Carlo method. Extensive numerical simulations demonstrate the accuracy and robustness of our algorithm.\n\nTitle: Estimation from lossy sensor data: jump linear modeling and Kalman filtering\nContent: Due to constraints in cost, power, and communication, losses often arise in large sensor networks. The sensor can be modeled as an output of a linear stochastic system with random losses of the sensor output samples. This paper considers the general problem of state estimation for jump linear systems where the discrete transitions are modeled as a Markov chain. Among other applications, this rich model can be used to analyze sensor networks. The sensor loss events are then modeled as Markov processes. Under the jump linear system model, many types of underlying losses can be easily considered, and the optimal estimator to be performed at the receiver in the presence of missing sensor data samples is given by a standard time-varying Kalman filter.We show that the asymptotic average estimation error variance converges and is given by a Linear Matrix Inequality, which can be easily solved. Under this framework, any arbitrary Markov loss process can be modeled, and its average asymptotic error variance can be directly computed. We include a few illustrative examples including .xed-length burst errors, a two-state model,and partial losses due to multiple SNR states. Our analysis encompasses modeling discrete changes not only in the received data as stated above, but also in the underlying system. In the context of the lossy sensor model, the former allows for variation in sensor positioning, power control, and loss of data communications; the latter could allow for discrete changes in the dynamics of the variable monitored by the sensor. This freedom in modeling yields a tool that is potentially valuable in various scenarios in which entities that share information are subjected to challenging and time-varying network conditions.\n\nTitle: Phase Unwrapping And Denoising For Time-Of-Flight Imaging Using Generalized Approximate Message Passing\nContent: We present a new method for simultaneously denoising and unwrapping phase in multi-frequency homodyne time-of-flight ranging for the formation of accurate depth maps despite low SNR of raw measurements. This is achieved with a new generalized approximate message passing (GAMP) algorithm for minimum mean-squared error estimation of the phase. A detailed, physically-accurate acquisition model is central in achieving high accuracy, and the use of the GAMP methodology allows low computational complexity despite dense dependencies and the nonlinearity and non-Gaussianity of the acquisition model. Numerical simulations demonstrate that our integrated approach performs better than separate unwrapping followed by denoising. This performance translates to lowering the optical power consumption of time-of-flight cameras for a fixed acquisition quality.\n\nTitle: Wave and equation based rate control using multicast round trip time\nContent: This paper introduces Wave and Equation Based Rate Control (WEBRC), the first multiple rate multicast congestion control protocol to be equation based. The equation-based approach enforces fairness to TCP with the benefit that fluctuations in the flow rate are small in comparison to TCP.This paper also introduces the multicast round trip time (MRTT), a multicast analogue of the unicast round trip time (RTT). The MRTT is fundamental to the equation-based protocol that each receiver uses to adjust its reception rate. Each receiver independently measures its own MRTT without placing any added messaging burden on the receiver, the sender or the intermediate network elements. Benefits provided by the MRTT include those that the RTT provides to TCP, e.g., reduced reception rates in reaction to buffer filling and fair sharing of bottleneck links. In addition, the use of MRTT is shown to synchronize and equalize the reception rates of proximate receivers and to cause reception rates to increase as the density of receivers increases.Another innovation of WEBRC is the idea of transmitting data with waves: the transmission rate on a channel is periodic, with an exponentially decreasing form during an active period followed by a quiescent period. Benefits of using waves include insensitivity to large IGMP leave latency; a frequency of joins and leaves by each receiver that is small and independent of the receiver reception rate; the use of a small number of multicast channels; fine-grained control over the receiver reception rate; and minimal, at times nonexistent, losses due to buffer overflow.\n\nTitle: Denoising by sparse approximation: error bounds based on rate-distortion theory\nContent: If a signal x is known to have a sparse representation with respect to a frame, it can be estimated from a noise-corrupted observation y by finding the best sparse approximation to y. Removing noise in this manner depends on the frame efficiently representing the signal while it inefficiently represents the noise. The mean-squared error (MSE) of this denoising scheme and the probability that the estimate has the same sparsity pattern as the original signal are analyzed. First an MSE bound that depends on a new bound on approximating a Gaussian signal as a linear combination of elements of an overcomplete dictionary is given. Further analyses are for dictionaries generated randomly according to a spherically-symmetric distribution and signals expressible with single dictionary elements. Easily-computed approximations for the probability of selecting the correct dictionary element and the MSE are given. Asymptotic expressions reveal a critical input signal-to-noise ratio for signal recovery.\n\n## Task\nCompose an abstract for the title 'Delay Estimation in the Presence of Timing Noise' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We consider the problem of delay estimation in the presence of timing noise. We introduce an iterative algorithm with superior performance compared to the traditional method of using only cross-correlation. This method can exploit statistical knowledge of the timing noise such as loop bandwidth, giving further improvement in performance."
      }
    ]
  },
  {
    "tid":"283",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Transactional Tasks: Parallelism in Software Transactions.\nContent: Many programming languages, such as Clojure, Scala, and Haskell, support different concurrency models.In practice these models are often combined, however the semantics of the combinations are not always well-defined.In this paper, we study the combination of futures and Software Transactional Memory.Currently, futures created within a transaction cannot access the transactional state safely, violating the serializability of the transactions and leading to undesired behavior.We define transactional tasks: a construct that allows futures to be created in transactions.Transactional tasks allow the parallelism in a transaction to be exploited, while providing safe access to the state of their encapsulating transaction.We show that transactional tasks have several useful properties: they are coordinated, they maintain serializability, and they do not introduce non-determinism.As such, transactional tasks combine futures and Software Transactional Memory, allowing the potential parallelism of a program to be fully exploited, while preserving the properties of the separate models where possible.\n\nTitle: Linguistic symbiosis between actors and threads\nContent: We describe a linguistic symbiosis between AmbientTalk, a flexible, domain-specific language for writing distributed programs and Java, a conventional object-oriented language. This symbiosis allows concerns related to distribution (service discovery, asynchronous communication, failure handling) to be handled in the domain-specific language, while still enabling the reuse of existing software components written in a conventional language. The symbiosis is novel in the sense that a mapping is defined between the concurrency models of both languages. AmbientTalk employs an inherently event-driven model based on actors, while conventional object-oriented languages employ a concurrency model based on threads. The contribution of this paper is a linguistic symbiosis which ensures that the invariants of the event-driven concurrency model are not violated by engaging in symbiosis with multi-threaded programs.\n\nTitle: Enriching the Internet By Acting and Reacting\nContent: A recent trend in application development for the web is a move towards rich internet applications (RIAs). As more and more of the application logic is moved to the client, RIAs can benefit from concurrency in order to increase overall performance as well as responsiveness of the application. Additionally, RIAs are often written in an event-driven style of programming to react to incoming events of a multitude of services that are integrated within the application. In this paper we argue that, while individual technologies exist to tackle both concerns, these technologies cannot easily be integrated in an ad hoc way. To increase the modularisability and composability of RIAs we propose a new programming model based on actors and reactors that encapsulate different parts of the application. We show that our model is able to exploit some of the available concurrency while reducing the required amount of event-driven code. Both actors and reactors are modular components that can be glued together via a unified communication mechanism. We evaluate our approach by means of a motivating example of a collaborative code editor.\n\nTitle: 43 years of actors: a taxonomy of actor models and their key properties.\nContent: The Actor Model is a message passing concurrency model that was originally proposed by Hewitt et al. in 1973. It is now 43 years later and since then researchers have explored a plethora of variations on this model. This paper presents a history of the Actor Model throughout those years. The goal of this paper is not to provide an exhaustive overview of every actor system in existence but rather to give an overview of some of the exemplar languages and libraries that influenced the design and rationale of other actor systems throughout those years. This paper therefore shows that most actor systems can be roughly classified into four families, namely: Classic Actors, Active Objects, Processes and Communicating Event-Loops. This paper also defines the Isolated Turn Principle as a unifying principle across those four families. Additionally this paper lists some of the key properties along which actor systems can be evaluated and formulates some general insights about the design and rationale of the different actor families across those dimensions.\n\nTitle: Tackling the awkward squad for reactive programming: the actor-reactor model.\nContent:  In his famous paper entitled \"Tackling the Awkward Squad\", Peyton Jones studies how features that traditionally did not fit in the functional programming paradigm can be added to a functional language via careful language design (e.g. using monads), instead of allowing programmers to sprinkle around impure expressions and ad-hoc library calls, thereby turning the entire program into a non-functional program. Similarly, in this paper, we identify a number of code characteristics that do not map onto the reactive programming paradigm but that are present in many real life reactive programs. We propose a novel Actor-Reactor model that can serve as the basis for future language designs that allow a programmer to use the awkward squad without making the reactive parts of the program accidentally non-reactive. \n\n\n\n## Task\nCompose an abstract for the title 'Transactional actors: communication in transactions.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Developers often require different concurrency models to fit the various concurrency needs of the different parts of their applications. Many programming languages, such as Clojure, Scala, and Haskell, cater to this need by incorporating different concurrency models. It has been shown that, in practice, developers often combine these concurrency models. However, they are often combined in an ad hoc way and the semantics of the combination is not always well-defined. The starting hypothesis of this paper is that different concurrency models need to be carefully integrated such that the properties of each individual model are still maintained.     This paper proposes one such combination, namely the combination of the actor model and software transactional memory. In this paper we show that, while both individual models offer strong safety guarantees, these guarantees are no longer valid when they are combined. The main contribution of this paper is a novel hybrid concurrency model called transactional actors that combines both models while preserving their guarantees. This paper also presents an implementation in Clojure and an experimental evaluation of the performance of the transactional actor model."
      }
    ]
  },
  {
    "tid":"284",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Quantitative Analysis of Current Practices in Optical Flow Estimation and the Principles Behind Them\nContent: The accuracy of optical flow estimation algorithms has been improving steadily as evidenced by results on the Middlebury optical flow benchmark. The typical formulation, however, has changed little since the work of Horn and Schunck. We attempt to uncover what has made recent advances possible through a thorough analysis of how the objective function, the optimization method, and modern implementation practices influence accuracy. We discover that \"classical\" flow formulations perform surprisingly well when combined with modern optimization and implementation techniques. One key implementation detail is the median filtering of intermediate flow fields during optimization. While this improves the robustness of classical methods it actually leads to higher energy solutions, meaning that these methods are not optimizing the original objective function. To understand the principles behind this phenomenon, we derive a new objective function that formalizes the median filtering heuristic. This objective function includes a non-local smoothness term that robustly integrates flow estimates over large spatial neighborhoods. By modifying this new term to include information about flow and image boundaries we develop a method that can better preserve motion details. To take advantage of the trend towards video in wide-screen format, we further introduce an asymmetric pyramid downsampling scheme that enables the estimation of longer range horizontal motions. The methods are evaluated on the Middlebury, MPI Sintel, and KITTI datasets using the same parameter settings.\n\nTitle: A Database and Evaluation Methodology for Optical Flow\nContent: The quantitative evaluation of optical flow algorithms by Barron et al. (1994) led to significant advances in performance. The challenges for optical flow algorithms today go beyond the datasets and evaluation methods proposed in that paper. Instead, they center on problems associated with complex natural scenes, including nonrigid motion, real sensor noise, and motion discontinuities. We propose a new set of benchmarks and evaluation methods for the next generation of optical flow algorithms. To that end, we contribute four types of data to test different aspects of optical flow algorithms: (1) sequences with nonrigid motion where the ground-truth flow is determined by tracking hidden fluorescent texture, (2) realistic synthetic sequences, (3) high frame-rate video used to study interpolation error, and (4) modified stereo sequences of static scenes. In addition to the average angular error used by Barron et al., we compute the absolute flow endpoint error, measures for frame interpolation error, improved statistics, and results at motion discontinuities and in textureless regions. In October 2007, we published the performance of several well-known methods on a preliminary version of our data to establish the current state of the art. We also made the data freely available on the web at http://vision.middlebury.edu/flow/ . Subsequently a number of researchers have uploaded their results to our website and published papers using the data. A significant improvement in performance has already been achieved. In this paper we analyze the results obtained to date and draw a large number of conclusions from them.\n\nTitle: Discrete-Continuous Optimization for Optical Flow Estimation\nContent: The accurate estimation of optical flow is a challenging task, which is often posed as an energy minimization problem. Most top-performing methods approach this using continuous optimization algorithms. In many cases, the employed models are assumed to be convex to ensure tractability of the optimization problem. This is in contrast to the related problem of narrow-baseline stereo matching, where the top-performing methods employ powerful discrete optimization algorithms such as graph cuts and message-passing to optimize highly non-convex energies.In this chapter, we demonstrate how similar non-convex energies can be formulated and optimized in the context of optical flow estimation using a combination of discrete and continuous techniques. Starting with a set of candidate solutions that are produced by either fast continuous flow estimation algorithms or sparse feature matching, the proposed method iteratively fuses these candidate solutions by the computation of minimum cuts on graphs. The obtained continuous-valued result is then further improved using local gradient descent. Experimentally, we demonstrate that the proposed energy is an accurate model and that the proposed discrete-continuous optimization scheme not only finds lower energy solutions than traditional discrete or continuous optimization techniques, but also leads to very accurate flow estimates.\n\nTitle: FusionFlow: Discrete-continuous optimization for optical flow estimation\nContent: Accurate estimation of optical flow is a challenging task, which often requires addressing difficult energy optimiza- tion problems. To solve them, most top-performing methods rely on continuous optimization algorithms. The modeling accuracy of the energy in this case is often traded for its tractability. This is in contrast to the related problem of narrow-baseline stereo matching, where the top-performing methods employ powerful discrete optimization algorithms such as graph cuts and message-passing to optimize highly non-convex energies. In this paper, we demonstrate how similar non-convex energies can be formulated and optimized discretely in the context of optical flow estimation. Starting with a set of candidate solutions that are produced by fast continuous flow estimation algorithms, the proposed method iteratively fuses these candidate solutions by the computation of min- imum cuts on graphs. The obtained continuous-valued fu- sion result is then further improved using local gradient de- scent. Experimentally, we demonstrate that the proposed energy is an accurate model and that the proposed discrete- continuous optimization scheme not only finds lower energy solutions than traditional discrete or continuous optimiza- tion techniques, but also leads to flow estimates that outper- form the current state-of-the-art.\n\nTitle: MirrorFlow: Exploiting Symmetries in Joint Optical Flow and Occlusion Estimation\nContent: Optical flow estimation is one of the most studied problems in computer vision, yet recent benchmark datasets continue to reveal problem areas of today's approaches. Occlusions have remained one of the key challenges. In this paper, we propose a symmetric optical flow method to address the well-known chicken-and-egg relation between optical flow and occlusions. In contrast to many state-of-the-art methods that consider occlusions as outliers, possibly filtered out during post-processing, we highlight the importance of joint occlusion reasoning in the optimization and show how to utilize occlusion as an important cue for estimating optical flow. The key feature of our model is to fully exploit the symmetry properties that characterize optical flow and occlusions in the two consecutive images. Specifically through utilizing forward-backward consistency and occlusion-disocclusion symmetry in the energy, our model jointly estimates optical flow in both forward and backward direction, as well as consistent occlusion maps in both views. We demonstrate significant performance benefits on standard benchmarks, especially from the occlusion-disocclusion symmetry. On the challenging KITTI dataset we report the most accurate two-frame results to date.\n\n## Task\nCompose an abstract for the title 'Secrets Of Optical Flow Estimation And Their Principles' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The accuracy of optical flow estimation algorithms has been improving steadily as evidenced by results on the Middlebury optical flow benchmark. The typical formulation, however, has changed little since the work of Horn and Schunck. We attempt to uncover what has made recent advances possible through a thorough analysis of how the objective function, the optimization method, and modern implementation practices influence accuracy. We discover that \"classical\" flow formulations perform surprisingly well when combined with modern optimization and implementation techniques. Moreover, we find that while median filtering of intermediate flow fields during optimization is a key to recent performance gains, it leads to higher energy solutions. To understand the principles behind this phenomenon, we derive a new objective that formalizes the median filtering heuristic. This objective includes a non-local term that robustly integrates flow estimates over large spatial neighborhoods. By modifying this new term to include information about flow and image boundaries we develop a method that ranks at the top of the Middlebury benchmark."
      }
    ]
  },
  {
    "tid":"285",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A genetic algorithm based double layer neural network for solving quadratic bilevel programming problem\nContent: In this paper, an intelligent genetic algorithm (IGA) and a double layer neural network (NN) are integrated into a hybrid intelligent algorithm for solving the quadratic bilevel programming problem. The intelligent genetic algorithm is used to select a set of potential solution combinations from the entire generated combinations of the upper level. Then a meta-controlled Boltzmann machine, which is formulated by comprising the Hopfield model (HM) and the Boltzmann machine (BM), is used to effectively and efficiently determine the optimal solution of the lower level. Numerical experiments on examples show that the genetic algorithm based double layer neural network enables us to efficiently and effectively solve quadratic bilevel programming problems.\n\nTitle: A hybrid intelligent algorithm for solving the bilevel programming models\nContent: In this paper, genetic algorithm (GA) and neural network (NN) are integrated to produce a hybrid intelligent algorithm for solving the bilevel programming models. GA is used to select a set of potential combination from the entire generated combination. Then a meta-controlled Boltzmann machine which is formulated by comprising a Hopfield network and a Boltzmann machine (BM) is used to effectively and efficiently determine the optimal solution. The proposed method is used to solve the examples of bilevel programming for two- level investment in a new facility. The upper layer will decide the optimal company investment. The lower layer is used to decide the optimal department investment. Typical application examples are provided to illustrate the effectiveness and practicability of the hybrid intelligent algorithm.\n\nTitle: Gaussian-PSO with fuzzy reasoning based on structural learning for training a Neural Network\nContent: This paper proposes Gaussian-PSO-based structural learning and fuzzy reasoning to optimize the weights and the structure of the Feed Forward Neural Network. The Neural Network is widely used for various applications; though it still has disadvantages such as learning capability and slow convergence. Back Propagation, the most used learning algorithm, has several difficulties such as the necessity for a priori specification of the network structure and sensibility to parameter settings. Recently, research studies have introduced evolutionary algorithms into the learning to improve its performance. The PSO is a population-based algorithm that has the advantage of faster convergence. However, the total number of the weights in the Neural Network determines the size of each particle, therefore the size of the network structure is computationally time consuming. The proposed method improves the learning and removes the stress by eliminating the necessity of determining a detailed network.\n\nTitle: A Modified Artificial Neural Network Learning Algorithm for Imbalanced Data Set Problem\nContent: A modified learning algorithm of Artificial Neural Networks (ANN) is introduced in this paper to solve imbalanced data set problems. In solving imbalanced data set, it is critical to predict the minority class due to their imbalanced nature. In order to improve the standard ANN classifier prediction performance, this paper focuses on optimizing the decision boundary of the step function at the output layer of ANN using particle swarm optimization (PSO). A feedforward ANN is chosen in this study. Firstly, a conventional back propagation algorithm is employed to train the ANN. PSO is then applied to train the real predicted output of training data from this trained network. As the result, the optimum value of decision boundary is found and applied to the classifier. Prediction performance is assessed by G-mean, which is a measure to indicate the efficiency of classifiers for imbalanced data sets. Based on experimental results, the proposed model is able to solve imbalanced data sets problem with better performance compared to the standard ANN.\n\nTitle: New Perspective for Structural Learning Method of Neural Networks\nContent: A neural network is developed to mimic a human brain. The neural network consists of units and links that connect between units. Various types of neural networks are categorized into two classes: (1) back-propagation hierarchical neural network and (2) mutual-connected neural network. Generally speaking, it is hard to fix the number of units to build a neural network for solving problems. So the number of units is decided on the basis of experts' experience.In this paper, we explain a learning method how to decide the structure of a neural network for problems. The learning method is named structural learning. Even if we give a sufficient number of units, the optimal structure will be decided in the process of learning.The objective of the paper is to explain the structural learning of both hierarchical and mutual connecting neural networks. Both networks obtained and showed the sufficiently good results. In the stock forecast by a general neural network, the operation and the system cost are very large because a lot of numbers of hidden layer units in the network are used. This research tried the optimization of the network by the structured learning, and evaluated the practicality. ...\n\n## Task\nCompose an abstract for the title 'Double-Layered Hybrid Neural Network Approach for Solving Mixed Integer Quadratic Bilevel Problems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper we build a double-layered hybrid neural network method to solve mixed integer quadratic bilevel programming problems. Bilevel programming problems arise when one optimization problem, the upper problem, is constrained by another optimization, the lower problem. In this paper, mixed integer quadratic bilevel programming problem is transformed into a double-layered hybrid neural network. We propose an efficient method for solving bilevel programming problems which employs a double-layered hybrid neural network. A two-layered neural network is formulate by comprising a Hopfield network, genetic algorithm, and a Boltzmann machine in order to effectively and efficiently select the limited number of units from those available. The Hopfield network and genetic algorithm are employed in the upper layer to select the limited number of units, and the Boltzmann machine is employed in the lower layer to decide the optimal solution/units from the limited number of units selected by the upper layer. The proposed method leads the mixed integer quadratic bilevel programming problem to a global optimal solution. To illustrate this approach, several numerical examples are solved and compared."
      }
    ]
  },
  {
    "tid":"286",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Experiences of Using a Public Key Infrastructure to Access Patient Confidential Data over the Internet\nContent: A project to enable health care professionals (GPs, practice nurses and diabetes nurse specialists) to access, via the Internet, confidential patient data held on a secondary care (hospital) diabetes information system, has been implemented. We describe the application that we chose to distribute (a diabetes register); the security mechanisms we used to protect the data (a public key infrastructure with strong encryption and digitally signed messages, plus a firewall); the reasons for the implementation decisions we made; the validation testing that we performed and the results of the first set of user trials. From a user acceptance perspective, we conclude that perceived usefulness and perceived ease of use on their own, are insufficient to guarantee that a new application will be used extensively in its new environment. Other domain specific factors, such as the compatibility and integration of the new computing system with the old, the working practices of the clinicians, the costs of using the new system compared to the old, and the actual location of the computing equipment all need to be taken into account when establishing untried information technology in 'real world' settings.\n\nTitle: Experiences of using a public key infrastructure for the preparation of examination papers\nContent: A project that piloted the secure electronic preparation of examination papers ran during the first semester of the academic year 1998/99 at the University of Salford. The examination papers were transferred between the participants (lecturers, administrators and external examiners) using secure electronic mail. Security was provided by a managed public key infrastructure. Users were profiled and interviewed in order to determine the likely success of further roll out within the institution, as well as the user friendliness of the existing paper-based and proposed electronic systems. The project found that, while the technology worked for some participants, others had severe problems with installation of the software and failed to grasp key concepts. There appear to be formidable obstacles to extending the system to cover the whole university, including compatibility of equipment and the reliability of the network infrastructure.\n\nTitle: Privacy Preserving Trust Authorization Framework Using XACML\nContent: Nowadays many organizations share sensitive services through open network systems and this raises the need for an authorization framework that can interoperate even when the parties have no pre-existing relationships. Trust Negotiation is the process used to establish these first relationships, through the transfer of attributes, embedded in digital credentials, between the two parties. However, these attributes may themselves be considered sensitive and so may need protection from disclosure. In some environments, the policies that govern the protected services may also be considered sensitive and their release to arbitrary strangers may leak confidential business information. This paper describes a way to unify the protection of services, sensitive credentials and policies in a synchronized trustworthy manner. We propose a trust authorization framework (TAF) that builds on the capabilities of XACML to support the bilateral exchange of policies and credentials through trust negotiation.\n\nTitle: Obligations Of Trust For Privacy And Confidentiality In Distributed Transactions\nContent: Purpose - This paper aims to describe a bilateral symmetric approach to authorization, privacy protection and obligation enforcement in distributed transactions. The authors introduce the concept of the obligation Of trust (OoT) protocol as a privacy assurance and authorization mechanism that is built upon the XACML standard. The OoT allows two communicating pal-ties to dynamically exchange their privacy and authorization requirements and capabilities, which the authors term a notification of obligation (NoB), as well as their commitments to fulfilling each other's requirements, which the authors term signed acceptance of obligations (SAO). The authors seek to describe some applicability of these concepts and to show how they can be integrated into distributed authorization systems for stricter privacy and confidentiality control.Design/methodology/approach - Existing access control and privacy protection systems are typically unilateral and provider-centric, in that the enterprise service provider assigns the access rights, makes the access control decisions, and determines the privacy policy. There is no negotiation between the client and the service provider about which access control or privacy policy to use. The authors adopt a symmetric, more user-centric approach to privacy protection and authorization, which treats the client and service provider as peers, in which both can stipulate their requirements and capabilities, and hence negotiate terms which are equally acceptable to both parties.Findings - The authors demonstrate how the obligation of trust protocol can be used in a number of different scenarios to improve upon the mechanisms that are currently available today.Practical implications - This approach will serve to increase trust in distributed transactions since each communicating party receives a difficult to repudiate digitally signed acceptance of obligations, in a standard language (XACML), which can be automatically enforced by their respective computing machinery.Originality/value - The paper adds to current research in trust negotiation, privacy protection and authorization by combining all three together into one set of standardized protocols. Furthermore, by providing hard to repudiate signed acceptance of obligations messages, this strengthens the legal case of the injured party should a dispute arise.\n\nTitle: The PERMIS X.509 role based privilege management infrastructure\nContent: This paper describes the output of the PERMIS project, which has developed a role based access control infrastructure that uses X.509 attribute certificates (ACs) to store the users' roles. All access control decisions are driven by an authorization policy, which is itself stored in an X.509 attribute certificate, thus guaranteeing its integrity. All the ACs can be stored in one or more LDAP directories, thus making them widely available. Authorization policies are written in XML according to a DTD that has been published at XML.org. The Access Control Decision Function (ADF) is written in Java and the Java API is simple to use, comprising of just 3 methods and a constructor. There is also a Privilege Allocator, which is a tool that constructs and signs attribute certificates and stores them in an LDAP directory for subsequent use by the ADF.\n\n\n\n## Task\nCompose an abstract for the title 'Evaluating Trust in a Public Key Certification Authority' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"With the growth of many different public key infrastructures on the Internet, relying parties have the difficult task of deciding whether the sender of digitally signed message is really who the public key certificate says they are. We have built an expert system that calculates the amount of trust, or trust quotient, that one can place in the name to public key binding in a certificate. The structure of the expert system is based on the CPS framework of Chokhani and Ford (RFC 2527), whilst the relative importance of the various factors that comprise the trust quotient, were determined by interviewing PKI experts from around the globe. This paper discusses the knowledge analysis strategy employed to collect this expert information and how we used it to develop the KBS. The analysis of the results of the interviews are also presented, and they can be summarised succinctly as \u201cthere are some factors concerning trust in a PKI which nearly all experts agree upon, and there are other factors in which there is very little agreement at all\u201d. The importance of identifying contextual factors when building a knowledge base is very important. In many cases, a disagreement between experts, as shown by a bimodal split in importance, was traced to differences in context and we show how this can be a source of new knowledge."
      }
    ]
  },
  {
    "tid":"287",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Subjectively Interesting Subgroup Discovery on Real-valued Targets.\nContent: Deriving insights from high-dimensional data is one of the core problems in data mining. The difficulty mainly stems from the large number of variable combinations to potentially consider. Hence, an obvious question is whether we can automate the search for interesting patterns. Here, we consider the setting where a user wants to learn as efficiently as possible about real-valued attributes. We introduce a method to find subgroups in the data that are maximally informative (in the Information Theoretic sense) with respect to one or more real-valued target attributes. The succinct subgroup descriptions are in terms of arbitrarily-typed description attributes. The approach is based on the Subjective Interestingness framework FORSIED to use prior knowledge when mining most informative patterns.\n\nTitle: Subjective Interestingness in Exploratory Data Mining.\nContent: Exploratory data mining has as its aim to assist a user in improving their understanding about the data. Considering this aim, it seems self-evident that in optimizing this process the data as well as the user need to be considered. Yet, the vast majority of exploratory data mining methods (including most methods for clustering, itemset and association rule mining, subgroup discovery, dimensionality reduction, etc) formalize interestingness of patterns in an objective manner, disregarding the user altogether. More often than not this leads to subjectively uninteresting patterns being reported. Here I will discuss a general mathematical framework for formalizing interestingness in a subjective manner. I will further demonstrate how it can be successfully instantiated for a variety of exploratory data mining problems. Finally, I will highlight some connections to other work, and outline some of the challenges and research opportunities ahead.\n\nTitle: Subjectively Interesting Component Analysis: Data Projections that Contrast with Prior Expectations\nContent: Methods that find insightful low-dimensional projections are essential to effectively explore high-dimensional data. Principal Component Analysis is used pervasively to find low-dimensional projections, not only because it is straightforward to use, but it is also often effective, because the variance in data is often dominated by relevant structure. However, even if the projections highlight real structure in the data, not all structure is interesting to every user. If a user is already aware of, or not interested in the dominant structure, Principal Component Analysis is less effective for finding interesting components. We introduce a new method called Subjectively Interesting Component Analysis (SICA), designed to find data projections that are subjectively interesting, i.e, projections that truly surprise the end-user. It is rooted in information theory and employs an explicit model of a user's prior expectations about the data. The corresponding optimization problem is a simple eigenvalue problem, and the result is a trade-off between explained variance and novelty. We present five case studies on synthetic data, images, time-series, and spatial data, to illustrate how SICA enables users to find (subjectively) interesting projections.\n\n\n\nTitle: Interactive Visual Data Exploration with Subjective Feedback.\nContent: Data visualization and iterative/interactive data mining are growing rapidly in attention, both in research as well as in industry. However, integrated methods and tools that combine advanced visualization and data mining techniques are rare, and those that exist are often specialized to a single problem or domain. In this paper, we introduce a novel generic method for interactive visual exploration of high-dimensional data. In contrast to most visualization tools, it is not based on the traditional dogma of manually zooming and rotating data. Instead, the tool initially presents the user with an \u2018interesting\u2019 projection of the data and then employs data randomization with constraints to allow users to flexibly and intuitively express their interests or beliefs using visual interactions that correspond to exactly defined constraints. These constraints expressed by the user are then taken into account by a projection-finding algorithm to compute a new \u2018interesting\u2019 projection, a process that can be iterated until the user runs out of time or finds that constraints explain everything she needs to find from the data. We present the tool by means of two case studies, one controlled study on synthetic data and another on real census data. The data and software related to this paper are available at http://www.interesting-patterns.net/forsied/interactive-visual-data-exploration-with-subjective-feedback/.\n\nTitle: Maximum entropy models and subjective interestingness: an application to tiles in binary databases\nContent: Recent research has highlighted the practical benefits of subjective interestingness measures, which quantify the novelty or unexpectedness of a pattern when contrasted with any prior information of the data miner (Silberschatz and Tuzhilin, Proceedings of the 1st ACM SIGKDD international conference on Knowledge discovery and data mining (KDD95), 1995; Geng and Hamilton, ACM Comput Surv 38(3):9, 2006). A key challenge here is the formalization of this prior information in a way that lends itself to the definition of a subjective interestingness measure that is both meaningful and practical. In this paper, we outline a general strategy of how this could be achieved, before working out the details for a use case that is important in its own right. Our general strategy is based on considering prior information as constraints on a probabilistic model representing the uncertainty about the data. More specifically, we represent the prior information by the maximum entropy (MaxEnt) distribution subject to these constraints. We briefly outline various measures that could subsequently be used to contrast patterns with this MaxEnt model, thus quantifying their subjective interestingness. We demonstrate this strategy for rectangular databases with knowledge of the row and column sums. This situation has been considered before using computation intensive approaches based on swap randomizations, allowing for the computation of empirical p-values as interestingness measures (Gionis et al., ACM Trans Knowl Discov Data 1(3):14, 2007). We show how the MaxEnt model can be computed remarkably efficiently in this situation, and how it can be used for the same purpose as swap randomizations but computationally more efficiently. More importantly, being an explicitly represented distribution, the MaxEnt model can additionally be used to define analytically computable interestingness measures, as we demonstrate for tiles (Geerts et al., Proceedings of the 7th international conference on Discovery science (DS04), 2004) in binary databases.\n\n## Task\nCompose an abstract for the title 'SICA: subjectively interesting component analysis.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The information in high-dimensional datasets is often too complex for human users to perceive directly. Hence, it may be helpful to use dimensionality reduction methods to construct lower dimensional representations that can be visualized. The natural question that arises is  We study this question from an information-theoretic perspective and introduce a new method for linear dimensionality reduction. The obtained model that quantifies the informativeness also allows us to flexibly account for prior knowledge a user may have about the data. This enables us to provide representations that are . We title the method Subjectively Interesting Component Analysis (SICA) and expect it is mainly useful for iterative data mining. SICA is based on a model of a user\u2019s belief state about the data. This belief state is used to search for surprising views. The initial state is chosen by the user (it may be empty up to the data format) and is updated automatically as the analysis progresses. We study several types of prior beliefs: if a user only knows the scale of the data, SICA yields the same cost function as Principal Component Analysis (PCA), while if a user expects the data to have outliers, we obtain a variant that we term -PCA. Finally, scientifically more interesting variants are obtained when a user has more complicated beliefs, such as knowledge about similarities between data points. The experiments suggest that SICA enables users to find subjectively more interesting representations."
      }
    ]
  },
  {
    "tid":"288",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Goal-oriented requirements analysis and reasoning in the Tropos methodology\nContent: Tropos is an agent-oriented software methodology proposed in (J. Autonomous Agents Multi-Agent Syst. 8(3) (2004) 203; Inf. Syst. 27(6) (2002) 365). The methodology is founded on the notions of agent and goal, and goal analysis is used extensively to support software development during different phases. This paper adopts a formal goal model defined and analyzed in (J. Data Semantics 1 (2003); Proceedings of the International Conference on Advanced Information Systems Engineering, CAISE'04, vol. 3804 of LNCS, Springer, Berlin, 2004, pp. 20-33) to make the goal analysis process concrete through the use of forward and backward reasoning for goal models. The formal goal analysis is illustrated through examples, using an implemented goal reasoning tool.\n\nTitle: A goal-based framework for contextual requirements modeling and analysis\nContent: Requirements engineering (RE) research often ignores or presumes a uniform nature of the context in which the system operates. This assumption is no longer valid in emerging computing paradigms, such as ambient, pervasive and ubiquitous computing, where it is essential to monitor and adapt to an inherently varying context. Besides influencing the software, context may influence stakeholders\u2019 goals and their choices to meet them. In this paper, we propose a goal-oriented RE modeling and reasoning framework for systems operating in varying contexts. We introduce contextual goal models to relate goals and contexts; context analysis to refine contexts and identify ways to verify them; reasoning techniques to derive requirements reflecting the context and users priorities at runtime; and finally, design time reasoning techniques to derive requirements for a system to be developed at minimum cost and valid in all considered contexts. We illustrate and evaluate our approach through a case study about a museum-guide mobile information system.\n\nTitle: Using Goal Models Downstream: A Systematic Roadmap And Literature Review\nContent: Goal models have proven useful for capturing, understanding, and communicating requirements during early stages of software development. However, the utility of goal models is greatly enhanced when they can be exploited during downstream stages of the requirements analysis process (e.g. requirements elaboration, validation, planning), and can be used as part of the entire system life cycle (e.g., architectural and behavioral process design, coding, testing, monitoring, adaptation, and evolution). In order to better understand the progress that has been made in integrating goal models with downstream system development, the authors ask: what approaches exist that map/integrate/transform goal models to later stage software artifacts? To answer this question, they conduct a systematic survey, producing a roadmap of work summarizing 243 publications. Results include a categorization of the \"why?\" and \"how?\" for each approach. Furthermore, they select the 50 most prominent publications, based on citation numbers, in order to perform an in-depth literature review. Findings show that there is a wide variety of proposals with a variety of proposed goal models and targets, covering multiple paradigms, motivated by a variety of purposes. The authors conclude that although much work has been done in this area, the work is fragmented, following multiple separate strands of goal-orientation, and is often still in early stages of maturity.\n\nTitle: From early to late requirements: a goal-based approach\nContent: The Software Engineering community is placing increasing emphasis on understanding the organizational context of a new software system before its development. In this context, several research projects focus on mechanisms that facilitate the generation of a software system from early requirements specifications. However, none of these has proposed so far a systematic process for transforming an organizational model into a late requirements one. This paper presents a methodological approach to precisely this problem. In the proposed method, business goals constitute the basis for determining the relevant plans to be supported by the system-to-be. A pattern language is then used to systematically carry out the transformation from an organizational model into a late requirements model. The Tropos framework serves as baseline for this work. However, our work extends Tropos by proposing rules that support the model transformation process, thereby making organizational modeling an integral part of the software development process.\n\nTitle: Goal-oriented requirements engineering: an extended systematic mapping study\nContent:  Over the last two decades, much attention has been paid to the area of goal-oriented requirements engineering (GORE), where goals are used as a useful conceptualization to elicit, model, and analyze requirements, capturing alternatives and conflicts. Goal modeling has been adapted and applied to many sub-topics within requirements engineering (RE) and beyond, such as agent orientation, aspect orientation, business intelligence, model-driven development, and security. Despite extensive efforts in this field, the RE community lacks a recent, general systematic literature review of the area. In this work, we present a systematic mapping study, covering the 246 top-cited GORE-related conference and journal papers, according to Scopus. Our literature map addresses several research questions: we classify the types of papers (e.g., proposals, formalizations, meta-studies), look at the presence of evaluation, the topics covered (e.g., security, agents, scenarios), frameworks used, venues, citations, author networks, and overall publication numbers. For most questions, we evaluate trends over time. Our findings show a proliferation of papers with new ideas and few citations, with a small number of authors and papers dominating citations; however, there is a slight rise in papers which build upon past work (implementations, integrations, and extensions). We see a rise in papers concerning adaptation/variability/evolution and a slight rise in case studies. Overall, interest in GORE has increased. We use our analysis results to make recommendations concerning future GORE research and make our data publicly available.\n\n## Task\nCompose an abstract for the title 'Method Construction By Goal Analysis' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Method engineering proposes the construction of methodologies by selecting method fragments from a repository and assembling them in an appropriate way. However, the rules by which the \"optimal\" method fragments are chosen are not clear, and such chores are usually done manually by an expert. This paper presents a goal analysis technique for the selection of the optimal method fragments from a repository, using backward reasoning to obtain the set of fragments that satisfy the desired goals with minimum effort. By using this technique, a methodologist can determine the goals that the organisation wants the methodology to satisfy, and then, preferably, rely on automated tools for the selection of the optimal solution."
      }
    ]
  },
  {
    "tid":"289",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Diffeomorphic Surface Registration with Atrophy Constraints.\nContent: Diffeomorphic registration using optimal control on the diffeomorphism group and on shape spaces has become widely used since the development of the large deformation diffeomorphic metric mapping (LDDMM) algorithm. More recently, a series of algorithms involving sub-Riemannian constraints have been introduced in which the velocity fields that control the shapes in the LDDMM framework are constrained in accordance with a specific deformation model. Here, we extend this setting by considering, for the first time, inequality constraints in order to estimate surface deformations that only allow for atrophy, introducing for this purpose an algorithm that uses the augmented Lagrangian method. We prove the existence of solutions of the associated optimal control problem and the consistency of our approximation scheme. These developments are illustrated by numerical experiments on simulated and real data.\n\nTitle: Robust diffeomorphic mapping via geodesically controlled active shapes\nContent: This paper presents recent advances in the use of diffeomorphic active shapes which incorporate the conservation laws of large deformation diffeomorphic metric mapping. The equations of evolution satisfying the conservation law are geodesics under the diffeomorphism metric and therefore termed geodesically controlled diffeomorphic active shapes (GDAS). Our principal application in this paper is on robust diffeomorphic mapping methods based on parameterized surface representations of subcortical template structures. Our parametrization of the GDAS evolution is via the initial momentum representation in the tangent space of the template surface. The dimension of this representation is constrained using principal component analysis generated fromtraining samples. In this work, we seek to use template surfaces to generate segmentations of the hippocampus with three data attachment terms: surface matching, landmark matching, and inside-outside modeling from grayscale T1 MR imaging data.This is formulated as an energy minimization problem,where energy describes shape variability and data attachment accuracy, and we derive a variational solution. A gradient descent strategy is employed in the numerical optimization. For the landmark matching case, we demonstrate the robustness of this algorithm as applied to the workflow of a large neuroanatomical study by comparing to an existing diffeomorphic landmark matching algorithm.\n\nTitle: Multi-contrast large deformation diffeomorphic metric mapping for diffusion tensor imaging.\nContent: Diffusion tensor imaging (DTI) can reveal detailed white matter anatomy and has the potential to detect abnormalities in specific white matter structures. Such detection and quantification are, however, not straightforward. The voxel-based analysis after image normalization is one of the most widely used methods for quantitative image analyses. To apply this approach to DTI, it is important to examine if structures in the white matter are well registered among subjects, which would be highly dependent on employed algorithms for normalization. In this paper, we evaluate the accuracy of normalization of DTI data using a highly elastic transformation algorithm, called large deformation diffeomorphic metric mapping. After simulation-based validation of the algorithm, DTI data from normal subjects were used to measure the registration accuracy. To examine the impact of morphological abnormalities on the accuracy, the algorithm was also tested using data from Alzheimer's disease (AD) patients with severe brain atrophy. The accuracy level was measured by using manual landmark-based white matter matching and surface-based brain and ventricle matching as gold standard. To improve the accuracy level, cascading and multi-contrast approaches were developed. The accuracy level for the white matter was 1.88\u00b10.55 and 2.19\u00b10.84\u00a0mm for the measured locations in the controls and patients, respectively.\n\nTitle: Parametric Surface Diffeomorphometry for Low Dimensional Embeddings of Dense Segmentations and Imagery.\nContent: In the field of Computational Anatomy, biological form (including our focus, neuroanatomy) is studied quantitatively through the action of the diffeomorphism group on example anatomies \u2013 a technique called diffeomorphometry. Here we design an algorithm within this framework to pass from dense objects common in neuromaging studies (binary segmentations, structural images) to a sparse representation defined on the surface boundaries of anatomical structures, and embedded into the low dimensional coordinates of a parametric model. Our main new contribution is to introduce an expanded group action to simultaneously deform surfaces through direct mapping of points, as well as images through functional composition with the inverse. This allows us to index the diffeomorphisms with respect to two-dimensional surface geometries like subcortical gray matter structures, but explicitly map onto cost functions determined by noisy 3-dimensional measurements. We consider models generated from empirical covariance of training data, as well as bandlimited (Laplace-Beltrami eigenfunction) models when no such data is available. We show applications to noisy or anomalous segmentations, and other typical problems in neuroimaging studies. We reproduce statistical results detecting changes in Alzheimer's disease, despite dimensionality reduction. Lastly we apply our algorithm to the common problem of segmenting subcortical structures from T1 MR images.\n\nTitle: Whole brain diffeomorphic metric mapping via integration of sulcal and gyral curves, cortical surfaces, and images.\nContent: This paper introduces a novel large deformation diffeomorphic metric mapping algorithm for whole brain registration where sulcal and gyral curves, cortical surfaces, and intensity images are simultaneously carried from one subject to another through a flow of diffeomorphisms. To the best of our knowledge, this is the first time that the diffeomorphic metric from one brain to another is derived in a shape space of intensity images and point sets (such as curves and surfaces) in a unified manner. We describe the Euler\u2013Lagrange equation associated with this algorithm with respect to momentum, a linear transformation of the velocity vector field of the diffeomorphic flow. The numerical implementation for solving this variational problem, which involves large-scale kernel convolution in an irregular grid, is made feasible by introducing a class of computationally friendly kernels. We apply this algorithm to align magnetic resonance brain data. Our whole brain mapping results show that our algorithm outperforms the image-based LDDMM algorithm in terms of the mapping accuracy of gyral/sulcal curves, sulcal regions, and cortical and subcortical segmentation. Moreover, our algorithm provides better whole brain alignment than combined volumetric and surface registration (Postelnicu et al., 2009) and hierarchical attribute matching mechanism for elastic registration (HAMMER) (Shen and Davatzikos, 2002) in terms of cortical and subcortical volume segmentation.\n\n## Task\nCompose an abstract for the title 'Principal component based diffeomorphic surface mapping.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We present a new diffeomorphic surface mapping algorithm under the framework of large deformation diffeomorphic metric mapping (LDDMM). Unlike existing LDDMM approaches, this new algorithm reduces the complexity of the estimation of diffeomorphic transformations by incorporating a shape prior in which a nonlinear diffeomorphic shape space is represented by a linear space of initial momenta of diff..."
      }
    ]
  },
  {
    "tid":"290",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Exploiting Rateless Codes and Belief Propagation to Infer Identity of Polluters in MANET\nContent: In this paper, we consider a scenario where nodes in a MANET disseminate data chunks using rateless codes. Any node is able to successfully decode any chunk by collecting enough coded blocks from several other nodes without any coordination. We consider the problem of identifying malicious nodes that launch a pollution attack by deliberately modifying the payload of coded blocks before transmitting. It follows that the original chunk can only be obtained if there are no malicious nodes among the chunk providers. In this paper we propose SIEVE, a fully distributed technique to infer the identity of malicious nodes. A node creates what we termed a check whenever a chunk is decoded; a check is a pair composed of the set of other nodes that provided coded blocks used to decode the chunk (the chunk uploaders) and a flag indicating whether the chunk is corrupted or not. SIEVE exploits rateless codes to detect chunk integrity and belief propagation to infer the identity of malicious nodes. In particular, every node autonomously constructs its own bipartite graph (a.k.a. factor graph in the literature) whose vertexes are checks and nodes, respectively. Then, it periodically runs the belief propagation algorithm on its factor graph to infer the probability of other nodes being malicious. We show by running detailed simulations using ns-3 that SIEVE is very accurate and robust under several attack scenarios and deceiving actions. We discuss how the topological properties of the factor graph impacts SIEVE performance and show that nodes speed in the MANET plays a role on the identification accuracy. Furthermore, an interesting trade-off between coding efficiency and SIEVE accuracy, completeness, and reactivity is discovered. We also show that SIEVE is efficient requiring low computational, memory, and communication resources.\n\nTitle: SIEVE: A Distributed, Accurate, and Robust Technique to Identify Malicious Nodes in Data Dissemination on MANET\nContent: In this paper we consider the following problem: nodes in a MANET must disseminate data chunks using rateless codes but some nodes are assumed to be malicious, i.e., before transmitting a coded packet they may modify its payload. Nodes receiving corrupted coded packets are prevented from correctly decoding the original chunk. We propose SIEVE, a fully distributed technique to identify malicious nodes. SIEVE is based on special messages called checks that nodes periodically transmit. A check contains the list of nodes identifiers that provided coded packets of a chunk as well as a flag to signal if the chunk has been corrupted. SIEVE operates on top of an otherwise reliable architecture and it is based on the construction of a factor graph obtained from the collected checks on which an incremental belief propagation algorithm is run to compute the probability of a node being malicious. Analysis is carried out by detailed simulations using ns-3. We show that SIEVE is very accurate and discuss how nodes speed impacts on its accuracy. We also show SIEVE robustness under several attack scenarios and deceiving actions.\n\nTitle: Pollution-resilient peer-to-peer video streaming with Band Codes\nContent: Band Codes (BC) have been recently proposed as a solution for controlled-complexity random Network Coding (NC) in mobile applications, where energy consumption is a major concern. In this paper, we investigate the potential of BC in a peer-to-peer video streaming scenario where malicious and honest nodes coexists. Malicious nodes launch the so called pollution attack by randomly modifying the content of the coded packets they forward to downstream nodes, preventing honest nodes from correctly recovering the video stream. Whereas in much of the related literature this type of attack is addressed by identifying and isolating the malicious nodes, in this work we propose to address it by adaptively adjusting the coding scheme so to introduce resilience against pollution propagation. We experimentally show the impact of a pollution attack in a defenseless system and in a system where the coding parameters of BC are adaptively modulated following the discovery of polluted packets in the network. We observe that just by tuning the coding parameters, it is possible to reduce the impact of a pollution attack and restore the quality of the video communication.\n\nTitle: DIP: Distributed Identification of Polluters in P2P Live Streaming\nContent: Peer-to-peer live streaming applications are vulnerable to malicious actions of peers that deliberately modify data to decrease or prevent the fruition of the media (pollution attack). In this article we propose DIP, a fully distributed, accurate, and robust algorithm for the identification of polluters. DIP relies on checks that are computed by peers upon completing reception of all blocks composing a data chunk. A check is a special message that contains the set of peer identifiers that provided blocks of the chunk as well as a bit to signal if the chunk has been corrupted. Checks are periodically transmitted by peers to their neighbors in the overlay network; peers receiving checks use them to maintain a factor graph. This graph is bipartite and an incremental belief propagation algorithm is run on it to compute the probability of a peer being a polluter. Using a prototype deployed over PlanetLab we show by extensive experimentation that DIP allows honest peers to identify polluters with very high accuracy and completeness, even when polluters collude to deceive them. Furthermore, we show that DIP is efficient, requiring low computational, communication, and storage overhead at each peer.\n\nTitle: Characterization of Band Codes for Pollution-Resilient Peer-to-Peer Video Streaming.\nContent: We provide a comprehensive characterization of band codes (BC) as a resilient-by-design solution to pollution attacks in network coding (NC)-based peer-to-peer live video streaming. Consider one malicious node injecting bogus coded packets into the network: the recombinations at the nodes generate an avalanche of novel coded bogus packets. Therefore, the malicious node can cripple the communication by injecting into the network only a handful of polluted packets. Pollution attacks are typically addressed by identifying and isolating the malicious nodes from the network. Pollution detection is, however, not straightforward in NC as the nodes exchange coded packets. Similarly, malicious nodes identification is complicated by the ambiguity between malicious nodes and nodes that have involuntarily relayed polluted packets. This paper addresses pollution attacks through a radically different approach which relies on BCs. BCs are a family of rateless codes originally designed for controlling the NC decoding complexity in mobile applications. Here, we exploit BCs for the totally different purpose of recombining the packets at the nodes so to avoid that the pollution propagates by adaptively adjusting the coding parameters. Our streaming experiments show that BCs curb the propagation of the pollution and restore the quality of the distributed video stream.\n\n## Task\nCompose an abstract for the title 'Simple Countermeasures to Mitigate the Effect of Pollution Attack in Network Coding-Based Peer-to-Peer Live Streaming' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Network coding (NC)-based peer-to-peer (P2P) streaming represents an effective solution to aggregate user capacities and to increase system throughput in live multimedia streaming. Nonetheless, such systems are vulnerable to pollution attacks where a handful of malicious peers can disrupt the communication by transmitting just a few bogus packets which are then recombined and relayed by unaware honest nodes, further spreading the pollution over the network. Whereas previous research focused on malicious nodes identification schemes and pollution-resilient coding, in this paper we show pollution countermeasures which make a standard NC scheme resilient to pollution attacks. Thanks to a simple yet effective analytical model of a reference node collecting packets by malicious and honest neighbors, we demonstrate that: i) packets received earlier are less likely to be polluted, and ii) short generations increase the likelihood to recover a clean generation. Therefore, we propose a recombination scheme where nodes draw packets to be recombined according to their age in the input queue, paired with a decoding scheme able to detect the reception of polluted packets early in the decoding process and short generations. The effectiveness of our approach is experimentally evaluated in a real system we developed and deployed on hundreds to thousands of peers. Experimental evidence shows that, thanks to our simple countermeasures, the effect of a pollution attack is almost canceled and the video quality experienced by the peers is comparable to pre-attack levels."
      }
    ]
  },
  {
    "tid":"291",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Hybrid Abductive Inductive Learning: A Generalisation of Progol\nContent: The learning system Progo15 and the underlying inference method of Bottom Generalisation are firmly established within Inductive Logic Programming (ILP). But despite their success, it is known that Bottom Generalisation, and therefore Progo15, are restricted to finding hypotheses that lie within the semantics of Plotkin's relative subsumption. This paper exposes a previously unknown incompleteness of Progo15 with respect to Bottom Generalisation, and proposes a new approach, called Hybrid Abductive Inductive Learning, that integrates the ILP principles of Progo15 with Abductive Logic Programming (ALP). A proof procedure is proposed, called HAIL, that not only overcomes this newly discovered incompleteness, but further generalises Progo15 by computing multiple clauses in response to a single seed example and deriving hypotheses outside Plotkin's relative subsumption. A semantics is presented, called Kernel Generalisation, which extends that of Bottom Generalisation. and includes the hypotheses constructed by HAIL.\n\nTitle: Iterative Learning of Answer Set Programs from Context Dependent Examples.\nContent: In recent years, several frameworks and systems have been proposed that extend Inductive Logic Programming (ILP) to the Answer Set Programming (ASP) paradigm. In ILP, examples must all be explained by a hypothesis together with a given background knowledge. In existing systems, the background knowledge is the same for all examples; however, examples may be context-dependent. This means that some examples should be explained in the context of some information, whereas others should be explained in different contexts. In this paper, we capture this notion and present a context-dependent extension of the Learning from Ordered Answer Sets framework. In this extension, contexts can be used to further structure the background knowledge. We then propose a new iterative algorithm, ILASP2i, which exploits this feature to scale up the existing ILASP2 system to learning tasks with large numbers of examples. We demonstrate the gain in scalability by applying both algorithms to various learning tasks. Our results show that, compared to ILASP2, the newly proposed ILASP2i system can be two orders of magnitude faster and use two orders of magnitude less memory, whilst preserving the same average accuracy.\n\nTitle: The complexity and generality of learning answer set programs.\nContent: Traditionally most of the work in the field of Inductive Logic Programming (ILP) has addressed the problem of learning Prolog programs. On the other hand, Answer Set Programming is increasingly being used as a powerful language for knowledge representation and reasoning, and is also gaining increasing attention in industry. Consequently, the research activity in ILP has widened to the area of Answer Set Programming, witnessing the proposal of several new learning frameworks that have extended ILP to learning answer set programs. In this paper, we investigate the theoretical properties of these existing frameworks for learning programs under the answer set semantics. Specifically, we present a detailed analysis of the computational complexity of each of these frameworks with respect to the two decision problems of deciding whether a hypothesis is a solution of a learning task and deciding whether a learning task has any solutions. We introduce a new notion of generality of a learning framework, which enables us to define a framework to be more general than another in terms of being able to distinguish one ASP hypothesis solution from a set of incorrect ASP programs. Based on this notion, we formally prove a generality relation over the set of existing frameworks for learning programs under answer set semantics. In particular, we show that our recently proposed framework, Context-dependent Learning from Ordered Answer Sets, is more general than brave induction, induction of stable models, and cautious induction, and maintains the same complexity as cautious induction, which has the highest complexity of these frameworks.\n\nTitle: Inductive Logic Programming as Abductive Search\nContent: We present a novel approach to non-monotonic ILP and its implementation called TAL (Top-directed Abductive Learning). TAL overcomes some of the completeness problems of ILP systems based on Inverse Entailment and is the first top-down ILP system that allows background theories and hypotheses to be normal logic programs. The approach relies on mapping an ILP problem into an equivalent ALP one. This enables the use of established ALP proof procedures and the specification of richer language bias with integrity constraints. The mapping provides a principled search space for an ILP problem, over which an abductive search is used to compute inductive solutions.\n\nTitle: Using Abduction to Evolve Inconsistent Requirements Specifications\nContent: Requirements specifications are often inconsistent. Inconsistencies may arise because multiple conflicting requirements are embodied in these specifications, or because the specifications themselves are in a transient stage of evolutionary development. In this paper we argue that such inconsistencies, rather than being undesirable, are actually useful drivers for changing the requirements specifications in which they arise. We present a formal technique to reason about inconsistency handling changes. Our technique is an adaptation of logical abduction - adapted to generate changes that address some specification inconsistencies, while leaving others. We represent our specifications in quasi-classical (QC) logic - an adaptation of classical logic that allows continued reasoning in the presence of inconsistency. The paper develops a sound algorithm for automating our abductive reasoning technique and presents illustrative examples drawn from a library system case study.\n\n## Task\nCompose an abstract for the title 'Generalised Kernel Sets for Inverse Entailment' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The task of inverting logical entailment is of central importance to the disciplines of Abductive and Inductive Logic Programming (ALP & ILP). Bottom Generalisation (BG) is a widely applied approach for Inverse Entailment (IE), but is limited to deriving single clauses from a hypothesis space restricted by Plotkin's notion of C-derivation. Moreover, known practical applications of BG are confined to Horn clause logic. Recently, a hybrid ALP-ILP proof procedure, called HAIL, was shown to generalise existing BG techniques by deriving multiple clauses in response to a single example, and constructing hypotheses outside the semantics of BG. The HAIL proof procedure is based on a new semantics, called Kernel Set Subsumption (KSS), which was shown to be a sound generalisation of BG. But so far KSS is defined only for Horn clauses. This paper extends the semantics of KSS from Horn clause logic to general clausal logic, where it is shown to remain a sound extension of BG. A generalisation of the C-derivation, called a K*-derivation, is introduced and shown to provide a sound and complete characterisation of KSS. Finally, the K*-derivation is used to provide a systematic comparison of existing proof procedures based on IE."
      }
    ]
  },
  {
    "tid":"292",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Least-squares Approach to Direct Importance Estimation\nContent: We address the problem of estimating the ratio of two probability density functions, which is often referred to as the importance. The importance values can be used for various succeeding tasks such as covariate shift adaptation or outlier detection. In this paper, we propose a new importance estimation method that has a closed-form solution; the leave-one-out cross-validation score can also be computed analytically. Therefore, the proposed method is computationally highly efficient and simple to implement. We also elucidate theoretical properties of the proposed method such as the convergence rate and approximation error bounds. Numerical experiments show that the proposed method is comparable to the best existing method in accuracy, while it is computationally more efficient than competing approaches.\n\nTitle: Conditional Density Estimation with Dimensionality Reduction via Squared-Loss Conditional Entropy Minimization\nContent: Regression aims at estimating the conditional mean of output given input. However, regression is not informative enough if the conditional density is multimodal, heteroskedastic, and asymmetric. In such a case, estimating the conditional density itself is preferable, but conditional density estimation (CDE) is challenging in high-dimensional space. A naive approach to coping with high dimensionality is to first perform dimensionality reduction (DR) and then execute CDE. However, a two-step process does not perform well in practice because the error incurred in the first DR step can be magnified in the second CDE step. In this letter, we propose a novel single-shot procedure that performs CDE and DR simultaneously in an integrated way. Our key idea is to formulate DR as the problem of minimizing a squared-loss variant of conditional entropy, and this is solved using CDE. Thus, an additional CDE step is not needed after DR. We demonstrate the usefulness of the proposed method through extensive experiments on various data sets, including humanoid robot transition and computer art.\n\nTitle: Model-Based Policy Gradients with Parameter-Based Exploration by Least-Squares Conditional Density Estimation.\nContent: The goal of reinforcement learning (RL) is to let an agent learn an optimal control policy in an unknown environment so that future expected rewards are maximized. The model-free RL approach directly learns the policy based on data samples. Although using many samples tends to improve the accuracy of policy learning, collecting a large number of samples is often expensive in practice. On the other hand, the model-based RL approach first estimates the transition model of the environment and then learns the policy based on the estimated transition model. Thus, if the transition model is accurately learned from a small amount of data, the model-based approach is a promising alternative to the model-free approach. In this paper, we propose a novel model-based RL method by combining a recently proposed model-free policy search method called policy gradients with parameter-based exploration and the state-of-the-art transition model estimator called least-squares conditional density estimation. Through experiments, we demonstrate the practical usefulness of the proposed method.\n\nTitle: Model-based policy gradients with parameter-based exploration by least-squares conditional density estimation\nContent: The goal of reinforcement learning (RL) is to let an agent learn an optimal control policy in an unknown environment so that future expected rewards are maximized. The model-free RL approach directly learns the policy based on data samples. Although using many samples tends to improve the accuracy of policy learning, collecting a large number of samples is often expensive in practice. On the other hand, the model-based RL approach first estimates the transition model of the environment and then learns the policy based on the estimated transition model. Thus, if the transition model is accurately learned from a small amount of data, the model-based approach is a promising alternative to the model-free approach. In this paper, we propose a novel model-based RL method by combining a recently proposed model-free policy search method called policy gradients with parameter-based exploration and the state-of-the-art transition model estimator called least-squares conditional density estimation. Through experiments, we demonstrate the practical usefulness of the proposed method.\n\nTitle: Constrained Least-Squares Density-Difference Estimation\nContent: We address the problem of estimating the difference between two probability densities. A naive approach is a two-step procedure that first estimates two densities separately and then computes their difference. However, such a two-step procedure does not necessarily work well because the first step is performed without regard to the second step and thus a small error in the first stage can cause a big error in the second stage. Recently, a single-shot method called the least-squares density-difference (LSDD) estimator has been proposed. LSDD directly estimates the density difference without separately estimating two densities, and it was demonstrated to outperform the two-step approach. In this paper, we propose a variation of LSDD called the constrained least-squares density-difference (CLSDD) estimator, and theoretically prove that CLSDD improves the accuracy of density difference estimation for correctly specified parametric models. The usefulness of the proposed method is also demonstrated experimentally.\n\n## Task\nCompose an abstract for the title 'Least-Squares Conditional Density Estimation' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Estimating the conditional mean of an input-output relation is the goal of regression. However, regression analysis is not sufficiently informative if the conditional distribution has multi-modality, is highly asymmetric, or contains heteroscedastic noise. In such scenarios, estimating the conditional distribution itself would be more useful. In this paper. we propose a novel method of conditional density estimation that is suitable for multi-dimensional continuous variables. The basic idea of the proposed method is to express the conditional density in terms of the density ratio and the ratio is directly estimated without going through density estimation. Experiments using benchmark and robot transition datasets illustrate the usefulness of the proposed approach."
      }
    ]
  },
  {
    "tid":"293",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Theory and implementation of a distributed event based platform.\nContent: This paper presents theory and an implementation of a Distributed Event Based System (DEBS) platform. The theory is based on a simple model that forms the basis of the implementation. Though this paper is about a DEBS platform, a description of the theory and model provides the motivation for the design. Many software libraries operate on \\\"data at rest', i.e. fixed data structures such as arrays and graphs. By contrast, DEBS systems operate on \\\"data in motion,\\\" i.e., data structures that change, in increments, over time. Many software libraries are designed for sequential execution or synchronous parallel execution. By contrast, DEBS systems have multiple agents executing asynchronously. The paper presents sufficient conditions that enable programs operating on data at rest to be reconfigured as networks of asynchronous agents operating on data structures that change incrementally as time progresses. The paper provides a brief description of a DEBS platform, called StreamPy, implemented in Python. StreamPy enables the use of libraries designed to operate on data at rest --- particularly for data analytics, artificial intelligence, and scientific computation --- for data in motion. An event is either defined by a pre-specified pattern or an event is learned from data. Learning what is, and what is not, an event requires the use of machine learning algorithms. A goal of StreamPy is to incorporate machine learning into data streaming to obtain a DEBS platform that learns what is an event and then to continually improve this learning.\n\nTitle: Control-Based Scheduling in a Distributed Stream Processing System\nContent: Stream processing systems receive continuous streams of messages with raw information and produce streams of messages with processed information. The utility of a stream-processing system depends, in part, on the accuracy and timeliness of the output. Streams in complex event processing systems are processed on distributed systems; several steps are taken on different processors to process each incoming message, and messages may be enqueued between steps. This paper deals with the problems of distributed dynamic control of streams to optimize the total utility provided by the system. A challenge of distributed control is that timeliness of output depends only on the total end-toend time and is otherwise independent of the delays at each separate processor whereas the controller for each processor takes action to control only the steps on that processor and cannot directly control the entire network. This paper identifies key problems in distributed control and analyzes two scheduling algorithms that help in an initial analysis of a difficult problem.\n\nTitle: Distributed snapshots: determining global states of distributed systems\nContent: This paper presents an algorithm by which a process in a distributed system determines a global state of the system during a computation. Many problems in distributed systems can be cast in terms of the problem of detecting global states. For instance, the global state detection algorithm helps to solve an important class of problems: stable property detection. A stable property is one that persists: once a stable property becomes true it remains true thereafter. Examples of stable properties are \u201ccomputation has terminated,\u201d \u201c the system is deadlocked\u201d and \u201call tokens in a token ring have disappeared.\u201d The stable property detection problem is that of devising algorithms to detect a given stable property. Global state detection can also be used for checkpointing.\n\nTitle: Systematic Composition of Objects in Distributed Internet Applications: Processes and Sessions\nContent: We consider a system with the infrastructure for the creation and interconnection of large numbers of distributed persistent objects. This system is exemplified by the Internet: potentially, every appliance and document on the Internet has both persistent state and the ability to interact with large numbers of other appliances and documents on the Internet. This paper elucidates the characteristics of such a system, and proposes the compositional requirements of its corresponding infrastructure. We explore the problems of specifying, composing, reasoning about, and implementing applications in such a system. A specific concern of our research is developing the infrastructure to support structuring distributed applications by using sequential, choice, and parallel composition, in the anarchic environment where application compositions may be unforeseeable, and interactions may be unknown prior to actually occurring. The structuring concepts discussed are relevant to a wide range of distributed applications; our implementation is illustrated with collaborative Java processes interacting over the Internet, but the methodology provided can be applied independent of specific platforms.\n\nTitle: Towards a discipline of geospatial distributed event based systems\nContent: A geospatial system is one in which the state space includes one, two or three-dimensional space and time. A geospatial event is one in which an event impacts points in space over time. Examples of geospatial events include floods, tsunamis, earthquakes, and emission of toxic plumes. This paper discusses aspects of the theory of geospatial distributed event based systems (GDEBS). The paper describes algorithms for rapid detection of geospatial events which can be used on Cloud computing architectures, in which many servers collaborate to detect events by analyzing data streams from large numbers of sensors. Sensor noise and timing errors may result in false detection or missed detection as well as incorrect identification of event attributes such as the location of the event source. The paper presents mathematical analyses and simulations dealing with rapid event detection for geospatial events of varying speeds in the presence of substantial sensor noise and timing error. The paper also describes some of the algorithmic and machine-learning techniques for improving event detection in the Cloud with large numbers of noisy sensors. Experience with GDEBS using a seismic network is described.\n\n## Task\nCompose an abstract for the title 'Using Announce-Listen With Global Events To Develop Distributed Control Systems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We specify an abstract model for dynamic distributed control systems in which the component objects make local decisions based on system-wide constraints and approximate global state. We focus an the issue of distributed resource management, exploring a solution that is both compositional and scalable because it builds global events into the Java infrastructure by exploiting its multicast facilities. (C) 1998 John Wiley & Sons, Ltd."
      }
    ]
  },
  {
    "tid":"294",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: We Are Family: Relating Information-Flow Trackers\nContent: While information-flow security is a well-established area, there is an unsettling gap between heavyweight information-flow control, with formal guarantees yet limited practical impact, and lightweight tainting techniques, useful for bug finding yet lacking formal assurance. This paper proposes a framework for exploring the middle ground in the range of enforcement from tainting (tracking data flows only) to fully-fledged information-flow control (tracking both data and control flows). We formally illustrate the trade-offs between the soundness and permissiveness that the framework allows to achieve. The framework is deployed in a staged fashion, statically embedding a dynamic monitor, being parametric in security policies, as they do not need to be fixed until the final deployment. This flexibility facilitates a secure app store architecture, where the static stage of verification is performed by the app store and the dynamic stage is deployed on the client. To illustrate the practicality of the framework, we implement our approach for a core of Java and evaluate it on a use case with enforcing privacy policies in the Android setting. We also show how a state-of-the-art dynamic monitor for JavaScript can be easily adapted to implement our approach.\n\nTitle: From dynamic to static and back: riding the roller coaster of information-flow control research\nContent: Historically, dynamic techniques are the pioneers of the area of information flow in the 70's. In their seminal work, Denning and Denning suggest a static alternative for information-flow analysis. Following this work, the 90's see the domination of static techniques for information flow. The common wisdom appears to be that dynamic approaches are not a good match for security since monitoring a single path misses public side effects that could have happened in other paths. Dynamic techniques for information flow are on the rise again, driven by the need for permissiveness in today's dynamic applications. But they still involve nontrivial static checks for leaks related to control flow. This paper demonstrates that it is possible for a purely dynamic enforcement to be as secure as Denning-style static information-flow analysis, despite the common wisdom. We do have the trade-off that static techniques have benefits of reducing runtime overhead, and dynamic techniques have the benefits of permissiveness (this, for example, is of particular importance in dynamic applications, where freshly generated code is evaluated). But on the security side, we show for a simple imperative language that both Denning-style analysis and dynamic enforcement have the same assurance: termination-insensitive noninterference.\n\nTitle: A Principled Approach to Tracking Information Flow in the Presence of Libraries.\nContent: There has been encouraging progress on information flow control for programs in increasingly complex programming languages, tracking the propagation of information from input sources to output sinks. Yet, programs are typically deployed in an environment with rich APIs and powerful libraries, posing challenges for information flow control when the code for these APIs and libraries is either unavailable or written in a different language. This paper presents a principled approach to tracking information flow in the presence of libraries. With the goal to strike the balance between security and precision, we present a framework that explores the middle ground between the \\\"shallow\\\", signature-based modeling of libraries and the \\\"deep\\\", stateful approach, where library models need to be supplied manually. We formalize our approach for a core language, extend it with lists and higher-order functions, and establish soundness results with respect to the security condition of noninterference.\n\nTitle: SeLINQ: tracking information across application-database boundaries\nContent: The root cause for confidentiality and integrity attacks against computing systems is insecure information flow. The complexity of modern systems poses a major challenge to secure end-to-end information flow, ensuring that the insecurity of a single component does not render the entire system insecure. While information flow in a variety of languages and settings has been thoroughly studied in isolation, the problem of tracking information across component boundaries has been largely out of reach of the work so far. This is unsatisfactory because tracking information across component boundaries is necessary for end-to-end security. This paper proposes a framework for uniform tracking of information flow through both the application and the underlying database. Key enabler of the uniform treatment is recent work by Cheney et al., which studies database manipulation via an embedded language-integrated query language (with Microsoft's LINQ on the backend). Because both the host language and the embedded query languages are functional F#-like languages, we are able to leverage information-flow enforcement for functional languages to obtain information-flow control for databases \\\"for free\\\", synergize it with information-flow control for applications and thus guarantee security across application-database boundaries. We develop the formal results in the form of a security type system that includes a treatment of algebraic data types and pattern matching, and establish its soundness. On the practical side, we implement the framework and demonstrate its usefulness in a case study with a realistic movie rental database.\n\nTitle: Value-Sensitive Hybrid Information Flow Control for a JavaScript-Like Language\nContent: Secure integration of third-party code is one of the prime challenges for securing today's web. Recent empirical studies give evidence of pervasive reliance on and excessive trust in third-party JavaScript, with no adequate security mechanism to limit the trust or the extent of its abuse. Information flow control is a promising approach for controlling the behavior of third-party code and enforcing confidentiality and integrity policies. While much progress has been made on static and dynamic approaches to information flow control, only recently their combinations have received attention. Purely static analysis falls short of addressing dynamic language features such as dynamic objects and dynamic code evaluation, while purely dynamic analysis suffers from inability to predict side effects in non-performed executions. This paper develops a value-sensitive hybrid mechanism for tracking information flow in a JavaScript-like language. The mechanism consists of a dynamic monitor empowered to invoke a static component on the fly. This enables us to achieve a sound yet permissive enforcement. We establish formal soundness results with respect to the security policy of non-interference. In addition, we demonstrate permissiveness by proving that we subsume the precision of purely static analysis and by presenting a collection of common programming patterns that indicate that our mechanism has potential to provide more permissiveness than dynamic mechanisms in practice.\n\n## Task\nCompose an abstract for the title 'Polyglots: crossing origins by crossing formats' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In a heterogeneous system like the web, information is exchanged between components in versatile formats. A new breed of attacks is on the rise that exploit the mismatch between the expected and provided content. This paper focuses on the root cause of a large class of attacks: polyglots. A polyglot is a program that is valid in multiple programming languages. Polyglots allow multiple interpretation of the content, providing a new space of attack vectors. We characterize what constitutes a dangerous format in the web setting and identify particularly dangerous formats, with PDF as the prime example. We demonstrate that polyglot-based attacks on the web open up for insecure communication across Internet origins. The paper presents novel attack vectors that infiltrate the trusted origin by syntax injection across multiple languages and by content smuggling of malicious payload that appears formatted as benign content. The attacks lead to both cross-domain leakage and cross-site request forgery. We perform a systematic study of PDF-based injection and content smuggling attacks. We evaluate the current practice in client/server content filtering and PDF readers for polyglot-based attacks, and report on vulnerabilities in the top 100 Alexa web sites. We identify five web sites to be vulnerable to syntax injection attacks. Further, we have found two major enterprise cloud storage services to be susceptible to content smuggling attacks. Our recommendations for protective measures on server side, in browsers, and in content interpreters (in particular, PDF readers) show how to mitigate the attacks."
      }
    ]
  },
  {
    "tid":"295",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Unlocking the semantics of multimedia presentations in the web with the multimedia metadata ontology\nContent: The semantics of rich multimedia presentations in the web such as SMIL, SVG, and Flash cannot or only to a very limited extend be understood by search engines today. This hampers the retrieval of such presentations and makes their archival and management a difficult task. Existing metadata models and metadata standards are either conceptually too narrow, focus on a specific media type only, cannot be used and combined together, or are not practically applicable for the semantic description of rich multimedia presentations. In this paper, we propose the Multimedia Metadata Ontology (M3O) for annotating rich, structured multimedia presentations. The M3O provides a generic modeling framework for representing sophisticated multimedia metadata. It allows for integrating the features provided by the existing metadata models and metadata standards. Our approach bases on Semantic Web technologies and can be easily integrated with multimedia formats such as the W3C standards SMIL and SVG. With the M3O, we unlock the semantics of rich multimedia presentations in the web by making the semantics machine-readable and machine-understandable. The M3O is used with our SemanticMM4U framework for the multi-channel generation of semantically-rich multimedia presentations.\n\nTitle: Designing core ontologies\nContent: One of the key factors that hinders integration of distributed, heterogeneous information systems is the lack of a formal basis for modeling the complex, structured knowledge that is to be exchanged. To alleviate this situation, we present an approach based on core ontologies. Core ontologies are characterized by a high degree of axiomatization and formal precision. This is achieved by basing on a foundational ontology. In addition, core ontologies should follow a pattern-oriented design approach. By this, they are modular and extensible. Core ontologies allow for reusing the structured knowledge they define as well as integrating existing domain knowledge. The structured knowledge of the core ontologies is clearly separated from the domain-specific knowledge. Such core ontologies allow for both formally conceptualize their particular fields and to be flexibly combined to cover the needs of concrete, complex application domains. Over the last years, we have developed three independent core ontologies for events and objects, multimedia annotations and personal information management. In this paper, we present the simultaneous use and integration of our core ontologies at the example of a complex, distributed socio-technical system of emergency response. We describe our design approach for core ontologies and discuss the lessons learned in designing them. Finally, we elaborate on the beauty aspects of our core ontologies.\n\nTitle: Requirements Elicitation Towards a Search Engine for Semantic Multimedia Content\nContent: We investigate user requirements regarding the interface design for a semantic multimedia search and retrieval based on a prototypical implementation of a search engine for multimedia content on the web. Thus, unlike existing image search engines and video search engines, we are interested in true multimedia content combining different media assets into multimedia documents like PowerPoint presentations and Flash files. In a user study with 20 participants, we conducted a formative evaluation based on the think-aloud method and semi-structured interviews in order to obtain requirements to a future web search engine for multimedia content. The interviews are complemented by a paper-and-pencil questionnaire to obtain quantitative information and present mockups demonstrating the user interface of a future multimedia search and retrieval engine.\n\nTitle: Ontology-Based Information Flow Control Of Network-Level Internet Communication\nContent: Information flow control on the Internet is a desirable feature when it comes to content such as neo-Nazi propaganda, child pornography, or material showing extreme violence or crimes. In order to provide for a flexible control of information flow on the Internet, we present the pattern system InFO (short for: Information Flow Ontology). InFO provides a common support for different enforcing systems such as routers, proxies, or name servers by abstracting from existing as well as possible future regulation types. Thus, unlike existing solutions, InFO provides information flow control on the Internet-layer, transport-layer, as well as application-layer. In addition, InFO allows for linking the technical implementation of a flow control policy with a human-readable representation including its legal background (law) and organizational motivation (code of conduct). Besides a detailed description of the pattern system, we also provide various examples demonstrating the practical applicability of InFO. InFO has been implemented for name servers, routers, as well as application-level proxy servers. Its source code is available to the public.\n\nTitle: On the Status of Experimental Research on the Semantic Web\nContent: Experimentation is an important way to validate results of Semantic Web and Computer Science research in general. In this paper, we investigate the development and the current status of experimental work on the Semantic Web. Based on a corpus of 500 papers collected from the International Semantic Web Conferences (ISWC) over the past decade, we analyse the importance and the quality of experimental research conducted and compare it to general Computer Science. We observe that the amount and quality of experiments are steadily increasing over time. Unlike hypothesised, we cannot confirm a statistically significant correlation between a paper's citations and the amount of experimental work reported. Our analysis, however, shows that papers comparing themselves to other systems are more often cited than other papers.\n\n## Task\nCompose an abstract for the title 'Declarative representation of programming access to ontologies' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Using ontologies in software applications is a challenging task due to the chasm between the logics-based world of ontologies and the object-oriented world of software applications. The logics-based representation emphasizes the meaning of concepts and properties, i.e., their semantics. The modeler in the object-oriented paradigm also takes into account the pragmatics, i.e., how the classes are used, by whom, and why. To enable a comprehensive use of logics-based representations in object-oriented software systems, a seamless integration of the two paradigms is needed. However, the pragmatic issues of using logic-based knowledge in object-oriented software applications has yet not been considered sufficiently. Rather, the pragmatic issues that arise in using an ontology, e.g., which classes to instantiate in which order, remains a task to be carefully considered by the application developer. In this paper, we present a declarative representation for designing and applying programming access to ontologies. Based on this declarative representation, we have build OntoMDE, a model-driven engineering toolkit that we have applied to several example ontologies with different Characteristics."
      }
    ]
  },
  {
    "tid":"296",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A scale-based forward-and-backward diffusion process for adaptive image enhancement and denoising\nContent: This work presents a scale-based forward-and-backward diffusion (SFABD) scheme. The main idea of this scheme is to perform\n local adaptive diffusion using local scale information. To this end, we propose a diffusivity function based on the Minimum\n Reliable Scale (MRS) of Elder and Zucker (IEEE Trans. Pattern Anal. Mach. Intell. 20(7), 699-716, 1998) to detect the details of local structures. The magnitude of the diffusion coefficient at each pixel is\n determined by taking into account the local property of the image through the scales. A scale-based variable weight is incorporated\n into the diffusivity function for balancing the forward and backward diffusion. Furthermore, as numerical scheme, we propose\n a modification of the Perona-Malik scheme (IEEE Trans. Pattern Anal. Mach. Intell. 12(7), 629-639, 1990) by incorporating edge orientations. The article describes the main principles of our method and illustrates\n image enhancement results on a set of standard images as well as simulated medical images, together with qualitative and quantitative\n comparisons with a variety of anisotropic diffusion schemes.\n\nTitle: A Visual Silence Detector Constraining Speech Source Separation\nContent: We propose an audiovisual source separation algorithm for speech signals. In our proposed algorithm we first extract the time segments with low activity of the mouth region from synchronous video recordings. An automatically selected optimal classifier is used to detect silent intervals in these instants of low visual mouth activity. Then, the source separation problem is formulated and solved for the entire signal duration. Our approach was tested on two challenging speech corpora with two speakers and two microphones, namely in the first corpus separate source signals were mixed in a simulated room, and the second corpus contains recorded conversations. The results are promising on both corpora: with the visual silence detector the performance of the source separation algorithm, measured by the signal to noise inference ratio increases.\n\nTitle: Multimodal Unit Selection for 2D Audiovisual Text-to-Speech Synthesis\nContent: Audiovisual text-to-speech systems convert a written text into an audiovisual speech signal. Lately much interest goes out to data-driven 2D photorealistic synthesis, where the system uses a database of pre-recorded auditory and visual speech data to construct the target output signal. In this paper we propose a synthesis technique that creates both the target auditory and the target visual speech by using a same audiovisual database. To achieve this, the well-known unit selection synthesis technique is extended to work with multimodal segments containing original combinations of audio and video. This strategy results in a multimodal output signal that displays a high level of audiovisual correlation, which is crucial to achieve a natural perception of the synthetic speech signal.\n\nTitle: SVRG-MKL: A Fast and Scalable Multiple Kernel Learning Solution for Features Combination in Multi-Class Classification Problems.\nContent: In this paper, we present a novel strategy to combine a set of compact descriptors to leverage an associated recognition task. We formulate the problem from a multiple kernel learning (MKL) perspective and solve it following a stochastic variance reduced gradient (SVRG) approach to address its scalability, currently an open issue. MKL models are ideal candidates to jointly learn the optimal combination of features along with its associated predictor. However, they are unable to scale beyond a dozen thousand of samples due to high computational and memory requirements, which severely limits their applicability. We propose SVRG-MKL, an MKL solution with inherent scalability properties that can optimally combine multiple descriptors involving millions of samples. Our solution takes place directly in the primal to avoid Gram matrices computation and memory allocation, whereas the optimization is performed with a proposed algorithm of linear complexity and hence computationally efficient. Our proposition builds upon recent progress in SVRG with the distinction that each kernel is treated differently during optimization, which results in a faster convergence than applying off-the-shelf SVRG into MKL. Extensive experimental validation conducted on several benchmarking data sets confirms a higher accuracy and a significant speedup of our solution. Our technique can be extended to other MKL problems, including visual search and transfer learning, as well as other formulations, such as group-sensitive (GMKL) and localized MKL (LMKL) in convex settings.\n\nTitle: A Nonlinear Probabilistic Curvature Motion Filter for Positron Emission Tomography Images\nContent: Positron Emission Tomography (PET) is an important nuclear medicine imaging technique which enhances the effectiveness of diagnosing many diseases. The raw-projection data, i.e. the sinogram, from which the PET is reconstructed, contains a very high level of Poisson noise. The latter complicates the PET image's interpretation which may lead to erroneous diagnoses. Suitable denoising techniques prior to reconstruction can significantly alleviate the problem. In this paper, we propose filtering the sinogram with a constraint curvature motion diffusion for which we compute the edge stopping function in terms of edge probability under the assumption of contamination by Poison noise. We demonstrate through simulations with images contaminated by Poisson noise that the performance of the proposed method substantially surpasses that of recently published methods, both visually and in terms of statistical measures.\n\n## Task\nCompose an abstract for the title 'Backward-Forward Combined Convolutive Blind Source Separation' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Many blind source separation algorithms only try to find the estimation of original signals and the de-mixing system. However, finding the mixing system is also an important task, which is a representation for the recording environment. Direct inversing the mixing system will introduce many noises. We proposed a correlation based backward-forward combined algorithm to do blind source separation. In the proposed algorithm, an alternative optimization strategy is applied. The estimation of mixing/de-mixing system is used as an initial guess to optimize the de-mixing/mixing system iteratively. This alternative optimization algorithm can obtain better estimation for mixing and de-mixing system at the same time. In addition, it can reduce the chance to be tracked into a local minimal. As a result, we can get a better estimation for the sources. Good experimental results were obtained on both simulated and real signals."
      }
    ]
  },
  {
    "tid":"297",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Collaborative ensemble learning: combining collaborative and content-based information filtering via hierarchical bayes\nContent: Collaborative filtering (CF) and content-based filtering (CBF) have widely been used information filtering applications, both approaches having their individual strengths and weaknesses. This paper proposes a novel probabilistic framework to unify CF and CBF, named collaborative ensemble learning. Based on content based probabilistic models for each user's preferences (the CBF idea), it combines a society of users' preferences to predict an active user's preferences (the CF idea). While retaining an intuitive explanation, the combination scheme can be interpreted as a hierarchical Bayesian approach in which a common prior distribution is learned from related experiments. It does not require a global training stage and thus can incrementally incorporate new data. We report results based on two data sets, the neuters-21578 text data set and a data base of user opionions on art images. For both data sets, collaborative ensemble achieved excellent performance in terms of recommendation accuracy. In addition to recommendation engines, collaborative ensemble learning is applicable to problems typically solved via classical hierarchical Bayes, like multisensor fusion and multitask learning.\n\nTitle: A nonparametric hierarchical bayesian framework for information filtering\nContent: Information filtering has made considerable progress in recent years. The predominant approaches are content-based methods and collaborative methods. Researchers have largely concentrated on either of the two approaches since a principled unifying framework is still lacking. This paper suggests that both approaches can be combined under a hierarchical Bayesian framework. Individual content-based user profiles are generated and collaboration between various user models is achieved via a common learned prior distribution. However, it turns out that a parametric distribution (e.g. Gaussian) is too restrictive to describe such a common learned prior distribution. We thus introduce a nonparametric common prior, which is a sample generated from a Dirichlet process which assumes the role of a hyper prior. We describe effective means to learn this nonparametric distribution, and apply it to learn users' information needs. The resultant algorithm is simple and understandable, and offers a principled solution to combine content-based filtering and collaborative filtering. Within our framework, we are now able to interpret various existing techniques from a unifying point of view. Finally we demonstrate the empirical success of the proposed information filtering methods.\n\nTitle: Multi-label informed latent semantic indexing\nContent: Latent semantic indexing (LSI) is a well-known unsupervised approach for dimensionality reduction in information retrieval. However if the output information (i.e. category labels) is available, it is often beneficial to derive the indexing not only based on the inputs but also on the target values in the training data set. This is of particular importance in applications with multiple labels, in which each document can belong to several categories simultaneously. In this paper we introduce the multi-label informed latent semantic indexing (MLSI) algorithm which preserves the information of inputs and meanwhile captures the correlations between the multiple outputs. The recovered \"latent semantics\" thus incorporate the human-annotated category information and can be used to greatly improve the prediction accuracy. Empirical study based on two data sets, Reuters-21578 and RCV1, demonstrates very encouraging results.\n\nTitle: Combining content and link for classification using matrix factorization\nContent: The world wide web contains rich textual contents that areinterconnected via complex hyperlinks. This huge database violates the assumption held by most of conventional statistical methods that each web page is considered as an independent and identical sample. It is thus difficult to apply traditional mining or learning methods for solving web mining problems, e.g., web page classification, by exploiting both the content and the link structure. The research in this direction has recently received considerable attention but are still in an early stage. Though a few methods exploit both the link structure or the content information, some of them combine the only authority information with the content information, and the others first decompose the link structure into hub and authority features, then apply them as additional document features. Being practically attractive for its great simplicity, this paper aims to design an algorithm that exploits both the content and linkage information, by carrying out a joint factorization on both the linkage adjacency matrix and the document-term matrix, and derives a new representation for web pages in a low-dimensional factor space, without explicitly separating them as content, hub or authority factors. Further analysis can be performed based on the compact representation of web pages. In the experiments, the proposed method is compared with state-of-the-art methods and demonstrates an excellent accuracy in hypertext classification on the WebKB and Cora benchmarks.\n\nTitle: High-dimensional Joint Sparsity Random Effects Model for Multi-task Learning.\nContent:   Joint sparsity regularization in multi-task learning has attracted much attention in recent years. The traditional convex formulation employs the group Lasso relaxation to achieve joint sparsity across tasks. Although this approach leads to a simple convex formulation, it suffers from several issues due to the looseness of the relaxation. To remedy this problem, we view jointly sparse multi-task learning as a specialized random effects model, and derive a convex relaxation approach that involves two steps. The first step learns the covariance matrix of the coefficients using a convex formulation which we refer to as sparse covariance coding; the second step solves a ridge regression problem with a sparse quadratic regularizer based on the covariance matrix obtained in the first step. It is shown that this approach produces an asymptotically optimal quadratic regularizer in the multitask learning setting when the number of tasks approaches infinity. Experimental results demonstrate that the convex formulation obtained via the proposed model significantly outperforms group Lasso (and related multi-stage formulations \n\n## Task\nCompose an abstract for the title 'Hierarchy-Regularized Latent Semantic Indexing' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Organizing textual documents into a hierarchical taxonomy is a common practice in knowledge management. Beside textual features, the hierarchical structure of directories reflect additional and important knowledge annotated by experts. It is generally desired to incorporate this information into text mining processes. In this paper, we propose hierarchy-regularized latent semantic indexing, which encodes the hierarchy into a similarity graph of documents and then formulates an optimization problem mapping each document into a low dimensional vector space. The new feature space preserves the intrinsic structure of the original taxonomy and thus provides a meaningful basis for various learning tasks like visualization and classification. Our approach employs the information about class proximity and class specificity, and can naturally cope with multi-labeled documents. Our empirical studies show very encouraging results on two real-world data sets, the new Reuters (RCV1) benchmark and the Swissprot protein database."
      }
    ]
  },
  {
    "tid":"298",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: ATL Transformation for the Generation of SCA Model\nContent: Service Component Architecture specification (SCA) is an emerging and promising technology for the development, deployment and integration of Internet applications. This technology supports the management of dynamic availability and treats the heterogeneity between the components of distributed applications. However, this technology is not able to solve all problems. Currently, software systems are evolving. This factor makes development and maintenance of systems more complex than before. One solution to remedy this was the use of the Model Driven Engineering (MDE) approach in the development process. The aim of this paper is to apply an MDE automation type ensuring the passage from an UML 2.0 model to SCA model. To achieve this, we study two metamodels: the UML 2.0 component metamodel and the SCA meta-model. To ensure trace ability between these two meta-models, we have defined transformation rules in ATL language.\n\nTitle: An Object-Oriented Metamodel For Inter-Enterprises Cooperative Processes Based On Web Services\nContent: In this paper, we are interested in inter-enterprises cooperation based on Web services and emerged problems. On one hand, the dynamic and non-deterministic nature of cooperation requires both flexibility and elasticity of the process that supports this cooperation. On the other hand, the current Web services emergence, in addition to the new requirements and challenges of inter-enterprises cooperation (due to virtual enterprises achievement), have lead to addressing and resolving the problem of finding a compromise between transparency and confidentiality. In this paper, we introduce a generic object-oriented metamodel for cooperative processes. This metamodel will be used as a basic micro-core for both cooperative processes description (trying to face problems raised above, and inter-enterprises cooperative processes modeling. We present a process abstraction, named service process, allowing enterprises to externalize their business processes as Web services, while mastering the compromise between transparency and confidentiality.\n\nTitle: MDE approach for the generation and verification of SCA model\nContent: Service Component Architecture specification (SCA) is an emerging and promising technology for the development, deployment and integration of Internet applications. This technology supports the management of dynamic availability and treats the heterogeneity between the components of distributed applications. However, this technology is not able to solve all problems. Currently, software systems are evolving. This factor makes development, verification and maintenance of systems more complex than before. One solution to remedy this was the use of the Model Driven Engineering (MDE) approach in the development and verification process. The purpose of this paper is to apply an approach MDE to obtain SCA models and to verify the properties of these models. To reach our purpose, we applied two transformations: The first one to obtain SCA models using UML 2.0 metamodel and the second transformation to ensure the verification of the properties of these models using event-B metamodel. To achieve this, we study the UML 2.0 component metamodel, the SCA metamodel and the event-B metamodel. We have defined transformation rules in ATL language.\n\nTitle: Extracting Configuration Guidance Models from Business Process Repositories\nContent: Configurable process models are gaining a great importance for the design and development of reusable business processes. As these processes tend to be very complex, their configuration becomes a difficult task. Therefore, many approaches propose to build decision support systems to assist users selecting desirable configuration choices. Nevertheless, these systems are to a large extent manually created by domain experts, which is a time-consuming and tedious task. In addition, relying solely on the expert knowledge is not only error-prone, but also challengeable. In this paper, we propose to learn from past experience in process configuration in order to automatically extract a configuration guidance model. Instead of starting from scratch, a configuration guidance model assists analysts creating business-driven support systems.\n\nTitle: From Workflow Models to Executable Web Service Interfaces\nContent: Workflow models have been used and refined for years to execute processes within organisations. To deal with collaborative processes (choreographies) these internal workflow models have to be aligned with the external behaviour advertised through Web service interfaces. However, traditional workflow management systems (WfMS) do not offer this functionality. Simply sharing and merging process models is often not possible, because workflow management lacks a widely accepted standard theory for workflow models.Multiple research and standardisation efforts to integrate different workflow theories have been proposed over the years. XPDL is the most widely used standard for process model interchange and supported by over 80 systems.However, XPDL also lacks the possibility to relate a workflow model to its possible choreography interface abstractions.To remedy this situation, we propose to abstract the XPDL model to a higher-level model, perform the integration and the compaction algorithms at that level and then ground it back to the desired choreography models. We develop and use an integrated ontology which is based on the XPDL standard for this purpose. To facilitate the abstraction and grounding, we present a mapping procedure to automatically translate XPDL and BPMN workflow models into this ontology. After translation, these models are annotated with a parameterised role model and other collaborative properties. We present a compaction procedure that automatically maps the annotated models into external choreography interfaces that expose only the relevant information for a particular partner collaboration. Our procedure is agnostic with respect to the target choreography model. We demonstrate our approach using WSMO choreographies which enables us to automatically generate interface models from any WfMSs that supports XPDL export.\n\n## Task\nCompose an abstract for the title 'Profile UML2.0 for Specification of the SCA Architectures' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Service Component Architecture (SCA) aims to simplify the construction of service oriented architecture (SOA) to encourage better the reuse and to be independent from used technologies. In the other hand, UML is a standard for modeling especially in industrial environments. To face this situation we recommend an adaptation of UML2.0 to SCA. It is in this context that we have defined a profile UML2.0 for SCA contain a set of stereotypes applied to metaclasses stemming from the metamodel UML2.0. These stereotypes are completed by formal constraints in OCL. Our profile introduces new elements to reflect the architectural concepts of SCA."
      }
    ]
  },
  {
    "tid":"299",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Towards Robust Teams with Many Agents\nContent: Agents in deployed multi-agent systems monitor other agents to coordinate and col- laborate robustly. However, as the number of agents monitored is scaled up, two key challenges arise: (i) the number of monitoring hypotheses to be considered can grow exponentially in the number of agents; and (ii) agents become physically and logically unconnected (unobservable) to their peers. This paper examines these challenges in teams of cooperating agents, focusing on a monitoring task that is of particular impor- tance to robust teamwork: detecting disagreements among team-members. We present YOYO, a highly scalable disagreement-detection algorithm which guarantees sound detection in time linear in the number of agents despite the exponential number of hy- potheses. In addition, we present new upper bounds about the number of agents that must be monitored in a team to guarantee disagreement detection. Both YOYO and the new bounds are explored analytically and empirically in thousands of monitoring problems, scaled to thousands of agents.\n\nTitle: Simultaneous adversarial multi-robot learning\nContent: Multi-robot learning faces all of the challenges of robot learning with all of the challenges of multiagent learning. There has been a great deal of recent research on multiagent reinforcement learning in stochastic games, which is the intuitive extension of MDPs to multiple agents. This recent work, although general, has only been applied to small games with at most hundreds of states. On the other hand robot tasks have continuous, and often complex, state and action spaces. Robot learning tasks demand approximation and generalization techniques, which have only received extensive attention in single-agent learning. In this paper we introduce GraWoLF, a general-purpose, scalable, multiagent learning algorithm. It combines gradient-based policy learning techniques with the WoLF (\"Win or Learn Fast\") variable learning rate. We apply this algorithm to an adversarial multi-robot task with simultaneous learning. We show results of learning both in simulation and on the real robots. These results demonstrate that GraWoLF can learn successful policies, overcoming the many challenges in multi-robot learning.\n\nTitle: CMUnited-98: a team of robotic soccer agents\nContent: In this paper, we present the main research contributions of our champion CMUnited-98 small robot team. The team is a multi-agent system in the robotic soccer entertainment application area. The robotic system has global perception and distributed cognition and action. We describe the main features of the hardware design of the physical robots, including differential drive, robust mechanical structure, and a kicking device. We briefly overview the CMUnited- 98 global vision processing algorithm. We then introduce our new robot motion algorithm which reactively generates motion control to account for the target point, the desired robot orientation, and obstacle avoidance. Our robots exhibit successful collisionfree motion in the highly dynamic robotic soccer environment. At the strategic and decision-making level, we present the role-based behaviors of the CMUnited- 98 robotic agents. Team collaboration is remarkably achieved through a new algorithm that allows for team agents to anticipate possible collaboration opportunities. Robots position themselves strategically in open positions that increase passing opportunities. The chapter terminates with a summary of the results of the RoboCup-98 games in which the CMUnited-98 small robot team scored a total of 25 goals and suffered 6 goals in the 5 games that it played.\n\nTitle: Computing Robust Counter-Strategies\nContent: Adaptation to other initially unknown agents often requires computing an effec- tive counter-strategy. In the Bayesian paradigm, one must find a good counter- strategy to the inferred posterior of the other agents' behavior. In the experts paradigm, one may want to choose experts that are good counter-strategies to the other agents' expected behavior. In this paper we introduce a technique for computing robust counter-strategies for adaptation in multiagent scenarios under a variety of paradigms. The strategies can take advantage of a suspected tendency in the decisions of the other agents, while bounding the worst-case performance when the tendency is not observed. The technique involves solving a modified game, and therefore can make use of recently developed algorithms for solving very large extensive games. We demonstrate the effectiveness of the technique in two-player Texas Hold'em. We show that the computed poker strategies are sub- stantially more robust than best response counter-strategies, while still exploiting a suspected tendency. We also compose the generated strategies in an experts al- gorithm showing a dramatic improvement in performance over using simple best responses.\n\nTitle: Accelerating best response calculation in large extensive games\nContent: One fundamental evaluation criteria of an AI technique is its performance in the worst-case. For static strategies in extensive games, this can be computed using a best response computation. Conventionally, this requires a full game tree traversal. For very large games, such as poker, that traversal is infeasible to perform on modern hardware. In this paper, we detail a general technique for best response computations that can often avoid a full game tree traversal. Additionally, our method is specifically well-suited for parallel environments. We apply this approach to computing the worst-case performance of a number of strategies in heads-up limit Texas hold'em, which, prior to this work, was not possible. We explore these results thoroughly as they provide insight into the effects of abstraction on worst-case performance in large imperfect information games. This is a topic that has received much attention, but could not previously be examined outside of toy domains.\n\n## Task\nCompose an abstract for the title 'Multi-Robot Team Response To A Multi-Robot Opponent Team' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Adversarial multi-robot problems, where teams of robots compete with one another, require the development of approaches that span all levels of control and integrate algorithms ranging from low-level robot motion control, through to planning, opponent modeling, and multiagent learning. Small-size robot soccer, a league within the RoboCup initiative, is a prime example of this multi-robot team adversarial environment. In this paper, we describe some of the algorithms and approaches of our robot soccer team, CMDragons'02, developed for RoboCup 2002. Our team represents an integration of many components, several of which that are in themselves state-of-the-art, into a framework designed for fast adaptation and response to the changing environment."
      }
    ]
  },
  {
    "tid":"300",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Mining past-time temporal rules from execution traces\nContent: Specification mining is a process of extracting specifications, often from program execution traces. These specifications can in turn be used to aid program understanding, monitoring and verification. There are a number of dynamic-analysis-based specification mining tools in the literature, however none so far extract past time temporal expressions in the form of rules stating: \"whenever a series of events occurs, previously another series of events has happened\". Rules of this format are commonly found in practice and useful for various purposes. Most rule-based specification mining tools only mine future-time temporal expression. Many past-time temporal rules like \"whenever a resource is used, it was allocated before\" are asymmetric as the other direction does not holds. Hence, there is a need to mine past-time temporal rules. In this paper, we describe an approach to mine significant rules of the above format occurring above a certain statistical thresholds from program execution traces. The approach start from a set of traces, each being a sequence of events (i.e., method invocations) and resulting in a set of significant rules obeying minimum thresholds of support and confidence. A rule compaction mechanism is employed to reduce the number of reported rules significantly. Experiments on traces of JBoss Application Server shows the utility of our approach in inferring interesting past-time temporal rules.\n\nTitle: Mining Iterative Generators and Representative Rules for Software Specification Discovery\nContent: Billions of dollars are spent annually on software-related cost. It is estimated that up to 45 percent of software cost is due to the difficulty in understanding existing systems when performing maintenance tasks (i.e., adding features, removing bugs, etc.). One of the root causes is that software products often come with poor, incomplete, or even without any documented specifications. In an effort to improve program understanding, Lo et al. have proposed iterative pattern mining which outputs patterns that are repeated frequently within a program trace, or across multiple traces, or both. Frequent iterative patterns reflect frequent program behaviors that likely correspond to software specifications. To reduce the number of patterns and improve the efficiency of the algorithm, Lo et al. have also introduced mining closed iterative patterns, i.e., maximal patterns without any superpattern having the same support. In this paper, to technically deepen research on iterative pattern mining, we introduce mining iterative generators, i.e., minimal patterns without any subpattern having the same support. Iterative generators can be paired with closed patterns to produce a set of rules expressing forward, backward, and in-between temporal constraints among events in one general representation. We refer to these rules as representative rules. A comprehensive performance study shows the efficiency of our approach. A case study on traces of an industrial system shows how iterative generators and closed iterative patterns can be merged to form useful rules shedding light on software design.\n\nTitle: Mining patterns and rules for software specification discovery\nContent: Software specifications are often lacking, incomplete and outdated in the industry. Lack and incomplete specifications cause various software engineering problems. Studies have shown that program comprehension takes up to 45% of software development costs. One of the root causes of the high cost is the lack-of documented specification. Also, outdated and incomplete specification might potentially cause bugs and compatibility issues. In this paper, we describe novel data mining techniques to mine or reverse engineer these specifications from the pool of software engineering data. A large amount of software data is available for analysis. One form of software data is program execution traces. A program trace can be viewed as a sequence of events collected when a program is run. A set of program traces in turn can be viewed as a sequence database. In this paper, we present some novel work in mining software specifications by employing novel pattern mining and rule mining techniques. Performance studies show the scalability of our technique. Case studies on traces of a real industrial application show the utility of our technique in recovering program specifications from execution traces.\n\nTitle: Efficient mining of iterative patterns for software specification discovery\nContent: Studies have shown that program comprehension takes up to 45% of software development costs. Such high costs are caused by the lack-of documented specification and further aggravated by the phenomenon of software evolution. There is a need for automated tools to extract specifications to aid program comprehension. In this paper, a novel technique to efficiently mine common software temporal patterns from traces is proposed. These patterns shed light on program behaviors, and are termed iterative patterns. They capture unique characteristic of software traces, typically not found in arbitrary sequences. Specifically, due to loops, interesting iterative patterns can occur multiple times within a trace. Furthermore, an occurrence of an iterative pattern in a trace can extend across a sequence of indefinite length. Since a program behavior can be manifested in numerous ways, analyzing a single trace will not be sufficient. Iterative pattern mining extends sequential pattern and episode minings to discover frequent iterative patterns which occur repetitively both within a program trace and across multiple traces. In this paper, we present CLIPER (CLosed Iterative Pattern minER) to efficiently mine a closed set of iterative patterns. A performance study on several simulated and real datasets shows the efficiency of our mining algorithm and effectiveness of our pruning strategy. Our case study on JBoss Application Server confirms the usefulness of mined patterns in discovering interesting software behavioral specification.\n\nTitle: Efficient mining of recurrent rules from a sequence database\nContent: We study a novel problem of mining signiflcant recurrent rules from a sequence database. Recurrent rules have the form \\when- ever a series of precedent events occurs, eventually a series of consequent events occurs\". Recurrent rules are intuitive and characterize behaviors in many domains. An example is in the domain of software speciflcations, in which the rules capture a family of program properties beneflcial to program veriflcation and bug detection. Recurrent rules generalize exist- ing work on sequential and episode rules by considering repeated occur- rences of premise and consequent events within a sequence and across multiple sequences, and by removing the \\window\" barrier. Bridging the gap between mined rules and program speciflcations, we formalize our rules in linear temporal logic. We introduce and apply novel notion of rule redundancy to ensure e-cient mining of a compact representative set of rules. Performance studies on benchmarks datasets and a case study on an industrial system have been performed to show the scalability and utility of our approach.\n\n## Task\nCompose an abstract for the title 'Mining temporal rules for software maintenance' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Software evolution incurs difficulties in program comprehensionand software verification, and hence it increases the cost ofsoftware maintenance. In this study, we propose a novel techniqueto mine from program execution traces a sound and complete set ofstatistically significant temporal rules of arbitrary lengths. Theextracted temporal rules reveal invariants that the programobserves, and will consequently guide developers to understand theprogram behaviors, and facilitate all downstream applications suchas verification and debugging. Different from previous studies thatwere restricted to mining two-event rules (e.g., (lock)\u2192(unlock)), our algorithm discovers rules of arbitrarylengths. In order to facilitate downstream applications, werepresent the mined rules as temporal logic expressions, so thatexisting model checkers or other formal analysis toolkit canreadily consume our mining results. Performance studies onbenchmark data sets and a case study on an industrial system havebeen performed to show the scalability and utility of our approach.We performed case studies on JBoss application server and a buggyconcurrent versions system application, and the result clearlydemonstrates the usefulness of our technique in recoveringunderlying program designs and detecting bugs. Copyright \u00a92008 John Wiley & Sons, Ltd.This work was done while the author was with School ofComputing, National University of Singapore."
      }
    ]
  },
  {
    "tid":"301",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A User Satisfaction Level Based Multi-objective Optimization Method for Grid Task Scheduling\nContent: A user satisfaction level model, which focuses on the real feeling of grid users, is proposed in this paper. The model can integrate not only multiple-aspect demands of different grid users, but also their preferences in different aspects. An abstract user satisfaction level function and its properties are also defined in the model. Based on the model, an effective grid task scheduling algorithm Sat Opt is provided and a multivariate Gaussian function is chosen as the practical user satisfaction level function, The Sat Opt algorithm is a three-stage heuristic multi-objective optimization algorithm and it can achieve near ideal results. To validate the idea provided in this paper, a MetaService architecture which can provide an extensible and feasible implementation of the Sat Opt algorithm is designed. Experiment results show that the Sat Opt algorithm based on the user satisfaction level model can significantly improve the user satisfaction level compared with the related methods\n\nTitle: QoS Enhancement for PDES Grid Based on Time Series Prediction\nContent: The combination of parallel/distributed discrete event simulation (PDES) and Grid technology is a new trend in simulation. QoS is important yet difficult in this process. With the special features of Gird architecture and PDES input, such as periodical and predictable inputs, we could enhance QoS with a PDES-specific prediction. A Grid-based framework is presented that is designed to help predict the performance of PDES. Based on this framework, a prediction algorithm using time series theory is presented in the context of large scale Grid simulations. Experiments are executed in the context of GridSim, which shows methods discussed before to help to improve QoS level.\n\nTitle: Massively parallel simulated annealing embedded with downhill-a SPMD algorithm for cluster computing\nContent: Simulated Annealing (SA) is a frequently used stochastic algorithm to deal with combinatorial optimization problems and it converges with probability infinitely close to 1. SA is an NP algorithm and the long executive time prevents it from being accepted for many real-time applications. This paper presents a SPMD (Single Program Multiple Data) algorithm which combines SA with local searching algorithm-downhill. The hybrid method not only keeps the convergence of SA but also improves the convergence speed of SA. Approximate solutions can be found quickly for complex optimization problems and more precise solutions can also be found by employing the same algorithm to fine-tune the approximate solutions. SA is an essential serial algorithm, but the SPMD algorithm breaks up the serial bottleneck of SA and its performance scales up linearly with the increase of processors, at the same time, the SPMD algorithm does not require careful choice of control parameters. Application cases show that the algorithm is robust and it can find high quality solution with high speed\n\nTitle: A Pipeline Virtual Service Pre-Scheduling Pattern and its Application in Astronomy Data Processing\nContent: Based on Open Grid Services Architecture (OGSA), the concept and the formal model of Pipeline Virtual Service (PVS) are proposed and presented in this paper. PVS is used to model a special type of grid workflow composed of a group of services that can be executed as a pipeline. A PVS Pre-Scheduling Pattern is described in detail, which can overlap the execution time of a PVS with the time of generating and optimizing the scheduling solution pool. The Pattern includes two independent components, the solution generating component (GenFunc) and the solution execution component (ExeFunc). GenFunc can take advantage of the powerful computing potential of grid environment to find the optimal or a near optimal scheduling solution, while ExeFunc can select a suitable scheduling solution and put a PVS into running as soon as possible. Two corresponding algorithms, SODGen and SODExe, are also developed. The analytic and simulation results show that the PVS Pre-Scheduling Pattern can not only balance the scheduling cost and scheduling precision, save the scheduling time greatly, but also reduce the execution time through selecting the optimized solution. A prototype is designed and implemented for a large scale astronomy data processing center. The early engineering evaluation experiments show that the PVS Pre-Scheduling Pattern is feasible, efficient, and flexible in real astronomy data processing.\n\nTitle: A New Scheduling Algorithm for Distributed Streaming Media System Based on Multicast\nContent: To meet the growing service scale, distributed streaming media system has been widely used for streaming media applications. The schedule algorithm of distributed streaming media system on how to reduce the network bandwidth consumption and balance the load among service nodes has become a challenging problem. We develop the Period Patching Algorithm with unfixed Period to achieve the best efficiency on reducing bandwidth consumption. Furthermore, the Minimum Bandwidth Schedule Algorithm is proposed. Its major objective is reducing network bandwidth consumption and at the same time balancing the load balancing between nodes. The experiment results show that this proposed algorithm outperforms the classical scheduling algorithms both in reducing total bandwidth consumption and service response time, which is used to reflect the quality of service and load balance effect.\n\n## Task\nCompose an abstract for the title 'The Triangular Pyramid Scheduling Model and algorithm for PDES in Grid' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Grid is a perfect environment for the large scale Parallel Discrete Event Simulation (PDES), because its distribution and collaboration features match the PDES applications well. The PDES tasks or applications are modeled as a Directed Acyclic Graph (DAG), in which the simulation resources consist of three critical factors, simulation hosting machine (SHM), simulation service (SS) and simulation data (SD) in Grid environment. By solving the model we attempt to obtain an optimized triangular matching of the simulation resources on Grid, so that it can support the PDES activities better. We name the algorithm of solving the model Triangular Pyramid Scheduling (TPS). The PDES DAG is divided into three basic graph structures: Sequential structure, Fork structure, and Join structure. The TPS algorithm is developed based on these graph structures. The simulation results show that TPS algorithm can reduce the makespan and congestion, improve the simulation efficiency, and increase the resource utilization efficiency, compared to the existing algorithms."
      }
    ]
  },
  {
    "tid":"302",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Fast Regular Expression Matching Using Small TCAM\nContent: Regular expression (RE) matching is a core component of deep packet inspection in modern networking and security devices. In this paper, we propose the first hardware-based RE matching approach that uses ternary content addressable memory (TCAM), which is available as off-the-shelf chips and has been widely deployed in modern networking devices for tasks such as packet classification. We propose three novel techniques to reduce TCAM space and improve RE matching speed: transition sharing, table consolidation, and variable striding. We tested our techniques on eight real-world RE sets, and our results show that small TCAMs can be used to store large deterministic finite automata (DFAs) and achieve potentially high RE matching throughput. For space, we can store each of the corresponding eight DFAs with 25$\\,$000 states in a 0.59-Mb TCAM chip. Using a different TCAM encoding scheme that facilitates processing multiple characters per transition, we can achieve potential RE matching throughput of 10\u201319 Gb/s for each of the eight DFAs using only a single 2.36-Mb TCAM chip.\n\nTitle: Fast regular expression matching using small TCAMs for network intrusion detection and prevention systems\nContent: Regular expression (RE) matching is a core component of deep packet inspection in modern networking and security devices. In this paper, we propose the first hardware-based RE matching approach that uses Ternary Content Addressable Memories (TCAMs), which are off-the-shelf chips and have been widely deployed in modern networking devices for packet classification. We propose three novel techniques to reduce TCAM space and improve RE matching speed: transition sharing, table consolidation, and variable striding. We tested our techniques on 8 real-world RE sets, and our results show that small TCAMs can be used to store large DFAs and achieve potentially high RE matching throughtput. For space, we were able to store each of the corresponding 8 DFAs with as many as 25,000 states in a 0.59Mb TCAM chip where the number of TCAM bits required per DFA state were 12, 12, 12, 13, 14, 26, 28, and 42. Using a different TCAM encoding scheme that facilitates processing multiple characters per transition, we were able to achieve potential RE matching throughputs of between 10 and 19 Gbps for each of the 8 DFAs using only a single 2.36 Mb TCAM chip.\n\nTitle: SigMatch: fast and scalable multi-pattern matching\nContent: Multi-pattern matching involves matching a data item against a large database of \"signature\" patterns. Existing algorithms for multi-pattern matching do not scale well as the size of the signature database increases. In this paper, we present sigMatch -- a fast, versatile, and scalable technique for multi-pattern signature matching. At its heart, sigMatch organizes the signature database into a (processor) cache-efficient q-gram index structure, called the sigTree. The sigTree groups patterns based on common sub-patterns, such that signatures that don't match can be quickly eliminated from the matching process. The sigTree also uses parallel Bloom filters and a technique to reduce imbalances across groups, for improved performance. Using extensive empirical evaluation across three diverse domains, we show that sigMatch often outperforms existing methods by an order of magnitude or more.\n\nTitle: A Padded Encoding Scheme to Accelerate Scans by Leveraging Skew\nContent: In-memory data analytic systems that use vertical bit-parallel scan methods generally use encoding techniques. We observe that in such environments, there is an opportunity to turn skew in both the data and predicate distributions (usually a problem for query processing) into a benefit that can be leveraged to encode the column values. This paper proposes a padded encoding scheme to address this opportunity. The proposed scheme creates encodings that map common attribute values to codes that can easily be distinguished from other codes by only examining a few bits in the full code. Consequently, scans on columns stored using the padded encoding scheme can safely prune the computation without examining all the bits in the code, thereby reducing the memory bandwidth and CPU cycles that are consumed when evaluating scan queries. Our padded encoding method results in a fixed-length encoding, as fixed-length encodings are easier to manage. However, the proposed padded encoding may produce longer (fixed-length) codes than those produced by popular order-preserving encoding methods, such as dictionary-based encoding. This additional space overhead has the potential to negate the gains from early pruning of the scan computation. However, as we demonstrate empirically, the additional space overhead is generally small, and the padded encoding scheme provides significant performance improvements.\n\nTitle: Efficient Evaluation Of All-Nearest-Neighbor Queries\nContent: The All Nearest Neighbor (ANN) operation is a commonly used primitive for analyzing large multi-dimensional datasets. Since computing ANN is very expensive, in previous works R*-tree based methods have been proposed to speed up this computation. These traditional index-based methods use a pruning metric called MAXMAXDIST, which allows the algorithms to prune out nodes in the index that need not be traversed during the ANN computation. In this paper we introduce a new pruning metric called the NXNDIST and show that this metric is far more effective than the traditional MAXMAXDIST metric.In this paper we also challenge the common practice of using R*-tree index for speeding up the ANN computation. We propose an enhanced bucket quadtree index structure, called the MBRQT, and using extensive experimental evaluation show that the MBRQT index can significantly speed up the ANN computation.In addition, we also present the MBA algorithm based on a depth-first index traversal and bi-directional node expansion strategy. Furthermore, our method can be easily extended to efficiently answer the more general All-k-Nearest-Neighbor (AkNN) queries.\n\n## Task\nCompose an abstract for the title 'Bypassing Space Explosion in High-Speed Regular Expression Matching' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Network intrusion detection and prevention systems commonly use regular expression (RE) signatures to represent individual security threats. While the corresponding deterministic finite state automata (DFA) for any one RE is typically small, the DFA that corresponds to the entire set of REs is usually too large to be constructed or deployed. To address this issue, a variety of alternative automata implementations that compress the size of the final automaton have been proposed such as extended finite automata (XFA) and delayed input DFA (D 2FA). The resulting final automata are typically much smaller than the corresponding DFA. However, the previously proposed automata construction algorithms do suffer from some drawbacks. First, most employ a \u201cUnion then Minimize\u201d framework where the automata for each RE are first joined before minimization occurs. This leads to an expensive nondeterministic finite automata (NFA) to DFA subset construction on a relatively large NFA. Second, most construct the corresponding large DFA as an intermediate step. In some cases, this DFA is so large that the final automaton cannot be constructed even though the final automaton is small enough to be deployed. In this paper, we propose a \u201cMinimize then Union\u201d framework for constructing compact alternative automata focusing on the D 2FA. We show that we can construct an almost optimal final D 2FA with small intermediate parsers. The key to our approach is a space- and time-efficient routine for merging two compact D 2FA into a compact D 2FA. In our experiments, our algorithm runs on average 155 times faster and uses 1500 times less memory than previous algorithms. For example, we are able to construct a D 2FA with over 80 000 000 states using only 1 GB of main memory in only 77 min."
      }
    ]
  },
  {
    "tid":"303",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Towards Secure Inter-Cloud Architectures\nContent: The research community and some commercial organizations are beginning to develop architectures, technologies, and standards to support the integration of multiple cloud systems (Inter-Clouds). In Inter-Cloud systems, security is one of the most important challenges, because of the need for dynamic sharing, federation, and collaboration across multiple clouds. Security in this type of systems relies heavily on the establishment of trust among the involved components. It can be harder to achieve security if a comprehensive and detailed architecture is not developed; the lack of a common architecture slows down the process of Inter-Cloud adoption and has a negative impact on the process of making these systems more secure. We propose to use patterns in order to achieve Secure Inter-Cloud Architectures. Patterns are tools of great value because they provide a better understanding of the functional aspects and can be complemented with security patterns to achieve a Secure Inter-cloud Architecture. In this work, a federated Inter-Cloud pattern is presented as a first step in to the process of developing a Secure Inter-cloud Reference Architecture. Threat analysis and security patterns will be added to this pattern for that purpose.\n\nTitle: Decomposing Distributed Software Architectures for the Determination and Incorporation of Security and Other Non-functional Requirements\nContent: Non-functional requirements (NFRs) such as security, reliability and performance play a crucial role in the development of modern distributed systems. The burden of incorporating NFRs into a system's architecture, as well the determination of new design-level NFRs, can be greatly eased by the use of a structured approach providing guidance to developers. Such structured approaches, however, require equally structured system characterisations. This is especially important for distributed systems, which are inherently complex and multi-faceted. In this paper we propose a form of characterisation which we term architectural decomposition, and present a multi-level conceptual framework for decomposing distributed software architectures. Using the framework for decomposing architectures can help guide the incorporation and, via complementary analysis processes, the determination of NFRs at the architectural level. We describe each of the levels of the framework in turn, propose a complementary analysis process for security based on threat modelling, as well as a process for using the framework itself, and demonstrate the utility of our approach via an example derived from a real-life distributed architecture.\n\nTitle: A security reference architecture for cloud systems\nContent: Security is a fundamental concern in clouds and several cloud vendors provide Security Reference Architectures (SRAs) to describe the security level of their services. A SRA is an abstract architecture without implementation details showing a conceptual model of security for a cloud system. In general, Reference Architectures (RAs) are becoming useful tools to understand and build complex systems. We propose here a Security Reference Architecture (SRA), defined using UML models and patterns, incorporating a specific approach to build secure systems. We present a metamodel and possible patterns to conceptualize the approach. We also describe some uses for this SRA, including its value for Service Level Agreements (SLAs), service certification, monitoring, and security evaluation. We show this latter use in some detail.\n\nTitle: Security solution frames and security patterns for authorization in distributed, collaborative systems\nContent: The design of an authorization infrastructure is one of the most important aspects of engineering a secure software system. Unlike other system types, distributed systems - and especially distributed collaborative systems - can require custom, fine-grained authorization models and enforcement approaches that are able to take into account a range of semantic subtleties. In this paper we present a comprehensive, pattern-oriented software engineering approach to authorization for general distributed systems - with particular applicability to distributed collaborative systems - that allows developers to build custom, application-specific conceptual authorization models in a simple yet extensible manner, and to make informed decisions regarding their enforcement in software, as well as how their supporting rule/policy infrastructure should be designed. Our authorization approach is embodied in two instances of a new pattern-based security engineering construct called a security solution frame, which groups together related patterns - both security \\\"product\\\" and micro-process patterns - in different sub-structures, horizontally and vertically, for a single high-level security policy (in our case authorization and policy management). By applying specific micro-process patterns in each solution frame, developers are guided in using relevant \\\"product\\\" patterns to progressively construct a distributed authorization infrastructure - from abstract concepts toward concrete designs, via a number of levels of abstraction implying solution refinement and corresponding to stages of the development life-cycle. The summary-form \\\"product\\\" patterns encapsulated in each frame also help developers to form a holistic, \\\"global\\\" view when analyzing existing infrastructures. We illustrate and evaluate the proposal in the context of greenfield system development by applying our solution frames to design the authorization infrastructure of a (new) distributed system for secure file sharing and collaborative editing; and also use our solution frames to briefly analyze and capture the design decisions underlying two existing distributed authorization infrastructures: one based on UCON for collaborative Grid systems and another based on ZBAC for SOA-based systems.\n\nTitle: A Comprehensive Pattern-Driven Security Methodology for Distributed Systems\nContent: Incorporating security features is one of the most important and challenging tasks in designing distributed systems. Over the last decade, researchers and practitioners have come to recognize that the incorporation of security features should proceed by means of a systematic approach, combining principles from both software and security engineering. Such systematic approaches, particularly those implying some sort of process aligned with the development life-cycle, are termed security methodologies. One of the most important classes of such methodologies is based on the use of security patterns. While the literature presents a number of pattern-driven security methodologies, none of them are designed specifically for general distributed systems. Going further, there are also currently no methodologies with mixed specific applicability, e.g. for both general and peer-to-peer distributed systems. In this paper we aim to fill these gaps by presenting a comprehensive pattern-driven security methodology specifically designed for general distributed systems, which is also capable of taking into account the specifics of peer-to-peer systems. Our methodology takes the principle of encapsulation several steps further, by employing patterns not only for the incorporation of security features (via security solution frames), but also for the modeling of threats, and even as part of its process. We illustrate and evaluate the presented methodology via a realistic example--the development of a distributed system for file sharing and collaborative editing. In both the presentation of the methodology and example our focus is on the early life-cycle phases (analysis and design).\n\n## Task\nCompose an abstract for the title 'Coordination of Security Levels for Internet Architectures' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Internet systems provide a variety of ways for exchanging information, contain a large amount and variety of data, and have become quite complex, making them vulnerable to attacks from determined hackers. There are many products currently being used to stop these attacks but they suffer from a lack of completeness, only applying to one type of attack. Several mechanisms are needed for a comprehensive defense but this incurs the problem of their lack of coordination, which can be exploited for attacks. We propose a way to coordinate different mechanisms based on a unified object-oriented modeling approach and a hierarchical architecture whose layers define the scope of each security mechanism"
      }
    ]
  },
  {
    "tid":"304",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: On The Preciseness Of Subtyping In Session Types\nContent: Subtyping in concurrency has been extensively studied since early 1990s as one of the most interesting issues in type theory. The correctness of subtyping relations has been usually provided as the soundness for type safety. The converse direction, the completeness, has been largely ignored in spite of its usefulness to define the greatest subtyping relation ensuring type safety. This paper formalises preciseness (i. e. both soundness and completeness) of subtyping for mobile processes and studies it for the synchronous and the asynchronous session calculi. We first prove that the well-known session subtyping, the branching-selection subtyping, is sound and complete for the synchronous calculus. Next we show that in the asynchronous calculus, this subtyping is incomplete for type-safety: that is, there exist session types T and S such that T can safely be considered as a subtype of S, but T 6 S is not derivable by the subtyping. We then propose an asynchronous subtyping system which is sound and complete for the asynchronous calculus. The method gives a general guidance to design rigorous channel-based subtypings respecting desired safety properties.\n\nTitle: A Subtyping for Extensible, Incomplete Objects\nContent: We extend the type system for the Lambda Calculus of Objects [16] with a mechanism of width subtyping and a treatment of incomplete objects. The main novelties over previous work are the use of subtype-bounded quantification to capture a new and more direct rendering of MyType polymorphism, and a uniform treatment for other features that were accounted for via different systems in subsequent extensions [7, 6] of [16]. The new system provides for (i) appropriate type specialization of inherited methods, (ii) static detection of errors, (iii) width subtyping compatible with object extension, and (iv) sound typing for partially specified objects.\n\nTitle: Subtyping Constraints for Incomplete Objects (Extended Abstract)\nContent: We extend the type system for the Lambda Calculus of Objects [14] to account for a notion of width subtyping. The main novelties over previous work are the use of bounded quantification to achieve a new and more direct rendering of MyType polymorphism, and a uniform treatment for other features that were accounted for via different systems in subsequent extensions [7, 6] of [14]. In particular, the new system provides for (i) appropriate type specialization of inherited methods, (ii) static detection of errors, (iii) width subtyping compatible with object extension, and (iv) complete freedom in the order of method addition.\n\nTitle: Intersection and union types: syntax and semantics\nContent: Type assignment systems with intersection and union types are introduced. Although the subject reduction property with respect to \u03b2-reduction does not hold for a natural deduction-like system, we manage to overcome this problem in two, different ways. The first is to adopt a notion of parallel reduction, which is a refinement of Gross-Knuth reduction. The second is to introduce type theories to refine the system, among which is the theory called \u03a0 that induces an assignment system preserving \u03b2-reduction. This type assignment system further clarifies the relation with the intersection discipline through the decomposition, first, of a disjunctive type into a set of conjunctive types and, second, of a derivation in the new type assignment system into a set of derivations in the intersection type assignment system. For this system we propose three semantics and prove soundness and completeness theorems.\n\nTitle: Towards a semantic model for Java wildcards\nContent: Wildcard types enrich the types expressible in Java, and extend the set of typeable Java programs. Syntactic models and proofs of soundness for type systems related to Java wildcards have been suggested in the past, however, the semantics of wildcards has not yet been studied. In this paper we propose a semantic model for Java wildcards, inspired by work on semantic subtyping, which traditionally interprets types as sets of possible values. To easily reflect the nominal type system of Java, our model is defined in terms of runtime types (closed class types) rather than the structure of the runtime values themselves. To reflect the variance introduced by wildcards, our model interprets types as sets of such closed class types. Having defined our model, we employ a standard semantic notion of subtyping. We show soundness of syntactic subtyping with respect to the semantic version, and demonstrate that completeness fails in the general case. We identify a restricted (but nonetheless rich) type language for which the syntactic notion of subtyping is both sound and complete.\n\n## Task\nCompose an abstract for the title 'The Relevance of Semantic Subtyping' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We compare Meyer and Routley's minimal relevant logic B+ with the recent semantics-based approach to subtyping introduced by Frisch, Castagna and Benzaken in the definition of a type system with intersection and union. We show that \u2013 for the functional core of the system \u2013 such notion of subtyping, which is defined in purely set-theoretical terms, coincides with the relevant entailment of the logic B+."
      }
    ]
  },
  {
    "tid":"305",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Logic Programming Approach to Scientific Workflow Provenance Querying\nContent: Scientific workflows have become increasingly important for enabling and accelerating many scientific discoveries. More and more scientists and researchers rely on workflow systems to integrate and structure various local and remote heterogeneous data and services to perform in silico experiments. In order to support understanding, validation, and reproduction of scientific results, provenance querying and management has become a critical component in scientific workflows. In this paper, we propose a logic programming approach to scientific workflow provenance querying and management with the following contributions: i) We identify a set of characteristics that are desirable for a scientific workflow provenance query language; ii) Based on these requirements, we propose FLOQ, a Frame Logic based query language for scientific workflow provenance, iii) We demonstrate that our previous relational database based provenance model, virtual data schema , can be easily mapped to the FLOQ model; and iv) We show by examples that FLOQ is expressive enough to formulate common provenance queries, including all the provenance challenge queries proposed in the provenance challenge series.\n\nTitle: A Dataflow-Oriented Atomicity and Provenance System for Pipelined Scientific Workflows\nContent: Scientific workflows have gained great momentum in recent years due to their critical roles in e-Science and cyberinfrastructure applications. However, some tasks of a scientific workflow might fail during execution. A domain scientist might require a region of a scientific workflow to be \"atomic\". Data provenance, which determines the source data that are used to produce a data item, is also essential to scientific workflows. In this paper, we propose: (i) an architecture for scientific workflow management systems that supports both provenance and atomicity; (ii) a dataflow-oriented atomicity model that supports the notions of commit and abort; and (iii) a dataflow-oriented provenance model that, in addition to supporting existing provenance graphs and queries, also supports queries related to atomicity and failure.\n\nTitle: Storing, reasoning, and querying OPM-compliant scientific workflow provenance using relational databases\nContent: Provenance, the metadata that records the derivation history of scientific results, is essential in scientific workflows to support the reproducibility of scientific discovery, result interpretation, and problem diagnosis. To promote and facilitate interoperability among heterogeneous provenance systems, the Open Provenance Model (OPM) was first proposed in 2008 and since then has played an important role in the community. In this paper, we present OPMProv, a relational database-based scientific workflow provenance system, that is compliant with OPM (v1.1). Our main contributions are: (i) we design an entity-relationship diagram for OPM and translate it into a relational database schema for the storage of provenance; (ii) we show that provenance reasoning defined in OPM (v1.1) can be sufficiently supported by OPMProv using recursive views and SQL queries alone without any additional reasoning engine. Experiments are conducted to evaluate the performance of OPMProv in data insertion and provenance querying. A case study is performed, demonstrating that OPMProv can answer all except two queries out of the 16 queries defined in the Third Provenance Challenge.\n\nTitle: Storing and Querying Scientific Workflow Provenance Metadata Using an RDBMS\nContent: Provenance management has become increasingly important to support scientific discovery reproducibility, result interpretation, and problem diagnosis in scientific workflow environments. This paper proposes an approach to provenance management that seamlessly integrates the interoperability, extensibility, and reasoning advantages of Semantic Web technologies with the storage and querying power of an RDBMS. Specifically, we propose: i) two schema mapping algorithms to map an arbitrary OWL provenance ontology to a relational database schema that is optimized for common provenance queries; ii) two efficient data mapping algorithms to map provenance RDF metadata to relational data according to the generated relational database schema, and iii) a schema-independent SPARQL-toSQL translation algorithm that is optimized on-the-fly by using the type information of an instance available from the input provenance ontology and the statistics of the sizes of the tables in the database. Experimental results are presented to show that our algorithms are efficient and scalable.\n\nTitle: Analysis of Scientific Workflow Provenance Access Control Policies\nContent: Provenance has become an important concept for services computing in general, and for scientific workflows in particular. Provenance often contains confidential data and dependencies whose access needs to be protected. Provenance access control policies control who can access which provenance information. Correct specification of provenance access control policies is critical to ensure system security. However, due to the sheer size of provenance, it is often difficult to comprehend the full effects of an access control policy by manual inspection alone due to complex multi-step dependencies and their interactions. In this paper, we present automated analysis algorithms and complexity results for three provenance analysis problems. We have also developed incremental strategies for these algorithms for evolving provenance and access control policies.\n\n## Task\nCompose an abstract for the title 'Scientific Workflow Provenance Querying with Security Views' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Provenance, the metadata that pertains to the derivation history of a data product, has become increasingly important in scientific workflow environments. In many cases, both data products and their provenance can be sensitive and effective access control mechanisms are essential to protect their confidentiality. In this paper, we propose i) a formalization of scientific workflow provenance as the basis for querying and access control; ii) a security specification mechanism for provenance at various granularity levels and the derivation of a full security specification based on inheritance, overriding, and conflict resolution rules; iii) a formalization of security views that are derived from a scientific workflow run provenance for different roles of users; and iv) a framework that integrates abstraction views and security views such that a user can examine provenance at different abstraction levels while respecting the security policy prescribed for her. We have developed the SecProv prototype to validate the effectiveness of our approach."
      }
    ]
  },
  {
    "tid":"306",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Monitoring Aggregate k-NN Objects in Road Networks\nContent: In recent years, there is an increasing need to monitor knearest neighbor (k-NN) in a road network. There are existing solutions on either monitoring k-NN objects from a single query point over a road network, or computing the snapshot k-NN objects over a road network to minimize an aggregate distance function with respect to multiple query points. In this paper, we study a new problem that is to monitor k-NN objects over a road network from multiple query points to minimize an aggregate distance function with respect to the multiple query points. We call it a continuous aggregate k-NN (CANN) query. We propose a new approach that can significantly reduce the cost of computing network distances when monitoring aggregate k-NN objects on road networks. We conducted extensive experimental studies and confirmed the efficiency of our algorithms.\n\nTitle: Extracting Mobility Statistics from Indexed Spatio-Temporal Datasets\nContent: With the recent progress of spatial information technologies and mobile computing technologies, spatio-temporal databases which store information on moving objects including vehicles and mobile users have gained a lot of research in- terests. In this paper, we propose an algorithm to extract mobility statistics from indexed spatio-temporal datasets for the interactive analysis of huge collections of moving object trajectories. We focus on a mobility statistics value called the Markov transition probability, which is based on a cell-based organization of a target space and the Markov chain model. The pro- posed algorithm efficiently computes the specified Markov transition probabilities with the help of a spatial index R-tree. We reduce the statistics computation task to a kind of constraint satisfaction problem that uses a spatial index, and utilize internal representation of R-tree in an efficient manner.\n\nTitle: CiNCT: Compression and retrieval for massive vehicular trajectories via relative movement labeling.\nContent: In this paper, we present a compressed data structure for moving object trajectories in a road network, which are represented as sequences of road edges. Unlike existing compression methods for trajectories in a network, our method supports pattern matching and decompression from an arbitrary position while retaining high compressibility with theoretical guarantees. Specifically, our method is based on FM-index, a fast and compact data structure for pattern matching. To further enhance the compression performance, we incorporate the sparsity of road networks. In particular, we present the novel concepts of relative movement labeling and PseudoRank, each contributing to significant reduction in data size and query processing time. Our theoretical analysis and experimental studies reveal the advantages of our proposed method as compared to existing trajectory compression methods and FM-index variants.\n\nTitle: An Efficient Algorithm For Location-Aware Query Autocompletion\nContent: Query autocompletion is an important and practical technique when users want to search for desirable information. As mobile devices become more and more popular, one of the main applications is location-aware service, such as Web mapping. In this paper, we propose a new solution to location-aware query autocompletion. We devise a trie-based index structure and integrate spatial information into trie nodes. Our method is able to answer both range and top-k queries. In addition, we discuss the extension of our method to support the error tolerant feature in case user's queries contain typographical errors. Experiments on real datasets show that the proposed method outperforms existing methods in terms of query processing performance.\n\nTitle: Extended Link Analysis for Extracting Spatial Information Hubs\nContent: Recently, web mining that tries to find useful knowledge from the vast amount of web pages has attracted a lot of research interests. Besides, it is becoming an essential task to provide web pages related to a user-specified geographic area. In this paper, we propose an approach to extract spatial information hubs from the web. A spatial information hub is a web page which is related to a specified geographic area and has much local information and/or many hyperlinks to local web pages. In the traditional approach of web link analysis, the importance and quality of pages are judged only by their contents and hyperlink structures. However, we take their geographic localities into consideration. In our approach, we first extract geographic information from web pages to create spatial nodes and spatial links, then conduct a link analysis based on the extended link structures. We also show our approach works well based on the experiments.\n\n## Task\nCompose an abstract for the title 'Enhanced Indexing and Querying of Trajectories in Road Networks via String Algorithms.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this article, we propose a novel indexing and querying method for trajectories constrained in a road network. We aim to provide efficient algorithms for various types of spatiotemporal queries that involve routing in road networks, such as (1) finding moving objects that have traveled along a given path during a given time interval, (2) extracting all paths traveled after a given spatiotemporal context, and (3) enumerating all paths between two locations traveled during a certain time interval. Unlike the existing methods in spatial database research, we employ indexing techniques and algorithms from string processing. This idea is based on the fact that we can represent spatial paths as strings, because trajectories in a network are represented as sequences of road segment IDs. The proposed SNT-index (<u>s</u>uffix-array-based <u>n</u>etwork-constrained <u>t</u>rajectory index) introduces two novel concepts to trajectory indexing. The first is FM-index, which is a compact in-memory data structure for pattern matching. The second is an inverse suffix array, which allows the FM-index to be integrated with the temporal information stored in a forest of B+-trees. Thanks to these concepts, we can reduce the number of B+-tree accesses required by the query processing algorithms to a constant number, something that cannot be achieved with existing methods. Although an FM-index is essentially a static index, we also propose a practical method of appending new data to the index. Finally, experiments show that our method can process the target queries for more than 1 million trajectories in a few tens of milliseconds, which is significantly faster than what the baseline algorithms can achieve without string algorithms.\n\n"
      }
    ]
  },
  {
    "tid":"307",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Achieving side-channel protection with dynamic logic reconfiguration on modern FPGAs\nContent: Reconfigurability is a unique feature of modern FPGA devices to load hardware circuits just on demand. This also implies that a completely different set of circuits might operate at the exact same location of the FPGA at different time slots, making it difficult for an external observer or attacker to predict what will happen at what time. In this work we present and evaluate a novel hardware implementation of the lightweight cipher PRESENT with built-in side-channel countermeasures based on dynamic logic reconfiguration. In our design we make use of Configurable Look-Up Tables (CFGLUT) integrated in modern Xilinx FPGAs to nearly instantaneously change hardware internals of our cipher implementation for improved resistance against side-channel attacks. We provide evidence from practical experiments based on a Spartan-6 platform that even with 10 million recorded power traces we were unable to detect a first-order leakage using the state-of-the-art leakage assessment.\n\nTitle: ParTI: Towards Combined Hardware Countermeasures against Side-Channeland Fault-Injection Attacks\nContent: Side-channel analysis and fault-injection attacks are known as major threats to any cryptographic implementation. Hardening cryptographic implementations with appropriate countermeasures is thus essential before they are deployed in the wild. However, countermeasures for both threats are of completely different nature: Side-channel analysis is mitigated by techniques that hide or mask key-dependent information while resistance against fault-injection attacks can be achieved by redundancy in the computation for immediate error detection. Since already the integration of any single countermeasure in cryptographic hardware comes with significant costs in terms of performance and area, a combination of multiple countermeasures is expensive and often associated with undesired side effects. In this work, we introduce a countermeasure for cryptographic hardware implementations that combines the concept of a provably-secure masking scheme i.e., threshold implementation with an error detecting approach against fault injection. As a case study, we apply our generic construction to the lightweight LED cipher. Our LED instance achieves first-order resistance against side-channel attacks combined with a fault detection capability that is superior to that of simple duplication for most error distributions at an increased area demand of 12\u00ef\u00be\u00bf%.\n\nTitle: Towards Side-Channel Resistant Implementations Of Qc-Mdpc Mceliece Encryption On Constrained Devices\nContent: Recent advances in code-based cryptography paved new ways for efficient asymmetric cryptosystems that combine decent performance with moderate key sizes. In this context, Misoczki et al. recently proposed the use of quasi-cyclic MDPC (QC-MDPC) codes for the McEliece cryptosystem. It was shown that these codes can provide both compact key representations and solid performance on high-end computing platforms. However, for widely used low-end microcontrollers only slow implementations for this promising construction have been presented so far.In this work we present an implementation of QC-MDPC McEliece encryption providing 80 bits of equivalent symmetric security on low-cost ARM Cortex-M4-based microcontrollers with a reasonable performance of 42 ms for encryption and 251-558 ms for decryption. Besides practical issues such as random error generation, we demonstrate side-channel attacks on a straightforward implementation of this scheme and finally propose timing-and instruction-invariant coding strategies and countermeasures to strengthen it against timing attacks as well as simple power analysis.\n\nTitle: Towards Practical Lattice-Based Public-Key Encryption on Reconfigurable Hardware.\nContent: With this work we provide further evidence that lattice-based cryptography is a promising and efficient alternative to secure embedded applications. So far it is known for solid security reductions but implementations of specific instances have often been reported to be too complex beyond any practicability. In this work, we present an efficient and scalable micro-code engine for Ring-LWE encryption that combines polynomial multiplication based on the Number Theoretic Transform (NTT), polynomial addition, subtraction, and Gaussian sampling in a single unit. This unit can encrypt and decrypt a block in 26.19 mu s and 16.80 mu s on a Virtex-6 LX75T FPGA, respectively - at moderate resource requirements of about 1506 slices and a few block RAMs. Additionally, we provide solutions for several practical issues with Ring-LWE encryption, including the reduction of ciphertext expansion, error rate and constant-time operation. We hope that this contribution helps to pave the way for the deployment of ideal lattice-based encryption in future real-world systems.\n\nTitle: Establishing Chain of Trust in Reconfigurable Hardware\nContent: Facing ubiquitous threats like computer viruses, trojans and theft of intellectual property, Trusted Computing (TC) is an emerging technology towards building trustworthy computing platforms. A recent initiative by the Trusted Computing Group (TCG) specifies the use of Trusted Platform Modules (TPM), currently implemented as dedicated, cost-effective crypto-chips mounted on the main board of computer systems. In this paper we propose implementations for TC functionalities based on more flexible and versatile approaches for reconfigurable and embedded architectures. Our approach allows for (i) a scalable design and update of TPM functionalities in embedded systems, (ii) the integration of the TPM hardware in the chain of trust to bind applications to the underlying TPM and the reconfigurable hardware, and (iii) the design of vendor independent TPMs.\n\n## Task\nCompose an abstract for the title 'Generic side-channel countermeasures for reconfigurable devices' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this work, we propose and evaluate generic hardware countermeasures against DPA attacks for recent FPGA devices. The proposed set of FPGA-specific countermeasures can be combined to resist a large variety of first-order DPA attacks, even with 100 million recorded power traces. This set includes generic and resource-efficient countermeasures for on-chip noise generation, random-data processing delays and S-box scrambling using dual-ported block memories. In particular, it is possible to build many of these countermeasures into a single IP-core or hard macro that then provides basic protection for any cryptographic implementation just by its inclusion in the design process - what is particularly useful for engineers with no or little background on security and side-channel attacks."
      }
    ]
  },
  {
    "tid":"308",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Toward Everyday Gaze Input: Accuracy and Precision of Eye Tracking and Implications for Design.\nContent: For eye tracking to become a ubiquitous part of our everyday interaction with computers, we first need to understand its limitations outside rigorously controlled labs, and develop robust applications that can be used by a broad range of users and in various environments. Toward this end, we collected eye tracking data from 80 people in a calibration-style task, using two different trackers in two lighting conditions. We found that accuracy and precision can vary between users and targets more than six-fold, and report on differences between lighting, trackers, and screen regions. We show how such data can be used to determine appropriate target sizes and to optimize the parameters of commonly used filters. We conclude with design recommendations and examples how our findings and methodology can inform the design of error-aware adaptive applications.\n\nTitle: What do you see when you're surfing?: using eye tracking to predict salient regions of web pages\nContent: An understanding of how people allocate their visual attention when viewing Web pages is very important for Web authors, interface designers, advertisers and others. Such knowledge opens the door to a variety of innovations, ranging from improved Web page design to the creation of compact, yet recognizable, visual representations of long pages. We present an eye-tracking study in which 20 users viewed 361 Web pages while engaged in information foraging and page recognition tasks. From this data, we describe general location-based characteristics of visual attention for Web pages dependent on different tasks and demographics, and generate a model for predicting the visual attention that individual page elements may receive. Finally, we introduce the concept of fixation impact, a new method for mapping gaze data to visual scenes that is motivated by findings in vision research.\n\nTitle: Demonstration of Enabling People with Visual Impairments to Navigate Virtual Reality with a Haptic and Auditory Cane Simulation.\nContent: Traditional virtual reality (VR) mainly focuses on visual feedback, which is not accessible for people with visual impairments. We created Canetroller [6], a haptic cane controller that simulates white cane interactions, enabling people with visual impairments to navigate a virtual environment by transferring their cane skills into the virtual world. Canetroller provides three types of feedback: (1) physical resistance generated by a wearable programmable brake mechanism that physically impedes the controller when the virtual cane comes in contact with a virtual object; (2) vibrotactile feedback that simulates the vibrations when a cane hits an object or touches and drags across various surfaces; and (3) spatial 3D auditory feedback simulating the sound of real-world cane interactions. We demonstrate the design of Canetroller in this paper.\n\n\n\nTitle: Improving Dwell-Based Gaze Typing with Dynamic, Cascading Dwell Times.\nContent: We present cascading dwell gaze typing, a novel approach to dwell-based eye typing that dynamically adjusts the dwell time of keys in an on-screen keyboard based on the likelihood that a key will be selected next, and the location of the key on the keyboard. Our approach makes unlikely keys more difficult to select and likely keys easier to select by increasing and decreasing their required dwell times, respectively. To maintain a smooth typing rhythm for the user, we cascade the dwell time of likely keys, slowly decreasing the minimum allowable dwell time as a user enters text. Cascading the dwell time affords users the benefits of faster dwell times while causing little disruption to users' typing cadence. Results from a longitudinal study with 17 non-disabled participants show that our dynamic cascading dwell technique was significantly faster than a static dwell approach. Participants were able to achieve typing speeds of 12.39 WPM on average with our cascading technique, whereas participants were able to achieve typing speeds of 10.62 WPM on average with a static dwell time approach. In a small evaluation conducted with five people with ALS, participants achieved average typing speeds of 9.51 WPM with our cascading dwell approach. These results show that our dynamic cascading dwell technique has the potential to improve gaze typing for users with and without disabilities.\n\nTitle: Smartphone-Based Gaze Gesture Communication for People with Motor Disabilities.\nContent: Current eye-tracking input systems for people with ALS or other motor impairments are expensive, not robust under sunlight, and require frequent re-calibration and substantial, relatively immobile setups. Eye-gaze transfer (e-tran) boards, a low-tech alternative, are challenging to master and offer slow communication rates. To mitigate the drawbacks of these two status quo approaches, we created GazeSpeak, an eye gesture communication system that runs on a smartphone, and is designed to be low-cost, robust, portable, and easy-to-learn, with a higher communication bandwidth than an e-tran board. GazeSpeak can interpret eye gestures in real time, decode these gestures into predicted utterances, and facilitate communication, with different user interfaces for speakers and interpreters. Our evaluations demonstrate that GazeSpeak is robust, has good user satisfaction, and provides a speed improvement with respect to an e-tran board; we also identify avenues for further improvement to low-cost, low-effort gaze-based communication technologies.\n\n## Task\nCompose an abstract for the title 'Let's Talk About X: Combining Image Recognition and Eye Gaze to Support Conversation for People with ALS.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Communicating at a natural speed is a significant challenge for users of augmentative and alternative communication (AAC) devices, especially when input is provided by eye gaze, as is common for people with ALS and similar conditions. One way to improve AAC throughput is by drawing on contextual information from the outside world. Toward this goal, we present SceneTalk, a prototype gaze-based AAC system that uses computer vision to identify objects in the user's field of view and suggests words and phrases related to the current scene. We conducted a formative evaluation of SceneTalk with six people with ALS, in which we evaluated their preference for user interface modes and output preferences. Participants agreed that integrating contextual awareness into their AAC device could be helpful across a diverse range of situations."
      }
    ]
  },
  {
    "tid":"309",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Vehicle Orientation Analysis Using Eigen Color, Edge Map, And Normalized Cut Clustering\nContent: This paper proposes a novel approach for estimating vehicles' orientations from still images using \"eigen color\" and edge map through a clustering framework. To extract the eigen color, a novel color transform model is used for roughly segmenting a vehicle from its background. The model is invariant to various situations like contrast changes, background, and lighting. It does not need to be re-estimated for any new vehicles. In this eigen color space, different vehicle regions can be easily identified. However, since the problem of object segmentation is still ill-posed, only with this model, the shape of a vehicle cannot be well extracted from its background and thus affects the accuracy of orientation estimation. In order to solve this problem, the distributions of vehicle edges and colors are then integrated together to form a powerful but high-dimensional feature space. Since the feature dimension is high, the normalized cut spectral clustering (Ncut) is then used for feature reduction and orientation clustering. The criterion in Ncut tries to minimize the ratio of the total dissimilarity between groups to the total similarity within the groups. Then, the vehicle orientation can be analyzed using the eigenvectors derived from the Ncut result. The proposed framework needs only one still image and is thus very different to traditional methods which need motion features to determine vehicle orientations. Experimental results reveal the superior performances in vehicle orientation analysis.\n\nTitle: Vehicle Retrieval Using Eigen Color and Multiple Instance Learning\nContent: This paper presents a novel approach for retrieving images from databases using eigen color and the concept of multiple instance learning. Usually, vehicles have various colors and shapes under different viewpoints, weathers, and lighting conditions. All the variations will increase many difficulties and challenges in selecting a general feature to describe vehicles. Thus, traditional methods to retrieve vehicles require their orientations or colors being fixed. To tackle this problem, this paper proposes a novel vehicle retrieval system for effectively retrieving vehicles from databases no matter what orientations and colors they are. First of all, this paper proposes a novel color transform model, which is global and does not need to be re-estimated for any new vehicles or new images, to extract different regions of interest (or vehicle analogues) from databases. Then, to more accurately locate desired vehicle images, this paper uses the MIL (multiple-instance learning) method to learn specific visual properties of vehicles from query images. However, the MIL technique requires the positive training data being strongly positive and the negative ones being strongly negative. This requirement is too constrained in real cases and will lead to lots of false detection. This problem can be easily tackled if an eigen color transform is introduced. The extra consideration \"eigen color\" will add more capabilities to the MIL learner for capturing the embedded concept more accurately. Furthermore, during the learning process, since no time-consuming optimization process is involved, all the desired visual concept can be obtained immediately and adapted to different user's requests. Experimental results reveal the feasibility and high accuracy of the proposed approach in vehicle retrieval system.\n\nTitle: Vehicle Occlusion Identification System by Perceptive Roadway Modeling\nContent: This paper presents a vehicle occlusion identification system based on the perceptive characters of a roadway. To tracking the vehicles, two stages are devised, initial parameter setting stage and occlusion handling stage. In the initial stage, a background extracting method is adopted to obtain the first clean background. Then, a road detection algorithm is used for finding the vanishing point. After the lane dividing line detection, the perceptive roadway models are established in each lane. These roadway parameters are fed into the occlusion handling stage for analyzing the vehicle occlusion situations. Due to the different occlusion cases, several novel segmentation methods are proposed for improving the accurate of the tracking vehicles. Experiment results are shown the feasibility of the proposed methods.\n\nTitle: Morphology-based license plate detection in images of differently illuminated and oriented cars\nContent: This paper presents a morphology-based method for extracting license plates from cluttered images. The proposed system consists of three major components. First, a morphology-based method is proposed for extracting important contrast features as guides to search for desired license plates. The contrast feature is robust to lighting changes and invariant to different transformations like image scaling, translation, and skewing. Then, a recovery algorithm is applied to reconstruct a license plate if it is fragmented into pieces. The last stage of this method is to do license plate verification. The criterion for verification is based on the number of characters appearing in the plate that can be extracted from a clustering algorithm. The morphology-based method can significantly reduce the number of possible characters extracted and thus speeds up subsequent plate recognition. Since the feature extracted is robust to different image changes, the proposed method works well in extracting differently illuminated and oriented license plates. The average accuracy of detection is 98%. Due to the simplicity of the proposed method, all the license plates can be extracted very fast (in less than 0.5 s). The experimental results show that the proposed method improves the state-of-the-art work in terms of effectiveness and robustness for license plate detection. (C) 2002 SPIE and IST.\n\nTitle: Morphology-Based License Plate Detection from Complex Scenes\nContent: This paper presents a morphology-based method for detecting license plates from cluttered images. The proposed system consists of three major components. At the first, a morphology-based method is proposed to extract important contrast features as guides to search the desired license plates. The contrast feature is robust to lighting changes and invariant to several transformations like scaling, translation, and skewing. Then, a recovery algorithm is applied for reconstructing a license plate if the plate is fragmented into several parts. The last step is to do license plate verification. The morphology-based method can significantly reduce the number of candidates extracted from the cluttered images and thusspeeds up the subsequent plate recognition. Under the experimental database, 128 examples got from 130 images were successfully detected. The average accuracy of license plate detection is 98%. Experimental results show that the proposed method improves the state-of-the-art work in terms of effectiveness and robustness of license plate detection.\n\n## Task\nCompose an abstract for the title 'Vehicle Orientation Detection Using Vehicle Color and Normalized Cut Clustering' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper proposes a novel approach for vehicle orientation detection using \"vehicle color\" and edge information based on clustering framework. To extract the \"vehicle color\", this paper proposes a novel color transform model which is global and does not need to be re-estimated for any new vehicles or new images. This model is invariant to various situations like contrast changes, background and lighting. Compared with traditional methods which use motion feature to determine vehicle orientations, this paper uses only one still image to finish this task. After feature extraction, the normalized cut spectral clustering (N-cut) is used for vehicle orientation clustering. The N-cut criterion tries to minimize the ratio of the total dissimilarity between groups to the total similarity within the groups. Then, the vehicle orientation can be detected using the eigenvector derived from the N-cut result. Experimental results reveal the superior performances in vehicle orientation estimation."
      }
    ]
  },
  {
    "tid":"310",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: On the Security of Beth's Identification Schemes against Active and Concurrent Adversaries\nContent: One of the earliest identification schemes was proposed by Beth in [6]. Since its introduction, variations and generalizations of this scheme have been considered, and, recently, the property of security against passive impersonation was shown, under a weak unforgeability assumption on the hashed El Gamal signature scheme, for two such variants: one in the standard (i.e., not identity-based) and one in the identity-based model. However, the security of both protocols under active and concurrent impersonation attacks was left open. In this paper we prove that very minor modifications to these schemes result in schemes that satisfy security under active and concurrent impersonation attacks, assuming a one-more-dlog assumption. The resulting protocols are just as efficient as the original variants, which are, in turn, somewhat more efficient (but less general) of the original one proposed by Beth.\n\nTitle: Implementing Conjunction Obfuscation Under Entropic Ring LWE\nContent: We address the practicality challenges of secure program obfuscation by implementing, optimizing, and experimentally assessing an approach to securely obfuscate conjunction programs proposed in [1]. Conjunction programs evaluate functions f (x_1,...,x_L) = wedge_i in _I y_i, where y_i is either x_i or -x_i and I subseteq [L], and can be used as classifiers. Our obfuscation approach satisfies distributional Virtual Black Box (VBB) security based on reasonable hardness assumptions, namely an entropic variant of the Ring Learning with Errors (Ring-LWE) assumption. Prior implementations of secure program obfuscation techniques support either trivial programs like point functions, or support the obfuscation of more general but less efficient branching programs to satisfy Indistinguishability Obfuscation (IO), a weaker security model. Further, the more general implemented techniques, rather than relying on standard assumptions, base their security on conjectures that have been shown to be theoretically vulnerable. Our work is the first implementation of non-trivial program obfuscation based on polynomial rings. Our contributions include multiple design and implementation advances resulting in reduced program size, obfuscation runtime, and evaluation runtime by many orders of magnitude. We implement our design in software and experimentally assess performance in a commercially available multi-core computing environment. Our implementation achieves runtimes of 6.7 hours to securely obfuscate a 64-bit conjunction program and 2.5 seconds to evaluate this program over an arbitrary input. We are also able to obfuscate a 32-bit conjunction program with 53 bits of security in 7 minutes and evaluate the obfuscated program in 43 milliseconds on a commodity desktop computer, which implies that 32-bit conjunction obfuscation is already practical. Our graph-induced (directed) encoding implementation runs up to 25 levels, which is higher than previously reported in the literature for this encoding. Our design and implementation advances are applicable to obfuscating more general compute-and-compare programs and can also be used for many cryptographic schemes based on lattice trapdoors.\n\nTitle: Security amplification by composition: The case of doubly-iterated, ideal ciphers.\nContent: We investigate, in the Shannon model, the security of constructions corresponding to double and (two-key) triple DES. That is, we consider Fk1(Fk2(\ufffd )) and Fk1(F 1 k2 (Fk1(\ufffd ))) with the component functions being ideal ciphers. This models the resistance of these constructions to \"generic\" attacks like meet in the middle attacks. We obtain the first proof that composition actually increases the security in some meaningful sense. We compute a bound on the probability of breaking the double cipher as a function of the number of computations of the base cipher made, and the number of examples of the composed cipher seen, and show that the success probability is the square of that for a single key cipher. The same bound holds for the two-key triple cipher. The first bound is tight and shows that meet in the middle is the best possible generic attack against the double cipher.\n\nTitle: Improved Setup Assumptions for 3-Round Resettable Zero Knowledge\nContent: In the bare public-key model, introduced by Canetti et al. [STOC 2000], it is only assumed that each verifier deposits during a setup phase a public key in a file accessible by all users at all times. As pointed out by Micali and Reyzin [Crypto 2001], the notion of soundness in this model is more subtle and complex than in the classical model. Indeed Micali and Reyzin have introduced four different notions which are called (from weaker to stronger): one-time, sequential, concurrent and resettable soundness. In this paper we introduce the counter public-key model (the cPK model for short) I an augmentation of the bare public-key model in which each verifier is equipped with a counter and, like in the original bare public-key model, the key of the verifier can be used for any polynomial number of interactions with provers. In the cPK model, we give a three-round concurrently-sound resettable zero-knowledge argument of membership for NP. Previously similar results were obtained by Micali and Reyzin [EuroCrypt 2001] and then improved by Zhao et al. [EuroCrypt 2003] in models in which, roughly speaking, each verifier is still equipped with a counter, but the key of the verifier could only be used for a fixed number of interactions.\n\nTitle: Conditional oblivious transfer and timed-release encryption\nContent: We consider the problem of sending messages \"into the future.\" Previous constructions for this task were either based on heuristic assumptions or did not provide anonymity to the sender of the message. In the public-key setting, we present an efficient and secure timed-release encryption scheme using a \"time server\" which inputs the current time into the system. The server has to only interact with the receiver and never learns the sender's identity. The scheme's computational and communicational cost per request are only logarithmic in the time parameter. The construction of our scheme is based on a novel cryptographic primitive: a variant of oblivious transfer which we call conditional oblivious transfer. We define this primitive (which may be of independent interest) and show an efficient construction for an instance of this new primitive based on the quadratic residuosity assumption.\n\n## Task\nCompose an abstract for the title 'Security of erasable memories against adaptive adversaries' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We study cryptographic modeling and encryption-based design techniques for guaranteeing privacy of data that is first stored in some type of computer memory and then deleted. We continue the investigation started in [3] by presenting an enhanced privacy notion that captures practical scenarios of adversaries repeatedly and adaptively attacking the memory to inspect its entire content before trying to obtain information about deleted data. We prove that the new notion is strictly stronger than the previous one considered in [3] (allowing the adversary a single intrusion), and show then that the efficient protocol in [3] still satisfies the new notion. One question implicitly raised by the previous work was whether it is indeed possible to define one meaningful and applicable notion of security even against adversaries that can repeatedly and adaptively obtain total control of the memory. Perhaps unexpectedly, our paper affirmatively answers this question."
      }
    ]
  },
  {
    "tid":"311",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Accurate and efficient target prediction using a potency-sensitive influence-relevance voter.\nContent: A number of algorithms have been proposed to predict the biological targets of diverse molecules. Some are structure-based, but the most common are ligand-based and use chemical fingerprints and the notion of chemical similarity. These methods tend to be computationally faster than others, making them particularly attractive tools as the amount of available data grows.Using a ChEMBL-derived database covering 490,760 molecule-protein interactions and 3236 protein targets, we conduct a large-scale assessment of the performance of several target-prediction algorithms at predicting drug-target activity. We assess algorithm performance using three validation procedures: standard tenfold cross-validation, tenfold cross-validation in a simulated screen that includes random inactive molecules, and validation on an external test set composed of molecules not present in our database.We present two improvements over current practice. First, using a modified version of the influence-relevance voter (IRV), we show that using molecule potency data can improve target prediction. Second, we demonstrate that random inactive molecules added during training can boost the accuracy of several algorithms in realistic target-prediction experiments. Our potency-sensitive version of the IRV (PS-IRV) obtains the best results on large test sets in most of the experiments. Models and software are publicly accessible through the chemoinformatics portal at http://chemdb.ics.uci.edu/.\n\nTitle: Towards Automating Precision Studies of Clone Detectors.\nContent: Current research in clone detection suffers from poor ecosystems for evaluating precision of clone detection tools. Corpora of labeled clones are scarce and incomplete, making evaluation labor intensive and idiosyncratic, and limiting inter-tool comparison. Precision-assessment tools are simply lacking.\n\nWe present a semiautomated approach to facilitate precision studies of clone detection tools. The approach merges automatic mechanisms of clone classification with manual validation of clone pairs. We demonstrate that the proposed automatic approach has a very high precision and it significantly reduces the number of clone pairs that need human validation during precision experiments. Moreover, we aggregate the individual effort of multiple teams into a single evolving dataset of labeled clone pairs, creating an important asset for software clone research.\n\n\n\nTitle: Improved residue contact prediction using support vector machines and a large feature set.\nContent: Predicting protein residue-residue contacts is an important 2D prediction task. It is useful for ab initio structure prediction and understanding protein folding. In spite of steady progress over the past decade, contact prediction remains still largely unsolved.Here we develop a new contact map predictor (SVMcon) that uses support vector machines to predict medium- and long-range contacts. SVMcon integrates profiles, secondary structure, relative solvent accessibility, contact potentials, and other useful features. On the same test data set, SVMcon's accuracy is 4% higher than the latest version of the CMAPpro contact map predictor. SVMcon recently participated in the seventh edition of the Critical Assessment of Techniques for Protein Structure Prediction (CASP7) experiment and was evaluated along with seven other contact map predictors. SVMcon was ranked as one of the top predictors, yielding the second best coverage and accuracy for contacts with sequence separation > or = 12 on 13 de novo domains.We describe SVMcon, a new contact map predictor that uses SVMs and a large set of informative features. SVMcon yields good performance on medium- to long-range contact predictions and can be modularly incorporated into a structure prediction pipeline.\n\nTitle: Combining protein secondary structure prediction models with ensemble methods of optimal complexity\nContent: Many sophisticated methods are currently available to perform protein secondary structure prediction. Since they are frequently based on different principles, and different knowledge sources, significant benefits can be expected from combining them. However, the choice of an appropriate combiner appears to be an issue in its own right. The first difficulty to overcome when combining prediction methods is overfitting. This is the reason why we investigate the implementation of Support Vector Machines to perform the task. A family of multi-class SVMs is introduced. Two of these machines are used to combine some of the current best protein secondary structure prediction methods. Their performance is consistently superior to the performance of the ensemble methods traditionally used in the field. They also outperform the decomposition approaches based on bi-class SVMs. Furthermore, initial experimental evidence suggests that their outputs could be processed by the biologist to perform higher-level treatments.\n\nTitle: Three-stage prediction of protein ?-sheets by neural networks, alignments and graph algorithms\nContent: Motivation: Protein beta-sheets play a fundamental role in protein structure, function, evolution and bioengineering. Accurate prediction and assembly of protein beta-sheets, however, remains challenging because protein beta-sheets require formation of hydrogen bonds between linearly distant residues. Previous approaches for predicting beta-sheet topological features, such as beta-strand alignments, in general have not exploited the global covariation and constraints characteristic of beta-sheet architectures. Results: We propose a modular approach to the problem of predicting/assembling protein beta-sheets in a chain by integrating both local and global constraints in three steps. The first step uses recursive neural networks to predict pairing probabilities for all pairs of interstrand beta-residues from profile, secondary structure and solvent accessibility information. The second step applies dynamic programming techniques to these probabilities to derive binding pseudoenergies and optimal alignments between all pairs of beta-strands. Finally, the third step uses graph matching algorithms to predict the beta-sheet architecture of the protein by optimizing the global pseudoenergy while enforcing strong global beta-strand pairing constraints. The approach is evaluated using cross-validation methods on a large non-homologous dataset and yields significant improvements over previous methods.\n\n## Task\nCompose an abstract for the title 'Assessing the accuracy of prediction algorithms for classification: an overview.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We provide a unified overview of methods that currently are widely used to assess the accuracy of prediction algorithms, from raw percentages, quadratic error measures and other distances, ann correlation coefficients, and to information theoretic measures such as relative entropy and mutual information. We briefly discuss the advantages and disadvantages of each approach. For classification tasks, we derive new learning algorithms for the design of prediction systems by directly optimising the correlation coefficient. We observe and prove several results relating sensitivity nod specificity of optimal systems. While the principles are general, we illustrate the applicability on specific problems such as protein secondary structure and signal peptide prediction."
      }
    ]
  },
  {
    "tid":"312",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: How to construct more accurate student models: comparing and optimizing knowledge tracing and performance factor analysis\nContent: Student modeling is a fundamental concept applicable to a variety of intelligent tutoring systems (ITS). However, there is not a lot of practical guidance on how to construct and train such models. This paper compares two approaches for student modeling, Knowledge Tracing (KT) and Perlbrmance Factors Analysis (PFA), by evaluating their predictive accuracy on individual student practice opportunities. We explore the space of design decisions for each approach and find a set of \"best practices\" for each. In a head to head comparison, we find that PFA has considerably higher predictive accuracy than KT. In addition to being more accurate, we lbund that PFA's parameter estimates were more plausible. Our best-performing model was a variant of PFA that ignored the tutor's transfer model; that is, it assumed all skills influenced performance on all problems. One possible implication is that this result is a general one suggesting there is benefit from considering models that incorporate information from more than the typical handful of skills associated with a problem in the transfer model. Alternately, an explanation for this result is the transfer model that our tutor uses is particularly weak. We also found that both KT and PFA have relatively low predictive accuracy for cases where students generate incorrect responses, and 2/3 of the model's errors are false positives, indicating a better means of determining when students will make mistakes is needed.\n\nTitle: Enhancing the efficiency and reliability of group differentiation through partial credit.\nContent: The focus of the learning analytics community bridges the gap between controlled educational research and data mining. Online learning platforms can be used to conduct randomized controlled trials to assist in the development of interventions that increase learning gains; datasets from such research can act as a treasure trove for inquisitive data miners. The present work employs a data mining approach on randomized controlled trial data from ASSISTments, a popular online learning platform, to assess the benefits of incorporating additional student performance data when attempting to differentiate between two user groups. Through a resampling technique, we show that partial credit, defined as an algorithmic combination of binary correctness, hint usage, and attempt count, can benefit assessment and group differentiation. Partial credit reduces sample sizes required to reliably differentiate between groups that are known to differ by 58%, and reduces sample sizes required to reliably differentiate between less distinct groups by 9%.\n\nTitle: Using Mixed-Effects Modeling to Analyze Different Grain-Sized Skill Models in an Intelligent Tutoring System\nContent: Student modeling and cognitive diagnostic assessment are important issues that need to be addressed for the development and successful application of intelligent tutoring systems (ITS). ITS needs the construction of complex models to represent the skills that students are using and their knowledge states, and practitioners want cognitively diagnostic information at a finer grained level. Traditionally, most assessments treat all questions on the test as sampling a single underlying knowledge component. Can we have our cake and eat it, too? That is, can we have a good overall prediction of a high stakes test, while at the same time be able to tell teachers meaningful information about fine-grained knowledge components? In this paper, we introduce an online intelligent tutoring system that has been widely used. We then present some encouraging results about a fine-grained skill model with the system that is able to predict state test scores. This model allows the system track about 106 knowledge components for eighth grade math. In total, 921 eighth grade students were involved in the study. We show that our fine-grained model could improve prediction compared to other coarser grained models and an IRT-based model. We conclude that this intelligent tutoring system can be a good predictor of performance.\n\nTitle: KT-IDEM: introducing item difficulty to the knowledge tracing model\nContent: Many models in computer education and assessment take into account difficulty. However, despite the positive results of models that take difficulty in to account, knowledge tracing is still used in its basic form due to its skill level diagnostic abilities that are very useful to teachers. This leads to the research question we address in this work: Can KT be effectively extended to capture item difficulty and improve prediction accuracy? There have been a variety of extensions to KT in recent years. One such extension was Baker's contextual guess and slip model. While this model has shown positive gains over KT in internal validation testing, it has not performed well relative to KT on unseen in-tutor data or post-test data, however, it has proven a valuable model to use alongside other models. The contextual guess and slip model increases the complexity of KT by adding regression steps and feature generation. The added complexity of feature generation across datasets may have hindered the performance of this model. Therefore, one of the aims of our work here is to make the most minimal of modifications to the KT model in order to add item difficulty and keep the modification limited to changing the topology of the model. We analyze datasets from two intelligent tutoring systems with KT and a model we have called KT-IDEM (Item Difficulty Effect Model) and show that substantial performance gains can be achieved with this minor modification that incorporates item difficulty.\n\nTitle: The Effect of Model Granularity on Student Performance Prediction Using Bayesian Networks\nContent: A standing question in the field of Intelligent Tutoring Systems and User Modeling in general is what is the appropriate level of model granularity (how many skills to model) and how is that granularity derived? In this paper we will explore models with varying levels of skill generality (1, 5, 39 and 106 skill models) and measure the accuracy of these models by predicting student performance within our tutoring system called ASSISTment as well as their performance on a state standardized test. We employ the use of Bayes nets to model user knowledge and to use for prediction of student responses. Our results show that the finer the granularity of the skill model, the better we can predict student performance for our online data. However, for the standardized test data we received, it was the 39 skill model that performed the best. We view this as support for fine-grained skill models despite the finest grain model not predicting the state test scores the best.\n\n## Task\nCompose an abstract for the title 'Improving Student Modeling Through Partial Credit and Problem Difficulty' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Student modeling within intelligent tutoring systems is a task largely driven by binary models that predict student knowledge or next problem correctness (i.e., Knowledge Tracing (KT)). However, using a binary construct for student assessment often causes researchers to overlook the feedback innate to these platforms. The present study considers a novel method of tabling an algorithmically determined partial credit score and problem difficulty bin for each student's current problem to predict both binary and partial next problem correctness. This study was conducted using log files from ASSISTments, an adaptive mathematics tutor, from the 2012-2013 school year. The dataset consisted of 338,297 problem logs linked to 15,253 unique student identification numbers. Findings suggest that an efficiently tabled model considering partial credit and problem difficulty performs about as well as KT on binary predictions of next problem correctness. This method provides the groundwork for modifying KT in an attempt to optimize student modeling."
      }
    ]
  },
  {
    "tid":"313",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Stochastic optimization for collision selection in high energy physics\nContent: Artificial intelligence has begun to play a critical role in basic science research. In high energy physics, AI methods can aid precision measurements that elucidate the underlying structure of matter, such as measurements of the mass of the top quark. Top quarks can be produced only in collisions at high energy particle accelerators. Most collisions, however, do not produce top quarks and making precise measurements requires culling these collisions into a sample that is rich in collisions producing top quarks (signal) and spare in collisions producing other particles (background). Collision selection is typically performed with heuristics or supervised learning methods. However, such approaches are suboptimal because they assume that the selector with the highest classification accuracy will yield a mass measurement with the smallest statistical uncertainty. In practice, however, the mass measurement is more sensitive to some backgrounds than others. This paper presents a new approach that uses stochastic optimization techniques to directly search for selectors that minimize statistical uncertainty in the top quark mass measurement. Empirical results confirm that stochastically optimized selectors have much smaller uncertainty. This new approach contributes substantially to our knowledge of the top quark's mass, as the new selectors are currently in use selecting real collisions.\n\nTitle: Empirical Studies in Action Selection with Reinforcement Learning\nContent: To excel in challenging tasks, intelligent agents need sophisticated mechanisms for action selection: they need policies that dictate what action to take in each situation. Reinforcement learning (RL) algorithms are designed to learn such policies given only positive and negative rewards. Two contrasting approaches to RL that are currently in popular use are temporal difference (TD) methods, which learn value functions, and evolutionary methods, which optimize populations of candidate policies. Both approaches have had practical successes but few studies have directly compared them. Hence, there are no general guidelines describing their relative strengths and weaknesses. In addition, there has been little cross-collaboration, with few attempts to make them work together or to apply ideas from one to the other. In this article we aim to address these shortcomings via three empirical studies that compare these methods and investigate new ways of making them work together.First, we compare the two approaches in a benchmark task and identify variations of the task that isolate factors critical to the performance of each method. Second, we investigate ways to make evolutionary algorithms excel at on-line tasks by borrowing exploratory mechanisms traditionally used by TD methods. We present empirical results demonstrating a dramatic performance improvement. Third, we explore a novel way of making evolutionary and TD methods work together by using evolution to automatically discover good representations for TD function approximators. We present results demonstrating that this novel approach can outperform both TD and evolutionary methods alone.\n\nTitle: Automatic Feature Selection for Model-Based Reinforcement Learning in Factored MDPs\nContent: Feature selection is an important challenge in machine learning. Unfortunately, most methods for automating feature selection are designed for supervised learning tasks and are thus either inapplicable or impractical for reinforcement learning. This paper presents a new approach to feature selection specifically designed for the challenges of reinforcement learning. In our method, the agent learns a model, represented as a dynamic Bayesian network, of a factored Markov decision process, deduces a minimal feature set from this network, and efficiently computes a policy on this feature set using dynamic programming methods. Experiments in a stock-trading benchmark task demonstrate that this approach can reliably deduce minimal feature sets and that doing so can substantially improve performance and reduce the computational expense of planning.\n\nTitle: Automatic feature selection in neuroevolution\nContent: Feature selection is the process of finding the set of inputs to a machine learning algorithm that will yield the best performance. Developing a way to solve this problem automatically would make current machine learning methods much more useful. Previous efforts to automate feature selection rely on expensive meta-learning or are applicable only when labeled training data is available. This paper presents a novel method called FS-NEAT which extends the NEAT neuroevolution method to automatically determine an appropriate set of inputs for the networks it evolves. By learning the network's inputs, topology, and weights simultaneously, FS-NEAT addresses the feature selection problem without relying on meta-learning or labeled data. Initial experiments in an autonomous car racing simulation demonstrate that FS-NEAT can learn better and faster than regular NEAT. In addition, the networks it evolves are smaller and require fewer inputs. Furthermore, FS-NEAT's performance remains robust even as the feature selection task it faces is made increasingly difficult.\n\nTitle: On-line evolutionary computation for reinforcement learning in stochastic domains\nContent: In reinforcement learning, an agent interacting with its environment strives to learn a policy that specifies, for each state it may encounter, what action to take. Evolutionary computation is one of the most promising approaches to reinforcement learning but its success is largely restricted to off-line scenarios. In on-line scenarios, an agent must strive to maximize the reward it accrues while it is learning. Temporal difference (TD) methods, another approach to reinforcement learning, naturally excel in on-line scenarios because they have selection mechanisms for balancing the need to search for better policies exploration) with the need to accrue maximal reward (exploitation). This paper presents a novel way to strike this balance in evolutionary methods by borrowing the selection mechanisms used by TD methods to choose individual actions and using them in evolution to choose policies for evaluation. Empirical results in the mountain car and server job scheduling domains demonstrate that these techniques can substantially improve evolution's on-line performance in stochastic domains.\n\n## Task\nCompose an abstract for the title 'Machine learning for event selection in high energy physics' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The field of high energy physics aims to discover the underlying structure of matter by searching for and studying exotic particles, such as the top quark and Higgs boson, produced in collisions at modern accelerators. Since such accelerators are extraordinarily expensive, extracting maximal information from the resulting data is essential. However, most accelerator events do not produce particles of interest, so making effective measurements requires event selection, in which events producing particles of interest (signal) are separated from events producing other particles (background). This article studies the use of machine learning to aid event selection. First, we apply supervised learning methods, which have succeeded previously in similar tasks. However, they are suboptimal in this case because they assume that the selector with the highest classification accuracy will yield the best final analysis; this is not true in practice, as such analyses are more sensitive to some backgrounds than others. Second, we present a new approach that uses stochastic optimization techniques to directly search for selectors that maximize either the precision of top quark mass measurements or the sensitivity to the presence of the Higgs boson. Empirical results confirm that stochastically optimized selectors result in substantially better analyses. We also describe a case study in which the best selector is applied to real data from the Fermilab Tevatron accelerator, resulting in the most precise top quark mass measurement of this type to date. Hence, this new approach to event selection has already contributed to our knowledge of the top quark's mass and our understanding of the larger questions upon which it sheds light."
      }
    ]
  },
  {
    "tid":"314",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Exploiting criticality to reduce bottlenecks in distributed uniprocessors\nContent: Composable multicore systems merge multiple independent cores for running sequential single-threaded workloads. The performance scalability of these systems, however, is limited due to partitioning overheads. This paper addresses two of the key performance scalability limitations of composable multicore systems. We present a critical path analysis revealing that communication needed for cross-core register value delivery and fetch stalls due to misspeculation are the two worst bottlenecks that prevent efficient scaling to a large number of fused cores. To alleviate these bottlenecks, this paper proposes a fully distributed framework to exploit criticality in these architectures at different granularities. A coordinator core exploits different types of block-level communication criticality information to fine-tune critical instructions at decode and register forward pipeline stages of their executing cores. The framework exploits the fetch criticality information at a coarser granularity by reissuing all instructions in the blocks previously fetched into the merged cores. This general framework reduces competing bottlenecks in a synergic manner and achieves scalable performance/power efficiency for sequential programs when running across a large number of cores.\n\nTitle: Author retrospective for a NUCA substrate for flexible CMP cache sharing\nContent: In 2005, as chip multiprocessors started to appear widely, it became possible for the on-chip cores to share the last-level cache. At the time, architects either considered the last-level cache to be divided into per-core private segments, or wholly shared. The shared cache utilized the capacity more efficiency but suffered from high, uniform latencies. This paper proposed a new direction: allowing the caches to be non-uniform, with a varying number of processors sharing each section of the cache. Sharing degree, the number of cores sharing a last-level cache, determines the level of replication in on-chip caches and also affects the capacity and latency for each shared cache. Building on our previous work that introduced non-uniform cache architectures (NUCA), this study explored the design space for shared multi-core caches, focusing on the effect of sharing degree. Our observation of a per-application optimal sharing degree led to a static NUCA design with a reconfigurable sharing degree. This work in multicore NUCA cache architectures has been influential in contemporary systems, including the level-3 cache in the IBM Power 7 and Power 8 processors.\n\nTitle: Exploiting ILP, TLP, and DLP with the polymorphous TRIPS architecture\nContent: This paper describes the polymorphous TRIPS architecture which can be configured for different granularities and types of parallelism. TRIPS contains mechanisms that enable the processing cores and the on-chip memory system to be configured and combined in different modes for instruction, data, or thread-level parallelism. To adapt to small and large-grain concurrency, the TRIPS architecture contains four out-of-order, 16-wide-issue Grid Processor cores, which can be partitioned when easily extractable fine-grained parallelism exists. This approach to polymorphism provides better performance across a wide range of application types than an approach in which many small processors are aggregated to run workloads with irregular parallelism. Our results show that high performance can be obtained in each of the three modes--ILP, TLP, and DLP-demonstrating the viability of the polymorphous coarse-grained approach for future microprocessors.\n\nTitle: TRIPS: A polymorphous architecture for exploiting ILP, TLP, and DLP\nContent: This paper describes the polymorphous TRIPS architecture that can be configured for different granularities and types of parallelism. The TRIPS architecture is the first in a class of post-RISC, dataflow-like instruction sets called explicit data-graph execution (EDGE). This EDGE ISA is coupled with hardware mechanisms that enable the processing cores and the on-chip memory system to be configured and combined in different modes for instruction, data, or thread-level parallelism. To adapt to small and large-grain concurrency, the TRIPS architecture prototype contains two out-of-order, 16-wide-issue grid processor cores, which can be partitioned when easily extractable fine-grained parallelism exists. This approach to polymorphism provides better performance across a wide range of application types than an approach in which many small processors are aggregated to run workloads with irregular parallelism. Our results show that high performance can be obtained in each of the three modes---ILP, TLP, and DLP---demonstrating the viability of the polymorphous coarse-grained approach for future microprocessors.\n\nTitle: Implementation and Evaluation of a Dynamically Routed Processor Operand Network\nContent: Microarchitecturally integrated on-chip networks, or micronets, are candidates to replace busses for processor component interconnect in future processor designs. For micronets, tight coupling between processor microarchitecture and network architecture is one of the keys to improving processor performance. This paper presents the design, implementation and evaluation of the TRIPS operand network (OPN). The TRIPS OPN is a 5x5, dynamically routed, 2D mesh micronet that is integrated into the TRIPS microprocessor core. The TRIPS OPN is used for operand passing, register file I/O, and primary memory system I/O. We discuss in detail the OPN design, including the unique features that arise from its integration with the processor core, such as its connection to the execution unit's wakeup pipeline and its in flight mis-speculated traffic removal. We then evaluate the performance of the network under synthetic and realistic loads. Finally, we assess the processor performance implications of OPN design decisions with respect to the end-to-end latency of OPN packets and the OPN's bandwidth.\n\n## Task\nCompose an abstract for the title 'Arbitrary Modulus Indexing' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Modern high performance processors require memory systems that can provide access to data at a rate that is well matched to the processor's computation rate. Common to such systems is the organization of memory into local high speed memory banks that can be accessed in parallel. Associative look up of values is made efficient through indexing instead of associative memories. These techniques lose effectiveness when data locations are not mapped uniformly to the banks or cache locations, leading to bottlenecks that arise from excess demand on a subset of locations. Address mapping is most easily performed by indexing the banks using a mod (2 N) indexing scheme, but such schemes interact poorly with the memory access patterns of many computations, making resource conflicts a significant memory system bottleneck. Previous work has assumed that prime moduli are the best choices to alleviate conflicts and has concentrated on finding efficient implementations for them. In this paper, we introduce a new scheme called Arbitrary Modulus Indexing (AMI) that can be implemented efficiently for all moduli, matching or improving the efficiency of the best existing schemes for primes while allowing great flexibility in choosing a modulus to optimize cost/performance trade-offs. We also demonstrate that, for a memory-intensive workload on a modern replay-style GPU architecture, prime moduli are not in general the best choices for memory bank and cache set mappings. Applying AMI to set of memory intensive benchmarks eliminates 98% of bank and set conflicts, resulting in an average speedup of 24% over an aggressive baseline system and a 64% average reduction in memory system replays at reasonable implementation cost."
      }
    ]
  },
  {
    "tid":"315",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: An Algorithm for Multi-Criteria Optimization in CSPs\nContent: Constraint Satisfaction and Optimization are important areas of Artificial Intelligence. However, in many real-life applications, more functions should be optimized at the same time; the user needs to be provided a set of solutions and a posteriori choose the most preferable. In this paper, we propose an algorithm for solving Multi-Criteria Optimization problems in this setting. The algorithm is complete, i.e., it finds all the non-dominated solutions, and does not make any assumption on the structure of the constraints nor on the type of the objective functions. It exploits Point Quad-Trees for the representation of the non-dominated frontier, in order to efficiently access the data. We describe the implementation and give experimental results showing that our algorithm outperforms widely used methods.\n\nTitle: Integration of Abductive Reasoning and Constraint Optimization in SCIFF\nContent: Abductive Logic Programming (ALP) and Constraint Logic Programming (CLP) share the feature to constrain the set of possible solutions to a program via integrity or CLP constraints. These two frameworks have been merged in works by various authors, who developed efficient abductive proof-procedures empowered with constraint satisfaction techniques. However, while almost all CLP languages provide algorithms for finding an optimal solution with respect to some objective function (and not just any solution), the issue has received little attention in ALP. In this paper we show how optimisation meta-predicates can be included in abductive proof-procedures, achieving in this way a significant improvement to research and practical applications of abductive reasoning. In the paper, we give the declarative and operational semantics of an abductive proof-procedure that encloses constraint optimization meta-predicates, and we prove soundness in the three-valued completion semantics. In the proof-procedure, the abductive logic program can invoke optimisation meta-predicates, which can invoke abductive predicates, in a recursive way.\n\nTitle: A CHR-based implementation of known arc-consistency\nContent: In classical CLP(FD) systems, domains of variables are completely known at the beginning of the constraint propagation process. However, in systems interacting with an external environment, acquiring the whole domains of variables before the beginning of constraint propagation may cause waste of computation time, or even obsolescence of the acquired data at the time of use. For such cases, the Interactive Constraint Satisfaction Problem (ICSP) model has been proposed (Cucchiara et al. 1999a) as an extension of the CSP model, to make it possible to start constraint propagation even when domains are not fully known, performing acquisition of domain elements only when necessary, and without the need for restarting the propagation after every acquisition. In this paper, we show how a solver for the two sorted CLP language, defined in previous work (Gavanelli et al. 2005) to express ICSPs, has been implemented in the Constraint Handling Rules (CHR) language, a declarative language particularly suitable for high level implementation of constraint solvers.\n\nTitle: RUNTIME ADDITION OF INTEGRITY CONSTRAINS IN AN ABDUCTIVE PROOF PROCEDURE\nContent: Abductive Logic Programming is a computationally founded representation of abductive reasoning. In most ALP frameworks, integrity constraints express domain-specific logical relationships that abductive answers are required to satisfy. Integrity constraints are usually known a priori. However, in some applications (such as interactive abductive logic programming, multi-agent interactions, contracting) it makes sense to relax this assumption, in order to let the abductive reasoning start with incomplete knowledge of integrity constraints, and to continue without restarting when new integrity constraints become known. In this paper, we propose a declarative semantics for abductive logic programming with addition of integrity constraints during the abductive reasoning process, an operational instantiation (with formal termination, soundness and completeness properties) and an implementation of such a framework based on the SCIFF language and proof procedure.\n\nTitle: Constraint Propagation and Value Acquisition: Why we should do it Interactively\nContent: In Constraint Satisfaction Problems (CSPs) values belonging to variable domains should be completely known before the constraint propagation process starts. In many applications, however, the acquisition of domain values is a computational expensive process or some domain values could not be available at the beginning of the computation. For this purpose, we introduce an Interactive Constraint Satisfaction Problem (ICSP) model as extension of the widely used CSP model. The variable domain values can be acquired when needed during the resolution process by means of Interactive Constraints, which retrieve (possibly consistent) information. Experimental results on randomly generated CSPs and for 3D object recognition show the effectiveness of the proposed approach.\n\n## Task\nCompose an abstract for the title 'Partially Ordered Constraint Optimization Problems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The Constraint Problems usually addressed fall into one of two models: the Constraint Satisfaction Problem (CSP) and the Constraint Op- timization Problem (COP). However, in many real-life applications, more functions should be optimized at the same time, and solutions are ranked by means of a Partial Order. Also, when dealing with Null values, the solution set is ranked through a Partial Order, and only the non dominated solutions are interesting for the user. For example, in 3D visual recognition, from any viewpoint there are always hidden features: assignments that are less expressive (that take more Null values than others) should be discarded. For these reasons, we propose the model of Partially-Ordered Constraint Optimization Problem and provide an algorithm for solving it, derived from the widely used Branch and Bound."
      }
    ]
  },
  {
    "tid":"316",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Joint Multiple Target Tracking and Classification Using Controlled Based Cheap JPDA-Multiple Model Particle Filter in Cluttered Environment\nContent: In this paper, we address the problem of jointly tracking and classifying several targets in cluttered environment. It is assumed that the motion model of a given target belongs to one of several classes. We propose to use the multiple model particle filter (MMPF) to perform nonlinear filtering with switching dynamic models. Moreover, the principle of joint probabilistic data association (JPDA) is used to determine the measurements origin. Besides, the joint probabilities are calculated using Fitzgerald's had hoc formulation (Cheap JPDA) whose efficiency has been proven in the literature. On the other hand, a controller based on the quality of the innovation has been implemented in order to tune the number of particles. The feasibility and the performances of the proposal have been demonstrated using a set of Monte Carlo simulations dealing with two maneuvering targets.\n\nTitle: Possibilistic Kalman filtering for radar 2D tracking\nContent: Standard Kalman filter (SKF) introduced by Kalman in the 60s has gained a non-estimated importance in control as well as in robotics community. Its importance arises from the obtained optimal result in the sense of variance minimization under stochastic, Gaussian and unbiased perturbations, and when the state model as well as the measurement model are precisely known. However, when the last requirement is relaxed such that one or more parameters governing the models are ill-defined and rather given in terms of interval evaluations, Chen et al. (IEEE Trans. Aerospace Electr. Syst. 33 (1) (1997) 251-259) have proposed Interval Kalman Filter (IKF) by extending the arithmetic operations to interval calculus. In this paper, we rather assume that the uncertainty pervading some parameters of the models are given in terms of possibility distributions [21]. This leads to a formulation of possibilistic Kalman Filtering (PKF), which agrees with IKF. The same example of 2D-radar tracking is tackled. Comparisons with IKF are investigated as well the influence of the modelling process on the performance of the filter. Besides, the proposal permits to capture certainty qualified information, which cannot be obtained from IKF. (C) 2000 Elsevier Science Inc. All rights reserved.\n\nTitle: New fusion methodology approach and application to mobile robotics: investigation in the framework of possibility theory\nContent: This paper describes a new fusion methodology based on the possibility theory applied to a robotics application. A new combination rule called the progressive rule, which is a refinement of the adaptive rule proposed by Dubois and Prade in order to tackle robustness with respect to shape modelling and handle consistently erroneous data, is put forward. The proposed rule explicitly accounts for the distance between each alternative and the consensus zone. The application deals with a mobile robot equipped with ultrasonic sensors and odometry. The fusion method consists in combining consistently odometric reading and ultrasonic measurements in a structured environment. This fusion methodology is elaborated and implemented in three steps. At the alignment step, the measurements are translated into the same temporal, spatial and attribute frame using the environmental knowledge. At the association and correlation step, the data tracking problems in the sense of measurement-origin are carried out. Finally, at the combination step, the resulting homogeneous data are combined using the progressive rule.\n\nTitle: Track-To-Track Measurement Fusion Architectures and Correlation Analysis\nContent: The purpose of this paper is to address some theoretical issues related to the track-to-track fusion problem when the measurements tracking the same target are inherently correlated by the common process noise of the underlying target. This problem has been intensively investigated using standard Kalman filter with some appealing theoretical results, however such results are no longer valid in case of suboptimality due to either the presence of strong nonlinearity or to the discrete uncertainty pervading the origin of the measurement. This paper reviews several architectures of parallelized blocks of Kalman filters, including the augmented stacked measurement, sequential and data compression architectures. Next, convex combination architecture will be investigated and some theoretical results concerning its extension as well as in case of presence of correlation are investigated. Two special cases of correlation are highlighted. This concerns the case of presence of only two correlated tracks among all tracks and the case of weak correlation. In both cases some original theoretical results are put forward. Finally, links with related fusion architectures is investigated.\n\nTitle: Robust Fuzzy Linear Regression And Application For Contact Identification\nContent: Linear regression has been used for a long time in various applications, with linear least squares as the best known tool. It is also known in probability/statistics theory that this method is particularly sensitive to outliers. Hence, statisticians introduced robust statistical tools that allow robustness and efficiency to overcome the effects of outlier. On the other hand, Tanaka and Hayashi provide basic ideas for fuzzy linear regression when data are rather ill-known and given in terms of fuzzy sets, even if the robustness of the method in the presence of outliers still is poor and over-estimated. This paper attempts to provide a hybrid approach by combining! both robust statistical tools and the fuzzy approach. Particularly, the least median of squares estimator and the least trimmed squares estimator have been considered. The method is then tested in a robotics application where a force-controlled contact situation is assessed.\n\n## Task\nCompose an abstract for the title 'Hybrid fuzzy probabilistic data association filter and joint probabilistic data association filter' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Multitarget tracking problems are theoretically interesting because, unlike other estimation problems, the origins of the measurements are not identified. This involves hypothesis generation and their evaluation in terms of degree of agreement between the given measurements and the underlying tracks. Typical algorithms to deal with such problems are the probabilistic data association filter (PDAF) in the case of single target tracking and joint probabilistic data association filter (JPDAF) in the case of multiple target tracking proposed by Bar-Shalom and his team. The basis of JPDAF is the calculus of the joint probabilities over all targets and hits. The algorithm assigns weights for reasonable hits and uses a weighted centroid of those hits to update the track. In this paper, we propose a new weight assignment based on fuzzy c-means methodology. Particularly, in order to take account for the false alarms (clutter) where none of the measurements is target originated, a new noisy fuzzy c-means algorithm is elaborated. The latter contrasts with that provided by Dave regarding the location of the noise prototype as well as the meaning of the universality of the noise class. The treatment of conflictual situations where, for instance, more than one hit fail in a target extension gate is accomplished using some weighted based procedure with respect to all feasible joint matrices involved in the construction of joint probabilities in JPDAF. In the meantime, the general methodology of PDAF and JPDAF remains unchanged. This leads to Hybrid Fuzzy PDAF in the case of single target tracking and Hybrid Fuzzy JPDAF in the case of multiple target tracking. This investigation shows a fruitful combination between fuzzy and probabilistic approaches in order to accomplish target tracking tasks."
      }
    ]
  },
  {
    "tid":"317",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Efficient Public-Key Cryptography in the Presence of Key Leakage.\nContent: We study the design of cryptographic primitives resistant to a large class of side-channel attacks, called \"memory attacks\", where an attacker can repeatedly and adaptively learn information about the secret key, subject only to the constraint that the overall amount of such information is bounded by some parameter l. Although the study of such primitives was initiated only recently by Akavia et al. [2], subsequent work already produced many such \"leakage-resilient\" primitives [48,4,42], including signature, encryption, identification (ID) and authenticated key agreement (AKA) schemes. Unfortunately, every existing scheme,-for any of the four fundamental primitives above,-fails to satisfy at least one of the following desirable properties: Efficiency. While the construction may be generic, it should have some efficient instantiations, based on standard cryptographic assumptions, and without relying on random oracles. Strong Security. The construction should satisfy the strongest possible definition of security (even in the presence of leakage). For example, encryption schemes should be secure against chosen ciphertext attack (CCA), while signatures should be existentially unforgeable. Leakage Flexibility. It should be possible to set the scheme parameters so that the leakage bound l can come arbitrarily close to the secret-key size. In this work we design the first signature, encryption, ID and AKA schemes which overcome these limitations, and satisfy all the properties above. Moreover, all our constructions are generic, in several cases elegantly simplifying and generalizing the prior constructions (which did not have any efficient instantiations). We also introduce several tools of independent interest, such as the abstraction (and constructions) of true-simulation extractable NIZK arguments, and a new deniable DH-based AKA protocol based on any CCA-secure encryption.\n\nTitle: Salvaging Merkle-Damg\u00e5rd for Practical Applications\nContent: Many cryptographic applications of hash functions are analyzed in the random oracle model. Unfortunately, most concrete hash functions, including the SHA family, use the iterative (strengthened) Merkle-Damg\u00e5rd transform applied to a corresponding compression function. Moreover, it is well known that the resulting \"structured\" hash function cannot be generically used as a random oracle, even if the compression function is assumed to be ideal. This leaves a large disconnect between theory and practice: although no attack is known for many concrete applications utilizing existing (Merkle-Damg\u00e5rd based) hash functions, there is no security guarantee either, even by idealizing the compression function. Motivated by this question, we initiate a rigorous and modular study of developing new notions of (still idealized) hash functions which would be (a) natural and elegant; (b) sufficient for arguing security of important applications; and (c) provably met by the (strengthened) Merkle-Damg\u00e5rd transform, applied to a \"strong enough\" compression function. In particular, we develop two such notions satisfying (a)-(c): a preimage aware function ensures that the attacker cannot produce a \"useful\" output of the function without already \"knowing\" the corresponding preimage, and a public-use random oracle , which is a random oracle that reveals to attackers messages queried by honest parties.\n\nTitle: On the Indifferentiability of Key-Alternating Ciphers.\nContent: The Advanced Encryption Standard (AES) is the most widely used block cipher. The high level structure of AES can be viewed as a (10-round) key-alternating cipher, where a t-round key-alternating cipher KA(t) consists of a small number t of fixed permutations P-i on n bits, separated by key addition: KA(t)(K, m) = kt circle plus P-t(... k(2) circle plus P-2(k(1) circle plus P-1(k(0) circle plus m)) ...), where (k(0), ..., k(t)) are obtained from the master key K using some key derivation function. For t = 1, KA(1) collapses to the well-known Even-Mansour cipher, which is known to be indistinguishable from a (secret) random permutation, if P-1 is modeled as a (public) random permutation. In this work we seek for stronger security of key-alternating ciphers - indifferentiability from an ideal cipher - and ask the question under which conditions on the key derivation function and for how many rounds t is the key-alternating cipher KA(t) indifferentiable from the ideal cipher, assuming P-1, ..., P-t are (public) random permutations? As our main result, we give an affirmative answer for t = 5, showing that the 5-round key-alternating cipher KA(5) is indifferentiable from an ideal cipher, assuming P-1, ..., P-5 are five independent random permutations, and the key derivation function sets all rounds keys k(i) = f(K), where 0 <= i <= 5 and f is modeled as a random oracle. Moreover, when vertical bar K vertical bar = vertical bar m vertical bar, we show we can set f(K) = P-0(K) circle plus K, giving an n-bit block cipher with an n-bit key, making only six calls to n-bit permutations P-0, P-1, P-2, P-3, P-4, P-5.\n\nTitle: Non-malleable extractors and symmetric key cryptography from weak secrets\nContent: We study the question of basing symmetric key cryptography on weak secrets. In this setting, Alice and Bob share an n-bit secret W, which might not be uniformly random, but the adversary has at least k bits of uncertainty about it (formalized using conditional min-entropy). Since standard symmetric-key primitives require uniformly random secret keys, we would like to construct an authenticated key agreement protocol in which Alice and Bob use W to agree on a nearly uniform key R, by communicating over a public channel controlled by an active adversary Eve. We study this question in the information theoretic setting where the attacker is computationally unbounded. We show that single-round (i.e. one message) protocols do not work when k \u2264 n/2, and require poor parameters even when n/2 On the other hand, for arbitrary values of k, we design a communication efficient two-round (challenge-response) protocol extracting nearly k random bits. This dramatically improves the previous construction of Renner and Wolf [32], which requires \u0398(\u03bb + log(n)) rounds where \u03bb is the security parameter. Our solution takes a new approach by studying and constructing \"non-malleable\" seeded randomness extractors -- if an attacker sees a random seed X and comes up with an arbitrarily related seed X', then we bound the relationship between R= Ext(W;X) and R' = Ext(W;X'). We also extend our two-round key agreement protocol to the \"fuzzy\" setting, where Alice and Bob share \"close\" (but not equal) secrets WA and WB, and to the Bounded Retrieval Model (BRM) where the size of the secret W is huge.\n\nTitle: How to Eat Your Entropy and Have It Too - Optimal Recovery Strategies for Compromised RNGs.\nContent: Random number generators (RNGs) play a crucial role in many cryptographic schemes and protocols, but their security proof usually assumes that their internal state is initialized with truly random seeds and remains secret at all times. However, in many practical situations these are unrealistic assumptions: The seed is often gathered after a reset/reboot from low entropy external events such as the timing of manual key presses, and the state can be compromised at unknown points in time via side channels or penetration attacks. The usual remedy (used by all the major operating systems, including Windows, Linux, FreeBSD, MacOS, iOS, etc.) is to periodically replenish the internal state through an auxiliary input with additional randomness harvested from the environment. However, recovering from such attacks in a provably correct and computationally optimal way had remained an unsolved challenge so far.In this paper we formalize the problem of designing an efficient recovery mechanism from state compromise, by considering it as an online optimization problem. If we knew the timing of the last compromise and the amount of entropy gathered since then, we could stop producing any outputs until the state becomes truly random again. However, our challenge is to recover within a time proportional to this optimal solution even in the hardest (and most realistic) case in which (a) we know nothing about the timing of the last state compromise, and the amount of new entropy injected since then into the state, and (b) any premature production of outputs leads to the total loss of all the added entropy used by the RNG, since the attacker can use brute force to enumerate all the possible low-entropy states. In other words, the challenge is to develop recovery mechanisms which are guaranteed to save the day as quickly as possible after a compromise we are not even aware of. The dilemma that we face is that any entropy used prematurely will be lost, and any entropy which is kept unused will delay the recovery.After developing our formal definitional framework for RNGs with inputs, we show how to construct a nearly optimal RNG which is secure in our model. Our technique is inspired by the design of the Fortuna RNG (which is a heuristic RNG construction that is currently used by Windows and comes without any formal analysis), but we non-trivially adapt it to our much stronger adversarial setting. Along the way, our formal treatment of Fortuna enables us to improve its entropy efficiency by almost a factor of two, and to show that our improved construction is essentially tight, by proving a rigorous lower bound on the possible efficiency of any recovery mechanism in our very general model of the problem.\n\n## Task\nCompose an abstract for the title 'Key Derivation Without Entropy Waste.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We revisit the classical problem of converting an imperfect source of randomness into a usable cryptographic key. Assume that we have some cryptographic application P that expects a uniformly random m-bit key R and ensures that the best attack (in some complexity class) against P(R) has success probability at most delta. Our goal is to design a key-derivation function (KDF) h that converts any random source X of min-entropy k into a sufficiently \"good\" key h(X), guaranteeing that P(h(X)) has comparable security delta' which is 'close' to delta. Seeded randomness extractors provide a generic way to solve this problem for all applications P, with resulting security delta' = 0(6), provided that we start with entropy k >= m + 2 log (1/delta) - O(1). By a result of Radhakrishnan and Ta-Shma, this bound on k (called the \"RT-bound\") is also known to be tight in general. Unfortunately, in many situations the loss of 2 log (1/delta) bits of entropy is unacceptable. This motivates the study KDFs with less entropy waste by placing some restrictions on the source X or the application P. In this work we obtain the following new positive and negative results in this regard: Efficient samplability of the source X does not help beat the RT-bound for general applications. This resolves the SRI (samplable RT) conjecture of Dachman-Soled et al. [DGKM12] in the affirmative, and also shows that the existence of computationally-secure extractors beating the RT-bound implies the existence of one-way functions. We continue in the line of work initiated by Barak et al. [BDK+11] and construct new information-theoretic KDFs which beat the RIbound for large but restricted classes of applications. Specifically, we design efficient KDFs that work for all unpredictability applications P (e.g., signatures, MACs, one-way functions, etc.) and can either: (1) extract all of the entropy k = m with a very modest security loss delta' = 0(delta center dot log (1/delta)), or alternatively, (2) achieve essentially optimal security delta' = 0(delta) with a very modest entropy loss k >= m + loglog (1/delta). In comparison, the best prior results from [BDK(+)11] for this class of applications would only guarantee delta' = O(root/delta) when k = m, and would need k >= m + log (1/delta) to get delta' = O(delta). The weaker bounds of [BDK(+)11] hold for a larger class of so-called \"square-friendly\" applications (which includes all unpredictability, but also some important indistinguishability, applications). Unfortunately, we show that these weaker bounds are tight for the larger class of applications. We abstract out a clean, information-theoretic notion of (k,delta,delta')'unpredictability extractors, which guarantee \"induced\" security delta' for any delta-secure unpredictability application P, and characterize the parameters achievable for such unpredictability extractors. Of independent interest, we also relate this notion to the previously-known notion of (min-entropy) condensers, and improve the state-of-the-art parameters for such condensers."
      }
    ]
  },
  {
    "tid":"318",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Optimal Neighborhood Kernel Clustering with Multiple Kernels.\nContent: Multiple kernel k-means (MKKM) aims to improve clustering performance by learning an optimal kernel, which is usually assumed to be a linear combination of a group of pre-specified base kernels. However, we observe that this assumption could: i) cause limited kernel representation capability; and ii) not sufficiently consider the negotiation between the process of learning the optimal kernel and that of clustering, leading to unsatisfying clustering performance. To address these issues, we propose an optimal neighborhood kernel clustering (ONKC) algorithm to enhance the representability of the optimal kernel and strengthen the negotiation between kernel learning and clustering. We theoretically justify this ONKC by revealing its connection with existing MKKM algorithms. Furthermore, this justification shows that existing MKKM algorithms can be viewed as a special case of our approach and indicates the extendability of the proposed ONKC for designing better clustering algorithms. An efficient algorithm with proved convergence is designed to solve the resultant optimization problem. Extensive experiments have been conducted to evaluate the clustering performance of the proposed algorithm. As demonstrated, our algorithm significantly outperforms the state-of-the-art ones in the literature, verifying the effectiveness and advantages of ONKC.\n\nTitle: Multiple Kernel k-Means Clustering with Matrix-Induced Regularization.\nContent: Multiple kernel k-means ( MKKM) clustering aims to optimally combine a group of pre-specified kernels to improve clustering performance. However, we observe that existing MKKM algorithms do not sufficiently consider the correlation among these kernels. This could result in selecting mutually redundant kernels and affect the diversity of information sources utilized for clustering, which finally hurts the clustering performance. To address this issue, this paper proposes an MKKM clustering with a novel, effective matrix-induced regularization to reduce such redundancy and enhance the diversity of the selected kernels. We theoretically justify this matrix-induced regularization by revealing its connection with the commonly used kernel alignment criterion. Furthermore, this justification shows that maximizing the kernel alignment for clustering can be viewed as a special case of our approach and indicates the extendability of the proposed matrix-induced regularization for designing better clustering algorithms. As experimentally demonstrated on five challenging MKL benchmark data sets, our algorithm significantly improves existing MKKM and consistently out-performs the state-of-the-art ones in the literature, verifying the effectiveness and advantages of incorporating the proposed matrix-induced regularization.\n\nTitle: Multiple Kernel Clustering with Local Kernel Alignment Maximization.\nContent: Kernel alignment has recently been employed for multiple kernel clustering (MKC). However, we find that most of existing works implement this alignment in a global manner, which: i) indiscriminately forces all sample pairs to be equally aligned with the same ideal similarity; and ii) is inconsistent with a well-established concept that the similarity evaluated for two farther samples in a high dimensional space is less reliable. To address these issues, this paper proposes a novel MKC algorithm with a \\\"local\\\" kernel alignment, which only requires that the similarity of a sample to its k-nearest neighbours be aligned with the ideal similarity matrix. Such an alignment helps the clustering algorithm to focus on closer sample pairs that shall stay together and avoids involving unreliable similarity evaluation for farther sample pairs. We derive a new optimization problem to implement this idea, and design a two-step algorithm to efficiently solve it. As experimentally demonstrated on six challenging multiple kernel learning benchmark data sets, our algorithm significantly outperforms the state-of-the-art comparable methods in the recent literature, verifying the effectiveness and superiority of maximizing local kernel alignment.\n\nTitle: An Efficient Approach to Integrating Radius Information into Multiple Kernel Learning.\nContent: Integrating radius information has been demonstrated by recent work on multiple kernel learning (MKL) as a promising way to improve kernel learning performance. Directly integrating the radius of the minimum enclosing ball (MEB) into MKL as it is, however, not only incurs significant computational overhead but also possibly adversely affects the kernel learning performance due to the notorious sensitivity of this radius to outliers. Inspired by the relationship between the radius of the MEB and the trace of total data scattering matrix, this paper proposes to incorporate the latter into MKL to improve the situation. In particular, in order to well justify the incorporation of radius information, we strictly comply with the radius-margin bound of support vector machines (SVMs) and thus focus on the <?TeX l(2)?>-norm soft-margin SVM classifier. Detailed theoretical analysis is conducted to show how the proposed approach effectively preserves the merits of incorporating the radius of the MEB and how the resulting optimization is efficiently solved. Moreover, the proposed approach achieves the following advantages over its counterparts: 1) more robust in the presence of outliers or noisy training samples; 2) more computationally efficient by avoiding the quadratic optimization for computing the radius at each iteration; and 3) readily solvable by the existing off-the-shelf MKL packages. Comprehensive experiments are conducted on University of California, Irvine, protein subcellular localization, and Caltech-101 data sets, and the results well demonstrate the effectiveness and efficiency of our approach.\n\nTitle: An Adaptive Approach to Learning Optimal Neighborhood Kernels.\nContent: Learning an optimal kernel plays a pivotal role in kernel-based methods. Recently, an approach called optimal neighborhood kernel learning (ONKL) has been proposed, showing promising classification performance. It assumes that the optimal kernel will reside in the neighborhood of a \"pre-specified\" kernel. Nevertheless, how to specify such a kernel in a principled way remains unclear. To solve this issue, this paper treats the pre-specified kernel as an extra variable and jointly learns it with the optimal neighborhood kernel and the structure parameters of support vector machines. To avoid trivial solutions, we constrain the pre-specified kernel with a parameterized model. We first discuss the characteristics of our approach and in particular highlight its adaptivity. After that, two instantiations are demonstrated by modeling the pre-specified kernel as a common Gaussian radial basis function kernel and a linear combination of a set of base kernels in the way of multiple kernel learning (MKL), respectively. We show that the optimization in our approach is a min-max problem and can be efficiently solved by employing the extended level method and Nesterov's method. Also, we give the probabilistic interpretation for our approach and apply it to explain the existing kernel learning methods, providing another perspective for their commonness and differences. Comprehensive experimental results on 13 UCI data sets and another two real-world data sets show that via the joint learning process, our approach not only adaptively identifies the pre-specified kernel, but also achieves superior classification performance to the original ONKL and the related MKL algorithms.\n\n## Task\nCompose an abstract for the title 'Multiple Kernel Clustering With Neighbor-Kernel Subspace Segmentation.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Multiple kernel clustering (MKC) has been intensively studied during the last few decades. Even though they demonstrate promising clustering performance in various applications, existing MKC algorithms do not sufficiently consider the intrinsic neighborhood structure among base kernels, which could adversely affect the clustering performance. In this paper, we propose a simple yet effective neighbor-kernel-based MKC algorithm to address this issue. Specifically, we first define a neighbor kernel, which can be utilized to preserve the block diagonal structure and strengthen the robustness against noise and outliers among base kernels. After that, we linearly combine these base neighbor kernels to extract a consensus affinity matrix through an exact-rank-constrained subspace segmentation. The naturally possessed block diagonal structure of neighbor kernels better serves the subsequent subspace segmentation, and in turn, the extracted shared structure is further refined through subspace segmentation based on the combined neighbor kernels. In this manner, the above two learning processes can be seamlessly coupled and negotiate with each other to achieve better clustering. Furthermore, we carefully design an efficient iterative optimization algorithm with proven convergence to address the resultant optimization problem. As a by-product, we reveal an interesting insight into the exact-rank constraint in ridge regression by careful theoretical analysis: it back-projects the solution of the unconstrained counterpart to its principal components. Comprehensive experiments have been conducted on several benchmark data sets, and the results demonstrate the effectiveness of the proposed algorithm."
      }
    ]
  },
  {
    "tid":"319",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Supporting Large Persistent Stores using Conventional Hardware\nContent: Persistent programming systems are generally supported by an object store, a conceptually infinite object repository. Objects in such a repository cannot be directly accessed by user programs; to be manipulated they must be fetched from the object store into virtual memory. Thus in these systems, two different kinds of object addresses may exist: those in the object store and those in virtual memory. The action of changing object store addresses into virtual memory addresses has become known as pointer swizzling and is the subject of this paper. The paper investigates three approaches to pointer swizzling: a typical software address translation scheme, a technique for performing swizzling at page fault time and finally a new hybrid scheme which performs swizzling in two phases. The hybrid scheme supports arbitrarily large pointers and object repositories using conventional hardware. The paper concludes with a comparison of these approaches.\n\nTitle: A remote execution mechanism for distributed homogeneous stable stores\nContent: Persistent languages and systems provide the ability to create and manipulate all data in a uniform manner regardless of how long it persists. Such systems are usually implemented above a stable persistent store which supports relaible long-term storage of persistent data. In this paper we consider the issue of distribution of the persistent store across nodes. A number of existing persistent languages with support for distribution are described in terms of a taxonomy of distributed stores. It is shown that there are considerable difficulties with these systems, particularly in terms of scalability. A new mechanism based on the exportation and remote execution of procedures is then described. A key feature of this mechanism is that an exported procedure may dynamically bind to data in the remote store. It is shown that the mechanism alleviates most of the problems of existing systems and provides considerable flexibility. The paper concludes with some examples of practical use of the proposed mechanism.\n\nTitle: Autonomic management of client concurrency in a distributed storage service\nContent: A distributed autonomic system adapts its constituent components to a changing environment. This paper reports on the application of autonomic management to a distributed storage service. We developed a simple analytic model which suggested potential benefit from tuning the degree of concurrency used in data retrieval operations, to suit dynamic conditions. We then validated this experimentally by developing an autonomic manager to control the degree of concurrency. We compared the resulting data retrieval performance with non-autonomic versions, using various combinations of network capacity, membership churn and workload patterns. Overall, autonomic management yielded improved retrieval performance. It also produced a distinct but not significant increase in network usage relative to one non-autonomic configuration, and a significant reduction relative to another.\n\nTitle: Lumberjack: A Log-Structured Persistent Object Store\nContent: Lumberjack is a log-structured persistent object store intended for use with conventional operating systems such as Unix. The design draws together facets of previous work, in particular, the CPOMS object store used to support PS-algol, and the page-based log-structured store used in the Grasshopper persistent operating system. Lumberjack exhibits a novel store design that provides a number of benefits: i) an efficient commit strategy with respect to meta-data, ii) the avoidance of installation reads and writes during commit, iii) support for incremental parallel garbage collection. Additionally, log-cleaning and garbage collection may be integrated providing the ability to re-cluster data to exploit locality. We see Lumberjack as delivering object-based logging for use in systems such as Napier88 and persistent Java, thereby providing an alternative to the traditional shadow-paged stores used in these systems.\n\nTitle: Casper: A Cached Architecture Supporting Persistence\nContent: Persistent object systems greatly simplify programming tasks since they hide the traditional distinction between short-term and long-term storage from the applications programmer. As a result, the programmer can operate at a level of abstraction in which short-term and long-term data are treated uniformly. It is important that such a persistent system be capable of being used concurrently; such concurrent usage may arise because of parallel process facilities in the programming language concerned, or because of multiple users of the same persistent store. Concurrent access has not been satisfactorily supported in existing persistent store implementations and a number of significant research issues remain to be investigated. This paper describes an architecture that supports concurrent access to a shared persistent object store. The persistent distributed architecture represented by our system exploits a number of the facilities provided by the Mach distributed operating system\n\n## Task\nCompose an abstract for the title 'A Generic Storage API' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We present a generic API suitable for provision of highly generic storage facilities that can be tailored to produce various individually customised storage infrastructures. The paper identifies a candidate set of minimal storage system building blocks, which are sufficiently simple to avoid encapsulating policy where it cannot be customised by applications, and composable to build highly flexible storage architectures. Four main generic components are defined: the store, the namer , the caster and the interpreter. It is hypothesised that these are sufficiently general that they could act as building blocks for any information storage and retrieval system. The essential characteristics of each are defined by an interface, which may be implemented by multiple implementing classes . ."
      }
    ]
  },
  {
    "tid":"320",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Launching an Efficient Participatory Sensing Campaign: A Smart Mobile Device-Based Approach\nContent: Participatory sensing is a promising sensing paradigm that enables collection, processing, dissemination and analysis of the phenomena of interest by ordinary citizens through their handheld sensing devices. Participatory sensing has huge potential in many applications, such as smart transportation and air quality monitoring. However, participants may submit low-quality, misleading, inaccurate, or even malicious data if a participatory sensing campaign is not launched effectively. Therefore, it has become a significant issue to establish an efficient participatory sensing campaign for improving the data quality. This article proposes a novel five-tier framework of participatory sensing and addresses several technical challenges in this proposed framework including: (1) optimized deployment of data collection points (DC-points); and (2) efficient recruitment strategy of participants. Toward this end, the deployment of DC-points is formulated as an optimization problem with maximum utilization of sensor and then a Wise-Dynamic DC-points Deployment (WD3) algorithm is designed for high-quality sensing. Furthermore, to guarantee the reliable sensing data collection and communication, a trajectory-based strategy for participant recruitment is proposed to enable campaign organizers to identify well-suited participants for data sensing based on a joint consideration of temporal availability, trust, and energy. Extensive experiments and performance analysis of the proposed framework and associated algorithms are conducted. The results demonstrate that the proposed algorithm can achieve a good sensing coverage with a smaller number of DC-points, and the participants that are termed as social sensors are easily selected, to evaluate the feasibility and extensibility of the proposed recruitment strategies.\n\nTitle: NextMe: Localization Using Cellular Traces in Internet of Things\nContent: The Internet of Things (IoT) opens up tremendous opportunities to location-based industrial applications that leverage both Internet-resident resources and phones' processing power and sensors to provide location information. Location-based service is one of the vital applications in commercial, economic, and public domains. In this paper, we propose a novel localization scheme called NextMe, which is based on cellular phone traces. We find that the mobile call patterns are strongly correlated with the co-locate patterns. We extract such correlation as social interplay from cellular calls, and use it for location prediction from temporal and spatial perspectives. NextMe consists of data preprocessing, call pattern recognition, and a hybrid predictor. To design the call pattern recognition module, we introduce the notions of critical calls and corresponding patterns. In addition, NextMe does not require that the cell tower addresses should be bounded with concrete coordinates, e.g., global positioning system (GPS) coordinates. We validate NextMe across MIT Reality Mining Dataset, involving 500 000 h of continuous behavior information and 112 508 cellular calls. Experimental results show that NextMe achieves fine-grained prediction accuracy at cell tower level in the forthcoming 1-6 h with 12% accuracy enhancement averagely from cellular calls.\n\nTitle: Mobile agent-based energy-aware and user-centric data collection in wireless sensor networks.\nContent: We design an efficient data-gathering system in wireless sensor networks (WSNs) with mobile agents to achieve energy- and time-efficient collection as well as intelligent monitoring to adapt to the numerous demands of users. We first consider a data-gathering system called MAMS where mobile agents (MAs) and a mobile server (MS) collaboratively collect data. MAs collect data over the WSN and intelligently return this to the MS. We then develop dynamic itinerary planning approach for an MA (DIPMA) to find an optimal itinerary that provides more flexible services using widespread WSNs to users. We focus on two key challenges: (1) developing a new data-searching mechanism for making an MA\u2019s itinerary under specified requirements and (2) designing data structures with minimal information stored in sensor nodes, where an MA decides on the next destination based on the information. We validate the proposed solutions by simulation experiments and show DIPMA outperforms the random migration of MAs in terms of execution time by considering the search accuracy of nodes that detect events.\n\nTitle: MobiFuzzyTrust: An Efficient Fuzzy Trust Inference Mechanism in Mobile Social Networks\nContent: Mobile social networks (MSNs) facilitate connections between mobile users and allow them to find other potential users who have similar interests through mobile devices, communicate with them, and benefit from their information. As MSNs are distributed public virtual social spaces, the available information may not be trustworthy to all. Therefore, mobile users are often at risk since they may not have any prior knowledge about others who are socially connected. To address this problem, trust inference plays a critical role for establishing social links between mobile users in MSNs. Taking into account the nonsemantical representation of trust between users of the existing trust models in social networks, this paper proposes a new fuzzy inference mechanism, namely MobiFuzzyTrust, for inferring trust semantically from one mobile user to another that may not be directly connected in the trust graph of MSNs. First, a mobile context including an intersection of prestige of users, location, time, and social context is constructed. Second, a mobile context aware trust model is devised to evaluate the trust value between two mobile users efficiently. Finally, the fuzzy linguistic technique is used to express the trust between two mobile users and enhance the human's understanding of trust. Real-world mobile dataset is adopted to evaluate the performance of the MobiFuzzyTrust inference mechanism. The experimental results demonstrate that MobiFuzzyTrust can efficiently infer trust with a high precision.\n\nTitle: Toward Offering More Useful Data Reliably to Mobile Cloud From Wireless Sensor Network\nContent: The integration of ubiquitous wireless sensor network (WSN) and powerful mobile cloud computing (MCC) is a research topic that is attracting growing interest in both academia and industry. In this new paradigm, WSN provides data to the cloud and mobile users request data from the cloud. To support applications involving WSN-MCC integration, which need to reliably offer data that are more useful to the mobile users from WSN to cloud, this paper first identifies the critical issues that affect the usefulness of sensory data and the reliability of WSN, then proposes a novel WSN-MCC integration scheme named TPSS, which consists of two main parts: 1) time and priority-based selective data transmission (TPSDT) for WSN gateway to selectively transmit sensory data that are more useful to the cloud, considering the time and priority features of the data requested by the mobile user and 2) priority-based sleep scheduling (PSS) algorithm for WSN to save energy consumption so that it can gather and transmit data in a more reliable way. Analytical and experimental results demonstrate the effectiveness of TPSS in improving usefulness of sensory data and reliability of WSN for WSN-MCC integration.\n\n## Task\nCompose an abstract for the title 'A GML-Based mobile device trace monitoring system' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Recently, the demands on information services have been increasing significantly. This is mainly due to the popularization of computer and mobile telecommunication devices and the rapid improvements on wireless communication technology. Specially, information services and their corresponding management for mobile devices, such as Location Based Service (LBS) and Telematics, become more and more important. However, the standard for geographical space data has not been finalized. Many commercial monitoring systems are using their own independent geographical information without making them compatible to others. Much efforts and resources have been wasted on managing and operating those different monitoring systems\u2019 geographical Information System (GIS) databases. Accordingly, a standard format called GML, based on the most commonly used geographical data format such as DXF, DWG and SHP, has been emerged. In this paper, our work on GML\u2019s visualization in Trace Monitoring Systems (TMS) is described fully. The details on how to trace and manage data moving among different mobile terminals are presented as well."
      }
    ]
  },
  {
    "tid":"321",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Dynamic Organization Schemes for Cooperative Proxy Caching\nContent: In a generic cooperative caching architecture, web proxies form a mesh network. When a proxy cannot satisfy a request, it forwards the request to the other nodes of the mesh. Since a local cache cannot fulfill the majority of the arriving requests (typical values of the local hit ratio are about 30-50%), the volume of queries diverted to neighboring nodes can substantially grow and may consume considerable amount of system resources. A proxy does not need to cooperate with every node of the mesh due to the following reasons: (i) the traffic characteristics may be highly diverse; (ii) the contents of some nodes may extensively overlap; (iii) the inter-node distance might be too large. Furthermore, organizing N proxies in a mesh topology introduces scalability problems, since the number of queries is of the order of N2. Therefore, restricting the number of neighbors for each proxy to k\n\nTitle: Active caching of on-line-analytical-processing queries in WWW proxies\nContent: Abstract: The Internet is offering more than just regular Web pages to the users. Decision makers can now issue analytical, as opposed to transactional, queries that involve massive data (such as, aggregations of millions of rows in a relational database) in order to identify useful trends and patterns. Such queries are referred to as On-Line-Analytical-Processing (ICPP '01) (OLAP) queries. Typically, pages carrying query results do not exhibit temporal locality and, therefore, are not considered for caching at WWW proxies. In OLAP Processing (ICPP '01), this becomes a major hurdle as the cost of such queries is much higher than traditional transactional queries. This paper proposes a systematic technique to reduce the response time for OLAP queries originating from geographically distributed private LANs and issued through the Web towards the central data warehouse (DW) of an enterprise. An active caching scheme is proposed that enables the LAN proxies to cache some parts of the data, together with the semantics of the DW, in order to process queries and construct the resulting pages. OLAP queries arriving at the proxy are either satisfied locally or from the DW, depending on the relative access costs. We formulate a cost model for characterizing the latencies of these queries, taking into consideration normal Web access as well as analytical Processing (ICPP '01). We propose a cache admittance and replacement algorithm that outperforms a widely accepted caching algorithm.\n\nTitle: Static and adaptive data replication algorithms for fast information access in large distributed systems\nContent: Creating replicas of frequently accessed objects across a read-intensive network can result in large bandwidth savings, which, in turn, can lead to reduction in user response time. On the contrary, data replication in the presence of writes incurs extra cost due to multiple updates. The set of sites at which an object is replicated constitutes its replication scheme. Finding an optimal replication scheme that minimizes the amount of network traffic, given read and write frequencies for various objects, is NP-complete in general.We propose two heuristics to deal with this problem for static read and write patterns. The first is a simple and fast greedy heuristic that yields good solutions when the system is predominantly read-oriented. The second is a genetic algorithm that through an efficient exploration of the solution space provides better solutions for cases where the greedy heuristic does not perform well. We also propose an extended genetic algorithm that rapidly adapts to the dynamically changing characteristics such as the frequency of reads and writes for particular objects.\n\nTitle: Heuristics-Based Replication Schemas For Fast Information Retrieval Over The Internet\nContent: This paper addresses the problem of fine-grained data replication on a set of Internet sites. With fine-grained replication, we allow reallocation of objects that are accessed and not the entire site. We solve this problem by developing a cost model and by proposing a number of data placement algorithms. We exploit the A-Star state space searching technique to identify optimal solution(s). This approach is complemented by three sub-optimal A-Star based heuristics, which sacrifice some solution quality but incur fast execution time and do not suffer from memory overflow problems associated with A-Star type algorithms. We also propose natural selection algorithms: Local and Global Min-Min. These algorithms are effective in various environments providing a choice of algorithms that guarantee fast or optimal or both types of solutions.\n\nTitle: Allocating Data Objects to Multiple Sites for Fast Browsing of Hypermedia Documents\nContent: Many world wide web applications require access, transfer, and synchronization of large multimedia data objects (MDOs) (such as, audio, video, and images) across the communication network. The transfer of large MDOs across the network contributes to the response time observed by the end users. As the end users expect strict adherence to response time constraints, the problem of allocating these MDOs so as to minimize response time becomes very challenging. The problem becomes more complex in the context of hypermedia documents (web pages), wherein these MDOs need to be synchronized during presentation to the end users. Since the basic problem of data allocation in distributed database systems is NP-complete, a need exists to pursue and evaluate solutions based on heuristics for generating near-optimal MDO allocations. In this paper, we i) conceptualize this problem by using a navigational model to represent hypermedia documents and their access behavior from end users, ii) formulate the problem by developing a base case cost model for response time, iii) design two algorithms to find near-optimal solutions for allocating MDOs of the hypermedia documents while adhering to the synchronization requirements, and iv) evaluate the trade-off between time complexity to get the solution and quality of solution by comparing the algorithms solution with exhaustive solution over a set of experiments.\n\n## Task\nCompose an abstract for the title 'Adaptive schemes for distributed web caching' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In distributed web caching architectures, institutional proxies take advantage of their neighbors' contents in order to reduce the number of requests forwarded to the server. Intuitively, the maximum benefit from this cooperation is expected when the proxies that exhibit similar requests are grouped together. The current practice is to follow a static and manual configuration of neighbors. Such an approach has a number of drawbacks: (i) static allocation may not determine the best neighbors, especially if global knowledge of the participating proxies is not available, (ii) a manual allocation places significant administrative burden, (iii) static schemes are insensitive to changes in access patterns, and (iv) they cannot deal with the introduction of new, potentially useful, proxies. In this paper, we propose a set of algorithms that allow proxies to independently explore the network for better neighbors and continuously update their configuration in an adaptive fashion. The simulation experiments illustrate that dynamic neighbor reconfiguration leads to significantly higher hit ratios compared to the static approach. Although some researchers in the past have recognized the need for adaptive caching, to the best of our knowledge this is the first study to propose concrete algorithms and evaluate their efficacy."
      }
    ]
  },
  {
    "tid":"322",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: MM: A bidirectional search algorithm that is guaranteed to meet in the middle.\nContent: Bidirectional search algorithms interleave two separate searches, a normal search forward from the start state, and a search backward from the goal. It is well known that adding a heuristic to unidirectional search dramatically reduces the search effort. By contrast, despite decades of research, bidirectional heuristic search has not yet had a major impact. Additionally, no comprehensive theory was ever devised to understand the nature of bidirectional heuristic search. In this paper we aim to close this gap. We first present MM, a novel bidirectional heuristic search algorithm. Unlike previous bidirectional heuristic search algorithms, MM's forward and backward searches are guaranteed to \u201cmeet in the middle\u201d, i.e. never expand a node beyond the solution midpoint. Based on this unique attribute we present a novel framework for comparing MM, A*, and their brute-force variants. We do this by dividing the entire state space into disjoint regions based on their distance from the start and goal. This allows us to perform a comparison of these algorithms on a per region basis and identify conditions favoring each algorithm. Finally, we present experimental results that support our theoretical analysis.\n\nTitle: Front-to-End Bidirectional Heuristic Search with Near-Optimal Node Expansions.\nContent: It is well-known that any admissible unidirectional heuristic search algorithm must expand all states whose $f$-value is smaller than the optimal solution cost when using a consistent heuristic. Such states are called surely expanded (s.e.). A recent study characterized s.e. pairs of states for bidirectional search with consistent heuristics: if a pair of states is s.e. then at least one of the two states must be expanded. This paper derives a lower bound, VC, on the minimum number of expansions required to cover all s.e. pairs, and present a new admissible front-to-end bidirectional heuristic search algorithm, Near-Optimal Bidirectional Search (NBS), that is guaranteed to do no more than 2VC expansions. We further prove that no admissible front-to-end algorithm has a worst case better than 2VC. Experimental results show that NBS competes with or outperforms existing bidirectional search algorithms, and often outperforms A* as well.\n\nTitle: Predicting optimal solution costs with bidirectional stratified sampling in regular search spaces\nContent: Optimal planning and heuristic search systems solve state-space search problems by finding a least-cost path from start to goal. As a byproduct of having an optimal path they also determine the optimal solution cost. In this paper we focus on the problem of determining the optimal solution cost for a state-space search problem directly, i.e., without actually finding a solution path of that cost. We present an algorithm, BiSS, which is a hybrid of bidirectional search and stratified sampling that produces accurate estimates of the optimal solution cost. BiSS is guaranteed to return the optimal solution cost in the limit as the sample size goes to infinity. We show empirically that BiSS produces accurate predictions in several domains. In addition, we show that BiSS scales to state spaces much larger than can be solved optimally. In particular, we estimate the average solution cost for the 6 \u00d7 6 , 7 \u00d7 7 , and 8 \u00d7 8 Sliding-Tile puzzle and provide indirect evidence that these estimates are accurate. As a practical application of BiSS, we show how to use its predictions to reduce the time required by another system to learn strong heuristic functions from days to minutes in the domains tested.\n\nTitle: Stratified tree search: a novel suboptimal heuristic search algorithm\nContent: Traditional heuristic search algorithms use the ranking of states that a heuristic function provides to guide the search. In this paper---with the objective of improving suboptimality and runtime of search algorithms when only weak heuristics are available---we present Stratified Tree Search (STS), a suboptimal heuristic search algorithm that uses a heuristic to partition the state space to guide the search. We call this partition a type system. STS assumes that nodes of the same type will lead to solutions of the same cost. Thus, STS expands only one node of each type in every level of search. We show that in general STS offers a good tradeoff between solution quality and search speed by varying the size of the type system. However, in some cases, STS might not provide a fine adjustment of this tradeoff. We present a variant of STS, Beam STS (BSTS), that allows one to make fine adjustments of this tradeoff. BSTS combines the ideas of STS with those of Beam Search. Our empirical results in benchmark domains show that both STS and BSTS can find solutions of lower suboptimality in less time than standard heuristic search algorithms for finding suboptimal solutions.\n\nTitle: Duality in permutation state spaces and the dual search algorithm\nContent: Geometrical symmetries are commonly exploited to improve the efficiency of search algorithms. A new type of symmetry in permutation state spaces, duality, is introduced. Each state has a dual state. Both states share important attributes such as their distance to the goal. Given a state S, it is shown that an admissible heuristic of the dual state of S is an admissible heuristic for S. This provides opportunities for additional heuristic evaluations. An exact definition of the class of problems where duality exists is provided. A new search algorithm, dual search, is presented which switches between the original state and the dual state when it seems likely that the switch will improve the chance of reaching the goal faster. The decision of when to switch is very important and several policies for doing this are investigated. Experimental results show significant improvements for a number of applications, for using the dual state's heuristic evaluation and/or dual search.\n\n## Task\nCompose an abstract for the title 'Bidirectional Search That Is Guaranteed to Meet in the Middle.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We present MM, the first bidirectional heuristic search algorithm whose forward and backward searches are guaranteed to \\\"meet in the middle\\\", i.e. never expand a node beyond the solution midpoint. We also present a novel framework for comparing MM, A*, and brute-force search, and identify conditions favoring each algorithm. Finally, we present experimental results that support our theoretical analysis."
      }
    ]
  },
  {
    "tid":"323",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Tractable Group Detection on Large Link Data Sets\nContent: Discovering underlying structure from co-occurrence data is an important task in a variety of fields, including: insurance, intelligence, criminal investigation, epidemiology, human resources, and marketing. Previously Kubica et al. presented the group detection algorithm (GDA) - an algorithm for finding underlying groupings of entities from co-occurrence data. This algorithm is based on a probabilistic generative model and produces coherent groups that are consistent with prior knowledge. Unfortunately, the optimization used in GDA is slow, potentially making it infeasible for many large data sets. To this end, we present k-groups - an algorithm that uses an approach similar to that of k-means to significantly accelerate the discovery of groups while retaining GDA's probabilistic model. We compare the performance of GDA and k-groups on a variety of data, showing that k-groups' sacrifice in solution quality is significantly offset by its increase in speed.\n\nTitle: Real-valued all-dimensions search: low-overhead rapid searching over subsets of attributes\nContent: This paper is about searching the combinatorial space of contingency tables during the inner loop of a nonlinear statistical optimization. Examples of this operation in various data analytic communities include searching for nonlinear combinations of attributes that contribute significantly to a regression (Statistics), searching for items to include in a decision list (machine learning) and association rule hunting (Data Mining). This paper investigates a new, efficient approach to this class of problems, called RADSEARCH (Real-valued All-Dimensions-tree Search). RADSEARCH finds the global optimum, and this gives us the opportunity to empirically evaluate the question: apart from algorithmic elegance what does this attention to optimality buy us? We compare RADSEARCH with other recent successful search algorithms such as CN2, PRIM, APriori, OPUS and DenseMiner. Finally, we introduce RADREG, a new regression algorithm for learning real-valued outputs based on RADSEARCHing for highorder interactions.\n\nTitle: Active Search and Bandits on Graphs using Sigma-Optimality.\nContent: Many modern information access problems involve highly complex patterns that cannot be handled by traditional keyword based search. Active Search is an emerging paradigm that helps users quickly find relevant information by efficiently collecting and learning from user feedback. We consider active search on graphs, where the nodes represent the set of instances users want to search over and the edges encode pairwise similarity among the instances. Existing active search algorithms are either short of theoretical guarantees or inadequate for graph data. Motivated by recent advances in active learning on graphs, namely the \u03a3-optimality selection criterion, we propose new active search algorithms suitable for graphs with theoretical guarantees and demonstrate their effectiveness on several real-world datasets. We relate our active search setting to multi-armed bandits whose rewards are binary values indicating search hits or misses and arms cannot be pulled more than once. We also discussed theoretical guarantees for applying \u03a3-optimality as the exploration term for bandits on graphs.\n\nTitle: Active search on graphs\nContent: Active search is an increasingly important learning problem in which we use a limited budget of label queries to discover as many members of a certain class as possible. Numerous real-world applications may be approached in this manner, including fraud detection, product recommendation, and drug discovery. Active search has model learning and exploration/exploitation features similar to those encountered in active learning and bandit problems, but algorithms for those problems do not fit active search. Previous work on the active search problem [5] showed that the optimal algorithm requires a lookahead evaluation of expected utility that is exponential in the number of selections to be made and proposed a truncated lookahead heuristic. Inspired by the success of myopic methods for active learning and bandit problems, we propose a myopic method for active search on graphs. We suggest selecting points by maximizing a score considering the potential impact of selecting a node, meant to emulate lookahead while avoiding exponential search. We test the proposed algorithm empirically on real-world graphs and show that it outperforms popular approaches for active learning and bandit problems as well as truncated lookahead of a few steps.\n\nTitle: Scaling Active Search using Linear Similarity Functions.\nContent: Active Search has become an increasingly useful tool in information retrieval problems where the goal is to discover as many target elements as possible using only limited label queries. With the advent of big data, there is a growing emphasis on the scalability of such techniques to handle very large and very complex datasets.  this paper, we consider the problem of Active Search where we are given a similarity function between data points.  look at an algorithm introduced by Wang et al. [2013] for Active Search over graphs and propose crucial modifications which allow it to scale significantly. Their approach selects points by minimizing an energy function over the graph induced by the similarity function on the data. Our modifications require the similarity function to be a dot-product between feature vectors of data points, equivalent to having a linear kernel for the adjacency matrix. With this, we are able to scale tremendously: for $n$ data points, the original algorithm runs in $O(n^2)$ time per iteration while ours runs in only $O(nr + r^2)$ given $r$-dimensional features. We also describe a simple alternate approach using a weighted-neighbor predictor which also scales well. In our experiments, we show that our method is competitive with existing semi-supervised approaches.  also briefly discuss conditions under which our algorithm performs well.\n\n## Task\nCompose an abstract for the title 'Finding Underlying Connections: A Fast Graph-Based Method for Link Analysis and Collaboration Queries' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Many techniques in the social sciences and graph theory deal with the problem of examining and analyzing patterns found in the underlying struc- ture and associations of a group of entities. How- ever, much of this work assumes that this un- derlying structure is known or can easily be in- ferred from data, which may often be an unreal- istic assumption for many real-world problems. Below we consider the problem of learning and querying a graph-based model of this underlying structure. The model is learned from noisy ob- servations linking sets of entities. We explicitly allow different types of links (representing differ- ent types of relations) and temporal information indicating when a link was observed. We quanti- tatively compare this representation and learning method against other algorithms on the task of predicting future links and new \"friendships\" in a variety of real world data sets."
      }
    ]
  },
  {
    "tid":"324",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: A Network Model Approach To Retrieval In The Semantic Web\nContent: While it is agreed that semantic enrichment of resources would lead to better search results, at present the low coverage of resources on the Web with semantic information presents a major hurdle in realizing the vision of search on the Semantic Web. To address this problem, we investigate how to improve retrieval performance in settings where resources are sparsely annotated with semantic information. Techniques from soft computing are employed to find relevant material that was not originally annotated with the concepts used in a query. We present an associative retrieval model for the Semantic Web and evaluate if and to what extent the use of associative retrieval techniques increases retrieval performance. The evaluation of new retrieval paradigms, such as retrieval based on technology for the Semantic Web, presents an additional challenge since no off-the-shelf test corpora exist. Hence, we give a detailed description of the approach taken to evaluate the information retrieval service we have built.\n\nTitle: A formal semantics for weighted ontology mappings\nContent: Ontology mappings are often assigned a weight or confidence factor by matchers. Nonetheless, few semantic accounts have been given so far for such weights. This paper presents a formal semantics for weighted mappings between different ontologies. It is based on a classificational interpretation of mappings: if O1 and O2 are two ontologies used to classify a common set X, then mappings between O1 and O2 are interpreted to encode how elements of X classified in the concepts of O1 are re-classified in the concepts of O2, and weights are interpreted to measure how precise and complete re-classifications are. This semantics is justifiable by extensional practice of ontology matching. It is a conservative extension of a semantics of crisp mappings. The paper also includes properties that relate mapping entailment with description logic constructors.\n\nTitle: Using Wrapper Agents to Answer Queries in Distributed Information Systems\nContent: Nowadays, one of the objectives of information integration is to provide a global view of the information contained in a set of information sources. This is an important objective as global views enable us to design applications that use information contained in a set of autonomous and heterogeneous information sources. The global view is represented using a global schema, which is usually different from the schemata of the information sources. Users interested in exploiting the global information must be aware of the global schema. One of the drawbacks of this approach is that users working with local applications, who are often interested in exploiting global information, cannot access global information using their own local applications, as the latter are based on local scemata and are hardly adaptable to the global schema. What we envisage is the possibility for local applications of exploiting information contained in other information sources through their local schema. As an important effect, users can utilize global information via their usual local applications. The architecture we adopt is based on a multi-agent structure. In this structure there is no central agent, which plays the role of a mediator to integrate all the information sources. The key role in the process of information integration is played by the wrapper agents. Wrapper agents (or simply wrappers) are provided both for each information source and, if any, for each global view\n\nTitle: A Context-Based Logic for Distributed Knowledge Representation and Reasoning\nContent: This paper is concerned with providing a logic, called Distributed First Order Logic (DFOL), for the formalization of distributed knowledge representation and reasoning systems. In such systems knowledge is contained in a set of heterogeneous subsystems. Each subsystem represents, using a possibly different language, partial knowledge about a subset of the whole domain, it is able to reason about such a knowledge, and it is able to exchange knowledge with other subsystems via query answering. Our approach is to represent each subsystem as a context, each context having its own language, a set of basic facts describing what is \"explicitly known\" by the subsystem, and a set of inference rules representing the reasoning capabilities of the subsystem. Knowledge exchange is represented by two different relations on contexts: the former on the languages (query mapping) and the latter on the domains (answer mapping) of different contexts. DFOL is based on a semantics for contextual reasoning, called Local Models Semantics, which allows to model contexts having different languages, basic knowledge, and reasoning capabilities, as well as relations between contexts. An axiomatization of DFOL is also presented.\n\nTitle: Formalizing Belief Reports - The Approach and a Case Study\nContent: One of the most interesting puzzles in formalizing belief contexts is the fact that many belief reports can be given both an opaque and a transparent readings. A traditional explanation is that the two readings are related to the failure and success of the principle of substitutivity respectively, and this in turn is explained with the de re/de dicto distinction. We propose an alternative analysis, based on the idea that another agent's beliefs can just be quoted (preserving opacity) or translated into the reporter's language (allowing for transparency). We show that MultiContext systems allow for the formalization of these two phenomena at the same time, thanks to their multi-language feature.\n\n## Task\nCompose an abstract for the title 'Querying the Web of Data: A Formal Approach' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The increasing amount of interlinked RDF data has finally made available the necessary building blocks for the web of data. This in turns makes it possible (and interesting) to query such a collection of graphs as an open and decentralized knowledge base. However, despite the fact that there are already implementations of query answering algorithms for the web of data, there is no formal characterization of what a satisfactory answer is expected to be. In this paper, we propose a preliminary model for such an open collection of graphs which goes beyond the standard single-graph RDF semantics, describes three different ways in which a query can be answered, and characterizes them semantically in terms of three incremental restrictions on the relation between the domain of interpretation of each single component graph."
      }
    ]
  },
  {
    "tid":"325",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Breaking The Recursivity: Towards A Model To Analyse Expert Finders\nContent: Expert Finding (EF) techniques help in discovering people having relevant knowledge and skills. But for their validation, EF techniques usually rely on experts, meaning using another EF technique, generally not properly validated, and exploit them mainly for output validations, meaning only at late stages. We propose a model, which builds on literature in Psychology and practice, to identify generic concepts and relations in order to support the analysis and design of EF techniques, thus inferring potential improvements during early stages in an expert-free manner. Our contribution lies in the identification and review of relevant literature, building the conceptual model, and illustrating its use through an analysis of existing EF techniques. Although the model can be improved, we can already identify strengths and limitations in recent EF techniques, thus supporting the usefulness of a model-based analysis and design for EF techniques.\n\nTitle: Expert Finding Using Markov Networks in Open Source Communities.\nContent: Expert finding aims at identifying knowledgeable people to help in decision processes, such as eliciting or analysing requirements in Requirements Engineering. Complementary approaches exist to tackle specific contexts like in forum-based communities, exploiting personal contributions, or in structured organisations like companies, where the social relationships between employees help to identify experts. In this paper, we propose an approach to tackle a hybrid context like an Open Source Software (OSS) community, which involves forums open to contributors, as well as companies providing OSS-related services. By representing and relating stakeholders, their roles, the topics discussed and the terms used, and by applying inference algorithms based on Markov networks, we are able to rank stakeholders by their inferred level of expertise in one topic or more. Two preliminary experiments are presented to illustrate the approach and to show its potential benefit.\n\nTitle: Exploiting User Feedback in Tool-Supported Multi-criteria Requirements Prioritization\nContent: As different types of user feedback are becoming available, from a variety of sources and in large amount, several analysis techniques have been developed with the purpose of extracting information that can be useful for requirements engineering purposes. For instance, automated extraction and prioritization of feature requests have been recently investigated for the specific case of app development, where the key prioritization criterion is value for the user. For other types of software applications and services, software evolution relies on multi-criteria requirements prioritization, which may take into account different stakeholders' perspectives, thus leading to a complex decision-making problem. Different automated reasoning techniques have been proposed to support multi-criteria requirements prioritization, aimed at reducing human effort and improving the quality of the resulting ranking of the candidate requirements.The goal of our research is to understand how we can exploit user feedback in tool-supported multi-criteria requirements prioritization processes. Towards this objective, we discuss the properties of user feedback which are relevant for requirements prioritization, formulate a multi-criteria requirements prioritization problem, and outline a possible solution that integrates state of the art automated reasoning techniques which we extend to cope with information derived from user feedback.\n\nTitle: Ahab's leg: exploring the issues of communicating semi-formal requirements to the final users\nContent: In this paper, we present our experience in using narrative scenarios as a tool to communicate and validate semi-formal requirements with the stakeholders in a large software project. The process of translating the semiformal language of Tropos into the narrative form of scenarios is introduced and some unintended implications of this process are discussed. In particular, we define the notion of Ahab's leg to describe the necessity to introduce new constraints or features in a description when moving to a different representational language. Starting from the lessons learned with this specific case study, we derive some general implications concerning the issue of requirement translation for validation tasks and we propose some methodological guidelines to address the Ahab's leg dilemma.\n\nTitle: Collaborative case-based preference elicitation\nContent: Preference elicitation is a well known bottleneck that prevents the acquisition of the utility function and consequently the set up of effective decision-support systems. In this paper we present a new approach to preference elicitation based on pairwise comparison. The exploitation of learning techniques allows to overcome the usual restrictions that prevent to scale up. Furthermore, we show how our approach can easily support a distributed process of preference elicitation combining both autonomy and coordination among different stakeholders. We argue that a collaborative approach to preference elicitation can be effective in dealing with non homogeneous data representations.The presentation of the model is followed by an empirical evaluation on a real world settings. We consider a case study on environmental risk assessment to test with real users the properties of our model.\n\n## Task\nCompose an abstract for the title 'Who is the expert? combining intention and knowledge of online discussants in collaborative RE tasks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Large, distributed software development projects rely on the collaboration of culturally heterogeneous and geographically distributed stakeholders. Software requirements, as well as solution ideas are elicited in distributed processes, which increasingly use online forums and mailing lists, in which stakeholders mainly use free or semi-structured natural language text. The identification of contributors of key information about a given topic --called experts, in both the software domain and code-- and in particular an automated support for retrieving information from available online resources, are becoming of crucial importance. In this paper, we address the problem of expert finding in mailing-list discussions, and propose an approach which combines content- and intent-based information extraction for ranking online discussants with respect to their expertise in the discussed topics. We illustrate its application on an example."
      }
    ]
  },
  {
    "tid":"326",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Event-Triggered Optimal Control With Performance Guarantees Using Adaptive Dynamic Programming.\nContent: This paper studies the problem of event-triggered optimal control (ETOC) for continuous-time nonlinear systems and proposes a novel event-triggering condition that enables designing ETOC methods directly based on the solution of the Hamilton\u2013Jacobi\u2013Bellman (HJB) equation. We provide formal performance guarantees by proving a predetermined upper bound. Moreover, we also prove the existence of a lower bound for interexecution time. For implementation purposes, an adaptive dynamic programming (ADP) method is developed to realize the ETOC using a critic neural network (NN) to approximate the value function of the HJB equation. Subsequently, we prove that semiglobal uniform ultimate boundedness can be guaranteed for states and NN weight errors with the ADP-based ETOC. Simulation results demonstrate the effectiveness of the developed ADP-based ETOC method.\n\nTitle: Optimal Operator State Migration for Elastic Data Stream Processing.\nContent:   A cloud-based data stream management system (DSMS) handles fast data by utilizing the massively parallel processing capabilities of the underlying platform. An important property of such a DSMS is elasticity, meaning that nodes can be dynamically added to or removed from an application to match the latter's workload, which may fluctuate in an unpredictable manner. For an application involving stateful operations such as aggregates, the addition / removal of nodes necessitates the migration of operator states. Although the importance of migration has been recognized in existing systems, two key problems remain largely neglected, namely how to migrate and what to migrate, i.e., the migration mechanism that reduces synchronization overhead and result delay during migration, and the selection of the optimal task assignment that minimizes migration costs. Consequently, migration in current systems typically incurs a high spike in result delay caused by expensive synchronization barriers and suboptimal task assignments. Motivated by this, we present the first comprehensive study on efficient operator states migration, and propose designs and algorithms that enable live, progressive, and optimized migrations. Extensive experiments using real data justify our performance claims. \n\nTitle: Dynamic plan migration for snapshot-equivalent continuous queries in data stream systems\nContent: A data stream management system executes a large number of continuous queries in parallel. As stream characteristics and query workload change over time, the plan initially installed for a continuous query may become inefficient. As a consequence, the query optimizer will re-optimize this plan based on the current statistics. The replacement of the running plan with a more efficient but semantically equivalent plan at runtime is called dynamic plan migration. In order to have a sound semantic foundation for query optimization, we investigate dynamic plan migration for snapshot-equivalent plans. We develop a general method for dynamic plan migration that treats the old and new plan as snapshot-equivalent black boxes. This enables the query optimizer to apply the conventional transformation rules during re-optimization. As a consequence, our approach supports the dynamic optimization of arbitrary continuous queries expressible in CQL, whereas existing solutions are limited in their scope.\n\nTitle: Convex Optimization for Linear Query Processing under Approximate Differential Privacy\nContent: Differential privacy enables organizations to collect accurate aggregates over sensitive data with strong, rigorous guarantees on individuals' privacy. Previous work has found that under differential privacy, computing multiple correlated aggregates as a batch, using an appropriate strategy, may yield higher accuracy than computing each of them independently. However, finding the best strategy that maximizes result accuracy is non-trivial, as it involves solving a complex constrained optimization program that appears to be non-convex. Hence, in the past much effort has been devoted in solving this non-convex optimization program. Existing approaches include various sophisticated heuristics and expensive numerical solutions. None of them, however, guarantees to find the optimal solution of this optimization problem. This paper points out that under (\u03b5, \u0f6c)-differential privacy, the optimal solution of the above constrained optimization problem in search of a suitable strategy can be found, rather surprisingly, by solving a simple and elegant convex optimization program. Then, we propose an efficient algorithm based on Newton's method, which we prove to always converge to the optimal solution with linear global convergence rate and quadratic local convergence rate. Empirical evaluations demonstrate the accuracy and efficiency of the proposed solution.\n\nTitle: DRS: Dynamic Resource Scheduling for Real-Time Analytics over Fast Streams\nContent: In a data stream management system (DSMS), users register continuous queries, and receive result updates as data arrive and expire. We focus on applications with real-time constraints, in which the user must receive each result update within a given period after the update occurs. To handle fast data, the DSMS is commonly placed on top of a cloud infrastructure. Because stream properties such as arrival rates can fluctuate unpredictably, cloud resources must be dynamically provisioned and scheduled accordingly to ensure real-time response. It is essential, for the existing systems or future developments, to possess the ability of scheduling resources dynamically according to the current workload, in order to avoid wasting resources, or failing in delivering correct results on time. Motivated by this, we propose DRS, a novel dynamic resource scheduler for cloud-based DSMSs. DRS overcomes three fundamental challenges: (a) how to model the relationship between the provisioned resources and query response time (b) where to best place resources, and (c) how to measure system load with minimal overhead. In particular, DRS includes an accurate performance model based on the theory of Jackson open queueing networks and is capable of handling arbitrary operator topologies, possibly with loops, splits and joins. Extensive experiments with real data confirm that DRS achieves real-time response with close to optimal resource consumption.\n\n## Task\nCompose an abstract for the title 'Adaptive Q-Learning for Data-Based Optimal Output Regulation With Experience Replay.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, the data-based optimal output regulation problem of discrete-time systems is investigated. An off-policy adaptive Q-learning (QL) method is developed by using real system data without requiring the knowledge of system dynamics and the mathematical model of utility function. By introducing the Q-function, an off-policy adaptive QL algorithm is developed to learn the optimal Q-functio..."
      }
    ]
  },
  {
    "tid":"327",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Mosaicing-by-recognition for video-based text recognition\nContent: Text recognition captured in multiple frames by a hand-held video camera is a challenging task because it is possible to capture and recognize a longer line of text while improving the quality of the text image by utilizing the redundancy of the overlapping areas between the frames. For this task, the video frames should be registered, i.e., mosaiced, after compensating for their distortions due to camera shakes. In this paper, a mosaicing-by-recognition technique is proposed where the problems of video mosaicing and text recognition are formulated as a unified optimization problem and solved by a dynamic programming-based optimization algorithm simultaneously and collaboratively. Experimental results indicate that, even if the frames undergo various distortions such as rotation, scaling, translation, and nonlinear speed fluctuation of camera movement, the proposed technique provides fine mosaic image by accurate distortion estimation (around 90% of perfect estimation) and character recognition accuracy (over 95%).\n\nTitle: A new method for multi-oriented graphics-scene-3D text classification in video\nContent: Text detection and recognition in video is challenging due to the presence of different types of texts, namely, graphics (video caption), scene (natural text), 2D, 3D, static and dynamic texts. Developing a universal method that works well for all the types is hard. In this paper, we propose a novel method for classifying graphics-scene and 2D-3D texts in video to enhance text detection and recognition accuracies. We first propose an iterative method to classify static and dynamic clusters based on the fact that static texts have zero velocity while dynamic texts have non-zero velocity. This results in text candidates for both static and dynamic texts regardless of 2D and 3D types. We then propose symmetry for text candidates using stroke width distances and medial axis values, which results in potential text candidates. We group potential text candidates using their geometrical properties to form text regions. Next, for each text region, we study the distribution of the dominant medial axis values given by ring radius transform in a new way to classify graphics and scene texts. Similarly, we study the proximity among the pixels that satisfy the gradient directions symmetry to classify 2D and 3D texts. We evaluate each step of the proposed method in terms of classification and recognition rates through classification with the existing methods to show that video text classification is effective and necessary for enhancing the capability of current text detection and recognition systems. We propose a novel method for classifying graphics-scene and 2D-3D texts in video.An iterative procedure to identify text candidates is presented.Stroke width and medial axis are explored for classifying graphics and scene texts.Gradient directions and medial axis are combined for classifying 2D and 3D texts.\n\nTitle: Fast Image Mosaicing Based on Histograms\nContent: This paper introduces a fast image mosaicing technique that does not require costly search on image domain (e.g., pixel-to-pixel correspondence search on the image domain) and the iterative optimization (e.g., gradient-based optimization, iterative optimization, and random optimization) of geometric transformation parameter. The proposed technique is organized in a two-step manner. At both steps, histograms are fully utilized for high computational efficiency. At the first step, a histogram of pixel feature values is utilized to detect pairs of pixels with the same rare feature values as candidates of corresponding pixel pairs. At the second step, a histogram of transformation parameter values is utilized to determine the most reliable transformation parameter value. Experimental results showed that the proposed technique can provide reasonable mosaicing results in most cases with very feasible computations.\n\nTitle: Scene Character Detection And Recognition With Cooperative Multiple-Hypothesis Framework\nContent: To handle the variety of scene characters, we propose a cooperative multiple-hypothesis framework which consists of an image operator set module, an Optical Character Recognition (OCR) module and an integration module. Multiple image operators activated by multiple parameters probe suspected character regions. The OCR module is then applied to each suspected region and returns multiple candidates with weight values for future integration. Without the aid of the heuristic rules which impose constraints on segmentation area, aspect ratio, color consistency, text line orientations, etc., the integration module automatically prunes the redundant detection/recognition and pads the missing detection/recognition. The proposed framework bridges the gap between scene character detection and recognition, in the sense that a practical OCR engine is effectively leveraged for result refinement. In addition, the proposed method achieves the detection and recognition at the character level, which enables dealing with special scenarios such as single character, text along arbitrary orientations or text along curves. We perform experiments on the benchmark ICDAR 2011 Robust Reading Competition dataset which includes a text localization task and a word recognition task. The quantitative results demonstrate that multiple hypotheses outperform a single hypothesis, and be comparable with state-of-the-art methods in terms of recall, precision, F-measure, character recognition rate, total edit distance and word recognition rate. Moreover, two additional experiments are conducted to confirm the simplicity of parameter setting in this proposal.\n\nTitle: Scene character detection and recognition based on multiple hypotheses framework\nContent: To handle the diversity of scene characters, we propose a multiple hypotheses framework which consists of an image operator set module, an optical character recognition (OCR) module, and an integration module. Image operators detect multiple suspicious character areas. The OCR engine is then applied to each detected area and returns multiple candidates with weight values for future integration. Without the aid of heuristic constraints on area, aspect ratio or color etc., the integration module prunes the redundant detection and pads the missing detection based on the outputs of OCR. The experimental results demonstrate that the whole multiple hypotheses outperforms each operator's hypotheses and be comparable with existing methods in terms of recall, precision, F-measure and recognition rate.\n\n## Task\nCompose an abstract for the title 'Mosaicing-by-recognition: a technique for video-based text recognition' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, a mosaicing-by-recognition technique is proposed where video mosaicing and text recognition are simultaneously and collaboratively optimized in a one-step manner. Specifically, multiple frames capturing a long text line are optimally concatenated with a guide of the text recognition framework. In this optimization process, rotation, scaling, vertical shift, and speed fluctuation, which often appear in video frames captured by hand-held cameras, are compensated. The optimization is performed by a DPbased algorithm. The results of experiments to evaluate not only the accuracy of text recognition but also that of video mosaicing indicates that the proposed technique is practical and can provide reasonable results in most cases."
      }
    ]
  },
  {
    "tid":"328",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: EMD-L1: an efficient and robust algorithm for comparing histogram-based descriptors\nContent: We propose a fast algorithm, EMD-L1, for computing the Earth Mover's Distance (EMD) between a pair of histograms. Compared to the original formulation, EMD-L1 has a largely simplified structure. The number of unknown variables in EMD-L1 is O(N) that is significantly less than O(N2) of the original EMD for a histogram with N bins. In addition, the number of constraints is reduced by half and the objective function is also simplified. We prove that the EMD-L1 is formally equivalent to the original EMD with L1 ground distance without approximation. Exploiting the L1 metric structure, an efficient tree-based algorithm is designed to solve the EMD-L1 computation. An empirical study demonstrates that the new algorithm has the time complexity of O(N2), which is much faster than previously reported algorithms with super-cubic complexities. The proposed algorithm thus allows the EMD to be applied for comparing histogram-based features, which is practically impossible with previous algorithms. We conducted experiments for shape recognition and interest point matching. EMD-L1 is applied to compare shape contexts on the widely tested MPEG7 shape dataset and SIFT image descriptors on a set of images with large deformation, illumination change and heavy noise. The results show that our EMD-L1-based solutions outperform previously reported state-of-the-art features and distance measures in solving the two tasks.\n\nTitle: Diffusion Distance for Histogram Comparison\nContent: In this paper we propose diffusion distance, a new dissimilarity measure between histogram-based descriptors. We define the difference between two histograms to be a temperature field. We then study the relationship between histogram similarity and a diffusion process, showing how diffusion handles deformation as well as quantization effects. As a result, the diffusion distance is derived as the sum of dissimilarities over scales. Being a cross-bin histogram distance, the diffusion distance is robust to deformation, lighting change and noise in histogram-based local descriptors. In addition, it enjoys linear computational complexity which significantly improves previously proposed cross-bin distances with quadratic complexity or higher. We tested the proposed approach on both shape recognition and interest point matching tasks using several multi-dimensional histogram-based descriptors including shape context, SIFT, and spin images. In all experiments, the diffusion distance performs excellently in both accuracy and efficiency in comparison with other state-of-the-art distance measures. In particular, it performs as accurately as the Earth Mover\u0092s Distance with much greater efficiency.\n\nTitle: Probabilistic index histogram for robust object tracking\nContent: Color histograms are widely used for visual tracking due to their robustness against object deformations. However, traditional histogram representation often suffers from problems of partial occlusion, background cluttering and other appearance corruptions. In this paper, we propose a probabilistic index histogram to improve the discriminative power of the histogram representation. With this modeling, an input frame is translated into an index map whose entries indicate indexes to a separate bin. Based on the index map, we introduce spatial information and the bin-ratio dissimilarity in histogram comparison. The proposed probabilistic indexing technique, together with the two robust measurements, greatly increases the discriminative power of the histogram representation. Both qualitative and quantitative evaluations show the robustness of the proposed approach against partial occlusion, noisy and clutter background.\n\nTitle: Minimum error bounded efficient \u21131 tracker with occlusion detection\nContent: Recently, sparse representation has been applied to visual tracking to find the target with the minimum reconstruction error from the target template subspace. Though effective, these L1 trackers require high computational costs due to numerous calculations for \u21131 minimization. In addition, the inherent occlusion insensitivity of the \u21131 minimization has not been fully utilized. In this paper, we propose an efficient L1 tracker with minimum error bound and occlusion detection which we call Bounded Particle Resampling (BPR)-L1 tracker. First, the minimum error bound is quickly calculated from a linear least squares equation, and serves as a guide for particle resampling in a particle filter framework. Without loss of precision during resampling, most insignificant samples are removed before solving the computationally expensive \u21131 minimization function. The BPR technique enables us to speed up the L1 tracker without sacrificing accuracy. Second, we perform occlusion detection by investigating the trivial coefficients in the \u21131 minimization. These coefficients, by design, contain rich information about image corruptions including occlusion. Detected occlusions enhance the template updates to effectively reduce the drifting problem. The proposed method shows good performance as compared with several state-of-the-art trackers on challenging benchmark sequences.\n\nTitle: Robust and Efficient Graph Correspondence Transfer for Person Re-identification.\nContent: Spatial misalignment caused by variations in poses and viewpoints is one of the most critical issues that hinder the performance improvement in existing person re-identification (Re-ID) algorithms. Although it is straightforward to explore correspondence learning algorithms for alignment, online learning is intractable for negative pairs due to the intrinsic visual difference between negative pairs and efficiency concern. To address this problem, in this paper, we present a robust and efficient graph correspondence transfer (REGCT) approach for explicit spatial alignment in Re-ID. Specifically, we propose the off-line correspondence learning and on-line correspondence transfer framework. During training, patch-wise correspondences between positive training pairs are established via graph matching. By exploiting both spatial and visual contexts of human appearance in graph matching, meaningful semantic correspondences can be obtained. During testing, the off-line learned patch-wise correspondence templates are transferred to test pairs with similar pose-pair configurations for local feature distance calculation. To enhance the robustness of correspondence transfer, we design a novel pose context descriptor to accurately model human body configurations, and present an approach to measure the similarity between a pair of pose context descriptors. Meanwhile, to improve testing efficiency, we propose a correspondence template ensemble method using the voting mechanism, which significantly reduces the amount of patch-wise matchings involved in distance calculation. With aforementioned strategies, the REGCT model can effectively and efficiently handle the spatial misalignment problem in Re-ID. Extensive experiments on five challenging benchmarks, including VIPeR, Road, PRID450S, 3DPES and CUHK01, evidence the superior performance of REGCT over other state-of-the-art approaches.\n\n## Task\nCompose an abstract for the title 'An efficient Earth Mover's Distance algorithm for robust histogram comparison.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We propose EMD-L1: a fast and exact algorithm for computing the Earth Mover's Distance (EMD) between a pair of histograms. The efficiency of the new algorithm enables its application to problems that were previously prohibitive due to high time complexities. The proposed EMD-L1 significantly simplifies the original linear programming formulation of EMD. Exploiting the L1 metric structure, the number of unknown variables in EMD-L1 is reduced to O(N) from O(N2) of the original EMD for a histogram with N bins. In addition, the number of constraints is reduced by half and the objective function of the linear program is simplified. Formally, without any approximation, we prove that the EMD-L1 formulation is equivalent to the original EMD with a L1 ground distance. To perform the EMD-L1 computation, we propose an efficient tree-based algorithm, Tree-EMD. Tree-EMD exploits the fact that a basic feasible solution of the simplex algorithm-based solver forms a spanning tree when we interpret EMD-L1 as a network flow optimization problem. We empirically show that this new algorithm has an average time complexity of O(N2), which significantly improves the best reported supercubic complexity of the original EMD. The accuracy of the proposed methods is evaluated by experiments for two computation-intensive problems: shape recognition and interest point matching using multidimensional histogram-based local features. For shape recognition, EMD-L1 is applied to compare shape contexts on the widely tested MPEG7 shape data set, as well as an articulated shape data set. For interest point matching, SIFT, shape context and spin image are tested on both synthetic and real image pairs with large geometrical deformation, illumination change, and heavy intensity noise. The results demonstrate that our EMD-L1-based solutions outperform previously reported state-of-the-art features and distance measures in solving the two tasks."
      }
    ]
  },
  {
    "tid":"329",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Towards Practical Oblivious RAM\nContent:   We take an important step forward in making Oblivious RAM (O-RAM) practical. We propose an O-RAM construction achieving an amortized overhead of 20X-35X (for an O-RAM roughly 1 terabyte in size), about 63 times faster than the best existing scheme. On the theoretic front, we propose a fundamentally novel technique for constructing Oblivious RAMs: specifically, we partition a bigger O-RAM into smaller O-RAMs, and employ a background eviction technique to obliviously evict blocks from the client-side cache into a randomly assigned server-side partition. This novel technique is the key to achieving the gains in practical performance. \n\nTitle: Oblivious RAM with o((logn)3) worst-case cost\nContent: Oblivious RAM is a useful primitive that allows a client to hide its data access patterns from an untrusted server in storage outsourcing applications. Until recently, most prior works on Oblivious RAM aim to optimize its amortized cost, while suffering from linear or even higher worst-case cost. Such poor worst-case behavior renders these schemes impractical in realistic settings, since a data access request can occasionally be blocked waiting for an unreasonably large number of operations to complete. This paper proposes novel Oblivious RAM constructions that achieves poly-logarithmic worst-case cost, while consuming constant client-side storage. To achieve the desired worst-case asymptotic performance, we propose a novel technique in which we organize the O-RAM storage into a binary tree over data buckets, while moving data blocks obliviously along tree edges.\n\nTitle: SCORAM: Oblivious RAM for Secure Computation\nContent: Oblivious RAMs (ORAMs) have traditionally been measured by their bandwidth overhead and client storage. We observe that when using ORAMs to build secure computation protocols for RAM programs, the size of the ORAM circuits is more relevant to the performance. We therefore embark on a study of the circuit-complexity of several recently proposed ORAM constructions. Our careful implementation and experiments show that asymptotic analysis is not indicative of the true performance of ORAM in secure computation protocols with practical data sizes. We then present SCORAM, a heuristic compact ORAM design optimized for secure computation protocols. Our new design is almost 10x smaller in circuit size and also faster than all other designs we have tested for realistic settings (i.e., memory sizes between 4MB and 2GB, constrained by 2-80 failure probability). SCORAM makes it feasible to perform secure computations on gigabyte-sized data sets.\n\nTitle: Onion ORAM: A Constant Bandwidth Blowup Oblivious RAM.\nContent: We present Onion ORAM, an Oblivious RAM (ORAM) with constant worst-case bandwidth blowup that leverages poly-logarithmic server computation to circumvent the logarithmic lower bound on ORAM bandwidth blowup. Our construction does not require fully homomorphic encryption, but employs an additively homomorphic encryption scheme such as the Damgard-Jurik cryptosystem, or alternatively a BGV-style somewhat homomorphic encryption scheme without bootstrapping. At the core of our construction is an ORAM scheme that has \"shallow circuit depth\" over the entire history of ORAM accesses. We also propose novel techniques to achieve security against a malicious server, without resorting to expensive and non-standard techniques such as SNARKs. To the best of our knowledge, Onion ORAM is the first concrete instantiation of a constant bandwidth blowup ORAM under standard assumptions (even for the semi-honest setting).\n\nTitle: Perfectly Secure Oblivious Parallel Ram\nContent: We show that PRAMs can be obliviously simulated with perfect security, incurring only O(logN log logN) blowup in parallel run-time, O(log(3) N) blowup in total work, and O(1) blowup in space relative to the original PRAM. Our results advance the theoretical understanding of Oblivious (Parallel) RAM in several respects. First, prior to our work, no perfectly secure Oblivious Parallel RAM (OPRAM) construction was known; and we are the first in this respect. Second, even for the sequential special case of our algorithm (i.e., perfectly secure ORAM), we not only achieve logarithmic improvement in terms of space consumption relative to the state-of-the-art, but also significantly simplify perfectly secure ORAM constructions. Third, our perfectly secure OPRAM scheme matches the parallel runtime of earlier statistically secure schemes with negligible failure probability. Since we remove the dependence (in performance) on the security parameter, our perfectly secure OPRAM scheme in fact asymptotically outperforms known statistically secure ones if (sub-)exponentially small failure probability is desired. Our techniques for achieving small parallel runtime are novel and we employ special expander graphs to derandomize earlier statistically secure OPRAM techniques-this is the first time such techniques are used in the constructions of ORAMs/OPRAMs.\n\n## Task\nCompose an abstract for the title 'Constants Count: Practical Improvements to Oblivious RAM' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Oblivious RAM (ORAM) is a cryptographic primitive that hides memory access patterns as seen by untrusted storage. This paper proposes Ring ORAM, the most bandwidth-efficient ORAM scheme for the small client storage setting in both theory and practice. Ring ORAM is the first tree-based ORAM whose bandwidth is independent of the ORAM bucket size, a property that unlocks multiple performance improvements. First, Ring ORAM's overall bandwidth is 2.3\u00d7 to 4\u00d7 better than Path ORAM, the prior-art scheme for small client storage. Second, if memory can perform simple untrusted computation, Ring ORAM achieves constant online bandwidth (\u223c 60\u00d7 improvement over Path ORAM for practical parameters). As a case study, we show Ring ORAM speeds up program completion time in a secure processor by 1.5\u00d7 relative to Path ORAM. On the theory side, Ring ORAM features a tighter and significantly simpler analysis than Path ORAM."
      }
    ]
  },
  {
    "tid":"330",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Reducing computational complexity of estimating multivariate histogram-based probabilistic model\nContent: In continuous domain, how to efficiently learn the complex probabilistic graphical model is a bottleneck problem for estimation of distribution algorithms (EDAs). The predominant researches focus on Gaussian probabilistic model instead of histogram distribution model because of its comparative superiority in the computational complexity. In this paper, however, we find that using the histogram model does not necessarily bring into exponential computational complexity. Based on the fact many bins are zero-height, we propose a novel method that can learn the multivariate- dependency histogram based probabilistic graphical model with acceptable polynomial computational complexity. Several strategies previously used in the HEDA are combined into the new algorithm to improve the convergence and diversity. Experiments showed the superior performance of the new algorithm on several continuous problems compared with UMDAc IDEA-G and sur-shr-HEDA.\n\nTitle: Extended probe method for linkage discovery over high-cardinality alphabets\nContent: The work addresses the problem of identifying the epistatic linkage of a function from high cardinality alphabets to the real numbers. It is a generalization of Heckendorn and Wright's work that restricts problem representation into the binary-string domain. Discrete Fourier transform is used to analyze underlying structure in high-cardinality alphabets space. Boolean operators are replaced with new operators such as \u2295, \u2296, \u2297 and so on in high cardinality alphabets. The \"probe\" formulation is redesigned to determine epistatic properties of non-binary function. Theoretical analysis shows the close relationship between probe value and problem structure. A deterministic and a stochastic algorithm based on properties of probes are proposed to completely identify the linkage structure and rigourously compute all Fourier coefficients. Some discussion about linkage detection for continuous problems is given.\n\nTitle: Genetic Algorithm Based Constrained Control Allocation for Tailless Fighter\nContent: A genetic algorithm based optimization approach is proposed for constrained control allocation problem via adjusting the components of output vector and finding a proper vector in the attainable moment set (AMS) autonomously. The Basic idea is to minimize the L2 norm of error between the desired moment and attainable moment using the designing freedom provided by redundant control surfaces. With the constraints of control surfaces, in order to obtain desired performance of aircraft such as stability and maneuverability, the weights of different components are updated by the genetic algorithm, which makes the close-loop system self-adaptation. As a demonstration, application of the proposed approach to the designing of control system for a tailless fighter is discussed. The results show good closed loop performance and validate the proposed intelligent optimization approach of constrained control allocation for flight control.\n\nTitle: A robust learning algorithm for feedforward neural networks with adaptive spline activation function\nContent: This paper proposed an adaptive robust learning algorithm for spline-based neural network. Adaptive influence function was dynamically added before objective function to modify the learning gain of back-propagate learning method in neural networks with spline activation functions. Besides the nonlinear activation functions in neurons and linear interconnections between neurons, objective function also changes the shape during iteration. This employed neural network the robust ability to reject gross errors and to learn the underlying input-output mapping from training data. Simulation results also conformed that compared to common learning method, convergence rate of this algorithm is improved for: 1) more free parameters are updated simultaneously in each iteration; 2) the influence of incorrect samples is gracefully suppressed.\n\nTitle: Optimizing continuous problems using estimation of distribution algorithm based on histogram model\nContent: In the field of estimation of distribution algorithms, choosing probabilistic model for optimizing continuous problems is still a challenging task. This paper proposes an improved estimation of distribution algorithm (HEDA) based on histogram probabilistic model. By utilizing both historical and current population information, a novel learning method \u2013 accumulation strategy \u2013 is introduced to update the histogram model. In the sampling phase, mutation strategy is used to increase the diversity of population. In solving some well-known hard continuous problems, experimental results support that HEDA behaves much better than the conventional histogram-based implementation both in convergence speed and scalability. Compared with UMDA-Gaussian, SGA and CMA-ES, the proposed algorithms exhibit excellent performance in the test functions.\n\n## Task\nCompose an abstract for the title 'Deterministic divide-and-conquer algorithm for decomposable black-box optimization problems with bounded difficulty' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"There is a class of GA-hard problems for which classical genetic algorithms often fail to obtain optimal solutions. In this paper, we focus on a class of very typical GA-hard problems that we call decomposable black-box optimization problems (DBBOP). Different from random methods in GA literature, two \u201cdeterministic\u201d divide-and-conquer algorithms DA1 and DA2 are proposed respectively for non-overlapping and overlapping DBBOP, in which there are no classical genetic operations and even no random operations. Given any DBBOP with dimension l and bounded order k, our algorithms can always reliably and accurately obtain the optimal solutions in deterministic way using O(lk) function evaluations."
      }
    ]
  },
  {
    "tid":"331",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Communication Efficient Coresets for Empirical Loss Minimization.\nContent: In this paper, we study the problem of empirical loss minimization with l2-regularization in distributed settings with significant communication cost. Stochastic gradient descent (SGD) and its variants are popular techniques for solving these problems in large-scale applications. However, the communication cost of these techniques is usually high, thus leading to considerable performance degradation. We introduce a novel approach to reduce the communication cost while retaining good convergence properties. The key to our approach is the construction of a small summary of the data, called coreset, at each iteration and solve an easy optimization problem based on the coreset. We present a general framework for analyzing coreset-based optimization and provide interesting insights into existing algorithms from this perspective. We then propose a new coreset construction and provide its convergence analysis for a wide class of problems that include logistic regression and support vector machines. Preliminary experiments show encouraging results for our algorithm on real-world datasets.\n\nTitle: Fast Stochastic Methods for Nonsmooth Nonconvex Optimization.\nContent: We analyze stochastic algorithms for optimizing nonconvex, nonsmooth finite-sum problems, where the nonconvex part is smooth and the nonsmooth part is convex. Surprisingly, unlike the smooth case, our knowledge of this fundamental problem is very limited. For example, it is not known whether the proximal stochastic gradient method with constant minibatch converges to a stationary point. To tackle this issue, we develop fast stochastic algorithms that provably converge to a stationary point for constant minibatches. Furthermore, using a variant of these algorithms, we show provably faster convergence than batch proximal gradient descent. Finally, we prove global linear convergence rate for an interesting subclass of nonsmooth nonconvex functions, that subsumes several recent works. This paper builds upon our recent series of papers on fast stochastic methods for smooth nonconvex optimization [22, 23], with a novel analysis for nonconvex and nonsmooth functions.\n\nTitle: Fast Incremental Method for Nonconvex Optimization.\nContent: We analyze a fast incremental aggregated gradient method for optimizing nonconvex problems of the form $min_x sum_i f_i(x)$. Specifically, we analyze the SAGA algorithm within an Incremental First-order Oracle framework, and show that it converges to a stationary point provably faster than both gradient descent and stochastic gradient descent. We also discuss a Polyaku0027s special class of nonconvex problems for which SAGA converges at a linear rate to the global optimum. Finally, we analyze the practically valuable regularized and minibatch variants of SAGA. To our knowledge, this paper presents the first analysis of fast convergence for an incremental aggregated gradient method for nonconvex problems.\n\nTitle: On Variance Reduction in Stochastic Gradient Descent and its Asynchronous Variants\nContent: We study optimization algorithms based on variance reduction for stochastic gradient descent (SGD). Remarkable recent progress has been made in this direction through development of algorithms like SAG, SVRG, SAGA. These algorithms have been shown to outperform SGD, both theoretically and empirically. However, asynchronous versions of these algorithms\u2014a crucial requirement for modern large-scale applications\u2014have not been studied. We bridge this gap by presenting a unifying framework for many variance reduction techniques. Subsequently, we propose an asynchronous algorithm grounded in our framework, and prove its fast convergence. An important consequence of our general approach is that it yields asynchronous versions of variance reduction algorithms such as SVRG and SAGA as a byproduct. Our method achieves near linear speedup in sparse settings common to machine learning. We demonstrate the empirical performance of our method through a concrete realization of asynchronous SVRG.\n\nTitle: Utilize Old Coordinates: Faster Doubly Stochastic Gradients for Kernel Methods.\nContent: To address the scalability issue of kernel methods, random features are commonly used for kernel approximation (Rahimi and Recht, 2007). They map the input data to a randomized low-dimensional feature space and apply fast linear learning algorithms on it. However, to achieve high precision results, one might still need a large number of random features, which is infeasible in large-scale applications. Dai et al. (2014) address this issue by recomputing the random features of small batches in each iteration instead of pre-generating for the whole dataset and keeping them in the memory. The algorithm increases the number of random features linearly with iterations, which can reduce the approximation error to arbitrarily small. A drawback of this approach is that the large number of random features slows down the prediction and gradient evaluation after several iterations. We propose two algorithms to remedy this situation by \\\"utilizing\\\" old random features instead of adding new features in certain iterations. By checking the expected descent amount, the proposed algorithm selects \\\"important\\\" old features to update. The resulting procedure is surprisingly simple without enhancing the complexity of the original algorithm but effective in practice. We conduct empirical studies on both medium and large-scale datasets, such as ImageNet, to demonstrate the power of the proposed algorithms.\n\n## Task\nCompose an abstract for the title 'AIDE: Fast and Communication Efficient Distributed Optimization.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, we present two new communication-efficient methods for distributed minimization of an average of functions. The first algorithm is an inexact variant of the DANE algorithm that allows any local algorithm to return an approximate solution to a local subproblem. We show that such a strategy does not affect the theoretical guarantees of DANE significantly. In fact, our approach can be viewed as a robustification strategy since the method is substantially better behaved than DANE on data partition arising in practice. It is well known that DANE algorithm does not match the communication complexity lower bounds. To bridge this gap, we propose an accelerated variant of the first method, called AIDE, that not only matches the communication lower bounds but can also be implemented using a purely first-order oracle. Our empirical results show that AIDE is superior to other communication efficient algorithms in settings that naturally arise in machine learning applications."
      }
    ]
  },
  {
    "tid":"332",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Parallel checkpoint/recovery on cluster of IA-64 computers\nContent: We design and implement a high availability parallel run-time system\u2014ChaRM64, a Checkpoint- based Rollback Recovery and Migration system for parallel running programs on a cluster of IA-64 computers. At first, we discuss our solution of a user-level, single process checkpoint/recovery library running on IA-64 systems. Based on this library, ChaRM64 is realized, which implements a user-transparent, coordinated checkpointing and rollback recovery (CRR) mechanism, quasi-asynchronous migration and the dynamic reconfiguration function. Owing to the above techniques and efficient error detection, ChaRM64 can handle cluster node crashes and hardware transient faults in a IA-64 cluster. Now ChaRM64 for PVM has been implemented in Linux and the MPI version is under construction. As we know, there are few similar projects accomplished for IA-64 architecture.\n\nTitle: Dynamic reusability-based replication with network address mapping in CMPs\nContent: In a Chip MultiProcessor(CMP) with shared caches, the last level cache is distributed across all the cores. This increases the on-chip communication delay and thus influence the processor's performance. Replication can be provided in shared caches to reduce the on-chip communication delay. However, current proposals do not take into account replicating blocks's access characteristics and how to make the best of replicas, which have limited performance benefit. In this paper, we observe that reusability of cache blocks influences the availability of replication scheme severely. Based on this observation, we propose Dynamic Reusability-based Replication (DRR), a novel cache design to exploit efficient replicas management using blocks's reuse pattern. DRR monitors the recent referenced cache blocks' access pattern, and replicates the blocks with high reusability to appropriate L2 slices, and the replicated copies can be shared by their nearby cores. We evaluate DRR for 16-core system using splash-2 and parsec benchmarks. DRR improves performance by 30% on average over conventional shared cache design, 16% over Victim Replication(VR), 8% over Adaptive Selected Replication (ASR), and 25% over R-NUCA.\n\nTitle: Distributed cache memory data migration strategy based on cloud computing\nContent: In view of the huge scale of cloud computing, cloud computing systems face many problems in terms of scalability and high availability. In view of the distributed data storage in the cloud computing environment, a recursion based N boundary network model is constructed. A data management model is given on the basis of the data center network structure. A replica distribution strategy and a copy selection strategy are designed to improve the data availability and load balance. On the premise of guaranteeing data availability, a data migration algorithm based on coverage set is proposed, which uses node selection strategy to reduce the migration cost as much as possible. Data migration makes more machines dormant and reduces energy consumption. By comparing and analyzing the experimental data, the correctness and effectiveness of the proposed network topology, data management model, and data migration technology are verified. In the edge cloud computing architecture, in order to ensure the response time of the cloud computing service and minimize the data redundancy in the system, this paper proposes and analyzes the data migration strategy and the pre stored data migration strategy for the network transmission, and gives the suitable application scene. On this basis, in order to control the balance of access network transmission and storage flexibly, a data migration strategy based on network performance is proposed, which ensures the real-time response of the service.\n\nTitle: An efficient computing-checkpoint based coordinated checkpoint algorithm\nContent: In this paper, the concept of \u201ccomputing checkpoint\u201d is introduced, and then an efficient coordinated checkpoint algorithm is proposed. The algorithm combines the two approaches of reducing the overhead associated with coordinated checkpointing, which one is to minimize the processes which take checkpoints and the other is to make the checkpointing process non-blocking. Through piggybacking the information including which processes have taken new checkpoint in the broadcast committing message, the checkpoint sequence number of every process can be kept consistent in all processes, so that the unnecessary checkpoints and orphan messages can be avoided in the future running. Evaluation result shows that the number of redundant computing checkpoints is less than 1/10 of the number of tentative checkpoints. Analyses and experiments show that the overhead of our algorithm is lower than that of other coordinated checkpoint algorithms\n\nTitle: A Novel Cache Organization for Tiled Chip Multiprocessor\nContent: Increased device density and working set size are driving a rise in cache capacity, which comes at the cost of high access latency. Based on the characteristic of shared data, which is accessed frequently and consumes a little capacity, a novel two-level directory organization is proposed to minimize the cache access time in this paper. In this scheme, a small Fast Directory is used to offer fast hits for a great fraction of memory accesses. Detailed simulation results show that on a 16-core tiled chip multiprocessor, this approach reduces average access latency by 17.9% compared to the general cache organization, and improves the overall performance by 13.3% on average.\n\n## Task\nCompose an abstract for the title 'Quasi-asynchronous migration: a novel migration protocol for PVM tasks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Process migration is a desirable and useful facility for the Network of Workstations (NOW). This paper presents a novel migration protocol called quasi-asynchronous migration for PVM tasks, which allows non-migrating tasks to execute during most of the time of migration. Message flushing and message delaying are the key mechanisms used in quasi-asynchronous migration. Because the protocol is implemented on top of PVM and at user-level, it is both transparent to users and portable. Both the analysis of the migration protocol and the experimental results show that quasi-asynchronous migration has lower overhead than other migration protocols."
      }
    ]
  },
  {
    "tid":"333",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Attractor reliability reveals deterministic structure in neuronal spike trains\nContent: When periodic current is injected into an integrate-and-fire model neuron, the voltage as a function of time converges from different initial conditions to an attractor that produces reproducible sequences of spikes. The attractor reliability is a measure of the stability of spike trains against intrinsic noise and is quantified here as the inverse of the number of distinct spike trains obtained in response to repeated presentations of the same stimulus. High reliability characterizes neurons that can support a spike-time code, unlike neurons with discharges forming a renewal process (such as a Poisson process). These two classes of responses cannot be distinguished using measures based on the spike-time histogram, but they can be identified by the attractor dynamics of spike trains, as shown here using a new method for calculating the attractor reliability.We applied these methods to spike trains obtained from current injection into cortical neurons recorded in vitro. These spike trains did not form a renewal process and had a higher reliability compared to renewal-like processes with the same spike-time histogram.\n\nTitle: Multiple Spike Time Patterns Occur at Bifurcation Points of Membrane Potential Dynamics.\nContent: The response of a neuron to repeated somatic fluctuating current injections in vitro can elicit a reliable and precisely timed sequence of action potentials. The set of responses obtained across trials can also be interpreted as the response of an ensemble of similar neurons receiving the same input, with the precise spike times representing synchronous volleys that would be effective in driving postsynaptic neurons. To study the reproducibility of the output spike times for different conditions that might occur in vivo, we somatically injected aperiodic current waveforms into cortical neurons in vitro and systematically varied the amplitude and DC offset of the fluctuations. As the amplitude of the fluctuations was increased, reliability increased and the spike times remained stable over a wide range of values. However, at specific values called bifurcation points, large shifts in the spike times were obtained in response to small changes in the stimulus, resulting in multiple spike patterns that were revealed using an unsupervised classification method. Increasing the DC offset, which mimicked an overall increase in network background activity, also revealed bifurcation points and increased the reliability. Furthermore, the spike times shifted earlier with increasing offset. Although the reliability was reduced at bifurcation points, a theoretical analysis showed that the information about the stimulus time course was increased because each of the spike time patterns contained different information about the input.\n\nTitle: Spike propagation synchronized by temporally asymmetric Hebbian learning\nContent: Synchronously spiking neurons have been observed in the cerebral cortex and the hippocampus. In computer models, synchronous spike volleys may be propagated across appropriately connected neuron pop- ulations. However, it is unclear how the appropriate synaptic connectivity is set up during development and maintained during adult learning. We performed com- puter simulations to investigate the influence of tempo- rally asymmetric Hebbian synaptic plasticity on the propagation of spike volleys. In addition to feedforward connections, recurrent connections were included be- tween and within neuron populations and spike trans- mission delays varied due to axonal, synaptic and dendritic transmission. We found that repeated presen- tations of input volleys decreased the synaptic conduc- tances of intragroup and feedback connections while synaptic conductances of feedforward connections with short delays became stronger than those of connections with longer delays. These adaptations led to the synchronization of spike volleys as they propagated across neuron populations. The findings suggests that temporally asymmetric Hebbian learning may enhance synchronized spiking within small populations of neu- rons in cortical and hippocampal areas and familiar stimuli may produce synchronized spike volleys that are rapidly propagated across neural tissue.\n\nTitle: Neural Network Analysis Of Event Related Potentials And Electroencephalogram Predicts Vigilance\nContent: Automated monitoring of vigilance in attention intensive tasks such as air traffic control or sonar operation is highly desirable. As the opera(cid:173) tor monitors the instrument, the instrument would monitor the operator, insuring against lapses. We have taken a first step toward this goal by us(cid:173) ing feedforward neural networks trained with backpropagation to interpret event related potentials (ERPs) and electroencephalogram (EEG) associ(cid:173) ated with periods of high and low vigilance. The accuracy of our system on an ERP data set averaged over 28 minutes was 96%, better than the 83% accuracy obtained using linear discriminant analysis. Practical vigilance monitoring will require prediction over shorter time periods. We were able to average the ERP over as little as 2 minutes and still get 90% correct prediction of a vigilance measure. Additionally, we achieved similarly good performance using segments of EEG power spectrum as short as 56 sec.\n\nTitle: Integrate-and-fire neurons driven by correlated stochastic input.\nContent: Neurons are sensitive to correlations among synaptic inputs. However, analytical models that explicitly include correlations are hard to solve analytically, so their influence on a neuron's response has been difficult to ascertain. To gain some intuition on this problem, we studied the firing times of two simple integrate-and-fire model neurons driven by a correlated binary variable that represents the total input current. Analytic expressions were obtained for the average firing rate and coefficient of variation (a measure of spike-train variability) as functions of the mean, variance, and correlation time of the stochastic input. The results of computer simulations were in excellent agreement with these expressions. In these models, an increase in correlation time in general produces an increase in both the average firing rate and the variability of the output spike trains. However, the magnitude of the changes depends differentially on the relative values of the input mean and variance: the increase in firing rate is higher when the variance is large relative to the mean, whereas the increase in variability is higher when the variance is relatively small. In addition, the firing rate always tends to a finite limit value as the correlation time increases toward infinity, whereas the coefficient of variation typically diverges. These results suggest that temporal correlations may play a major role in determining the variability as well as the intensity of neuronal spike trains.\n\n## Task\nCompose an abstract for the title 'Finding the event structure of neuronal spike trains.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Neurons in sensory systems convey information about physical stimuli in their spike trains. In vitro, single neurons respond precisely and reliably to the repeated injection of the same fluctuating current, producing regions of elevated firing rate, termed events. Analysis of these spike trains reveals that multiple distinct spike patterns can be identified as trial-to-trial correlations between spike times (Fellous, Tiesinga, Thomas, & Sejnowski, 2004 ). Finding events in data with realistic spiking statistics is challenging because events belonging to different spike patterns may overlap. We propose a method for finding spiking events that uses contextual information to disambiguate which pattern a trial belongs to. The procedure can be applied to spike trains of the same neuron across multiple trials to detect and separate responses obtained during different brain states. The procedure can also be applied to spike trains from multiple simultaneously recorded neurons in order to identify volleys of near-synchronous activity or to distinguish between excitatory and inhibitory neurons. The procedure was tested using artificial data as well as recordings in vitro in response to fluctuating current waveforms."
      }
    ]
  },
  {
    "tid":"334",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Composability of bellare-rogaway key exchange protocols\nContent: In this paper we examine composability properties for the fundamental task of key exchange. Roughly speaking, we show that key exchange protocols secure in the prevalent model of Bellare and Rogaway can be composed with arbitrary protocols that require symmetrically distributed keys. This composition theorem holds if the key exchange protocol satisfies an additional technical requirement that our analysis brings to light: it should be possible to determine which sessions derive equal keys given only the publicly available information. What distinguishes our results from virtually all existing work is that we do not rely, neither directly nor indirectly, on the simulation paradigm. Instead, our security notions and composition theorems exclusively use a game-based formalism.We thus avoid several undesirable consequences of simulation-based security notions and support applicability to a broader class of protocols. In particular, we offer an abstract formalization of game-based security that should be of independent interest in other investigations using game-based formalisms.\n\nTitle: A composable computational soundness notion\nContent: Computational soundness results show that under certain conditions it is possible to conclude computational security whenever symbolic security holds. Unfortunately, each soundness result is usually established for some set of cryptographic primitives and extending the result to encompass new primitives typically requires redoing most of the work. In this paper we suggest a way of getting around this problem. We propose a notion of computational soundness that we term deduction soundness. As for other soundness notions, our definition captures the idea that a computational adversary does not have any more power than a symbolic adversary. However, a key aspect of deduction soundness is that it considers, intrinsically, the use of the primitives in the presence of functions specified by the adversary. As a consequence, the resulting notion is amenable to modular extensions. We prove that a deduction sound implementation of some arbitrary primitives can be extended to include asymmetric encryption and public data-structures (e.g. pairings or list), without repeating the original proof effort. Furthermore, our notion of soundness concerns cryptographic primitives in a way that is independent of any protocol specification language. Nonetheless, we show that deduction soundness leads to computational soundness for languages (or protocols) that satisfy a so called commutation property.\n\nTitle: Separating Trace Mapping and Reactive Simulatability Soundness: The Case of Adaptive Corruption\nContent: Computational soundness is the research direction that aims to translate security guarantees with respect to Dolev-Yao models into guarantees with resepect to the stronger computational models of modern cryptography. There are essentially two different approaches that aim to achieve computational soundness. One approach is based on the so-called trace mapping theorems, and one based on reactive simulatability. In a recent paper, Backes, D\u00fcrthmuth, and K\u00fcsters have shown that the stronger requirements needed for reactive simulatability-based soundness imply that a trace mapping theorem also holds. It was left as an open problem whether there exists interesting settings where the simulatability framework breaks down but mapping theorems still exist.In this paper we describe one such setting, and thus give a separation between the two frameworks. Specifically, we show that adaptive corruption of symmetric encryption keys (a problematic setting for simulation-based frameworks) can be smoothly treated in a mapping theorem-based soundness framework.A crucial ingredient of our proof, and a result of independent interest, is a new (indistinguishability based) security notion for encryption. The central feature of our definition is that in addition to standard chosen-ciphertext attacks in multi-user settings, it also directly accounts for adaptive corruption of decryption keys. We show that our notion satisfies the intuitively appealing property that it is equivalent to standard security requirements on encryption.\n\nTitle: Groth\u2013Sahai proofs revisited\nContent: Since their introduction in 2008, the non-interactive zero-knowledge (NIZK) and non-interactive witness indistinguishable (NIWI) proofs designed by Groth and Sahai have been used in numerous applications. In this paper, we offer two contributions to the study of these proof systems. First, we identify and correct some errors, present in the oringal online manuscript, that occur in two of the three instantiations of the Groth-Sahai NIWI proofs for which the equation checked by the verifier is not valid for honest executions of the protocol. In particular, implementations of these proofs would not work correctly. We explain why, perhaps surprisingly, the NIZK proofs that are built from these NIWI proofs do not suffer from a similar problem. Secondly, we study the efficiency of existing instantiations and note that only one of the three instantiations has the potential of being practical. We therefore propose a natural extension of an existing assumption from symmetric pairings to asymmetric ones which in turn enables Groth-Sahai proofs based on new classes of efficient pairings.\n\nTitle: Practical Zero-Knowledge Proofs for Circuit Evaluation\nContent: Showing that a circuit is satisfiable without revealing information is a key problem in modern cryptography. The related (and more general) problem of showing that a circuit evaluates to a particular value if executed on the input contained in a public commitment has potentially multiple practical applications. Although numerous solutions for the problem had been proposed, their practical applicability is poorly understood. In this paper, we take an important step towards moving existent solutions to practice. We implement and evaluate four solutions for the problem. We investigate solutions both in the common reference string model and the random oracle model. In particular, in the CRS model we use the recent techniques of Groth---Sahai for proofs that use bilinear groups in the asymmetric pairings environment. We provide various optimizations to the different solutions we investigate. We present timing results for two circuits the larger of which is an implementation of AES that uses about 30000 gates.\n\n## Task\nCompose an abstract for the title 'Completeness theorems for the Abadi-Rogaway language of encrypted expressions' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"We show that the Abadi-Rogaway logic of indistinguishability for cryptographic expressions is not complete by giving a natural example of a secure encryption function and a pair of expressions, such that the distributions associated to the two expressions are computationally indistinguishable, but equality cannot be proved within the logic. We then introduce a new property for encryption schemes, which we call confusion freeness, and show that the Abadi-Rogaway logic is sound and complete, whenever the encryption scheme used satisfies this property. We relate confusion freeness with standard cryptographic security notions, showing that any authenticated encryption scheme is confusion free. We also consider two extensions of the basic logic. The first is a refinement of the Abadi-Rogaway logic that overcomes certain limitations of the original proposal, allowing for encryption functions that do not hide the length of the message being sent. Both the soundness theorem of Abadi and Rogaway, and our completeness result for confusion free (or authenticated) encryption easily extend to this more realistic notion of secrecy. The second is an extension of the logic due to Abadi and J\u00fcrjens that allows to study more complex protocols in the presence of a passive adversary. Our completeness results holds for this extended logic as well."
      }
    ]
  },
  {
    "tid":"335",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Bayesian Active Clustering with Pairwise Constraints\nContent: Clustering can be improved with pairwise constraints that specify similarities between pairs of instances. However, randomly selecting constraints could lead to the waste of labeling effort, or even degrade the clustering performance. Consequently, how to actively select effective pairwise constraints to improve clustering becomes an important problem, which is the focus of this paper. In this work, we introduce a Bayesian clustering model that learns from pairwise constraints. With this model, we present an active learning framework that iteratively selects the most informative pair of instances to query an oracle, and updates the model posterior based on the obtained pairwise constraints. We introduce two information-theoretic criteria for selecting informative pairs. One selects the pair with the most uncertainty, and the other chooses the pair that maximizes the marginal information gain about the clustering. Experiments on benchmark datasets demonstrate the effectiveness of the proposed method over state-of-the-art.\n\nTitle: Active Learning of Constraints for Semi-Supervised Clustering\nContent: Semi-supervised clustering aims to improve clustering performance by considering user supervision in the form of pairwise constraints. In this paper, we study the active learning problem of selecting pairwise must-link and cannot-link constraints for semi-supervised clustering. We consider active learning in an iterative manner where in each iteration queries are selected based on the current clustering solution and the existing constraint set. We apply a general framework that builds on the concept of neighborhood, where neighborhoods contain \"labeled examples\" of different clusters according to the pairwise constraints. Our active learning method expands the neighborhoods by selecting informative points and querying their relationship with the neighborhoods. Under this framework, we build on the classic uncertainty-based principle and present a novel approach for computing the uncertainty associated with each data point. We further introduce a selection criterion that trades off the amount of uncertainty of each data point with the expected number of queries (the cost) required to resolve this uncertainty. This allows us to select queries that have the highest information rate. We evaluate the proposed method on the benchmark data sets and the results demonstrate consistent and substantial improvements over the current state of the art.\n\nTitle: Constrained instance clustering in multi-instance multi-label learning\nContent: In multi-instance multi-label (MIML) learning, datasets are given in the form of bags, each of which contains multiple instances and is associated with multiple labels. This paper considers a novel instance clustering problem in MIML learning, where the bag labels are used as background knowledge to help group instances into clusters. The goal is to recover the class labels or to find the subclasses within each class. Prior work on constraint-based clustering focuses on pairwise constraints and cannot fully utilize the bag-level label information. We propose to encode the bag-label knowledge into soft bag constraints that can be easily incorporated into any optimization based clustering algorithm. As a specific example, we demonstrate how the bag constraints can be incorporated into a popular spectral clustering algorithm. Empirical results on both synthetic and real-world datasets show that the proposed method achieves promising performance compared to state-of-the-art methods that use pairwise constraints.\n\nTitle: Random Projection for High Dimensional Data Clustering: A Cluster Ensemble Approach\nContent: We investigate how random projection can best be used for clustering high dimensional data. Random projection has been shown to have promising theoretical properties. In practice, however, we nd that it results in highly unstable clustering performance. Our solution is to use random projection in a cluster ensemble approach. Empirical results show that the proposed approach achieves better and more robust clustering perfor- mance compared to not only single runs of random projection/clustering but also clus- tering with PCA, a traditional data reduction method for high dimensional data. To gain insights into the performance improvement obtained by our ensemble method, we ana- lyze and identify the influence of the quality and the diversity of the individual clustering solutions on the nal ensemble performance.\n\nTitle: Learning multiple nonredundant clusterings\nContent: Real-world applications often involve complex data that can be interpreted in many different ways. When clustering such data, there may exist multiple groupings that are reasonable and interesting from different perspectives. This is especially true for high-dimensional data, where different feature subspaces may reveal different structures of the data. However, traditional clustering is restricted to finding only one single clustering of the data. In this article, we propose a new clustering paradigm for exploratory data analysis: find all non-redundant clustering solutions of the data, where data points in the same cluster in one solution can belong to different clusters in other partitioning solutions. We present a framework to solve this problem and suggest two approaches within this framework: (1) orthogonal clustering, and (2) clustering in orthogonal subspaces. In essence, both approaches find alternative ways to partition the data by projecting it to a space that is orthogonal to the current solution. The first approach seeks orthogonality in the cluster space, while the second approach seeks orthogonality in the feature space. We study the relationship between the two approaches. We also combine our framework with techniques for automatically finding the number of clusters in the different solutions, and study stopping criteria for determining when all meaningful solutions are discovered. We test our framework on both synthetic and high-dimensional benchmark data sets, and the results show that indeed our approaches were able to discover varied clustering solutions that are interesting and meaningful.\n\n## Task\nCompose an abstract for the title 'Comparing Clustering with Pairwise and Relative Constraints: A Unified Framework.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Clustering can be improved with the help of side information about the similarity relationships among instances. Such information has been commonly represented by two types of constraints: pairwise constraints and relative constraints, regarding similarities about instance pairs and triplets, respectively. Prior work has mostly considered these two types of constraints separately and developed individual algorithms to learn from each type. In practice, however, it is critical to understand/compare the usefulness of the two types of constraints as well as the cost of acquiring them, which has not been studied before. This paper provides an extensive comparison of clustering with these two types of constraints. Specifically, we compare their impacts both on human users that provide such constraints and on the learning system that incorporates such constraints into clustering. In addition, to ensure that the comparison of clustering is performed on equal ground (without the potential bias introduced by different learning algorithms), we propose a probabilistic semi-supervised clustering framework that can learn from either type of constraints. Our experiments demonstrate that the proposed semi-supervised clustering framework is highly effective at utilizing both types of constraints to aid clustering. Our user study provides valuable insights regarding the impact of the constraints on human users, and our experiments on clustering with the human-labeled constraints reveal that relative constraint is often more efficient at improving clustering."
      }
    ]
  },
  {
    "tid":"336",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Location-Based Distance Measures for Geosocial Similarity.\nContent: This article investigates the problem of geosocial similarity among users of online social networks, based on the locations of their activities (e.g., posting messages or photographs). Finding pairs of geosocially similar users or detecting that two sets of locations (of activities) belong to the same user has important applications in privacy protection, recommendation systems, urban planning, and public health, among others. It is explained and shown empirically that common distance measures between sets of locations are inadequate for determining geosocial similarity. Two novel distance measures between sets of locations are introduced. One is the mutually nearest distance that is based on computing a matching between two sets. The second measure uses a quad-tree index. It is highly scalable but incurs the overhead of creating and maintaining the index. Algorithms with optimization techniques are developed for computing the two distance measures and also for finding the k-most-similar users of a given one. Extensive experiments, using geotagged messages from Twitter, show that the new distance measures are both more accurate and more efficient than existing ones.\n\nTitle: Combined geo-social search: computing top-k join queries over incomplete information\nContent: Geo-social data sets, which fuse the social and the geospatial facets of data, are vibrant data sources that associate people and activities with locations. In a , several search queries are posed over geospatial and social data sources, or over data sources with both geospatial and social facets; and the search results, provided as ranked lists of items, are integrated by associating matching items, yielding . Each combination has a score which is a function of the scores of the items it comprises, and the goal is to compute the  combinations with the highest score, that is, the top-k combinations. However, since geo-social data sources are heterogeneous, data items may not have matching items in all the ranked lists. Such items cannot be included in complete combinations. Hence, we study the approach where combinations are padded by nulls for missing items, as in outer-join. A combination is  if it cannot be extended by replacing a null by an item. We show that if some of the top-k maximal combinations contain null values, the computation requires reading entire lists, and hence, traditional top-k algorithms and optimization techniques are not as effective as in the case of an ordinary top-k join. Thus, we present two novel algorithms for computing the top-k maximal combinations. One novel algorithm is instance optimal over the class of algorithms that compute a -approximation to the answer. The second algorithm is more efficient than the modification of two common top-k algorithms to compute maximal combinations. We show this analytically, and experimentally over real and synthetic data.\n\nTitle: Finding corresponding objects when integrating several geo-spatial datasets\nContent: When integrating geo-spatial datasets, a join algorithm is used for finding sets of corresponding objects (i.e., objects that represent the same real-world entity). Algorithms for joining two datasets were studied in the past. This paper investigates integration of three datasets and proposes methods that can be easily generalized to any number of datasets. Two approaches that use only locations of objects are presented and compared. In one approach, a join algorithm for two datasets is applied sequentially. In the second approach, all the integrated datasets are processed simultaneously. For the two approaches, join algorithms are given and their performances, in terms of recall and precision, are compared. The algorithms are designed to perform well even when locations are imprecise and each dataset represents only some of the real-world entities. Results of extensive experiments show that one of the algorithms has the best (or close to the best) performances under all circumstances. This algorithm has a much better performance than applying sequentially the one-sided nearest-neighbor join.\n\nTitle: Heuristic algorithms for route-search queries over geographical data\nContent: In a geographical route search, given search terms, the goal is to find an effective route that (1) starts at a given location, (2) ends at a given location, and (3) travels via geographical entities that are relevant to the given terms. A route is effective if it does not exceed a given distance limit whereas the ranking scores of the visited entities, with respect to the search terms, are maximal. This paper introduces route-search queries, suggests three semantics for such queries and deals with the problem of efficiently answering queries under the different semantics. Since the problem of answering route-search queries is a generalization of the traveling salesman problem, it is unlikely to have an efficient solution, i.e., there is no polynomial-time algorithm that solves the problem (unless P=NP). Hence, in this work we consider heuristics for the problem. Methods for effectively computing routes are presented. The methods are compared analytically and experimentally. For these methods, experiments on both synthetic and real-world data illustrate their efficiency and their effectiveness in computing a route that satisfies the constraints of a route-search query.\n\nTitle: Geosocial Search: Finding Places based on Geotagged Social-Media Posts\nContent: Geographic search -- where the user provides keywords and receives relevant locations depicted on a map -- is a popular web application. Typically, such search is based on static geographic data. However, the abundant geotagged posts in microblogs such as Twitter and in social networks like Instagram, provide contemporary information that can be used to support geosocial search---geographic search based on user activities in social media. Such search can point out where people talk (or tweet) about different topics. For example, the search results may show where people refer to ``jogging'', to indicate popular jogging places. The difficulty in implementing such search is that there is no natural partition of the space into ``documents'' as in ordinary web search. Thus, it is not always clear how to present results and how to rank and filter results effectively. In this paper, we demonstrate a two-step process of first, quickly finding the relevant areas by using an arbitrary indexed partition of the space, and secondly, applying clustering on discovered areas, to present more accurate results. We introduce a system that utilizes geotagged posts in geographic search and illustrate how different ranking methods can be used, based on the proposed two-step search process. The system demonstrates the effectiveness and usefulness of the approach.\n\n## Task\nCompose an abstract for the title 'Location-based algorithms for finding sets of corresponding objects over several geo-spatial data sets' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"When integrating geo-spatial data sets, a join algorithm is used for finding sets of corresponding objects (i.e., objects that represent the same real-world entity). This article investigates location-based join algorithms for integration of several data sets. First, algorithms for integration of two data sets are presented and their performances, in terms of recall and precision, are compared. Then, two approaches for integration of more than two data sets are described. In one approach, all the integrated data sets are processed simultaneously. In the second approach, a join algorithm for two data sets is applied sequentially, either in a serial manner, where in each join at least one of the joined data sets is a single source, or in a hierarchical manner, where two join results can be joined. For the two approaches, join algorithms are given. The algorithms are designed to perform well even when location of objects are imprecise and each data set represents only some of the real-world entities. Results of extensive experiments with the different approaches are provided and analyzed. The experiments show the differences, in accuracy and efficiency, between the approaches, under different circumstances. The results also show that all our algorithms have much better accuracy than applying the commonly used one-sided nearest-neighbor join."
      }
    ]
  },
  {
    "tid":"337",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Content-based image retrieval by using tree-structured features and multi-layer self-organizing map\nContent: A new approach for content-based image retrieval (CBIR) is described. In this study, a tree-structured image representation together with a multi-layer self-organizing map (MLSOM) is proposed for efficient image retrieval. In the proposed tree-structured image representation, a root node contains the global features, while child nodes contain the local region-based features. This approach hierarchically integrates more information of image contents to achieve better retrieval accuracy compared with global and region features individually. MLSOM in the proposed method provides effective compression and organization of tree-structured image data. This enables the retrieval system to operate at a much faster rate than that of directly comparing query images with all images in databases. The proposed method also adopts a relevance feedback scheme to improve the retrieval accuracy by a respectable level. Our obtained results indicate that the proposed image retrieval system is robust against different types of image alterations. Comparative results corroborate that the proposed CBIR system is promising in terms of accuracy, speed and robustness.\n\nTitle: Content-based hierarchical document organization using multi-layer hybrid network and tree-structured features\nContent: Automatic organizing documents through a hierarchical tree is demanding in many real applications. In this work, we focus on the problem of content-based document organization through a hierarchical tree which can be viewed as a classification problem. We proposed a new document representation to enhance the classification accuracy. We developed a new hybrid neural network model to handle the new document representation. In our document representation, a document is represented by a tree-structure that has a superior capability of encoding document characteristics. Compared to traditional feature representation that encodes only global characteristics of a document, the proposed approach can encode both global and local characteristics of a document through a hierarchical tree. Unlike traditional representation, the tree representation reflects the spatial organizations of words through pages and paragraphs of a document that help to encode better semantics of a document. Processing hierarchical tree is another challenging task in terms of computational complexity. We developed a hybrid neural network model, composed of SOM and MLP, for this task. Experimental results corroborate that our approach is efficient and effective in registering documents into organized tree compared with other approach.\n\nTitle: A flexible multi-layer self-organizing map for generic processing of tree-structured data\nContent: A new multi-layer self-organizing map (MLSOM) is proposed for unsupervised processing tree-structured data. The MLSOM is an improved self-organizing map for handling structured data. By introducing multiple SOM layers, the MLSOM can overcome the computational speed and visualization problems of SOM for structured data (SOM-SD). Node data in different levels of a tree are processed in different layers of the MLSOM. Root nodes are dedicatedly processed on the top SOM layer enabling the MLSOM a better utilization of SOM map compared with the SOM-SD. Thus, the MLSOM exhibits better data organization, clustering, visualization, and classification results of tree-structured data. Experimental results on three different data sets demonstrate that the proposed MLSOM approach can be more efficient and effective than the SOM-SD.\n\nTitle: Multilayer SOM with tree-structured data for efficient document retrieval and plagiarism detection\nContent: This paper proposes a new document retrieval (DR) and plagiarism detection (PD) system using multilayer self-organizing map (MLSOM). A document is modeled by a rich tree-structured representation, and a SOM-based system is used as a computationally effective solution. Instead of relying on keywords/lines, the proposed scheme compares a full document as a query for performing retrieval and PD. The tree-structured representation hierarchically includes document features as document, pages, and paragraphs. Thus, it can reflect underlying context that is difficult to acquire from the currently used word-frequency information. We show that the tree-structured data is effective for DR and PD. To handle tree-structured representation in an efficient way, we use an MLSOM algorithm, which was previously developed by the authors for the application of image retrieval. In this study, it serves as an effective clustering algorithm. Using the MLSOM, local matching techniques are developed for comparing text documents. Two novel MLSOM-based PD methods are proposed. Detailed simulations are conducted and the experimental results corroborate that the proposed approach is computationally efficient and accurate for DR and PD.\n\nTitle: Automatic image annotation via compact graph based semi-supervised learning.\nContent: The insufficiency of labeled samples is major problem in automatic image annotation. However, unlabeled samples are readily available and abundant. Hence, semi-supervised learning methods, which utilize partly labeled samples and a large amount of unlabeled samples, have attracted increased attention in the field of image annotation. During the past decade, graph-based semi-supervised learning has been becoming one of the most important research areas in semi-supervised learning. In this paper, we propose a novel and effective graph based semi-supervised learning method for image annotation. The new method is derived by a compact graph that can well grasp the manifold structure. In addition, we theoretically prove that the proposed semi-supervised learning method can be analyzed under a regularized framework. It can also be easily extended to deal with out-of-sample data. Simulation results show that the proposed method can achieve better performance compared with other state-of-the-art graph based semi-supervised learning methods.\n\n## Task\nCompose an abstract for the title 'Content-based image retrieval using growing hierarchical self-organizing quadtree map' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, a growing hierarchical self-organizing quadtree map (GHSOQM) is proposed and used for a content-based image retrieval (CBIR) system. The incorporation of GHSOQM in a CBIR system organizes images in a hierarchical structure. The retrieval time by GHSOQM is less than that by using direct image comparison using a flat structure. Furthermore, the ability of incremental learning enables GHSOQM to be a prospective neural-network-based approach for CBIR systems. We also propose feature matrices, image distance and relevance feedback for region-based images in the GHSOQM-based CBIR system. Experimental results strongly demonstrate the effectiveness of the proposed system."
      }
    ]
  },
  {
    "tid":"338",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Termontography and DOGMA for Knowledge Engineering within PROLIX\nContent: In this article, we describe our ongoing research to combine two approaches, i.e. Termontography and DOGMA, for knowledge engineering. Both approaches have in common that they mainly rely on natural language to describe meaning. Termontography is a special form of terminography that results in an ontologically structured terminological resource. DOGMA is an abbreviation of Developing Ontology Guided Mediation for Agents. The DOGMA approach results in a scalable and modular ontology that can easily be (re)used for different domains and applications. Both Termontography and DOGMA have already been used separately during several research projects. In this article we explain how both approaches are being combined within the PROLIX project, and what the advantages of this combination are. The goal of PROLIX is to develop an open, integrated reference architecture for process-oriented learning and information exchange.\n\nTitle: Towards community-based evolution of knowledge-intensive systems\nContent: This article wants to address the need for a research effort and framework that studies and embraces the novel, difficult but crucial issues of adaptation of knowledge resources to their respective user communities, and vice versa, as a fundamental property within knowledge-intensive internet systems. Through a deep understanding of real-time, community-driven evolution of so-called ontologies, a knowledge-intensive system can be made operationally relevant and sustainable over longer periods of time. To bootstrap our framework, we adopt and extend the DOGMA ontology framework, and its community-grounded ontology engineering methodology DOGMA-MESS, with an ontology that models community concepts such as business rules, norms, policies, and goals as firstclass citizens of the ontology evolution process. Doing so ontology evolution can be tailored to the needs of a particular community. Finally, we illustrate with an example from an actual real-world problem setting, viz. interorganisational exchange of HR-related knowledge.\n\nTitle: DIY-CDR: an ontology-based, Do-It-Yourself component discoverer and recommender\nContent: The do-it-yourself (DIY) culture has been continuously articulated since mid-1920s. The DIY spirit gets gradually spread because people realize that DIY activities are a way to confirm their personal creativities, outsource results, and expand their social contacts. We are motivated to design and implement a computing environment to support users to DIY their personalized Internet-of-things (IoT) applications. Before a person starts his DIY processes, it is important for him to find proper components that meet his needs. This article records our recent results concerning how to automatically discover and recommend existing components to users. The controlled fully automated ontology-assisted matching strategy (C-FOAM) is a matching strategy that contains algorithms at three different levels--string, lexical, and conceptual (or graphical). In this paper, we extend C-FOAM and embed it in a component discovery and recommendation module, which is called do-it-yourself component discoverer and recommender (DIY-CDR). DIY-CDR also uses semantic decision tables (SDT) to gather user specific decision rules and set up parameters for C-FOAM.\n\nTitle: Semantic interoperation of information systems by evolving ontologies through formalized social processes\nContent: For autonomously developed information systems to interoperate in a meaningful manner, ontologies capturing the intended semantics of that interoperation have to be developed by a community of stakeholders in those information systems. As the requirements of the ontology and the ontology itself evolve, so in general will the community, and vice versa. Ontology construction should thus be viewed as a complex activity leading to formalized semantic agreement involving various social processes within the community, and that may translate into a number of ontology evolution operators to be implemented. The hybrid ontologies that emerge in this way indeed need to support both the social agreement processes in the stakeholder communities and the eventual reasoning implemented in the information systems that are governed by these ontologies. In this paper, we discuss formal aspects of the social processes involved, a so-called fact-oriented methodology and formalism to structure and describe these, as well as certain relevant aspects of the communities in which they occur. We also report on a prototypical tool set that supports such a methodology, and on examples of some early experiments.\n\nTitle: Organizing meaning evolution supporting systems using semantic decision tables\nContent: DOGMA-MESS (Meaning Evolution Support System) is a system and methodology for supporting scalable, community-grounded ontology engineering. It uses a socio-technical process of meaning negotiation to tackle the scalability problems in ontology engineering. In order to improve the effectiveness of DOGMA-MESS, we adopt the idea of Semantic Decision Table (SDT). An SDT contains semantically rich decision rules that guide DOGMA-MESS micro-processes. It separates the decision rules from DOGMA-MESS. SDT with different decision rules results in different final decisions, which can be evaluated. In this paper, we illustrate how SDTs are used and apply our approach in the domain of Human Resource Management.\n\n## Task\nCompose an abstract for the title 'Persistent ET0L systems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"A new class of parallel rewriting devices called persistent ET0L systems is presented. It is shown that these systems are equivalent (in their language-generating power) to some other classes of grammars investigated in the literature. Also this new mechanism sheds some light on the nature of nondeterminism in (variations of) ET0L systems."
      }
    ]
  },
  {
    "tid":"339",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Computational anatomy: computing metrics on anatomical shapes\nContent: An important area of research in Computational Anatomy is to assign a metric space structure to 2D/3D images of anatomical structures. The images are registered in the non-rigid dense large deformation setting by computing a diffeomorphic transformation between the given images. The metric distance on the images follows from the Lie Group structure of diffeomorphisms, which allows measurement of lengths of curves on the manifold of diffeomorphisms. We present here a gradient-based method to compute the diffeomorphism matching the given images and estimating the metric distance for the pair. We show results for matching 2D sections of canine heart images.\n\nTitle: Geodesic Shooting for Computational Anatomy\nContent: Studying large deformations with a Riemannian approach has been an efficient point of view to generate metrics between deformable objects, and to provide accurate, non ambiguous and smooth matchings between images. In this paper, we study the geodesics of such large deformation diffeomorphisms, and more precisely, introduce a fundamental property that they satisfy, namely the conservation of momentum. This property allows us to generate and store complex deformations with the help of one initial \"momentum\" which serves as the initial state of a differential equation in the group of diffeomorphisms. Moreover, it is shown that this momentum can be also used for describing a deformation of given visual structures, like points, contours or images, and that, it has the same dimension as the described object, as a consequence of the normal momentum constraint we introduce.\n\nTitle: Metric Space Structures for Computational Anatomy\nContent: This paper describes a method based on metric structures for anatomical analysis on a large set of brain MR images. A geodesic distance between each pair was measured using large deformation diffeomorphic metric mapping (LDDMM). Manifold learning approaches were applied to seek a low-dimensional embedding in the high- dimensional shape space, in which inference between healthy control and disease groups can be done using standard classification algorithms. In particular, the proposed method was evaluated on ADNI, a dataset for Alzheimer's disease study. Our work demonstrates that the high-dimensional anatomical shape space of the amygdala and hippocampi can be approximated by a relatively low dimension manifold.\n\nTitle: Computing Large Deformation Metric Mappings via Geodesic Flows of Diffeomorphisms\nContent: This paper examine the Euler-Lagrange equations for the solution of the large deformation diffeomorphic metric mapping problem studied in Dupuis et al. (1998) and Trouv\u00e9 (1995) in which two images I 0, I 1 are given and connected via the diffeomorphic change of coordinates I 0\u9a74\u9a74\u9a741=I 1 where \u9a74=\u9a741 is the end point at t= 1 of curve \u9a74 t , t\u9a74[0, 1] satisfying .\u9a74 t =v t (\u9a74 t ), t\u9a74 [0,1] with \u9a740=id. The variational problem takes the form $$\\mathop {\\arg {\\text{m}}in}\\limits_{\\upsilon :\\dot \\phi _t = \\upsilon _t \\left( {\\dot \\phi } \\right)} \\left( {\\int_0^1 {\\left\\| {\\upsilon _t } \\right\\|} ^2 {\\text{d}}t + \\left\\| {I_0 \\circ \\phi _1^{ - 1} - I_1 } \\right\\|_{L^2 }^2 } \\right),$$ where \u9a74v t\u9a74 V is an appropriate Sobolev norm on the velocity field v t(\u00b7), and the second term enforces matching of the images with \u9a74\u00b7\u9a74L 2 representing the squared-error norm.In this paper we derive the Euler-Lagrange equations characterizing the minimizing vector fields v t, t\u9a74[0, 1] assuming sufficient smoothness of the norm to guarantee existence of solutions in the space of diffeomorphisms. We describe the implementation of the Euler equations using semi-lagrangian method of computing particle flows and show the solutions for various examples. As well, we compute the metric distance on several anatomical configurations as measured by \u9a740 1\u9a74v t\u9a74 V dt on the geodesic shortest paths.\n\nTitle: The emerging discipline of Computational Functional Anatomy\nContent: Computational Functional Anatomy (CFA) is the study of functional and physiological response variables in anatomical coordinates. For this we focus on two things: (i) the construction of bijections (via diffeomorphisms) between the coordinatized manifolds of human anatomy, and (ii) the transfer (group action and parallel transport) of functional information into anatomical atlases via these bijections. We review advances in the unification of the bijective comparison of anatomical submanifolds via point-sets including points, curves and surface triangulations as well as dense imagery. We examine the transfer via these bijections of functional response variables into anatomical coordinates via group action on scalars and matrices in DTI as well as parallel transport of metric information across multiple templates which preserves the inner product.\n\n## Task\nCompose an abstract for the title 'Evolutions equations in computational anatomy.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"One of the main purposes in computational anatomy is the measurement and statistical study of anatomical variations in organs, notably in the brain or the heart. Over the last decade, our group has progressively developed several approaches for this problem, all related to the Riemannian geometry of groups of diffeomorphisms and the shape spaces on which these groups act. Several important shape evolution equations that are now used routinely in applications have emerged over time."
      }
    ]
  },
  {
    "tid":"340",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: The function of the spine and its morphological effect in quadruped robot locomotion\nContent: In quadruped animals, spinal movements contribute to locomotion in terms of controlling body posture, providing the foundation to generate leg movement, and integrating limb and trunk actions. Inspired by this biological findings, we develop two quadruped models featuring different numbers of spinal joints to demonstrate the spine-driven locomotion behaviors. To gain a deep understanding of how the locomotion is achieved by axial driven propulsion and how the spinal morphology affects locomotion, we exclusively employ actuated spinal joint(s) to the model with a minimalistic control strategy. We choose three individuals from these two models and analyze their behaviors in terms of gait properties, i.e., angle of attack, ground clearance, and movement of the center of mass. The results show that employing the spinal morphology with two joints can greatly enhance the stability and speed of locomotion. Among several advantageous properties of the two spinal joint model we identify the following. First, it allows the robot to adjust the movement of the center of mass to stabilize itself. Second, by providing more freedom to bend the spine, the robot can pull the rear legs forward, thus increasing the stride length. Finally, locomotion with this model exhibits two flight phases and greater flight proportion during each stride, similar to what it is observed from running cheetahs, which make significant difference in the speed and the gait.\n\nTitle: Spine as an engine: effect of spine morphology on spine-driven quadruped locomotion.\nContent: In quadruped animals, spinal movements contribute to locomotion in terms of controlling body posture and integrating limb and trunk actions. In this paper, we develop quadruped models with different numbers of spinal joints to demonstrate the spine-driven locomotion. Actuated spinal joint(s) are exclusively employed to these models with a minimalistic control strategy. We choose some typical individuals from two models and analyze them on gait properties. Results show that employing the spine morphology with two joints can greatly enhance the stability and speed of locomotion by readjusting the center of mass, increasing the stride length, and generating double flight phases similar to running cheetahs' gait, which makes significant difference in the speed and the gait. Furthermore, we explore and compare models with more spinal joints. Results show that all gaits emerged from them can be categorized into three types (bounding, bounding with double flight phase, and stotting gaits). Overall, bounding gait with double flight phases is a more biologically inspired locomotive behavior; model with two spinal joints could be sufficient to emulate biological spine-driven locomotive behaviors.\n\nTitle: Spine Dynamics As A Computational Resource In Spine-Driven Quadruped Locomotion\nContent: Recent results suggest that compliance and non-linearity in physical bodies of soft robots may not be disadvantageous properties with respect to control, but rather of advantage. In the context of morphological computation one could see such complex structures as potential computational resources. In this study, we implement and exploit this view point in a spine-driven quadruped robot called Kitty by using its flexible spine as a computational resource. The spine is an actuated multi-joint structure consisting of a sequence of soft silicone blocks. Its complex dynamics are captured by a set of force sensors and used to construct a closed-loop to drive the motor commands. We use simple static, linear readout weights to combine the sensor values to generate multiple gait patterns (bounding, trotting, turning behavior). In addition, we demonstrate the robustness of the setup by applying strong external perturbations in form of additional loads. The system is able to fully recover to its nominal gait patterns (which are encoded in the linear readout weights) after the perturbation has vanished.\n\nTitle: A Variable Stiffness Mechanism For Improving Energy Efficiency Of A Planar Single-Legged Hopping Robot\nContent: Recently, variable stiffness actuators (VSAs) have been considered as actuation approaches to improve energy efficiency of legged locomotion robots. In this paper, we present the design and implementation of a variable stiffness actuator, named L-MESTRAN, which allows for improving energy efficiency of a planar single-legged robot over different stride frequencies. The leg in our setup consists of an actuated hip joint and a passive knee joint equipped with the L-MESTRAN. This mechanism is capable of varying stiffness in a large range, maintaining stiffness with almost no energy, and offers a linear joint stiffness. We empirically demonstrate that the LMESTRAN actuator can increase energy efficiency for hopping locomotion for various stride frequencies. Furthermore, we also demonstrate the capability of the L-MESTRAN to adjust stiffness to improve energy efficiency during locomotion.\n\nTitle: A novel mechanism for varying stiffness via changing transmission angle\nContent: Compliant actuation contributes enormously in legged locomotion robotics since it is able to alleviate control efforts in improving the robot's adaptability and energy efficiency. In this paper, we present a novel design of a variable stiffness rotary actuator, called MESTRAN, which was especially targeted to address the limitations in terms of the amount of energy and time required to vary the stiffness of an actuated joint. We have constructed a mechanical model in simulation and a physical prototype. We conducted a series of experiments to validate the performance of the MESTRAN actuator prototype. The results from the simulation and experiments show that MESTRAN allows independent control of stiffness and position of an actuated rotary joint with a large operational range and high speed. The torque-displacement relationship is close to linear. Lastly, the MESTRAN actuator is energy-efficient since a certain stiffness level is maintained without energy input.\n\n## Task\nCompose an abstract for the title 'The Effect Of Spine Actuation And Stiffness On A Pneumatically-Driven Quadruped Robot For Cheetah-Like Locomotion' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Biological research has concluded that the actuation of the spine contributes significantly to the performance of quadrupeds in terms of controlling body posture, and integrating limbs and trunk actions. Inspired by this biological findings, we develop a pneumatically-driven quadruped robot called Renny with configurable spine morphology to study how the spine contributes to cheetah-like running. Three spine morphologies: rigid spine, passive spine, and actuated spine, are introduced and tested in Renny robot. In addition, we investigate the effect of the stiffness distribution of the spine muscles in the passive case. The experimental results show that the passive one where the dorsal stiffness is higher than the ventral stiffness can run faster, even faster than the rigid case. Moreover, the coordination between the legs and the actuated spine is studied in actuated spine morphology. We found that when the spinal movements are synchronized with the legs movements, the speed is much faster. In the actuated case, both flexion and extension benefit the increase of the speed by advancing limbs rapidly and increasing the limb swing."
      }
    ]
  },
  {
    "tid":"341",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Shared values, new vision: Collaboration and communities of practice in virtual reference and SQA.\nContent: This investigation of new approaches to improving collaboration, user/librarian experiences, and sustainability for virtual reference services VRS reports findings from a grant project titled \\\"Cyber Synergy: Seeking Sustainability between Virtual Reference and Social Q&A Sites\\\" Radford, Connaway, & Shah, -2014. In-depth telephone interviews with 50 VRS librarians included questions on collaboration, referral practices, and attitudes toward Social Question and Answer SQA services using the Critical Incident Technique Flanagan, . The Community of Practice CoP Wenger, ; Davies, framework was found to be a useful conceptualization for understanding VRS professionals' approaches to their work. Findings indicate that participants usually refer questions from outside of their area of expertise to other librarians, but occasionally refer them to nonlibrarian experts. These referrals are made possible because participants believe that other VRS librarians are qualified and willing collaborators. Barriers to collaboration include not knowing appropriate librarians/experts for referral, inability to verify credentials, and perceived unwillingness to collaborate. Facilitators to collaboration include knowledge of appropriate collaborators who are qualified and willingness to refer. Answers from SQA services were perceived as less objective and authoritative, but participants were open to collaborating with nonlibrarian experts with confirmation of professional expertise or extensive knowledge.\n\nTitle: Modalities, motivations, and materials - investigating traditional and social online Q&A services\nContent: With the advent of ubiquitous connectivity and a constant flux of user-generated content, people's online information-seeking behaviours are rapidly changing, one o f which includes seeking information from peers through online questioning. Ways to understand this new behaviour can be broken down into three aspects, also referred to as the three M's - the modalities (sources and strategies) that people use when asking their questions online, their motivations behind asking these questions and choosing specific services, and the types and quality of the materials (content) generated in such an online Q&A environment. This article will provide a new framework - three M's - based on the synthesis of relevant literature. It will then identify some of the gaps in our knowledge about online Q&A based on this framework. These gaps will be transformed into six research questions, stemming from the three M's, and addressed by (a) consolidating and synthesizing findings previously reported in the literature, (b) conducting new analyses of data used in prior work, and (c) administering a new study to answer questions unaddressed by the pre-existing and new analyses of prior work.\n\nTitle: Questioning the Question -- Addressing the Answerability of Questions in Community Question-Answering\nContent: In this paper, we investigate question quality among questions posted in Yahoo! Answers to assess what factors contribute to the goodness of a question and determine if we can flag poor quality questions. Using human assessments of whether a question is good or bad and extracted textual features from the questions, we built an SVM classifier that performed with relatively good classification accuracy for both good and bad questions. We then enhanced the performance of this classifier by using additional human assessments of question type as well as additional question features to first separate questions by type and then classify them. This two-step classifier improved the performance of the original classifier in identifying Type II errors and suggests that our model presents a novel approach for identifying bad questions with implications for query revision and routing.\n\nTitle: Collaborative information seeking: art and science of achieving 1+12 in IR\nContent: The assumption of information seekers being independent and IR problem being individual has been challenged often in the recent past, with an argument that the next big leap in search and retrieval will come through incorporating social and collaborative aspects of information seeking. This half-day tutorial will introduce the student to theories, methodologies, and tools that focus on information retrieval/seeking in collaboration. The student will have an opportunity to learn about the social aspect of IR with a focus on collaborative information seeking (CIS) situations, systems, and evaluation techniques. The course is intended for those interested in social and collaborative aspects of IR (from both academia and industry), and requires only a general understanding of IR systems and evaluation.\n\nTitle: Retrieving Rising Stars In Focused Community Question-Answering\nContent: In Community Question Answering (CQA)' forums, there is typically a small fraction of users who provide high-quality posts and earn a very high reputation status from the community. These top contributors are critical to the community since they drive the development of the site and attract traffic from Internet users. Identifying these individuals could be highly valuable, but this is not an easy task. Unlike publication or social networks, most CQA sites lack information regarding peers, friends, or collaborators, which can be an important indicator signaling future success or performance. In this paper, we attempt to perform this analysis by extracting different sets of features to predict future contribution. The experiment covers 376,000 users who remain active in Stack Overflow for at least one year and together contribute more than 21 million posts. One of the highlights of our approach is that we can identify rising stars after short observations. Our approach achieves high accuracy, 85 %, when predicting whether a user will become a top contributor after a few weeks of observation. As a slightly different problem in which we could observe a few posts by a user, our method achieves accuracy higher than 90 %. Our approach provides higher accuracy than baselines methods including a popular time series analysis. Furthermore, our methods are robust to different classifier algorithms. Identifying the rising stars early could help CQA administrators gain an overview of the site's future and ensure that enough incentive and support is given to potential contributors.\n\n## Task\nCompose an abstract for the title 'Social Q&A and virtual reference\u2014comparing apples and oranges with the help of experts and users' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Online question-answering (Q&A) services are becoming increasingly popular among information seekers. We divide them into two categories, social Q&A (SQA) and virtual reference (VR), and examine how experts (librarians) and end users (students) evaluate information within both categories. To accomplish this, we first performed an extensive literature review and compiled a list of the aspects found to contribute to a \u201cgood\u201d answer. These aspects were divided among three high-level concepts: relevance, quality, and satisfaction. We then interviewed both experts and users, asking them first to reflect on their online Q&A experiences and then comment on our list of aspects. These interviews uncovered two main disparities. One disparity was found between users\u2019 expectations with these services and how information was actually delivered among them, and the other disparity between the perceptions of users and experts with regard to the aforementioned three characteristics of relevance, quality, and satisfaction. Using qualitative analyses of both the interviews and relevant literature, we suggest ways to create better hybrid solutions for online Q&A and to bridge the gap between experts\u2019 and users\u2019 understandings of relevance, quality, and satisfaction, as well as the perceived importance of each in contributing to a good answer. \u00a9 2012 Wiley Periodicals, Inc."
      }
    ]
  },
  {
    "tid":"342",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: The effects of diversity on group productivity and member withdrawal in online volunteer groups\nContent: The \"wisdom of crowds\" argument emphasizes the importance of diversity in online collaborations, such as open source projects and Wikipedia. However, decades of research on diversity in offline work groups have painted an inconclusive picture. On the one hand, the broader range of insights from a diverse group can lead to improved outcomes. On the other hand, individual differences can lead to conflict and diminished performance. In this paper, we examine the effects of group diversity on the amount of work accomplished and on member withdrawal behaviors in the context of WikiProjects. We find that increased diversity in experience with Wikipedia increases group productivity and decreases member withdrawal -- up to a point. Beyond that point, group productivity remains high, but members are more likely to withdraw. Strikingly, no such diminishing returns were observed for differences in member interest, which increases productivity and decreases member withdrawal in a linear fashion. Our results suggest that the low visibility of individual differences in online groups may allow them to harvest more of the benefits of diversity while bearing less of the cost. We discuss how our findings can inform further research of online collaboration.\n\nTitle: The Impact and Evolution of Group Diversity in Online Open Collaboration.\nContent: Online open collaboration efforts, such as Wikipedia articles and open source software development, often involve a large crowd with diverse experiences and interests. Diversity, on the one hand, facilitates the access to and integration of a wide variety of information; on the other hand, it may cause conflict and hurt group performance. Although diversity's effects have been the subject of many studies in offline work groups (with the results remaining inconclusive), its effects in online self-organizing groups are underexplored. In this paper, we examine 648 WikiProjects to understand (1) how tenure disparity and interest variety affect group productivity and member withdrawal and (2) how the two types of diversity evolve over time. Our results show a curvilinear effect of tenure disparity, which increases productivity and decreases member withdrawal, up to a point. Beyond that point, productivity slightly decreases, and members are more likely to withdraw. In comparison, our results show a positive effect of interest variety on productivity and no significant effects on withdrawal. We also find that, over a project's life cycle, tenure disparity decreases and interest variety increases, with both converging toward the level that is optimal for group performance. Overall, our study highlights the importance of having diverse experiences and perspectives in online open collaboration and the power of self-organizing that helps groups evolve toward their high-performing zones. It also has practical implications on the design of collaboration tools and new forms of organizing work in traditional organizations.\n\nTitle: Searching for the goldilocks zone: trade-offs in managing online volunteer groups\nContent: Dedicated and productive members who actively contribute to community efforts are crucial to the success of online volunteer groups such as Wikipedia. What predicts member productivity? Do productive members stay longer? How does involvement in multiple projects affect member contribution to the community? In this paper, we analyze data from 648 WikiProjects to address these questions. Our results reveal two critical trade-offs in managing online volunteer groups. First, factors that increase member productivity, measured by the number of edits on Wikipedia articles, also increase likelihood of withdrawal from contributing, perhaps due to feelings of mission accomplished or burnout. Second, individual membership in multiple projects has mixed effects. It decreases the amount of work editors contribute to both the individual projects and Wikipedia as a whole. It increases withdrawal for each individual project yet reduces withdrawal from Wikipedia. We discuss how our findings expand existing theories to fit the online context and inform the design of new tools to improve online volunteer work.\n\nTitle: An Empirical Analysis of Design Choices in Neighborhood-Based Collaborative Filtering Algorithms\nContent: Collaborative filtering systems predict a user's interest in new items based on the recommendations of other people with similar interests. Instead of performing content indexing or content analysis, collaborative filtering systems rely entirely on interest ratings from members of a participating community. Since predictions are based on human ratings, collaborative filtering systems have the potential to provide filtering based on complex attributes, such as quality, taste, or aesthetics. Many implementations of collaborative filtering apply some variation of the neighborhood-based prediction algorithm. Many variations of similarity metrics, weighting approaches, combination measures, and rating normalization have appeared in each implementation. For these parameters and others, there is no consensus as to which choice of technique is most appropriate for what situations, nor how significant an effect on accuracy each parameter has. Consequently, every person implementing a collaborative filtering system must make hard design choices with little guidance. This article provides a set of recommendations to guide design of neighborhood-based prediction systems, based on the results of an empirical study. We apply an analysis framework that divides the neighborhood-based prediction approach into three components and then examines variants of the key parameters in each component. The three components identified are similarity computation, neighbor selection, and rating combination.\n\nTitle: Building member attachment in online communities: applying theories of group identity and interpersonal bonds\nContent: Online communities are increasingly important to organizations and the general public, but there is little theoretically based research on what makes some online communities more successful than others. In this article, we apply theory from the field of social psychology to understand how online communities develop member attachment, an important dimension of community success. We implemented and empirically tested two sets of community features for building member attachment by strengthening either group identity or interpersonal bonds. To increase identity-based attachment, we gave members information about group activities and intergroup competition, and tools for group-level communication. To increase bond-based attachment, we gave members information about the activities of individual members and interpersonal similarity, and tools for interpersonal communication. Results from a six-month field experiment show that participants' visit frequency and self-reported attachment increased in both conditions. Community features intended to foster identity-based attachment had stronger effects than features intended to foster bond-based attachment. Participants in the identity condition with access to group profiles and repeated exposure to their group's activities visited their community twice as frequently as participants in other conditions. The new features also had stronger effects on newcomers than on old-timers. This research illustrates how theory from the social science literature can be applied to gain a more systematic understanding of online communities and how theory-inspired features can improve their success.\n\n## Task\nCompose an abstract for the title 'The effects of group composition on decision quality in a social production community' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Online social production communities allow efficient construction of valuable and high-quality information sources. To be successful, community members must be effective at collaboration, including makink collective decisions in the presence of disagreement. We examined over 100,000 decisions made by small working groups in Wikipedia, and analyzed how decision quality in these online groups is influenced by four group composition factors: the size of the group, how members were invited to the group, the prior experience of group members, and apparent bias shown by the group administrator. Our findings lead us to recommendations for designers of social production communities."
      }
    ]
  },
  {
    "tid":"343",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Markov chain-based optimization of multihop IEEE 802.15.4 wireless sensor networks\nContent: In this work, we propose an optimization framework for the IEEE 802.15.4 medium access control (MAC) protocol. More precisely, we derive a theoretical tool providing reliable guidelines for tuning the parameters of the MAC protocol. The presented tool could be used in two different directions: (i) for fixed network topology, it might be of interest to determine the MAC protocol configuration able to guarantee the best performance according to some quality of service metric; (ii) for fixed parameters of the MAC protocol, it might be of interest to determine the optimal network topology. Both these situations appear in practical situations. In particular, the first scenario happens in all cases where one cannot decide a-priori the node positions, because of some randomness or for some physical constraints, so that the only degree of freedom is given by the MAC protocol itself. The second set of scenarios occurs in the circumstances where one can decide the nodes displacement, thus introducing more degrees of freedom, but making also the problem solution more complicated. The proposed optimization tool applies some classical operative research instruments to a recently proposed Markov-Chain based model that has shown to be suitable for the performance analysis of a generic Cluster-Tree (CT) multihop IEEE 802.15.4 network. We will also show that this tool could be effectively used in a real scenario with a low-cost low-energy hardware platform.\n\nTitle: Optimum Topology in Clustered IEEE 802.15.4 Sensor Networks with Decentralized Detection\nContent: In this paper, we present a mathematical framework to study decentralized detection in IEEE 802.15.4-compliant clustered wireless sensor networks (WSNs). Sensors are organized in clusters, with fusion centers (FCs) acting as cluster heads, and are supposed to observe the same common binary phenomenon. The FCs periodically send queries to sensors and wait for replies. Once data are received, the FCs perform majority fusion. Decisions are sent to an access point (AP), where a final data fusion is carried out and an estimate of the phenomenon is obtained. A data aggregation strategy at sensors is also investigated. The goal of this paper is to jointly investigate decentralized detection, medium access control (MAC), and data aggregation issues. Our results show that an optimum WSN clustering configuration exists and it depends on the data aggregation strategy used and the MAC parameters.\n\nTitle: Design of Optimized Convolutional and Serially Concatenated Convolutional Codes in the Presence of A-priori Information\nContent: In this paper, we focus on the design of optimized binary convolutional codes (CCs) and serially concatenated convolutional codes (SCCCs) in the presence of a-priori information (API) at the receiver. For large signal-to-noise ratios (SNRs), we first propose a CC design criterion based on the minimization of a union bound on the bit error probability (BEP). In this case, relevant performance gains, with respect to previously proposed CCs, are obtained. These gains persist even in the presence of estimation errors on the API. Then, we apply the same union bound-based design criterion to SCCCs. Since the BEP of SCCCs is characterized by a typical waterfall shape, the proposed union bound-based design criterion is accurate only at large SNR, to estimate the BEP floor. In order to complement this analysis, we propose a density evolution-based approach to optimize the SCCC design in terms of minimization of the SNR of the \"knee\" of the BEP curve. The obtained simulation results show substantial gains with respect to previously proposed parallel concatenated convolutional coding (PCCCing) schemes optimized under the assumption of no API at the decoder. Moreover, in the presence of strong API the proposed SCCCs allow to approach the Shannon limit (SL) more than any previously proposed turbo coding scheme.\n\nTitle: Markov Chain-based performance analysis of multihop IEEE 802.15.4 wireless networks\nContent: In this paper, we propose a Markov chain-based analytical framework for modeling the behavior of the medium access control (MAC) protocol in IEEE 802.15.4 wireless networks. Two scenarios are of interest. First, we consider networks where the (sensor) nodes communicate directly to the network coordinator (the final sink). Then, we consider cluster-tree (CT) scenarios where the sources communicate to the coordinator through a series of intermediate relay, which forward the received packets and do not generate traffic on their own. In both scenarios, no acknowledgment messages are used to confirm successful data packet deliveries and communications are beaconed (i.e., they rely on synchronization packets denoted as ''beacons''). In all cases, our focus is on networks where the sources and the relays have finite queues (denoted as buffers) to store data packets. The network performance is evaluated in terms of aggregate network throughput and packet delivery delay. The performance predicted by the proposed analytical framework is in very good agreement with realistic ns-2 simulation results.\n\nTitle: Energy-efficient mobile target detection in Wireless Sensor Networks with random node deployment and partial coverage\nContent: This paper addresses the problem of engineering energy-efficient target detection applications, using unattended Wireless Sensor Networks (WSNs) with random node deployment and partial coverage, for long-lasting surveillance of areas of interest. As battery energy depletion is a crucial issue, an effective approach consists in switching on and off, according to proper duty cycles, sensing and communication modules of wireless sensor nodes. Making these modules work in an intermittent fashion has an impact on (i) the latency of notification transmission (depending on the communication duty cycle), (ii) the probability of missed target detection (depending on the number of deployed nodes, the sensing duty cycle, and the number of incoming targets), and (iii) the delay in detecting an incoming target. In order to optimize the system parameters to reach given performance objectives, we first derive an analytical framework which allows us to evaluate the probability of missed target detection (in the presence of either single or multiple incoming targets), the notification transmission latency, the detection delay, and the network lifetime. Then, we show how this ''toolbox'' can be used to optimally configure system parameters under realistic performance constraints.\n\n## Task\nCompose an abstract for the title 'Optimized Anchors Placement: An Analytical Approach In Uwb-Based Tdoa Localization' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper, we consider the problem of locating an Automated Guided Vehicle (AGV) which moves on a plane in an industrial environment by means of Ultra-Wide Band (UWB) signaling from fixed Anchors Nodes (ANs) situated in the (three-dimensional) space. An analytical approach to optimize, under proper (realistic) constraints, the placement of the ANs used to locate the AGV is proposed. Analytical results are confirmed by simulations."
      }
    ]
  },
  {
    "tid":"344",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Performance of a Policy-Based Management System in IPv6 Networks Using COPS-PR\nContent: To guarantee the correct QoS, the new generation core networks are best managed by the use of a centralized server. Most of these networks use DiffServ due to its scalability, an important feature for large networks. A Bandwidth Broker architecture is a policy-based management system which purpose is to manage DiffServ networks in a centralized manner performing admission control and policy deliveries to network nodes. This paper presents the implementation of a bandwidth broker architecture for the management of QoS in DiffServ dual networks (IPv4/IPv6). This implementation, which has been programmed in JAVA, uses COPS-PR as an intra-domain communication protocol and manages the policies based on SLA/SLS preconfigured by the administrator. The performance of the proposed architecture has been tested by implementing a real IPv6 DiffServ domain testbed controlled by the developed Bandwidth Broker. The results of these tests and the corresponding conclusions are also presented.\n\nTitle: Implementing End-to-End Register Data-Flow Continuous Self-Test\nContent: While Moore's Law predicts the ability of semiconductor industry to engineer smaller and more efficient transistors and circuits, there are serious issues not contemplated in that law. One concern is the verification effort of modern computing systems, which has grown to dominate the cost of system design. On the other hand, technology scaling leads to burn-in phase out. As a result, in-the-field error rate may increase due to both actual errors and latent defects. Whereas data can be protected with arithmetic codes, there is a lack of cost-effective mechanisms for control logic. This paper presents a light-weight microarchitectural mechanism that ensures that data consumed through registers are correct. The structures protected include the issue queue logic and the data associated (i.e., tags and control signals), input multiplexors, rename data, replay logic, register free-list and release logic, and register file logic. Our results show a coverage around 90 percent for the targeted structures with a cost in power and area of about four percent, and without impact in performance.\n\nTitle: End-to-end register data-flow continuous self-test\nContent: While Moore's Law predicts the ability of semi-conductor industry to engineer smaller and more efficient transistors and circuits, there are serious issues not contemplated in that law. One concern is the verification effort of modern computing systems, which has grown to dominate the cost of system design. On the other hand, technology scaling leads to burn-in phase out. As a result, in-the-field error rate may increase due to both actual errors and latent defects. Whereas data can be protected with arithmetic codes (like parity or ECC), there is a lack of cost-effective mechanisms for control logic. This paper presents a light-weight microarchitectural mechanism that ensures that data consumed through registers are correct. Microarchitecture presents a new way to manage reliability and testing without significantly sacrificing cost and performance, offering a unique opportunity to detect errors in the field at low cost. Our results show a coverage around 90% for the targeted structures with a cost in power and area of about 4%. The structures protected include the issue queue logic and the data associated (i.e., tags, control signals), input multiplexors, rename data, replay logic, register free list, bypasses data and logic, MOB data and addresses, register file logic, register file storage and functional units.\n\nTitle: Microarchitectural Online Testing for Failure Detection in Memory Order Buffers\nContent: Technology scaling leads to burn-in phase out and higher postsilicon test complexity, which increases in-the-field failure rate due to both latent defects and actual errors, respectively. As a consequence, current reliability qualification methods will likely be infeasible. Microarchitecture knowledge of application runtime behavior offers a possibility to have low-cost continuous online testing techniques detect hard errors in the field. Whereas data can be protected with redundancy (like parity or ECC), there is a lack of mechanism for control logic. This paper proposes a microarchitectural approach for validating that the memory order buffer logic works correctly. Our design relies on a small cache-like structure that keeps track of the last store to each cached address. Each load is checked to have obtained the data from the youngest older producing store. We present three different implementations of this idea, offering different trade-offs for error coverage, performance overhead, and design complexity.\n\nTitle: IEC-61508 SIL 3 Compliant Pseudo-Random Number Generators for Probabilistic Timing Analysis\nContent: Probabilistic Timing Analysis (PTA), especially its measurement based variant (MBPTA), has shown to be competitive with state-of-the-art timing analysis techniques. The use of MBPTA to analyse the timing behaviour of safety-critical systems rests on its ability to derive trustworthy WCET bounds. This ability depends on the soundness of the MBPTA method per se, as well as on the satisfaction of safety requirements placed on the pseudo-random number generator (prng) that plays a key role in the platform-level randomisation needed by MBPTA. This paper presents the design of a low-area, low-power prng that meets IEC-61508 SIL 3 safety requirements and allows for seamless integration in a real-world multicore architecture. This work enables the development and the IEC-61508 certification of mixed-criticality systems that use MBPTA for deriving timing bounds for mixed-criticality software programs running on multicore processors.\n\n## Task\nCompose an abstract for the title 'IPv6 Conformance and Interoperability Testing' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The fear of incompatibility issues between legacy IPv4 systems and new IPv6 implementations can delay the migration towards a full IPv6 native Internet. Network engineers need to ensure that heterogeneous IPv6 devices will behave consistently to the IPv6 specifications and will interoperate correctly in a multi-vendor environment. This paper analyzes IPv6 Conformance Testing, a set of rigorous test methodologies that allows to throughout verify the quality of IPv6 devices during their entire life cycle, and can give confidence to the customers. We analyze the TAHI IPv6 Conformance and Interoperability Test Suite and present our Conformance Test results for the latest versions of IPv6 host implementations."
      }
    ]
  },
  {
    "tid":"345",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Sliding Mode Congestion Control for data center Ethernet networks\nContent: Recently, Ethernet is being enhanced as the unified switch fabric of data centers, called Data Center Ethernet. The end-to-end congestion management is one of the indispensable enhancements, and Quantized Congestion Notification (QCN) has been ratified to be the standard. Our experiments show that QCN suffers from the oscillation of the queue at the bottleneck link. With the changes of system parameters and network configurations, the oscillation may become so serious that the queue is emptied frequently. As a result, the utilization of the bottleneck link degrades. Theoretical analysis shows that QCN approaches to the equilibrium point mainly through the sliding mode motion. But whether QCN enters into the sliding mode motion also depends on both system parameters and network configurations. Hence, we present the Sliding Mode Congestion Control (SMCC) scheme, which can drive the system into the sliding mode motion under any conditions. SMCC benefits from the advantage that the sliding mode motion is insensitive to system parameters and external disturbances. Moreover, SMCC is simple, stable and has short response time. QCN can be replaced by SMCC easily since both of them follow the framework developed by the IEEE 802.1 Qau work group. Experiments on the NetFPGA platform show that SMCC is superior to QCN, especially in the condition that the traffic pattern and the network state are variable.\n\nTitle: Analysing and improving convergence of quantized congestion notification in Data Center Ethernet.\nContent: Quantized Congestion Notification (QCN) has been approved as the standard congestion management mechanism for the Data Center Ethernet (DCE). However, lots of work pointed out that QCN suffers from the problem of unfairness among different flows. In this paper, we found that QCN could achieve fairness, merely the convergence time to fairness is quite long. Thus, we build a convergence time model to investigate the reasons of the slow convergence process of QCN. We validate the precision of our model by comparing with experimental data on the NetFPGA platform. The results show that the proposed model accurately well characterizes the convergence time to fairness of QCN. Based on the model, the impact of QCN parameters, network parameters, and QCN variants on the convergence time is analysed in detail. Results indicate that the convergence time of QCN can be decreased if sources have the same rate increase probability or the rate increase step becomes larger at steady state. Enlightened by the analysis, we proposed a mechanism called QCN-T, which replaces the original Byte Counter and Timer at sources with a single modified Timer to reduce the convergence time. Finally, evaluations show great improvements of QCN-T in both convergence and stability.\n\nTitle: Modeling and Solving TCP Incast Problem in Data Center Networks\nContent: TCP Incast problem attracts much attention due to the catastrophic goodput drop. In this paper, a goodput model of the problem is built to understand why goodput collapse occurs and a solution to the problem based on the theoretical analysis is proposed. We found that the TCP Incast goodput deterioration is mainly caused by two types of timeouts, one happens at the tail of data blocks and dominates the goodput when the number of senders is small, while the other one at the head of data blocks and governs the goodput when the number of senders is large. The proposed model describes the relationship between these two types of timeouts and the Incast communication pattern, block size, bottleneck buffer size, and so on. The simulation results indicate that the model well characterizes the features of the TCP Incast problem. Enlightened by the analysis, a PRiority-based solution to the TCP INcast problem (PRIN) is proposed, which avoids timeouts at the head of blocks by reducing TCP send window and prevents timeouts at the tail of blocks by leveraging priority technology. The experimental results show that PRIN solves the TCP Incast problem.\n\nTitle: Phase Plane Analysis of Quantized Congestion Notification for Data Center Ethernet\nContent: Currently, Ethernet is being enhanced to become the unified switch fabric in data centers. With the unified switch fabric, the cost on redundant devices is reduced, while the design and management of data center networks are simplified. Congestion management is one of the indispensable enhancements on Ethernet, and Quantized Congestion Notification (QCN) has just been ratified as the formal standard. Though QCN has been investigated for several years, there exist few in-depth theoretical analyses on QCN. The most possible reason is that QCN is heuristically designed and involves the property of variable structure. The classic linear analysis method is incapable of handling the segmented nonlinearity of the variable structure system. In this paper, we use the phase plane method, which is suitable for systems of segmented nonlinearity, to analyze the QCN system. The overall dynamic behaviors of the QCN system are presented, and the sufficient conditions for the stable QCN system are deduced. These sufficient conditions serve as guidelines toward proper parameters setting. Moreover, we find that the stability of QCN is mainly promised by the sliding mode motion, which is the underlying reason for QCN's stable queue shown in numerous simulations and experiments. Experiments on the NetFPGA platform verify that the analytical results can explain the complex behaviors of QCN.\n\nTitle: Taming TCP incast throughput collapse in data center networks\nContent: The TCP incast problem attracts a lot of attention due to its wide existence in cloud services and catastrophic performance degradation. Some effort has been made to solve it. However, the industry is still struggling with it, such as Facebook. Based on the investigation that the TCP incast problem is mainly caused by the TimeOuts (TOs) occurring at the boundary of the stripe units, this paper presents a simple and effective TCP enhanced mechanism, called GIP (Guarantee Important Packets), for the applications with the TCP incast problem. The main idea is making TCP aware of the boundaries of the stripe units, and reducing the congestion window of each flow at the start of each stripe unit as well as redundantly transmitting the last packet of each stripe unit. GIP modifies TCP a little at the end hosts, thus it can be easily implemented. Also, it poses no impact on the other TCP-based applications. The results of both experiments on our testbed and simulations on the ns-2 platform demonstrate that TCP with GIP can avoid almost all of the TOs and achieve high goodput for applications with the incast communication pattern.\n\n## Task\nCompose an abstract for the title 'Sliding Mode Congestion Control for Data Center Ethernet Networks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Recently, Ethernet is enhanced as the unified switch fabric of data centers, called Data Center Ethernet. One of the indispensable enhancements is end-to-end congestion management, and currently Quantized Congestion Notification (QCN) has been ratified as the corresponding standard. However, our experiments show that QCN suffers from large oscillations of the queue length at the bottleneck link such that the buffer is emptied frequently and accordingly the link utilization degrades, with certain system parameters and network configurations. This phenomenon is corresponding to our theoretical analysis result that QCN fails to enter into the Sliding Mode Motion (SMM) pattern with certain system parameters and network configurations. Knowing the drawbacks of QCN and realizing the advantage that congestion management system is insensitive to the changes of parameters and network configurations in the SMM pattern, we present Sliding Mode Congestion Control (SMCC), which can enter into the SMM pattern under any conditions. SMCC is simple, stable, fair, has short response time, and can be easily used to replace QCN because both of them follow the framework developed by the IEEE 802.1Qau work group. Experiments on the NetFPGA platform show that SMCC is superior to QCN, especially when traffic pattern and network states are variable."
      }
    ]
  },
  {
    "tid":"346",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Improving Metaheuristic Algorithms With Information Feedback Models.\nContent: In most metaheuristic algorithms, the updating process fails to make use of information available from individuals in previous iterations. If this useful information could be exploited fully and used in the later optimization process, the quality of the succeeding solutions would be improved significantly. This paper presents our method for reusing the valuable information available from previous individuals to guide later search. In our approach, previous useful information was fed back to the updating process. We proposed six information feedback models. In these models, individuals from previous iterations were selected in either a fixed or random manner. Their useful information was incorporated into the updating process. Accordingly, an individual at the current iteration was updated based on the basic algorithm plus some selected previous individuals by using a simple fitness weighting method. By incorporating six different information feedback models into ten metaheuristic algorithms, this approach provided a number of variants of the basic algorithms. We demonstrated experimentally that the variants outperformed the basic algorithms significantly on 14 standard test functions and 10 CEC 2011 real world problems, thereby, establishing the value of the information feedback models.\n\nTitle: The Effect of Information Utilization: Introducing a Novel Guiding Spark in the Fireworks Algorithm.\nContent: The fireworks algorithm (FWA) is a competitive swarm intelligence algorithm which has been shown to be very useful in many applications. In this paper, a novel guiding spark (GS) is introduced to further improve its performance by enhancing the information utilization in the FWA. The idea is to use the objective function\u2019s information acquired by explosion sparks to construct a guiding vector (GV) with promising direction and adaptive length, and to generate an elite solution called a GS by adding the GV to the position of the firework. The FWA with GS is called the guided FWA (GFWA). Experimental results show that the GS contributes greatly to both exploration and exploitation of the GFWA. The GFWA outperforms previous versions of the FWA and other swarm and evolutionary algorithms on a large variety of test functions and it is also a useful method for large scale optimization. The principle of the GS is very simple but efficient, which can be easily transplanted to other population-based algorithms.\n\nTitle: Iterative improvement of the Multiplicative Update NMF algorithm using nature-inspired optimization.\nContent: Low-rank approximations of data (e. g. based on the Singular Value Decomposition) have proven very useful in various data mining applications. The Non-negative Matrix Factorization (NMF) leads to special low-rank approximations which satisfy non-negativity constraints. The Multiplicative Update (MU) algorithm is one of the two original NMF algorithms and is still one of the fastest NMF algorithms per iteration. Nevertheless, MU demands a quite large number of iterations in order to provide an accurate approximation of the original data. In this paper we present a new iterative update strategy for the MU algorithm based on nature-inspired optimization algorithms. The goal is to achieve a better accuracy per runtime compared to the standard version of MU. Several properties of the NMF objective function underlying the MU algorithm motivate the utilization of heuristic search algorithms. Indeed, this function is usually non-differentiable, discontinuous, and may possess many local minima. Experimental results show that our new iterative update strategy for the MU algorithm achieves the same approximation error than the standard version in significantly fewer iterations and in faster overall runtime. \u00a9 2011 IEEE.\n\nTitle: Comparison of random number generators in Particle Swarm Optimization algorithm\nContent: Intelligent optimization algorithms are very effective to tackle complex problems that would be difficult or impossible to solve exactly. A key component within these algorithms is the random number generators (RNGs) which provide random numbers to drive the stochastic search process. Much effort is devoted to develop efficient RNGs with good statistical properties, and many highly optimized libraries are ready to use for generating random numbers fast on both CPUs and other hardware platforms such as GPUs. However, few study is focused on how different RNGs can effect the performance of specific intelligent optimization algorithms. In this paper, we empirically compared 13 widely used RNGs with uniform distribution based on both CPUs and GPUs, with respect to algorithm efficiency as well as their impact on Particle Swarm Optimization (PSO). Two strategies were adopted to conduct comparison among multiple RNGs for multiple objectives. The experiments were conducted on well-known benchmark functions of diverse landscapes, and were run on the GPU for the purpose of accelerating. The results show that RNGs have very different efficiencies in terms of speed, and GPU-based RNGs can be much faster than their CPU-based counterparts if properly utilized. However, no statistically significant disparity in solution quality was observed. Thus it is reasonable to use more efficient RNGs such as Mersenne Twister. The framework proposed in this work can be easily extended to compare the impact of non-uniformly distributed RNGs on more other intelligent optimization algorithms.\n\nTitle: Hybrid particle swarm optimizer with advance and retreat strategy and clonal mechanism for global numerical optimization\nContent: A novel particle swarm optimization algorithm based on advance and retreat strategy and clone mechanism (ARC-PSO) is proposed in this paper. It is well known that the advance-and-retreat strategy is a simple and effective method of one-dimensional search. We use the advance-and-retreat strategy to endow the clones with faster speed to find nearby local basins before next clonal operation. Furthermore, in the next clonal operation, the search space is enlarged greatly and the diversity of clones is increased. When the fitness value turns better after last ldquoflyingrdquo, the cloned particle advances. On the contrary, the cloned particle retreats then searches in the reverse direction of the last ldquoflyingrdquo with a small step-size of the previous velocity. Thus, the swarm has strong optimization ability. Comparisons among the proposed ARC-PSO, the conventional standard particle swarm optimization (SPSO) and the pure clone particle swarm optimization (CPSO) on thirteen benchmark test functions are presented in this paper. Experimental results show that the proposed ARC-PSO is capable of speeding up the evolution process significantly and improving the performance of global optimizer greatly.\n\n## Task\nCompose an abstract for the title 'Information Utilization Ratio in Heuristic Optimization Algorithms.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Heuristic algorithms are able to optimize objective functions efficiently because they use intelligently the information about the objective functions. Thus, information utilization is critical to the performance of heuristics. However, the concept of information utilization has remained vague and abstract because there is no reliable metric to reflect the extent to which the information about the objective function is utilized by heuristic algorithms. In this paper, the metric of information utilization ratio (IUR) is defined, which is the ratio of the utilized information quantity over the acquired information quantity in the search process. The IUR proves to be well-defined. Several examples of typical heuristic algorithms are given to demonstrate the procedure of calculating the IUR. Empirical evidences on the correlation between the IUR and the performance of a heuristic are also provided. The IUR can be an index of how finely an algorithm is designed and guide the invention of new heuristics and the improvement of existing ones."
      }
    ]
  },
  {
    "tid":"347",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Join query optimization techniques for complex event processing applications\nContent: AbstractComplex event processing (CEP) is a prominent technology used in many modern applications for monitoring and tracking events of interest in massive data streams. CEP engines inspect real-time information flows and attempt to detect combinations of occurrences matching predefined patterns. This is done by combining basic data items, also called \"primitive events\", according to a pattern detection plan, in a manner similar to the execution of multi-join queries in traditional data management systems. Despite this similarity, little work has been done on utilizing existing join optimization methods to improve the performance of CEP-based systems.In this paper, we provide the first theoretical and experimental study of the relationship between these two research areas. We formally prove that the CEP Plan Generation problem is equivalent to the Join Query Plan Generation problem for a restricted class of patterns and can be reduced to it for a considerably wider range of classes. This result implies the NP-completeness of the CEP Plan Generation problem. We further show how join query optimization techniques developed over the last decades can be adapted and utilized to provide practically efficient solutions for complex event detection. Our experiments demonstrate the superiority of these techniques over existing strategies for CEP optimization in terms of throughput, latency, and memory consumption.\n\nTitle: Monitoring distributed streams using convex decompositions\nContent: Emerging large-scale monitoring applications rely on continuous tracking of complex data-analysis queries over collections of massive, physically-distributed data streams. Thus, in addition to the space- and time-efficiency requirements of conventional stream processing (at each remote monitor site), effective solutions also need to guarantee communication efficiency (over the underlying communication network). The complexity of the monitored query adds to the difficulty of the problem --- this is especially true for non-linear queries (e.g., joins), where no obvious solutions exist for distributing the monitored condition across sites. The recently proposed geometric method, based on the notion of covering spheres, offers a generic methodology for splitting an arbitrary (non-linear) global condition into a collection of local site constraints, and has been applied to massive distributed stream-monitoring tasks, achieving state-of-the-art performance. In this paper, we present a far more general geometric approach, based on the convex decomposition of an appropriate subset of the domain of the monitoring query, and formally prove that it is always guaranteed to perform at least as good as the covering spheres method. We analyze our approach and demonstrate its effectiveness for the important case of sketch-based approximate tracking for norm, range-aggregate, and join-aggregate queries, which have numerous applications in streaming data analysis. Experimental results on real-life data streams verify the superiority of our approach in practical settings, showing that it substantially outperforms the covering spheres method.\n\nTitle: Scalable Proactive Event-Driven Decision Making\nContent: This paper proposes a methodology for proactive event-driven decision making. Proper decisions are made by forecasting events prior to their occurrence. Motivation for proactive decision making stems from social and economic factors, and is based on the fact that prevention is often more effective than the cure. The decisions are made in real time and require swift and immediate processing of Big Data, that is, extremely large amounts of noisy data flooding in from various locations, as well as historical data. The methodology will recognize and forecast opportunities and threats, making the decision to capitalize on the opportunities and mitigate the threats. This will be explained through user-interaction and the decisions of human operators, in order to ultimately facilitate proactive decision making.\n\nTitle: A geometric approach to monitoring threshold functions over distributed data streams\nContent: Monitoring data streams in a distributed system is the focus of much research in recent years. Most of the proposed schemes, however, deal with monitoring simple aggregated values, such as the frequency of appearance of items in the streams. More involved challenges, such as the important task of feature selection (e.g., by monitoring the information gain of various features), still require very high communication overhead using naive, centralized algorithms. We present a novel geometric approach by which an arbitrary global monitoring task can be split into a set of constraints applied locally on each of the streams. The constraints are used to locally filter out data increments that do not affect the monitoring outcome, thus avoiding unnecessary communication. As a result, our approach enables monitoring of arbitrary threshold functions over distributed data streams in an efficient manner. We present experimental results on real-world data which demonstrate that our algorithms are highly scalable, and considerably reduce communication load in comparison to centralized algorithms.\n\nTitle: Communication-Efficient Distributed Variance Monitoring and Outlier Detection for Multivariate Time Series\nContent: Modern scale-out services are comprised of thousands of individual machines, which must be continuously monitored for unexpected failures. One recent approach to monitoring is latent fault detection, an adaptive statistical framework for scale-out, load-balanced systems. By periodically measuring hundreds of performance metrics and looking for outlier machines, it attempts to detect subtle problems such as misconfigurations, bugs, and malfunctioning hardware, before they manifest as machine failures. Previous work on a large, real-world Web service has shown that many failures are indeed preceded by such latent faults. Latent fault detection is an offline framework with large bandwidth and processing requirements. Each machine must send all its measurements to a centralized location, which is prohibitive in some settings and requires data-parallel processing infrastructure. In this work we adapt the latent fault detector to provide an online, communication- and computation-reduced version. We utilize stream processing techniques to trade accuracy for communication and computation. We first describe a novel communication-efficient online distributed variance monitoring algorithm that provides a continuous estimate of the global variance within guaranteed approximation bounds. Using the variance monitor, we provide an online distributed outlier detection framework for non-stationary multivariate time series common in scale-out systems. The adapted framework reduces data size and central processing cost by processing the data in situ, making it usable in wider settings. Like the original framework, our adaptation admits different comparison functions, supports non-stationary data, and provides statistical guarantees on the rate of false positives. Simulations on logs from a production system show that we are able to reduce bandwidth by an order of magnitude, with below 1% error compared to the original algorithm.\n\n## Task\nCompose an abstract for the title 'Lazy evaluation methods for detecting complex events' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The goal of Complex Event Processing (CEP) systems is to efficiently detect complex patterns over a stream of primitive events. A pattern of particular significance is a sequence, where we are interested in identifying that a number of primitive events have arrived on the stream in a predefined order. Many popular CEP systems employ Non-deterministic Finite Automata (NFA) arranged in a chain topology to detect such sequences. Existing NFA-based mechanisms incrementally extend previously observed prefixes of a sequence until a match is found. Consequently, each newly arriving event needs to be processed to determine whether a new prefix is to be initiated or an existing one extended. This approach may be very inefficient when events at the beginning of the sequence are very frequent. We address the problem by introducing a lazy evaluation mechanism that is able to process events in descending order of selectivity. We employ this mechanism in a chain topology NFA, which waits until the most selective event in the sequence arrives and then adds events to partial matches according to a predetermined order of selectivity. In addition, we propose a tree topology NFA that does not require the selectivity order to be defined in advance. Finally, we experimentally evaluate our mechanism on real-world stock trading data, demonstrating a performance gain of two orders of magnitude, with significantly reduced memory resource requirements."
      }
    ]
  },
  {
    "tid":"348",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: NP-Completeness and Inapproximability of the Virtual Network Embedding Problem and Its Variants.\nContent: Many resource allocation problems in the cloud can be described as a basic Virtual Network Embedding Problem (VNEP): the problem of finding a mapping of a request graph (describing a workload) onto a substrate graph (describing the physical infrastructure). Applications range from mapping testbeds (from where the problem originated), over the embedding of batch-processing workloads (virtual clusters) to the embedding of service function chains. The different applications come with their own specific requirements and constraints, including node mapping constraints, routing policies, and latency constraints. While the VNEP has been studied intensively over the last years, complexity results are only known for specific models and we lack a comprehensive understanding of its hardness. This paper charts the complexity landscape of the VNEP by providing a systematic analysis of the hardness of a wide range of VNEP variants, using a unifying and rigorous proof framework. In particular, we show that the problem of finding a feasible embedding is already $mathcal{NP}$-complete in general, and, hence, the VNEP cannot be approximated under any objective, unless $mathcal{NP} = mathcal{P}$ holds. Importantly, we derive $mathcal{NP}$-completeness results also for finding approximate embeddings, which may violate, e.g., capacity constraints by certain factors. Lastly, we prove that our results still pertain when restricting the request graphs to planar or degree-bounded graphs.\n\nTitle: Competitive and Deterministic Embeddings of Virtual Networks\nContent: Network virtualization is a paradigm that allows for flexible and efficient\nallocation of the resources among multiple virtual networks (VNets). In this\npaper we deal with the problem of embedding dynamically arriving VNet requests.\nWe describe a generic algorithm for the online VNet embedding problem and\nanalyze its competitive ratio. This means that we compare the benefit\naccumulated by the algorithm with the benefit of an optimal offline algorithm.\nWe prove that the competitive ratio of our online algorithm is, loosely\nspeaking, logarithmic in the sum of the resources. Our algorithm is generic in\nthe sense that it supports multiple traffic models, multiple routing models,\nand allows for nonuniform benefits and durations of VNet requests. Concretely,\nthe routing models considered in this paper include: multipaths, single paths,\nand tree routing. For modeling traffic, we study the customer-pipe model, the\nhose model, and a new traffic model, called aggregate ingress model, that is\nwell suited for modeling multicasts and multi-party video conferences.\n\nTitle: (FPT-)Approximation Algorithms for the Virtual Network Embedding Problem.\nContent: Many resource allocation problems in the cloud can be described as a basic Virtual Network Embedding Problem (VNEP): finding mappings of request graphs (describing the workloads) onto a substrate graph (describing the physical infrastructure). In the offline setting, the two natural objectives are profit maximization, i.e., embedding a maximal number of request graphs subject to the resource constraints, and cost minimization, i.e., embedding all requests at minimal overall cost. The VNEP can be seen as a generalization of classic routing and call admission problems, in which requests are arbitrary graphs whose communication endpoints are not fixed. Due to its applications, the problem has been studied intensively in the networking community. However, the underlying algorithmic problem is hardly understood. This paper presents the first fixed-parameter tractable approximation algorithms for the VNEP. Our algorithms are based on randomized rounding. Due to the flexible mapping options and the arbitrary request graph topologies, we show that a novel linear program formulation is required. Only using this novel formulation the computation of convex combinations of valid mappings is enabled, as the formulation needs to account for the structure of the request graphs. Accordingly, to capture the structure of request graphs, we introduce the graph-theoretic notion of extraction orders and extraction width and show that our algorithms have exponential runtime in the request graphsu0027 maximal width. Hence, for request graphs of fixed extraction width, we obtain the first polynomial-time approximations. Studying the new notion of extraction orders we show that (i) computing extraction orders of minimal width is NP-hard and (ii) that computing decomposable LP solutions is in general NP-hard, even when restricting request graphs to planar ones.\n\nTitle: Online strategies for intra and inter provider service migration in virtual networks\nContent: Network virtualization allows one to build dynamic distributed systems in which resources can be dynamically allocated at locations where they are most useful. In order to fully exploit the benefits of this new technology, protocols need to be devised which react efficiently to changes in the demand. This paper argues that the field of online algorithms and competitive analysis provides useful tools to deal with and reason about the uncertainty in the request dynamics, and to design algorithms with provable performance guarantees. As a case study, we describe a system (e.g., a gaming application) where network virtualization is used to support thin client applications for mobile devices to improve their Quality-of-Service (QoS). By decoupling the service from the underlying resource infrastructure, it can be migrated closer to the current client locations while taking into account migration cost. This paper identifies the major cost factors in such a system, and formalizes the corresponding optimization problem. Both randomized and deterministic, gravity center based online algorithms are presented which achieve a good tradeoff between improved QoS and migration cost in the worst-case, both for service migration within an infrastructure provider as well as for networks supporting cross-provider migration. We report on our simulation results and also present an explicit construction of an optimal offline algorithm which can be used, e.g., to evaluate the competitive ratio empirically.\n\nTitle: Online Admission Control and Embedding of Service Chains\nContent: The virtualization and softwarization of modern computer networks enables the definition and fast deployment of novel network services called service chains: sequences of virtualized network functions e.g., firewalls, caches, traffic optimizers through which traffic is routed between source and destination. This paper attends to the problem of admitting and embedding a maximum number of service chains, i.e., a maximum number of source-destination pairs which are routed via a sequence of \u2113 to-be-allocated, capacitated network functions. We consider an Online variant of this maximum Service Chain Embedding Problem, short OSCEP, where requests arrive over time, in a worst-case manner. Our main contribution is a deterministic Olog\u2113-competitive online algorithm, under the assumption that capacities are at least logarithmic in \u2113. We show that this is asymptotically optimal within the class of deterministic and randomized online algorithms. We also explore lower bounds for offline approximation algorithms, and prove that the offline problem is APX-hard for unit capacities and small\u00ef\u00be\u017a\u2113\u00ef\u00be\u017a3, and even Poly-APX-hard in general, when there is no bound on\u00ef\u00be\u017a\u2113. These approximation lower bounds may be of independent interest, as they also extend to other problems such as Virtual Circuit Routing. Finally, we present an exact algorithm based on 0-1 programming, implying that the general offline SCEP is in NP and, by the above hardness results, it is NP-complete for constant \u2113.\n\n## Task\nCompose an abstract for the title 'Service Chain and Virtual Network Embeddings: Approximations using Randomized Rounding.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"The SDN and NFV paradigms enable novel network services which can be realized and embedded in a flexible and rapid manner. For example, SDN can be used to flexibly steer traffic from a source to a destination through a sequence of virtualized middleboxes, in order to realize so-called service chains. The service chain embedding problem consists of three tasks: admission control, finding suitable locations to allocate the virtualized middleboxes and computing corresponding routing paths. This paper considers the offline batch embedding of multiple service chains. Concretely, we consider the objectives of maximizing the profit by embedding an optimal subset of requests or minimizing the costs when all requests need to be embedded. Interestingly, while the service chain embedding problem has recently received much attention, so far, only non- polynomial time algorithms (based on integer programming) as well as heuristics (which do not provide any formal guarantees) are known. This paper presents the first polynomial time service chain approximation algorithms both for the case with admission and without admission control. Our algorithm is based on a novel extension of the classic linear programming and randomized rounding technique, which may be of independent interest. In particular, we show that our approach can also be extended to more complex service graphs, containing cycles or sub-chains, hence also providing new insights into the classic virtual network embedding problem."
      }
    ]
  },
  {
    "tid":"349",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Automatic web services composition in trustaware communities\nContent: The promise of Web Service Computing is to utilize Web ser- vices as fundamental elements for realizing distributed ap- plications/solutions. In particular, when no available service can satisfy client request, (parts of) available services can be composed and orchestrated in order to satisfy such a request. In this paper, we address the automatic composition when component services have access control & authorization con- straints, and impose further reputation constraints on other component services. In particular, access & authorization control is based on credentials, component services may (or not) trust of credentials issued by other component services and the service behavior is modeled by the possible conver- sations the service can have with its clients. We propose an automatic composition synthesis technique, based on reduc- tion to satisflability in Propositional Dynamic Logic, that is sound, complete and decidable. Moreover, we will charac- terize the computational complexity of the problem.\n\nTitle: Automatic Service Composition Based On Behavioral Descriptions\nContent: This paper addresses the issue of automatic service composition. We first develop a framework in which the exported behavior of a service is described in terms of a so-called execution tree, that is an abstraction for its possible executions. We then study the case in which such exported behavior (i.e. the execution tree of the service) can be represented by a finite state machine (i.e. finite state transition system). In this specific setting, we devise sound, complete and terminating techniques both to check for the existence of a composition, and to return a composition, if one exists. We also analyze the computational complexity of the proposed algorithms. Finally, we present an open source prototype tool, called ESC (E-Service Composer), that implements our composition technique. To the best of our knowledge, our work is the first attempt to provide a provably correct technique for the automatic synthesis of service composition, in a framework where the behavior of services is explicitly specified.\n\nTitle: : A Tool for Automatic Composition ofServices Based on Logics of Programs\nContent: Abstract. In this paper we discuss an effective technique for automatic service composition and we present the prototype software that imple- ments it. In particular, we characterize the behavior of a service in terms of a finite state machine. In this setting we discuss a technique based on satisfiability in a variant of Propositional Dynamic Logic that solves the automatic composition problem. Specifically, given (i) a client spec- ification of his desired service, i.e., the service he would like to interact with, and (ii) a set of available services, our technique synthesizes the orchestration schema of a composite service that uses only the available services and fully realizes the client specification. The developed system is an open-source software tool, called ESC (e-service composer), that im- plements our composition technique starting from services, each of them described in terms of a WSDL specification and a behavioral description expressed in any language that can capture finite state machines.\n\nTitle: Automatic Web Service Composition\nContent: The tutorial aims at providing a deep comprehension of the Web Service Composition problem and automated techniques to tackle it. Web Service Composition is currently one of the most hyped and addressed issue in the Service Oriented Computing. Starting from an analysis of current technologies and standards for Web Service Composition, the tutorial will lead the attendees to consider formal models at the base of current proposals, and techniques that can be fruitfully considered to address automatic composition synthesis in each of them. More in detail, attendees will consider: (i) basic technologies and standards for Web Service invocation and description (SOAP, UDDI, WSDL, ...); (ii) advanced technologies and standards for orchestration and inter-organizational process enactment, in particular WSBPEL and WS-CDL; (iii) models for Web Service composition; (iv) formal tools for both data-centric and process-centric synthesis, including query reformulation a'la Data Integration, transition-systems based formalisms, trace-based formalisms, logics of programs and processes. In particular, we will show how these formal tools can be applied for Automatic Web Service Composition; (v) current state-of-the-art research results in automatic service composition, drawing a comparison and defining a unifying framework.\n\nTitle: Automatic Composition of E-services That Export Their Behavior\nContent: The main focus of this paper is on automatic e-Service composition. We start by developing a framework in which the exported behavior of an e-Service is described in terms of its possible executions (execution trees). Then we specialize the framework to the case in which such exported behavior (i.e., the execution tree of the e-Service) is represented by a finite state machine. In this specific setting, we analyze the complexity of synthesizing a composition, and develop sound and complete algorithms to check the existence of a composition and to return one such a composition if one exists. To the best of our knowledge, our work is the first attempt to provide an algorithm for the automatic synthesis of e-Service composition, that is both proved to be correct, and has an associated computational complexity characterization.\n\n## Task\nCompose an abstract for the title 'Automatic composition of transition-based semantic web services with messaging' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"In this paper we present Colombo, a framework in which web services are characterized in terms of (i) the atomic processes (i.e., operations) they can perform; (ii) their impact on the \"real world\" (modeled as a relational database); (iii) their transition-based behavior; and (iv) the messages they can send and receive (from/to other web services and \"human\" clients). As such, Colombo combines key elements from the standards and research literature on (semantic) web services. Using Colombo, we study the problem of automatic service composition (synthesis) and devise a sound, complete and terminating algorithm for building a composite service. Specifically, the paper develops (i) a technique for handling the data, which ranges over an infinite domain, in a finite, symbolic way, and (ii) a technique to automatically synthesize composite web services, based on Propositional Dynamic Logic."
      }
    ]
  },
  {
    "tid":"350",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: On provisioning in all-optical networks: an impairment-aware approach\nContent: We investigate connection provisioning in an all-optical wavelength-division multiplexing (WDM) network in the presence of physical-layer impairments. As the channel bit rate increases, impairments pose a more serious problem, and lightpaths need to be routed intelligently, so that the destination node receives the signal with adequate quality. We study the models of major physical impairments that affect optical signals. With reasonable assumptions, we model the major impairments as link-based metrics and we formulate schemes for routing, which consider the impairment constraints imposed by the underlying physical infrastructure. Prior work in this area has focused on enumerating a set of paths based on criteria such as minimum hops/distances, and then selecting a path from this set based on complex calculations of signal quality. Our approach integrates verification of impairment constraints while searching for paths. Routing with multiple additive constraints being NP-hard, we give an optimal heuristic algorithm with fast execution time for practical networks, as well as a dynamic programming pseudo-polynomial-time algorithm for the case when one impairment constraint dominates. We also examine the case of a practical \"all-optical\" network with signal regeneration facility at some nodes. We explore routing in such a translucent network and discover that finding a simple feasible path passing through a regenerator is NP-complete.\n\nTitle: Energy-efficient dynamic provisioning for spectrum elastic optical networks\nContent: Spectrum elastic optical networks support flexible central frequency and spectrum assignment for lightpaths. In this paper, we investigate energy-efficient dynamic provisioning for such networks. When provisioning a connection, the routing problems in both electrical layer (routing over multiple lightpaths) and optical layer (routing over multiple fibers) have to be addressed. Also, the control plane has to determine the optical-layer spectrum assignment considering the spectrum-continuity constraint as well as the bandwidth variability of transponders. We adopt a novel auxiliary graph based on which a new dynamic provisioning policy called Time-Aware Provisioning with Bandwidth Reservation (TAP-BR) is proposed. TAP-BR incorporates two important factors to facilitate energy-efficient provisioning: time awareness and bandwidth reservation. We compare TAP-BR with previously-proposed dynamic provisioning policies and show that TAP-BR can save significant amount of energy and make efficient use of spectrum resources.\n\nTitle: Backup reprovisioning with partial protection for disaster-survivable software-defined optical networks\nContent: As networks grow in size, large-scale failures caused by disasters may lead to huge data loss, especially in an optical network employing wavelength-division multiplexing (WDM). Providing 100\u00a0% protection against disasters would require massive and economically unsustainable bandwidth overprovisioning, as disasters are difficult to predict, statistically rare, and may create large-scale failures. Backup reprovisioning schemes are proposed to remedy this problem, but in case of a large-scale disaster, even the flexibility provided by backup reprovisioning may not be enough, given the sudden reduction in available network resource, i.e., resource crunch. To mitigate the adverse effects of resource crunch, an effective resource reallocation is possible by exploiting service heterogeneity, specifically degraded-service tolerance, which makes it possible to provide some level of service, e.g., reduced capacity, to connections that can tolerate degraded service, versus no service at all. Software-Defined Networking (SDN) is a promising approach to perform such dynamic changes (redistribution of network resources) as it simplifies network management via centralized control logic. By exploiting these new opportunities, we propose a Backup Reprovisioning with Partial Protection (BRPP) scheme supporting dedicated-path protection, where backup resources are reserved but not provisioned (as in shared-path protection), such that the amount of bandwidth reserved for backups as well as their routings are subject to dynamic changes, given the network state, to increase utilization. The performance of the proposed scheme is evaluated by means of SDN emulation using Mininet environment and OpenDaylight as the controller.\n\nTitle: Risk-Aware Provisioning for Optical WDM Mesh Networks\nContent: A service-level agreement (SLA) typically specifies, among other metrics, the availability a service provider (SP) promises to a customer. In an optical wavelength division multiplexing (WDM) network, connection-oriented provisioning is commonly based on whether the path's statistical availability complies with the SLA-requested availability. Because of the stochastic nature of network failures, the actually provisioned availability over a specific time period is subject to uncertainty, and hence the SLA is usually at risk. We consider this uncertainty and study provisioning to minimize SLA violations. We show that the SLA Violation Risk is affected by a number of factors (e.g., failure profiles, availability target, and penalty period), and hence cannot simply be characterized by statistical path availability. We formulate the problem of risk-aware provisioning in WDM mesh networks, where path selection is dictated by SLA Violation Risk. In particular, we focus on devising an efficient scheme capable of computing path(s) that are likely to successfully accommodate the SLA-requested availability. A novel technique is applied to convert links with heterogeneous failure profiles to reference links that capture the main risk features in a relative manner. Based on the \u201creference link\u201d concept, our Risk-Aware Provisioning scheme uses only limited failure information. We also extend our Risk-Aware Provisioning to use shared-path protection (SPP) for connections with strict availability requirements. We evaluate the performance and demonstrate the effectiveness of our schemes in terms of SLA violation ratio compared to the generic availability-aware approaches.\n\nTitle: Dynamic Traffic Grooming in Elastic Optical Networks\nContent: Spectrum elastic optical networks support flexible central frequency and spectrum assignment for lightpaths. When provisioning a new connection in an elastic optical network that allows traffic grooming, the control plane has to solve two problems: the electrical-layer routing and optical-layer routing and spectrum assignment (RSA). The electrical-layer routing determines how to route the new connection through a combination of new and existing lightpaths, while the optical-layer RSA decides how to establish new lightpaths under the spectrum-continuity constraint. The flexibility (e.g., bandwidth variability of lightpaths) provided by elastic optical networks makes it suitable for accommodating dynamic traffic. It is important and challenging to exploit the full potential of the flexibility when dealing with the above two problems. In this study, we propose a multi-layer auxiliary graph to jointly solve the electrical-layer routing and optical-layer RSA. Various traffic-grooming policies (objectives) can be achieved by properly adjusting the edge weights in the auxiliary graph. Also, we propose a spectrum reservation scheme that can efficiently utilize the bandwidth variability of lightpaths by reserving bandwidth for non-fully utilized lightpaths and grooming future connections onto them. We show that there is a tradeoff among different traffic-grooming policies, and the spectrum reservation scheme can be easily incorporated into various traffic-grooming policies and lead to a significant reduction in operational expenditure (OPEX) and better spectrum efficiency.\n\n## Task\nCompose an abstract for the title 'Provisioning in Elastic Optical Networks with Non-Disruptive Defragmentation' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"How serious is spectrum fragmentation in an elastic optical network? It is addressed by the tremendous provisioning gain achieved by eliminating a particular kind of fragmentation, namely, one that could be removed by Hitless Optical Path Shift (HOPS), a non-disruptive defragmentation technology. We devised a series of HOPS-based defragmentation algorithms, ranging from proactive to reactive strategies. Analysis and simulation results show that for typical backbone networks operated with a light load, using our optimal reactive defragmentation scheme, 98% of the requested spectrum bandwidth, that is otherwise blocked, can be provisioned. Moreover, the corresponding defragmentation costs (in terms of impact on existing connections, and defragmentation delay) are kept at minimum."
      }
    ]
  },
  {
    "tid":"351",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Separating the contributions of variability and parameter uncertainty in probability distributions.\nContent: This paper proposes a computational methodology to quantify the individual contributions of variability and distribution parameter uncertainty to the overall uncertainty in a random variable. Even if the distribution type is assumed to be known, sparse or imprecise data leads to uncertainty about the distribution parameters. If uncertain distribution parameters are represented using probability distributions, then the random variable can be represented using a family of probability distributions. The family of distributions concept has been used to obtain qualitative, graphical inference of the contributions of natural variability and distribution parameter uncertainty. The proposed methodology provides quantitative estimates of the contributions of the two types of uncertainty. Using variance-based global sensitivity analysis, the contributions of variability and distribution parameter uncertainty to the overall uncertainty are computed. The proposed method is developed at two different levels; first, at the level of a variable whose distribution parameters are uncertain, and second, at the level of a model output whose inputs have uncertain distribution parameters.\n\nTitle: Role of calibration, validation, and relevance in multi-level uncertainty integration.\nContent: Calibration of model parameters is an essential step in predicting the response of a complicated system, but the lack of data at the system level makes it impossible to conduct this quantification directly. In such a situation, system model parameters are estimated using tests at lower levels of complexity which share the same model parameters with the system. For such a multi-level problem, this paper proposes a methodology to quantify the uncertainty in the system level prediction by integrating calibration, validation and sensitivity analysis at different levels. The proposed approach considers the validity of the models used for parameter estimation at lower levels, as well as the relevance at the lower level to the prediction at the system level. The model validity is evaluated using a model reliability metric, and models with multivariate output are considered. The relevance is quantified by comparing Sobol indices at the lower level and system level, thus measuring the extent to which a lower level test represents the characteristics of the system so that the calibration results can be reliably used in the system level. Finally the results of calibration, validation and relevance analysis are integrated in a roll-up method to predict the system output.\n\nTitle: Performance evaluation of smart systems under uncertainty\nContent: This paper develops a model-based framework for the quantification and propagation of multiple uncertainty sources affecting the performance of a smart system. A smart system, in general, performs sensing, control and actuation for proper functioning of a physical subsystem (also referred to as a plant). With strong feedback coupling between several subsystems, the uncertainty in the quantities of interest (QoI) amplifies over time. The coupling in a generic smart system occurs at two levels: (1) coupling between individual subsystems (plant, cyber, actuation, sensors), and (2) coupling between nodes in a distributed computational subsystem. In this paper, a coupled smart system is decoupled and considered as a feed-forward system over time and modeled using a two-level Dynamic Bayesian Network (DBN), one at each level of coupling (between subsystems and between nodes). A DBN can aggregate uncertainty from multiple sources within a time step and across time steps. The DBN associated with a smart system can be learned using available system models, physics models and data. The proposed methodology is demonstrated for the design of a smart indoor heating system (identification of sensors and a wireless network) within cost constraints that enables room-by-room temperature control. We observe that sensor uncertainty has a higher impact on the performance of the heating system compared to the uncertainty in the wireless network.\n\nTitle: Bayesian Sensitivity Analysis and Uncertainty Integration for Robust Optimization.\nContent: This paper presents a comprehensive methodology that combines uncertainty quantification, uncertainty propagation, and design optimization using a Bayesian framework. The epistemic uncertainty due to input data uncertainty is considered. Two types of uncertainty models for input variables and/or their distribution parameters are addressed: 1) uncertainty modeled as family of distributions, and 2) uncertainty modeled as interval data. A Bayesian approach is adopted to update the uncertainty models, where the likelihood functions are constructed using limited experimental data. Global sensitivity analysis, which previously only considered aleatory inputs in the context of probabilistic representation, is extended in this paper to quantify the contributions of both aleatory and epistemic uncertainty sources for multioutput problems using an auxiliary variable approach. Gaussian process surrogate modeling is employed to replace the expensive physics models and improve the computational efficiency. Apreviously developed bias-minimization technique, which only dealt with single-output functions, is extended to reduce the surrogate model error for a multioutput function. A decoupled robustness-based design optimization framework is developed to include both aleatory and epistemic uncertainties. The proposed methodology is illustrated using the NASA Langley Research Center's multidisciplinary uncertainty quantification challenge problem.\n\nTitle: A probabilistic approach for representation of interval uncertainty\nContent: In this paper, we propose a probabilistic approach to represent interval data for input variables in reliability and uncertainty analysis problems, using flexible families of continuous Johnson distributions. Such a probabilistic representation of interval data facilitates a unified framework for handling aleatory and epistemic uncertainty. For fitting probability distributions, methods such as moment matching are commonly used in the literature. However, unlike point data where single estimates for the moments of data can be calculated, moments of interval data can only be computed in terms of upper and lower bounds. Finding bounds on the moments of interval data has been generally considered an NP-hard problem because it includes a search among the combinations of multiple values of the variables, including interval endpoints. In this paper, we present efficient algorithms based on continuous optimization to find the bounds on second and higher moments of interval data. With numerical examples, we show that the proposed bounding algorithms are scalable in polynomial time with respect to increasing number of intervals. Using the bounds on moments computed using the proposed approach, we fit a family of Johnson distributions to interval data. Furthermore, using an optimization approach based on percentiles, we find the bounding envelopes of the family of distributions, termed as a Johnson p-box. The idea of bounding envelopes for the family of Johnson distributions is analogous to the notion of empirical p-box in the literature. Several sets of interval data with different numbers of intervals and type of overlap are presented to demonstrate the proposed methods. As against the computationally expensive nested analysis that is typically required in the presence of interval variables, the proposed probabilistic representation enables inexpensive optimization-based strategies to estimate bounds on an output quantity of interest.\n\n## Task\nCompose an abstract for the title 'Quantification of margins and uncertainties of complex systems in the presence of aleatoric and epistemic uncertainty' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Performance assessment of complex systems is ideally done through full system-level testing which is seldom available for high consequence systems. Further, a reality of engineering practice is that some features of system behavior are not known from experimental data, but from expert assessment, only. On the other hand, individual component data, which are part of the full system are more readily available. The lack of system level data and the complexity of the system lead to a need to build computational models of a system in a hierarchical or building block approach (from simple components to the full system). The models are then used for performance prediction in lieu of experiments, to estimate the confidence in the performance of these systems. Central to this are the need to quantify the uncertainties present in the system and to compare the system response to an expected performance measure. This is the basic idea behind Quantification of Margins and Uncertainties (QMU). QMU is applied in decision making\u2014there are many uncertainties caused by inherent variability (aleatoric) in materials, configurations, environments, etc., and lack of information (epistemic) in models for deterministic and random variables that influence system behavior and performance. This paper proposes a methodology to quantify margins and uncertainty in the presence of both aleatoric and epistemic uncertainty. It presents a framework based on Bayes networks to use available data at multiple levels of complexity (i.e. components, subsystem, etc.) and demonstrates a method to incorporate epistemic uncertainty given in terms of intervals on a model parameter."
      }
    ]
  },
  {
    "tid":"352",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Efficient Network Management System with DACS Scheme\nContent: Where customers with different membership and position, use computers as in the university network systems, it often takes much time and efforts for them to cope with the change of the system management. This is because the requirements for the respective computer usage are different in the network and security policies. In this paper, a new destination addressing control system (DACS) scheme for the university network services is proposed. The DACS Scheme performs the network services efficiently through the communication management of a client. As the characteristic of DACS Scheme, only the setup modification is required by a system administrator, when the configuration change is needed in the network server. Then, the setup modification is unnecessary by a customer, which shows a merit for both a system administrator and a customer. This paper describes the instruction and the prototype for DACS Protocol as the implementation of DACS Scheme. Then, the simplicity of the system management in DACS Scheme, is examined from the customer and the system administrator viewpoints.\n\nTitle: Simplified Network Management with DACS Scheme\nContent: When operation and management in the university network are performed, various problems occur. As one of those problems, user support problems in the network are often shown. As an example, coping with annoying communication to other users is described. The word of \"annoying communication\" means the communication which adversely affects processing performance and communication speed of other clients. In this case, under the management by DHCP Service, much time and effort are spent to specify where the client with annoying communication is. To simplify user supports such as this case, new form of user support by DACS (Destination Addressing Control System) Scheme, which is an efficient scheme for the network operation and management in university network, is proposed and examined. Without the virus infection client being specified by a system administrator, the communication port of the client is intercepted and the warning message is displayed on the screen of the client. The user who watches the warning message does an inquiry to the computer management section. Because the system administrator does not need to identify the virus infection client physically, network management is largely simplified.\n\nTitle: Evaluation of processing load in the network with DACS scheme\nContent: The study of DACS Scheme, which manages a whole network system by communication control on a client computer, has been advanced. The problem for applying DACS Scheme to practical network is that, processing load is heavy at the time of controlling communication. In this paper, we propose models for managing a whole network system; Course management type model (COMTM) and Client management type model (CLMTM). Each processing load which occurs at the time of controlling communication in these two models is compared. It is shown that, DACS Scheme which is CLMTM is advantageous in comparison with COMTM from the point of processing load, which occurs at the time of control communication.\n\nTitle: Establishment of Virtual Policy Based Network Management Scheme by Load Experimets in Virtual Environment\nContent: In the current Internet-based systems, there are many problems using anonymity of the network communication such as personal information leak and crimes using the Internet systems. This is because the TCP/IP protocol used in Internet systems does not have the user identification information on the communication data, and it is difficult to supervise the user performing the above acts immediately. As a solution for solving the above problem, there is the approach of Policy-based Network Management (PBNM). This is the scheme for managing a whole Local Area Network (LAN) through communication control of every user. In this PBNM, two types of schemes exist. The first is the scheme for managing the whole LAN by locating the communication control mechanisms on the course between network servers and clients. The second is the scheme of managing the whole LAN by locating the communication control mechanisms on clients. As the second scheme, we have been studied theoretically about the Destination Addressing Control System (DACS) Scheme. By applying this DACS Scheme to Internet system management, we intend to realize the policy-based Internet system management finally. In the DACS Scheme, inspection is not done about compatibility to cloud environment with virtualization technology that spreads explosively. As the result, the coverage of the DACS Scheme is limited only in physical environment now. In this study, we inspect compatibility of the DACS Scheme for the cloud environment with virtualization technology, and enlarge coverage of this scheme. With it, the Virtual DACS Scheme (vDACS Scheme) is established.\n\nTitle: Efficient User Support with DACS Scheme\nContent: When operation and management in the university network are performed, various problems occur. As one of those problems, user support problems in the network are often shown. As an example, support at the time of changing setups of a client computer by a user, is described. In this case, it take trouble very much for a system administrator because it is often necessary to support individual users, respectively. As another example, it is necessary to take some measure for annoying communication to other users. In this case, much time and effort are spent to specify where the client with annoying communication is under the management by DHCP Service. To simplify user supports such as those cases, a new user support by DACS (Destination Addressing Control System) Scheme, which is an efficient scheme for the network operation and management in university network, is examined.\n\n## Task\nCompose an abstract for the title 'DACS scheme as next generation policy-based network management scheme' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"As the work for managing a whole network effectively without a limited purpose, there is the work of PBNM (Policy-based network management). PBNM has two structural problems such as communication concentration from many clients to a communication control mechanism called PEP (Policy Enhancement Point) and the necessity of the network system updating at the time of introducing PBNM into LAN. Moreover, user support problems in campus-like computer networks such as trouble user support at updating a client's setups and coping with annoying communication can not be improved by PBNM. To improve these problems, we show a next generation PBNM which overcomes theses problems and has the function which does not exist in existing PBNM, and called it DACS (Destination Addressing Control System) Scheme. By DACS Scheme, communication concentration from many clients to PEP is solved, and system updating becomes unnecessary. Moreover, user support at updating the client's setups and coping with annoying communication by DACS Scheme becomes very effective."
      }
    ]
  },
  {
    "tid":"353",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: On Peer Selection and Protection for Reliable Peer-to-Peer Video Streaming\nContent: P2P live video streaming is an emerging technique for video-on-demand services. In a P2P network, the bandwidth, memory, and computation ability of peers are redistributed to provide large-scale services. The key concept of P2P streaming is that each peer should forward the received data to its neighboring peers. However, peers might unexpectedly join and leave the P2P network, namely, peer churns, which can seriously disturb the video bitstream sharing. When a parent peer leaves, the receiving peers must find a replacement peer in time; otherwise, the receiving peers cannot resume their video transport. In addition, peers that connect to those receiving peers will also be influenced. This effect is known as chain reaction. In this paper, we propose a new P2P streaming method involving peer selection, video coding, and packet protection. Simulation results show that our proposed method can achieve better video quality and more reliable P2P video streaming compared to existing schemes.\n\nTitle: A Hybrid Sender/Receiver-Driven Error Protection Scheme For Reliable P2p Scalable Video Streaming\nContent: This paper proposes a hybrid sender/receiver-driven error protection scheme to transmit scalable video packets over packet-lossy peer-to-peer networks. In our scheme, given an estimated system uplink capacity, a joint source-channel coding (JSCC) mechanism based on receiver-driven subscriptions is proposed to minimize the visual distortion received by child-peers by subscribing to appropriate amounts of source and channel coding packets. Because the bandwidth for inter-peer transmissions may fluctuate largely due to peer dynamics, in our method peers estimate the available system uplink capacity based on consensus propagation to avoid the fluctuating allocations of JSCC. To efficiently utilize the uplink bandwidth of peers, parent-peers use sender-driven contribution-guided peer selection to reject the low-contribution subscriptions requested from candidate child-peers. Simulation results demonstrate that our method significantly improves visual quality, compared to other state-of-the-art schemes.\n\nTitle: An unequal error protection scheme for reliable peer-to-peer scalable video streaming.\nContent: \u2022This paper proposes a systematic error protection scheme, involving packet-loss modeling, receiver-driven JSCC, and sender-driven peer selection. In the proposed scheme, given an estimated system uplink capacity, a receiver-driven JSCC mechanism is proposed by which each child-peer minimize its received visual distortion by subscribing to an appropriate amount of source and channel coding packets. To maximize the uplink bandwidth of peers, parent-peers can adaptively select child-peers to transmit subscribed packets according to the rate-distortion contribution of the subscriptions requested from child-peers.\u2022This paper also proposes a scheme for accurately and stably estimating the parent peers\u2019 average uplink capacity based on consensus propagation rather than estimating the average available uplink bandwidth of neighboring peers, which is usually unstable due to the dynamics of neighboring peers and thereby leads to fluctuating allocations of JSCC.\n\nTitle: A packet loss estimation model and its application to reliable mesh-based P2P video streaming\nContent: This paper proposes a model to estimate the packet loss probability in a mesh-based P2P network. Because of the irregular mesh structure, packet loss estimation for a mesh-based P2P network is more complicated than that in a tree-based network. The proposed model takes into account the channel packet drop rate, peer dynamics, and FEC protection to capture the heterogeneous packet loss behavior of individual video substreams transmitted over the irregular transmission paths of a mesh network. The simulation results show that the proposed packet loss model can accurately estimate the packet loss in a mesh-based P2P network. Based on the proposed model, we also propose a peer selection mechanism which can effectively mitigate packet loss propagation by selecting at a parent-peer the candidate child-peers that can achieve the minimal packet loss probability compared to others, to transmit the FEC redundant substream.\n\nTitle: A Control-Theoretic Approach to Rate Adaption for DASH Over Multiple Content Distribution Servers\nContent: Recently, dynamic adaptive streaming over HTTP (DASH) has been widely deployed on the Internet. However, the research about DASH over multiple content distribution servers (MCDS-DASH) is limited. Compared with traditional single-server DASH, MCDS-DASH is able to offer expanded bandwidth, link diversity, and reliability. It is, however, a challenging problem to smooth video bitrate switching over multiple servers due to their diverse bandwidths. In this paper, we propose a block-based rate adaptation method considering both the diverse bandwidths and feedback buffered video time. In our method, multiple fragments are grouped into a block and the fragments are downloaded in parallel from multiple servers. We propose to adapt video bitrate at the block level rather than at the fragment level. By dynamically adjusting the block length and scheduling fragment requests to multiple servers, the requested video bitrates from the multiple servers are synchronized, making the fragments download in an orderly way. Then, we propose a control-theoretic approach to select an appropriate bitrate for each block. By modeling and linearizing the rate adaption system, we propose a novel proportional-derivative controller to adapt video bitrate with high responsiveness and stability. Theoretical analysis and extensive experiments on our network testbed and the Internet demonstrate the good efficiency of the proposed method.\n\n## Task\nCompose an abstract for the title 'Contribution-Guided Peer Selection for Reliable Peer-to-Peer Video Streaming Over Mesh Networks' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"This paper proposes a sender-driven peer selection scheme, including estimation of packet loss propagation, evaluation of peers' contributions, and peer selection based on child-peers' contributions, for mesh-based peer-to-peer (P2P) video streaming systems. The proposed packet loss propagation model takes into account the link packet drop rate, peer dynamics, and forward error correction protection to capture the heterogeneous packet loss behavior of individual substreams transmitted over a mesh network. The evaluation of candidate peers' contributions is modeled through Markov random fields to significantly reduce complexity. Simulation results demonstrate that our peer selection scheme significantly mitigates packet loss in a mesh-based P2P network, compared to other state-of-the-art schemes."
      }
    ]
  },
  {
    "tid":"354",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Convergence of discrete-time approximations of constrained linear-quadratic optimal control problems\nContent: Continuous-time linear constrained optimal control problems are in practice often solved using discretization techniques, e.g. in model predictive control (MPC). This requires the discretization of the (linear time-invariant) dynamics and the cost functional leading to discrete-time optimization problems. Although the question of convergence of the sequence of optimal controls, obtained by solving the discretized problems, to the true optimal continuous-time control signal when the discretization parameter (the sampling interval) approaches zero has been addressed in the literature, we provide some new results under less restrictive assumptions for a class of constrained continuous-time linear quadratic (LQ) problems with mixed state-control constraints by exploiting results from mathematical programming extensively. As a byproduct of our analysis, a regularity result regarding the costate trajectory is also presented.\n\nTitle: A Time-Stepping Complementarity Approach for Frictionless Systems of Rigid Bodies\nContent: This paper presents a time-stepping complementarity approach for computing a numerical trajectory of a system of rigid bodies under frictionless contact. Such a system is formulated as a time-dependent (ordinary) differential complementarity problem (DCP), which consists of an ordinary differential equation (ODE) coupled with a complementarity condition. The phenomenon of impact plays an essential role when modeling a mechanical system of this type. Within the time-stepping scheme, which uses a high-order ODE discretization, we include an impact detection routine and an impact law governing the behavior of the system over the duration of an impact. We report computational results for the application of the overall numerical scheme to computing the solution trajectories of several realistic mechanical systems.\n\nTitle: A unified numerical scheme for linear-quadratic optimal control problems with joint control and state constraints\nContent: This paper presents a numerical scheme for solving the continuous-time convex linear-quadratic LQ optimal control problem with mixed polyhedral state and control constraints. Unifying a discretization of this optimal control problem as often employed in model predictive control and that obtained through time-stepping methods based on the differential variational inequality reformulation, the scheme solves a sequence of finite-dimensional convex quadratic programs QPs whose optimal solutions are employed to construct a sequence of discrete-time trajectories dependent on the time step. Under certain technical primal\u2013dual assumptions primarily to deal with the algebraic constraints involving the state variable, we prove that such a numerical trajectory converges to an optimal trajectory of the continuous-time control problem as the time step goes to zero, with both the limiting optimal state and costate trajectories being absolutely continuous. This provides a constructive proof of the existence of a solution to the optimal control problem with such regularity properties. Additional properties of the optimal solutions to the LQ problem are also established that are analogous to those of the finite-dimensional convex QP. Our results are applicable to problems with convex but not necessarily strictly convex objective functions and with possibly unbounded mixed state\u2013control constraints.\n\nTitle: Convergence of splitting and Newton methods for complementarity problems: an application of some sensitivity results\nContent: This paper is concerned with two well-known families of iterative methods for solving the linear and nonlinear complementarity problems. For the linear complementarity problem, we consider the class of matrix splitting methods and establish, under a finiteness assumption on the number of solutions, a necessary and sufficient condition for the convergence of the sequence of iterates produced. A rate of convergence result for this class of methods is also derived under a stability assumption on the limit solution. For the nonlinear complementarity problem, we establish the convergence of the Newton method under the assumption of a \u201cpseudo-regular\u201d solution which generalizes Robinson's concept of a \u201cstrongly regular\u201d solution. In both instances, the convergence proofs rely on a common sensitivity result of the linear complementarity problem under perturbation.\n\nTitle: Parallel Successive Convex Approximation for Nonsmooth Nonconvex Optimization.\nContent: Consider the problem of minimizing the sum of a smooth (possibly non-convex) and a convex (possibly nonsmooth) function involving a large number of variables. A popular approach to solve this problem is the block coordinate descent (BCD) method whereby at each iteration only one variable block is updated while the remaining variables are held fixed. With the recent advances in the developments of the multi-core parallel processing technology, it is desirable to parallelize the BCD method by allowing multiple blocks to be updated simultaneously at each iteration of the algorithm. In this work, we propose an inexact parallel BCD approach where at each iteration, a subset of the variables is updated in parallel by minimizing convex approximations of the original objective function. We investigate the convergence of this parallel BCD method for both randomized and cyclic variable selection rules. We analyze the asymptotic and non-asymptotic convergence behavior of the algorithm for both convex and non-convex objective functions. The numerical experiments suggest that for a special case of Lasso minimization problem, the cyclic block selection rule can outperform the randomized rule.\n\n## Task\nCompose an abstract for the title 'Convergence of Time-Stepping Schemes for Passive and Extended Linear Complementarity Systems' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Generalizing recent results in [M. K. Camlibel, Complementarity Methods in the Analysis of Piecewise Linear Dynamical Systems, Ph.D. thesis, Center for Economic Research, Tilburg University, Tilburg, The Netherlands, 2001], [M. K. Camlibel, W. P. M. H. Heemels, and J. M. Schumacher, IEEE Trans. Circuits Systems I: Fund. Theory Appl., 49 (2002), pp. 349-357], and [J.-S. Pang and D. Stewart, Math. Program. Ser. A, 113 (2008), pp. 345-424], this paper provides an in-depth analysis of time-stepping methods for solving initial-value and boundary-value, non-Lipschitz linear complementarity systems (LCSs) under passivity and broader assumptions. The novelty of the methods and their analysis lies in the use of \u201cleast-norm solutions\u201d in the discrete-time linear complementarity subproblems arising from the numerical scheme; these subproblems are not necessarily monotone and are not guaranteed to have convex solution sets. Among the principal results, it is shown that, using such least-norm solutions of the discrete-time subproblems, an implicit Euler scheme is convergent for passive initial-value LCSs; generalizations under a strict copositivity assumption and for boundary-value LCSs are also established."
      }
    ]
  },
  {
    "tid":"355",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Exploring the Performance Limits of Out-of-order Commit.\nContent: Out-of-order execution is essential for high performance, general-purpose computation, as it can find and execute useful work instead of stalling. However, it is limited by the requirement of visibly sequential, atomic instruction execution --- in other words in-order instruction commit. While in-order commit has its advantages, such as providing precise interrupts and avoiding complications with the memory consistency model, it requires the core to hold on to resources (reorder buffer entries, load/store queue entries, registers) until they are released in program order. In contrast, out-of-order commit releases resources much earlier, yielding improved performance with fewer traditional hardware resources. However, out-of-order commit is limited in terms of correctness by the conditions described in the work of Bell and Lipasti. In this paper we revisit out-of-order commit from a different perspective, not by proposing another hardware technique, but by examining these conditions one by one and in combination with respect to their potential performance benefit for both non-speculative and speculative out-of-order commit. While correctly handling recovery for all out-of-order commit conditions currently requires complex tracking and expensive checkpointing, this work aims to demonstrate the potential for selective, speculative out-of-order commit using an oracle implementation without speculative rollback costs. We learn that: a) there is significant untapped potential for aggressive variants of out-of-order commit; b) it is important to optimize the commit depth, or the search distance for out-of-order commit, for a balanced design: smaller cores can benefit from shorter depths while larger cores continue to benefit from aggressive parameters; c) the focus on a subset of out-of-order commit conditions could lead to efficient implementations; d) the benefits for out-of-order commit increase with higher memory latency and works well in conjunction with prefetching to continue to improve performance.\n\nTitle: Maximizing Limited Resources: a Limit-Based Study and Taxonomy of Out-of-Order Commit\nContent: Out-of-order execution is essential for high performance, general-purpose computation, as it can find and execute useful work instead of stalling. However, it is typically limited by the requirement of visibly sequential, atomic instruction execution\u2014in other words, in-order instruction commit. While in-order commit has a number of advantages, such as providing precise interrupts and avoiding complications with the memory consistency model, it requires the core to hold on to resources (reorder buffer entries, load/store queue entries, physical registers) until they are released in program order. In contrast, out-of-order commit can release some resources much earlier, yielding improved performance and/or lower resource requirements. Non-speculative out-of-order commit is limited in terms of correctness by the conditions described in the work of Bell and Lipasti (2004). In this paper we revisit out-of-order commit by examining the potential performance benefits of lifting these conditions one by one and in combination, for both non-speculative and speculative out-of-order commit. While correctly handling recovery for all out-of-order commit conditions currently requires complex tracking and expensive checkpointing, this work aims to demonstrate the potential for selective, speculative out-of-order commit using an oracle implementation without speculative rollback costs. Through this analysis of the potential of out-of-order commit, we learn that: a) there is significant untapped potential for aggressive variants of out-of-order commit; b) it is important to optimize the out-of-order commit depth for a balanced design, as smaller cores benefit from reduced depth while larger cores continue to benefit from deeper designs; c) the focus on implementing only a subset of the out-of-order commit conditions could lead to efficient implementations; d) the benefits of out-of-order commit increases with higher memory latency and in conjunction with prefetching; e) out-of-order commit exposes additional parallelism in the memory hierarchy.\n\nTitle: SWOOP: software-hardware co-design for non-speculative, execute-ahead, in-order cores.\nContent: Increasing demands for energy efficiency constrain emerging hardware. These new hardware trends challenge the established assumptions in code generation and force us to rethink existing software optimization techniques. We propose a cross-layer redesign of the way compilers and the underlying microarchitecture are built and interact, to achieve both performance and high energy efficiency. \n\nIn this paper, we address one of the main performance bottlenecks \u2014 last-level cache misses \u2014 through a software-hardware co-design. Our approach is able to hide memory latency and attain increased memory and instruction level parallelism by orchestrating a non-speculative, execute-ahead paradigm in software (SWOOP). While out-of-order (OoO) architectures attempt to hide memory latency by dynamically reordering instructions, they do so through expensive, power-hungry, speculative mechanisms.We aim to shift this complexity into software, and we build upon compilation techniques inherited from VLIW, software pipelining, modulo scheduling, decoupled access-execution, and software prefetching. In contrast to previous approaches we do not rely on either software or hardware speculation that can be detrimental to efficiency. Our SWOOP compiler is enhanced with lightweight architectural support, thus being able to transform applications that include highly complex control-flow and indirect memory accesses.\n\n\n\nTitle: Efficient invisible speculative execution through selective delay and value prediction\nContent: Speculative execution, the base on which modern high-performance general-purpose CPUs are built on, has recently been shown to enable a slew of security attacks. All these attacks are centered around a common set of behaviors: During speculative execution, the architectural state of the system is kept unmodified, until the speculation can be verified. In the event that a misspeculation occurs, then anything that can affect the architectural state is reverted (squashed) and re-executed correctly. However, the same is not true for the microarchitectural state. Normally invisible to the user, changes to the microarchitectural state can be observed through various side-channels, with timing differences caused by the memory hierarchy being one of the most common and easy to exploit. The speculative side-channels can then be exploited to perform attacks that can bypass software and hardware checks in order to leak information. These attacks, out of which the most infamous are perhaps Spectre and Meltdown, have led to a frantic search for solutions.\n\nIn this work, we present our own solution for reducing the microarchitectural state-changes caused by speculative execution in the memory hierarchy. It is based on the observation that if we only allow accesses that hit in the L1 data cache to proceed, then we can easily hide any microarchitectural changes until after the speculation has been verified. At the same time, we propose to prevent stalls by value predicting the loads that miss in the L1. Value prediction, though speculative, constitutes an invisible form of speculation, not seen outside the core. We evaluate our solution and show that we can prevent observable microarchitectural changes in the memory hierarchy while keeping the performance and energy costs at 11% and 7%, respectively. In comparison, the current state of the art solution, InvisiSpec, incurs a 46% performance loss and a 51% energy increase.\n\n\n\nTitle: Multicore Cache Simulations Using Heterogeneous Computing on General Purpose and Graphics Processors\nContent: Traditional trace-driven memory system simulation is a very time consuming process while the advent of multicores simply exacerbates the problem. We propose a framework for accelerating trace-driven multicore cache simulations by utilizing the capabilities of the modern many core GPUs. A straightforward way towards this direction is to rely on the inherent parallelism in cache simulations: communicating cache sets can be simulated independently and concurrently to other sets. Based on this, we map collections of communicating cache sets (each belonging to a different target cache) on the same GPU block so that the simulated coherence traffic is local traffic in the GPU. However, this is not enough due to the great imbalance in the activity in the different cache sets: some sets receive a flurry of activity while others do not. Our solution is to load balance the simulated sets (based on activity) on the computing element (host-CPU or GPU) that can manage them in the most efficient way. We propose a heterogeneous computing approach in which the host-CPU simulates the few but most active sets, while the GPU is responsible for the many more but less active sets. Our experimental findings using the SPLASH-2 suite demonstrate that our cache simulator based on the CPU-GPU cooperation achieves on average 5.88x speedup over alternative implementations running on CPU, speedups which scale well with the size of the simulated system.\n\n## Task\nCompose an abstract for the title 'Transcending Hardware Limits with Software Out-of-Order Processing.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Building high-performance, next-generation processors require novel techniques to enable improved performance given today&#39;s power- and energy-efficiency requirements. Additionally, a widening gap between processor and memory performance makes it even more difficult to improve efficiency with conventional techniques. While out-of-order architectures attempt to hide this memory latency with dynamica..."
      }
    ]
  },
  {
    "tid":"356",
    "conversations":[
      {
        "from":"user",
        "value":"You are now a helpful personal AI assistant. You should emulate the author's style and tone based on provided history content. Your responses should be detailed and informative, matching the author's unique writing approach. Aim for insightful and high-quality solutions that make users satisfied.\n\n## History Paper Abstracts\nTitle: Smashing the Gadgets: Hindering Return-Oriented Programming Using In-place Code Randomization\nContent: The wide adoption of non-executable page protections in recent versions of popular operating systems has given rise to attacks that employ return-oriented programming (ROP) to achieve arbitrary code execution without the injection of any code. Existing defenses against ROP exploits either require source code or symbolic debugging information, or impose a significant runtime overhead, which limits their applicability for the protection of third-party applications. In this paper we present in-place code randomization, a practical mitigation technique against ROP attacks that can be applied directly on third-party software. Our method uses various narrow-scope code transformations that can be applied statically, without changing the location of basic blocks, allowing the safe randomization of stripped binaries even with partial disassembly coverage. These transformations effectively eliminate about 10%, and probabilistically break about 80% of the useful instruction sequences found in a large set of PE files. Since no additional code is inserted, in-place code randomization does not incur any measurable runtime overhead, enabling it to be easily used in tandem with existing exploit mitigations such as address space layout randomization. Our evaluation using publicly available ROP exploits and two ROP code generation toolkits demonstrates that our technique prevents the exploitation of the tested vulnerable Windows 7 applications, including Adobe Reader, as well as the automated construction of alternative ROP payloads that aim to circumvent in-place code randomization using solely any remaining unaffected instruction sequences.\n\nTitle: Protecting COTS Binaries from Disclosure-guided Code Reuse Attacks.\nContent: Code diversification, combined with execute-only memory, provides an effective defense against just-in-time code reuse attacks. However, existing techniques for combining code diversification and hardware-assisted memory protections typically require compiler support, as well as the deployment or modification of a hypervisor. These requirements often cannot be met, either because source code is not available, or because the required hardware features may not be available on the target system. In this paper we present SECRET, a software hardening technique tailored to legacy and closed-source software that provides equivalent protection to execute-only memory without relying on hardware features or recompilation. This is achieved using two novel techniques, code space isolation and code pointer remapping, which prevent read accesses to the executable memory of the protected code. Furthermore, SECRET thwarts code pointer harvesting attacks on ELF files by remapping existing code pointers to use random values. SECRET has been implemented on 32-bit Linux systems. Our evaluation shows that it introduces just 2% additional runtime overhead on top of a state-of-the-art CFI implementation, bringing the total average overhead to about 16%. In addition, it achieves better protection coverage compared to compiler-based techniques, as it can handle low-level machine code such as inline assembly or extra code introduced by the linker and loader.\n\nTitle: kR^X: Comprehensive Kernel Protection against Just-In-Time Code Reuse.\nContent: The abundance of memory corruption and disclosure vulnerabilities in kernel code necessitates the deployment of hardening techniques to prevent privilege escalation attacks. As more strict memory isolation mechanisms between the kernel and user space, like Intel's SMEP, become commonplace, attackers increasingly rely on code reuse techniques to exploit kernel vulnerabilities. Contrary to similar attacks in more restrictive settings, such as web browsers, in kernel exploitation, non-privileged local adversaries have great flexibility in abusing memory disclosure vulnerabilities to dynamically discover, or infer, the location of certain code snippets and construct code-reuse payloads. Recent studies have shown that the coupling of code diversification with the enforcement of a \\\"read XOR execute\\\" (R^X) memory safety policy is an effective defense against the exploitation of userland software, but so far this approach has not been applied for the protection of the kernel itself. In this paper, we fill this gap by presenting kR^X: a kernel hardening scheme based on execute-only memory and code diversification. We study a previously unexplored point in the design space, where a hypervisor or a super-privileged component is not required. Implemented mostly as a set of GCC plugins, kR^X is readily applicable to the x86-64 Linux kernel and can benefit from hardware support (e.g., MPX on modern Intel CPUs) to optimize performance. In full protection mode, kR^X incurs a low runtime overhead of 4.04%, which drops to 2.32% when MPX is available.\n\nTitle: Compiler-Assisted Code Randomization\nContent: Despite decades of research on software diversification, only address space layout randomization has seen widespread adoption. Code randomization, an effective defense against return-oriented programming exploits, has remained an academic exercise mainly due to i) the lack of a transparent and streamlined deployment model that does not disrupt existing software distribution norms, and ii) the inherent incompatibility of program variants with error reporting, whitelisting, patching, and other operations that rely on code uniformity. In this work we present compiler-assisted code randomization (CCR), a hybrid approach that relies on compiler\u2013rewriter cooperation to enable fast and robust fine-grained code randomization on end-user systems, while maintaining compatibility with existing software distribution models. The main concept behind CCR is to augment binaries with a minimal set of transformation-assisting metadata, which i) facilitate rapid fine-grained code transformation at installation or load time, and ii) form the basis for reversing any applied code transformation when needed, to maintain compatibility with existing mechanisms that rely on referencing the original code. We have implemented a prototype of this approach by extending the LLVM compiler toolchain, and developing a simple binary rewriter that leverages the embedded metadata to generate randomized variants using basic block reordering. The results of our experimental evaluation demonstrate the feasibility and practicality of CCR, as on average it incurs a modest file size increase of 11.46% and a negligible runtime overhead of 0.28%, while it is compatible with link-time optimization and control flow integrity.\n\nTitle: Return to the Zombie Gadgets: Undermining Destructive Code Reads via Code Inference Attacks.\nContent: The concept of destructive code reads is a new defensive strategy that prevents code reuse attacks by coupling fine-grained address space layout randomization with a mitigation for online knowledge gathering that destroys potentially useful gadgets as they are disclosed by an adversary. The intuition is that by destroying code as it is read, an adversary is left with no usable gadgets to reuse in a control-flow hijacking attack. In this paper, we examine the security of this new mitigation. We show that while the concept initially appeared promising, there are several unforeseen attack tactics that render destructive code reads ineffective in practice. Specifically, we introduce techniques for leveraging constructive reloads, wherein multiple copies of native code are loaded into a processu0027 address space (either side-by-side or one-after-another). Constructive reloads allow the adversary to disclose one code copy, destroying it in the process, then use another code copy for their code reuse payload. For situations where constructive reloads are not viable, we show that an alternative, and equally powerful, strategy exists: leveraging code association via implicit reads, which allows an adversary to undo in-place code randomization by inferring the layout of code that follows already disclosed bytes. As a result, the implicitly learned code is not destroyed, and can be used in the adversaryu0027s code reuse attack. We demonstrate the effectiveness of our techniques with concrete instantiations of these attacks against popular applications. In light of our successes, we argue that the code inference strategies presented herein paint a cautionary tale for defensive approaches whose security blindly rests on the perceived inability to undo the application of in-place randomization.\n\n## Task\nCompose an abstract for the title 'Juggling the Gadgets: Binary-level Code Randomization using Instruction Displacement.' that matches the author's unique content, style and tone.\n"
      },
      {
        "from":"assistant",
        "value":"Code diversification is an effective mitigation against return-oriented programming attacks, which breaks the assumptions of attackers about the location and structure of useful instruction sequences, known as \\\"gadgets\\\". Although a wide range of code diversification techniques of varying levels of granularity exist, most of them rely on the availability of source code, debug symbols, or the assumption of fully precise code disassembly, limiting their practical applicability for the protection of closed-source third-party applications. In-place code randomization has been proposed as an alternative binary-compatible diversification technique that is tolerant of partial disassembly coverage, in the expense though of leaving some gadgets intact, at the disposal of attackers. Consequently, the possibility of constructing robust ROP payloads using only the remaining non-randomized gadgets is still open. In this paper we present instruction displacement, a code diversification technique based on static binary instrumentation that does not rely on complete code disassembly coverage. Instruction displacement aims to improve the randomization coverage and entropy of existing binary-level code diversification techniques by displacing any remaining non-randomized gadgets to random locations. The results of our experimental evaluation demonstrate that instruction displacement reduces the number of non-randomized gadgets in the extracted code regions from 15.04% for standalone in-place code randomization, to 2.77% for the combination of both techniques. At the same time, the additional indirection introduced due to displacement incurs a negligible runtime overhead of 0.36% on average for the SPEC CPU2006 benchmarks."
      }
    ]
  }
]