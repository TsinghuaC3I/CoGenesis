[
  {
    "tid":"0",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Cross Concept Local Fisher Discriminant Analysis for Image Classification.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Distance metric learning is widely used in many visual computing methods, especially image classification. Among various metric learning approaches, Fisher Discriminant Analysis (FDA) is a classical metric learning approach utilizing the pair-wise semantic similarity and dissimilarity in image classification. Moreover, Local Fisher Discriminant Analysis (LFDA) takes advantage of local data structure in FDA and achieves better performance. Both FDA and LFDA can only deal with images with simple concept relations, where images either belong to the same concept category or come from different categories. However, in real application scenarios, images usually contain multiple concepts, and relations of concepts and images are complex. In this paper, to improve the flexibility of LFDA on the complex image-concept relations, we propose a new pairwise constraints method called Cross Concept Local Fisher Discriminant Analysis (C2 LFDA) for image classification. By considering the cross concept images as a special case of within-class samples, C2 LFDA models the semantic relations of images for distance metric learning under the framework of LFDA.We calculate within-class and between-class scatter matrix based on the proposed re-weighting scheme and local manifold structure. By solving the objective function of discriminant analysis using the proposed scheme, a set of projected representation is obtained to better reflect the complex semantic relations among images. Experimental evaluations and comparisons show the effectiveness of the proposed method. \u00a9 Springer-Verlag 2013."
      }
    ]
  },
  {
    "tid":"1",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Application-centric SSD Cache Allocation for Hadoop Applications.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Flash-based Solid State Drive (SSD) is widely used in the virtualization environment, usually as the cache of the hard disk drive-based Virtual Machine (VM) storage, to improve the IO performance. Existing SSD caching schemes are mainly driven by VM-centric metrics. They treat the VMs as independent units and focus on critical low-level performance metrics of individual VMs, such as the working set, the IO latency, or the throughput. However, for elastic Hadoop applications consisting of multiple VMs, the workload is rapidly changing, and the importance of differnet VMs may be different even if they have the same low-level IO pattern. In this situation, the VM-centric SSD caching schemes may not lead to the best performance, i.e., the shortest job completion time. Considering the importance of VMs and relationships among VMs inside the application may potentially better improve the performance, which we regard as the application-centric metrics. We propose the Application-Centric SSD caching for Hadoop applications (ACSSD), which reduces the job completion time from the application level. AC-SSD uses the genetic algorithm based approach to calculate the nearly optimal weights of virtual machines for allocating SSD cache space and controlling the I/O Operations Per Second (IOPS) based on the importance of the VMs. Moreover, AC-SSD introduces the closed-loop adaptation to face the rapidly changing workload. The evaluation shows that AC-SSD reduces the job completion time by up to 39% for IO sensitive workloads, and up to 29% for rapidly changing workloads."
      }
    ]
  },
  {
    "tid":"2",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Light-Weight Trust-Based On-Demand Multipath Routing Protocol for Mobile Ad Hoc Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mobile ad hoc networks are originally designed for the environment that is cooperative and in which every node has constrained resources such as energy and communication radius. In virulent environments, compared with traditional methods, trust-based routing protocol could bring better service quality. In this study, considering the black-hole attack and the grey-hole attack, we build a novel trust model. And then a light-weight trust-based multipath routing protocol called LWT-AOMDV is proposed based on the proposed model, extending from the AOMDV. The key issue of our work is to establish multiple trustworthy paths and to detect malicious nodes in time. Our on-demand route maintenance mechanism in this protocol reduces control overhead by presenting the notion of path error instead of route error. Moreover, our new protocol explores the tradeoff between the overhead and the service quality. Simulation results via ns-2 simulator demonstrate that our approach achieves a great improvement in the packet delivery ratio at the expense of few additional resources."
      }
    ]
  },
  {
    "tid":"3",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Enhanced Cell Range Expansion and Radio Resource Allocation for a Better Traffic Offloading in HetNets' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cell range expansion (CRE) is a promising solution to offload macrocells' traffic and improve the radio capacity in heterogeneous network (HetNet). On the other hand, enhanced inter-cell interference coordination (eICIC) techniques are introduced in HetNets to mitigate the severe interference caused between the small cells and the macrocell. In this paper, we present a novel application of the CRE coupled with a new time and frequency domain based eICIC technique. Our approach offers a dynamic radio resource allocation scheme which takes into account the number of femtocells inside the macrocell, the femtocells' loads and the macrocell load. First, we propose a so called FT-CRE scheme which alternates the transmission of the macrocell and the femtocells offering high traffic offloading. Then, a more elaborated scheme called enhanced FT-CRE (eFT-CRE) is presented. In this scheme, the need in traffic offloading is estimated so that time and frequency resources are allocated between the macrocell and the femtocells accordingly. The performance of both proposed FT-CRE and eFT-CRE schemes is evaluated through simulations. The results showed that when FT-CRE scheme is applied, the achieved throughput of the victim macro-users served by femtocells which apply the CRE is increased by 6 times as compared to a common application of CRE technique based on time domain multiplexing eICIC only. The outage probability of these victim macro-users is also significantly decreased. More importantly, the eFT-CRE scheme reveals more effective in terms of total achieved throughput in the system, the outage probability and energy efficiency at the macrocell."
      }
    ]
  },
  {
    "tid":"4",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Architecture for a collaborative research environment based on reading list sharing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Scholarly research involves a systematic study of information sources in order to establish facts and reach new conclusions. It encompasses survey, analysis, evaluation, and creation as distinct phases that are performed iteratively and often in parallel by accessing a range of local and remote resources. Throughout these activities scholars create collections of relevant work, ranging from publication references to new information acquired through experiments or correspondence with other scholars. We use the term reading list to refer to such collections. Existing software packages or web services for managing publication lists, like CiteULike, lack integration with researchers' workflow which may require access to both desktop and online resources. In this paper we describe the architecture and system design of ScholarLynk, a desktop tagging tool that enables researchers to build and maintain reading lists across distributed data stores, in collaboration with other researchers."
      }
    ]
  },
  {
    "tid":"5",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A new data pre-processing approach for the dendritic cellalgorithm based on fuzzy rough set theory' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The aim of this paper is to develop a new data pre-processing method for the dendritic cell algorithm (DCA) based on Fuzzy Rough Set Theory (FRST). In this new fuzzy-rough model, the data pre-processing phase is based on the fuzzy positive region and the fuzzy dependency degree concepts. Results show that applying FRST is more convenient for the DCA data pre-processing phase yielding much better performance in terms of accuracy."
      }
    ]
  },
  {
    "tid":"6",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Steerable Needle Technology Using Curved Concentric Tubes' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A new approach to steerable needle design is proposed for use in minimally invasive surgery. The technology is based on sets of curved concentric tubes. By rotating and extending the tubes with respect to each other, the position and orientation of the needle tip, as well as the shape of the inserted length, can be controlled. A mechanics model is presented for computing the shape of the needle. Forward and inverse kinematic equations are also derived. In addition, experimental results are presented as validation of the approach."
      }
    ]
  },
  {
    "tid":"7",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'OpenGate: Towards an open network services gateway' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose an extensible open network services gateway (OpenGate) for high-performance network processing at the edge of high-speed networks. The OpenGate system embraces recent advances of open network technologies: the performance is guaranteed by using open-standard ATCA platforms; and the extensibility is achieved by employing parallelized open source software. As an application example of OpenGate, a high-performance security gateway, OpenGate-SG, was developed using existing ATCA platforms and open source software. This system provides multiple security services, including stateful firewall, intrusion prevention and anti-virus. Experimental results show that, OpenGate-SG can achieve up to 200Gbps stateful firewall throughput with 8Gbps intrusion prevention and anti-virus, which is competitive to the performance of today's high-end security products. OpenGate-SG has also been tested as a security gateway for a university campus network with more than 1000 students."
      }
    ]
  },
  {
    "tid":"8",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Detection of rotationally symmetric shape orientations by fold-invariant shape-specific points' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Two new types of shape-specific points, called fold-invariant centroid (FIC) and fold-invariant radius weighted mean (FIRWM), are introduced for detecting the orientations of rotationally symmetric shapes. A given rotationally symmetric shape is first transformed into a new fold-invariant shape. The centroid and the radius weighted mean of the new shape are then transformed back to obtain the proposed FIC and FIRWM, respectively. FIC and FIRWM points are proved to be invariant to translation, rotation and scaling, and so can be used for matching rotationally symmetric shapes. Using the proposed fold-invariant shape-specific points to detect the orientations of rotationally symmetric shapes is easier to implement than that of using the fold principal axes. Some illustrative examples and a method to avoid the case where the proposed fold-invariant shape-specific points are too close to the centroid are also given."
      }
    ]
  },
  {
    "tid":"9",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'GeoDialogue: A Software Agent Enabling Collaborative Dialogues between a User and a Conversational GIS' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A fundamental challenge that must be met to achieve a usable conversational interface to geographic information system (GIS) is how to enable a more natural interaction between the user and the system. This paper presents a design of an agent-based computational model, PlanGraph, and implementation of this model in a software agent, GeoDialogue, as a dialogue manager for a conversational GIS. This dialogue agent enables intelligent collaborative human-GIS dialogues. The capabilities of this agent are demonstrated through its performance in a conversational GIS, Dave_G."
      }
    ]
  },
  {
    "tid":"10",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Data Dependence-based Optimistic Data Consistency Maintenance Method' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Data replication introduces well-known consistency issues. This paper puts forward the question about data dependence in data consistency, which embodies pseudo-conflict updates and update dependency. According to that, an optimistic data consistency method is proposed. In our method, data object is partitioned into data blocks by fixed size, as the basic unit of data management. Updates are compressed by Bloom filter technique and propagated in double-path. Negotiation algorithms detect and reconcile update conflicts, and dynamic data management algorithms accommodate dynamic data processing. The results of the performance evaluation show that our method is an efficient method to achieve consistency, good dynamic property, and strong robustness."
      }
    ]
  },
  {
    "tid":"11",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Algorithms for cost- and deadline-constrained provisioning for scientific workflow ensembles in IaaS clouds' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Large-scale applications expressed as scientific workflows are often grouped into ensembles of inter-related workflows. In this paper, we address a new and important problem concerning the efficient management of such ensembles under budget and deadline constraints on Infrastructure as a Service (IaaS) clouds. IaaS clouds are characterized by on-demand resource provisioning capabilities and a pay-per-use model. We discuss, develop, and assess novel algorithms based on static and dynamic strategies for both task scheduling and resource provisioning. We perform the evaluation via simulation using a set of scientific workflow ensembles with a broad range of budget and deadline parameters, taking into account task granularity, uncertainties in task runtime estimations, provisioning delays, and failures. We find that the key factor determining the performance of an algorithm is its ability to decide which workflows in an ensemble to admit or reject for execution. Our results show that an admission procedure based on workflow structure and estimates of task runtimes can significantly improve the quality of solutions. We define the problem of scheduling prioritized workflow ensembles on IaaS clouds.We analyze and develop several dynamic (online) and static (offline) algorithms.We address both problems of task scheduling and resource provisioning.Our simulator models uncertainties in estimates, provisioning delays, and failures.We use synthetic workflow ensembles based on important, real scientific applications."
      }
    ]
  },
  {
    "tid":"12",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Hybrid manifold embedding.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this brief, we present a novel supervised manifold learning framework dubbed hybrid manifold embedding (HyME). Unlike most of the existing supervised manifold learning algorithms that give linear explicit mapping functions, the HyME aims to provide a more general nonlinear explicit mapping function by performing a two-layer learning procedure. In the first layer, a new clustering strategy called geodesic clustering is proposed to divide the original data set into several subsets with minimum nonlinearity. In the second layer, a supervised dimensionality reduction scheme called locally conjugate discriminant projection is performed on each subset for maximizing the discriminant information and minimizing the dimension redundancy simultaneously in the reduced low-dimensional space. By integrating these two layers in a unified mapping function, a supervised manifold embedding framework is established to describe both global and local manifold structure as well as to preserve the discriminative ability in the learned subspace. Experiments on various data sets validate the effectiveness of the proposed method."
      }
    ]
  },
  {
    "tid":"13",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Bagging and Feature Selection for Classification with Incomplete Data.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Missing values are an unavoidable issue of many real-world datasets. Dealing with missing values is an essential requirement in classification problem, because inadequate treatment with missing values often leads to large classification errors. Some classifiers can directly work with incomplete data, but they often result in big classification errors and generate complex models. Feature selection and bagging have been successfully used to improve classification, but they are mainly applied to complete data. This paper proposes a combination of bagging and feature selection to improve classification with incomplete data. To achieve this purpose, a wrapper-based feature selection which can directly work with incomplete data is used to select suitable feature subsets for bagging. The experiments on eight incomplete datasets were designed to compare the proposed method with three other popular methods that are able to deal with incomplete data using C4.5/REPTree as classifiers and using Particle Swam Optimisation as a search technique in feature selection. Results show that the combination of bagging and feature selection can not only achieve better classification accuracy than the other methods but also generate less complex models compared to the bagging method."
      }
    ]
  },
  {
    "tid":"14",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Millimeter Wave Cellular Wireless Networks: Potentials and Challenges.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Millimeter-wave (mmW) frequencies between 30 and 300 GHz are a new frontier for cellular communication that offers the promise of orders of magnitude greater bandwidths combined with further gains via beamforming and spatial multiplexing from multielement antenna arrays. This paper surveys measurements and capacity studies to assess this technology with a focus on small cell deployments in urban e..."
      }
    ]
  },
  {
    "tid":"15",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fully Distributed Self Certified Key Management for Large-Scale MANETs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mobile ad hoc networks (MANETs) have received considerable attention while their special characteristics make them vulnerable against different attacks. Providing security in such networks becomes a challenge and cryptography is an essential solution for it, to implement a cryptosystem, key management is the main challenge. In this paper a new deterministic scheme for key management in MANETs is proposed. Since public and private keys in this algorithm have an unforgeable relationship, there is no need for any certificate. Proposed scheme distributes the role of the key generation center (KGC) among all nodes, therefore the private key is issued by distributed KGCs (DKGCs) and the node itself. Furthermore, each pair of nodes can share a symmetric key in a non-interactive way while communicating with each other. The proposed scheme is certificate-less, does not require any trusted authority and also it can solve the key escrow problem. Moreover, the performance of the proposed algorithm is analyzed analytically and it is compared with previous works."
      }
    ]
  },
  {
    "tid":"16",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Upper bounds on the capacities of non-controllable finite-state channels using dynamic programming methods' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A non-controllable finite-state channel (FSC) is a finite-state channel in which the user can't control channel states. That is, the channel state of a non-controllable FSC evolves freely according to an uncontrollable probability law. Thus far, good upper bounds on capacities of general non-controllable FSCs remain unknown. Here we develop upper bounds that use delayed feedback and delayed state information, and propose dynamic programming methods to numerically evaluate the bounds."
      }
    ]
  },
  {
    "tid":"17",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'AutoSync: Automatic duty-cycle control for synchronous low-power listening' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Low power listening (LPL) has been widely adopted to save energy in wireless sensor networks. However, LPL is ineffective in adapting to dynamic networks with asymmetric traffic patterns, as it sets a network-wide check interval. As a result, nodes with low data traffic waste significant energy resources doing idle listening. This problem is particularly exacerbated in multi-radio networks where majority of data comes through the most reliable radio and the duty cycles of other radios could be reduced. We address this issue in AutoSync, a protocol that combines synchronous LPL with automatic selection of check intervals to reduce energy consumption in both single and multi-radio networks. We first present the justification for AutoSync's design, and we then discuss our implementation of AutoSync in TinyOS. We compare AutoSync against existing protocols in both simulations and empirical experiments. Results show that AutoSync attains a substantial increase in the operational lifetime and mean power consumption over existing protocols in single radio networks and even more in dual radio networks."
      }
    ]
  },
  {
    "tid":"18",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Context-Aware Online Spatiotemporal Traffic Prediction' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With the availability of traffic sensors data, various techniques have been proposed to make congestion prediction by utilizing those datasets. One key challenge in predicting traffic congestion is how much to rely on the historical data v.s. The real-time data. To better utilize both the historical and real-time data, in this paper we propose a novel online framework that could learn the current situation from the real-time data and predict the future using the most effective predictor in this situation from a set of predictors that are trained using historical data. In particular, the proposed framework uses a set of base predictors (e.g. A Support Vector Machine or a Bayes classifier) and learns in real-time the most effective one to use in different contexts (e.g. Time, location, weather condition). As real-time traffic data arrives, the context space is adaptively partitioned in order to efficiently estimate the effectiveness of each predictor in different contexts. We obtain and prove both short-term and long-term performance guarantees (bounds) for our online algorithm. Our experiments with real-world data in real-life conditions show that the proposed approach significantly outperforms existing solutions."
      }
    ]
  },
  {
    "tid":"19",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Loop transformations for flash memory: cost models and performance effects' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Loop optimization, made of a sequence of loop transformations, plays an important role in performance improvement in data centric applications. Programs using flash memory are no exception to this, but, under certain conditions careless applications of specific loop transformations might cause unexpected results, due to the characteristics of flash memory and underlying management systems. In this article, we analyze how loop transformations affect the performance in flash translation layers (FTLs). First, we choose four loop structures which have distinct reference patterns and propose a cost model for each structure, reflecting the properties of flash memory. Then, using these cost models, we investigate how loop transformations affect the block associative sector translation (BAST)'s and fully associative sector translation (FAST)'s internal operations and analyze the performance effect of loop transformations experimentally. As a result, we find that some of the major loop transformations cause unexpected performance effects in those major FTLs under certain conditions."
      }
    ]
  },
  {
    "tid":"20",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A joint PHY/MAC cross-layer design for UWB under power control' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Power control (PC) is a critical parameter for the design and evaluation of UWB-based WPAN networks due to its distributed control nature and non-fixed topology. The main issues in UWB PC are the channel gain fluctuations induced by indoor channel fluctuation and interference arising from narrowband systems. In this paper we introduce a joint PHY/MAC technique for DS-UWB power control design by exploiting the high time resolution of the UWB signal for channel gain improvement and mitigate the narrowband interference to reduce transmitted power. The results indicate that the proposed approach achieves better BER and throughput over previous works."
      }
    ]
  },
  {
    "tid":"21",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Using a medical digital library for education purposes' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Athens Medical School is one of the largest medical research institutions in Greece, where educational activities are combined with every-day practice in the University Hospital. The laboratories operating in the University Hospital produce a large amount of research material, mainly consisting of medical images and videos in digital format. In this paper, we present the Digital Library System built to support the collections of Athens Medical School laboratories. The system facilitates access through the Web to medical research material and on-line presentations/lessons. Laboratory collections have different requirement regarding digital material structure and metadata modelling. Thus, while designing the Digital Library architecture, the organisation and administration of dynamically created collections was explored. We introduced the term dynamic collection management to denote the support of automated collection definition and management within an integrated Digital Library environment. Laboratory staff is responsible for selecting the material to be added in the library, while they also participate in metadata creation. Furthermore, they use this material to create presentation or on-line tutorials, also stored in the Digital Library. To facilitate the aforementioned functionality the Digital Library system also supports a complex workflow environment."
      }
    ]
  },
  {
    "tid":"22",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Updatable and Universal Common Reference Strings with Applications to zk-SNARKs.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"By design, existing (pre-processing) zk-SNARKs embed a secret trapdoor in a relation-dependent common reference strings (CRS). The trapdoor is exploited by a (hypothetical) simulator to prove the scheme is zero knowledge, and the secret-dependent structure facilitates a linear-size CRS and linear-time prover computation. If known by a real party, however, the trapdoor can be used to subvert the security of the system. The structured CRS that makes zk-SNARKs practical also makes deploying zk-SNARKS problematic, as it is difficult to argue why the trapdoor would not be available to the entity responsible for generating the CRS. Moreover, for pre-processing zk-SNARKs a new trusted CRS needs to be computed every time the relation is changed. In this paper, we address both issues by proposing a model where a number of users can update a universal CRS. The updatable CRS model guarantees security if at least one of the users updating the CRS is honest. We provide both a negative result, by showing that zk-SNARKs with private secret-dependent polynomials in the CRS cannot be updatable, and a positive result by constructing a zk-SNARK based on a CRS consisting only of secret-dependent monomials. The CRS is of quadratic size, is updatable, and is universal in the sense that it can be specialized into one or more relation-dependent CRS of linear size with linear-time prover computation."
      }
    ]
  },
  {
    "tid":"23",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'SoftMobile: control evolution for future heterogeneous mobile networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Heterogeneous mobile networks, HMNs, with flexible spectrum use, densified cell deployment, and multi-layer multiple types of radio access technologies, are expected to be key to meeting the 1000 times increase of mobile data traffic in 2020 and beyond. The increasing complexity in HMNs renders the control and coordination of networks a challenging task. The control frameworks of current cellular networks, which were previously designed for sparse network deployment, hit the wall for HMNs. HMNs need good separation of control and data planes, and call for novel control methods to handle the highly complex dynamics therein. In this article, we first briefly review the control planes of 2G to 4G cellular networks, and then identify their constraints to support HMNs. We analyze the complexity in HMNs and examine enabling control technologies for HMNs. We believe new thinking is needed for efficient control of HMNs. SDN is a promising technology to solve complex control problems in the Internet. Principle-based control methods applied in SDN are promising to solve control problems in HMNs. Several SDN approaches have been proposed for mobile networks. However, most of them are targeted at mobile core networks. We propose an SDNbased control framework named SoftMobile to coordinate complex radio access in HMNs. The main features of SoftMobile are low-layer abstraction, separation of control and data planes, and network-wide high-layer programmable control. Important research problems in SDN for mobile networks are highlighted. We believe SDN for mobile networks will be the controlling evolution of future HMNs."
      }
    ]
  },
  {
    "tid":"24",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Region-based weighted-norm with adaptive regularization for resolution enhancement' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"As digital video cameras spread rapidly, critical locations such as airports, train stations, military compounds and airbases and public ''hot-spots'' are placed in the spotlight of video surveillance. Video surveillance provides visual information, in order to maintain security at the monitored areas. Indeed, super-resolution (SR) technique can be useful for extracting additional information from the captured video sequences. Generally, video surveillance cameras capture moving objects while cameras are moving in some cases, which means that many of the existing SR algorithms, that cannot cope with moving objects, are not applicable in this case. In this paper, we propose a SR algorithm that takes into account inaccurate registration at the moving regions and therefore copes with moving objects. We propose to adaptively weight each region according to its reliability where regions that have local motion and/or occlusion have different registration error level. Also, the regularization parameter is simultaneously estimated for each region. The regions are generated by segmenting the reference frame using watershed segmentation. Our approach is tested on simulated and real data coming from videos with different difficulties taken by a hand-held camera. The experimental results show the effectiveness of the proposed algorithm compared to four state-of-the-art SR algorithms."
      }
    ]
  },
  {
    "tid":"25",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A novel point-based nonrigid image registration scheme based on learning optimal landmark configurations' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Image registration plays an increasingly important role in the field of medical image processing given the plurality of images often acquired from different sensors, time points, or viewpoints. Landmark-based registration schemes represent the most popular class of registration methods due to their simplicity and high accuracy. Previous studies have shown that these registration schemes are sensitive to the number and location of landmarks. Identifying important landmarks to perform an accurate registration remains a very challenging task. Current landmark selection methods, such as feature-based approaches, focus on optimization of global transformation and may have poor performance in recovering local deformation, e.g. subtle tissue changes caused by tumor resection, making them inappropriate for registering pre- and post-surgery images as a small cancerous region will be deformed after removing a tumor. In this work, a novel method is introduced to estimate optimal landmark configurations. An important landmark configuration that will be used as a training landmark set was learned for an image pair with a known deformation. This landmark configuration can be considered as a collection of discrete points. A generic transformation matrix between a pair of training landmark sets with different deformation locations was computed via an iterative close point (ICP) alignment technique. A new landmark configuration was determined by simply transforming the training landmarks to the current displacement location while preserving the topological structure of the configuration of landmarks. Two assumptions are made: 1) In a new pair of images the deformation is approximately the same size and has only been spatially relocated in the image, and that by a simple affine transformation one can identify the optimal configuration on this new pair of images; and 2) The deformation is of similar size and shape on the original pair of images. These are reasonable assumptions in many cases where one seeks to register tumor images at multiple time points following application of therapy and to evaluate changes in tumor size. The experiments were conducted on 286 pairs of synthetic MRI brain images. The training landmark configurations were obtained through 2000 iterations of registration where the points with consistently best registration performance were selected. The estimated landmarks greatly improved the quality metrics compared to a uniform grid placement scheme and a speeded-up robust features (SURF) based method as well as a generic free-form deformation (FFD) approach. The quantitative results showed that the new landmark configuration achieved 95% improvement in recovering the local deformation compared to 89% for the uniform grid placement, 79% for the SURF-based approach, and 10% for the generic FFD approach."
      }
    ]
  },
  {
    "tid":"26",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'HybridPlan: a capacity planning technique for projecting storage requirements in hybrid storage systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Economic forces, driven by the desire to introduce flash into the high-end storage market without changing existing software-base, have resulted in the emergence of solid-state drives (SSDs), flash packaged in HDD form factors and capable of working with device drivers and I/O buses designed for HDDs. Unlike the use of DRAM for caching or buffering, however, certain idiosyncrasies of NAND Flash-based solid-state drives (SSDs) make their integration into hard disk drive (HDD)-based storage systems nontrivial. Flash memory suffers from limits on its reliability, is an order of magnitude more expensive than the magnetic hard disk drives (HDDs), and can sometimes be as slow as the HDD (due to excessive garbage collection (GC) induced by high intensity of random writes). Given the complementary properties of HDDs and SSDs in terms of cost, performance, and lifetime, the current consensus among several storage experts is to view SSDs not as a replacement for HDD, but rather as a complementary device within the high-performance storage hierarchy. Thus, we design and evaluate such a hybrid storage system with HybridPlan that is an improved capacity planning technique to administrators with the overall goal of operating within cost-budgets. HybridPlan is able to find the most cost-effective hybrid storage configuration with different types of SSDs and HDDs"
      }
    ]
  },
  {
    "tid":"27",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Bronchoscope tracking based on image registration using multiple initial starting points estimated by motion prediction.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a method for tracking a bronchoscope based on motion prediction and image registration from multiple initial starting points as a function of a bronchoscope navigation system. We try to improve performance of bronchoscope tracking based on image registration using multiple initial guesses estimated using motion prediction. This method basically tracks a bronchoscopic camera by image registration between real bronchoscopic images and virtual ones derived from CT images taken prior to the bronchoscopic examinations. As an initial guess for image registration, we use multiple starting points to avoid falling into local minima. These initial guesses are computed using the motion prediction results obtained from the Kalman filter's output. We applied the proposed method to nine pairs of X-ray CT images and real bronchoscopic video images. The experimental results showed significant performance in continuous tracking without using any positional sensors."
      }
    ]
  },
  {
    "tid":"28",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Dynamics of human body skeletons convey significant information for human action recognition. Conventional approaches for modeling skeletons usually rely on hand-crafted parts or traversal rules, thus resulting in limited expressive power and difficulties of generalization. In this work, we propose a novel model of dynamic skeletons called Spatial-Temporal Graph Convolutional Networks (ST-GCN), which moves beyond the limitations of previous methods by automatically learning both the spatial and temporal patterns from data. This formulation not only leads to greater expressive power but also stronger generalization capability. On two large datasets, Kinetics and NTU-RGBD, it achieves substantial improvements over mainstream methods."
      }
    ]
  },
  {
    "tid":"29",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'ExpertRank: A topic-aware expert finding algorithm for online knowledge communities' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With increasing knowledge demands and limited availability of expertise and resources within organizations, professionals often rely on external sources when seeking knowledge. Online knowledge communities are Internet based virtual communities that specialize in knowledge seeking and sharing. They provide a virtual media environment where individuals with common interests seek and share knowledge across time and space. A large online community may have millions of participants who have accrued a large knowledge repository with millions of text documents. However, due to the low information quality of user-generated content, it is very challenging to develop an effective knowledge management system for facilitating knowledge seeking and sharing in online communities. Knowledge management literature suggests that effective knowledge management should make accessible not only written knowledge but also experts who are a source of information and can perform a given organizational or social function. Existing expert finding systems evaluate one's expertise based on either the contents of authored documents or one's social status within his or her knowledge community. However, very few studies consider both indicators collectively. In addition, very few studies focus on virtual communities where information quality is often poorer than that in organizational knowledge repositories. In this study we propose a novel expert finding algorithm, ExpertRank, that evaluates expertise based on both document-based relevance and one's authority in his or her knowledge community. We modify the PageRank algorithm to evaluate one's authority so that it reduces the effect of certain biasing communication behavior in online communities. We explore three different expert ranking strategies that combine document-based relevance and authority: linear combination, cascade ranking, and multiplication scaling. We evaluate ExpertRank using a popular online knowledge community. Experiments show that the proposed algorithm achieves the best performance when both document-based relevance and authority are considered."
      }
    ]
  },
  {
    "tid":"30",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Structured tiles: directed subgraphs of recurring path patterns in board games' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Tiles have been widely used in 2d games as highly recurring primitive small-sized elements used to structure large terrains. In many categories of board games repeating path-patterns are commonly used. In this context, we propose the notion of structured tiles as a systematic way to capture and such express recurring path-patterns. They constitute reusable building blocks of a far larger-scale compared to primitive tiles, embodying both geometric (for terrain formation) and path structure (for game play) information. Technically, structured tiles are subgraphs whose vertices are polygonal areas of a respective tile image. Through structured tiles, terrain composition is essentially graph assembling enabling to perform semantic checking like node connectivity and reachability, or other tests concerning graphs. Finally, common mechanics like plausible path computation, path selection by players, and in-path movement control are standardized and incorporated in the game engine. We discuss the notion of repeating path-patterns and introduce structured tiles, then elaborate on terrain composition and game mechanics."
      }
    ]
  },
  {
    "tid":"31",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The Affective Body Argument in Technology Design.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, I argue that the affective body is underused in the design of interactive technology despite what it has to offer. Whilst the literature shows it to be a powerful affective communication channel, it is often ignored in favor of the more commonly studied facial and vocal expression modalities. This is despite it being as informative and in some situations even more reliable than the other affective channels. In addition, due to the proliferation of increasingly cheaper and ubiquitous movement sensing technologies, the regulatory affective functions of the body could open new possibilities in various application areas. In this paper, after presenting a brief summary of the opportunities that the affective body offers to technology designers, I will use the case of physical rehabilitation to discuss how its use could lead to interesting new solutions and more effective therapies."
      }
    ]
  },
  {
    "tid":"32",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Novel Cooperative Content Fetching-Based Strategy to Increase the Quality of Video Delivery to Mobile Users in Wireless Networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cooperatively fetching video content with the help of other mobile nodes can release some of the pressure on storage and bandwidth of the constrained mobile devices in wireless networks and speeds up the localization process of video resources so as to support better quality of viewing experience. In this context, the discovery of the appropriate mobile cooperative node becomes a key factor and is also a challenge for the deployment of such a fetching scheme. In this paper, we introduce a novel cooperative content fetching-based strategy to increase the quality of video delivery to mobile users in wireless networks (CCF). By intelligently monitoring the real-time variation in the state of the one-hop neighbors (immediate-neighbors) of the video resource downloader, CCF employs an innovative estimation model to measure the stability of these immediate-neighbors. In order to enhance the cooperative fetching efficiency, CCF designs a communication quality forecast model to measure link reliability and forecast the available bandwidth. By making use of a newly proposed cooperative fetching algorithm, CCF can speed up fetching and disseminating of video resources with the help of cooperative neighbors selected in terms of stability and communication quality. Simulation results show how CCF obtains higher selection accuracy of cooperative neighbors, lower average end-to-end delay, lower average packet loss ratio, higher average throughput, higher video quality, and lower maintenance overhead in comparison with state-of-the-art solutions."
      }
    ]
  },
  {
    "tid":"33",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Shotgun Protein Sequencing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Despite significant advances in the identification of known proteins, the analysis of unknown proteins by tandem mass spectrometry (MS/MS) still remains a challenging open problem. Although Klaus Biemann recognized the potential of mass spectrometry for sequencing of unknown proteins in the 1980s, low-throughput Edman degradation followed by cloning still remains the main method to sequence unknown proteins. The automated spectral interpretation has been limited by a focus on individual spectra and has not capitalized on the information contained in spectra of overlapping peptides. Indeed, the powerful Shotgun DNA Sequencing strategies have not been extended to protein sequencing yet.We demonstrate, for the first time, the feasibility of Shotgun Protein Sequencing of protein mixtures and validate this approach by generating highly accurate de novo reconstructions of various proteins in western diamondback rattlesnake venom. We further argue that Shotgun Protein Sequencing has the potential to overcome the limitations of current protein sequencing approaches and thus catalyze the otherwise impractical applications of proteomics methodologies in studies of unknown proteins. We further describe applications of this technique to analyzing proteins that are not directly inscribed in DNA sequences (like antibodies and fusion proteins in cancer).This is a joint work with Nuno Bandeira (UCSD) and Karl Clauser (Broad)."
      }
    ]
  },
  {
    "tid":"34",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Performance evaluation of packet data services over cellular voice networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we develop a Markov chain modeling framework for throughput/delay analysis of data services over cellular voice networks, using the dynamic channel stealing method. Effective approximation techniques are also proposed and verified for simplification of modeling analysis. Our study identifies the average voice call holding time as the dominant factor to affect data delay performance. Especially in heavy load conditions, namely when the number of free voice channels becomes momentarily less, the data users will experience large network access delay in the range of several minutes or longer on average. The study also reveals that the data delay performance deteriorates as the number of voice channels increases at a fixed voice call blocking probability, due to increased voice trunking efficiency. We also examine the data performance improvement by using the priority data access scheme and speech silence detection technique."
      }
    ]
  },
  {
    "tid":"35",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Letter to the editor: Differential rain attenuation statistics on two converging point-to-point terrestrial links located in a tropical climatic region' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"An analytical procedure for the prediction of differential rain attenuation statistics on two converging point-to-point terrestrial\n links located in a tropical climatic region is presented. It is based on an appropriate convective raincell model and the\n fundamental assumption that the point rainfall rate statistics follows a gamma form. A tropical raindrop size distribution\n is also adopted. Numerical results are compared with a set of available experimental data over pairs of converging terrestrial\n links in Brazil."
      }
    ]
  },
  {
    "tid":"36",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Accurate Image Retrieval with Unsupervised 2-Stage k-NN Re-Ranking' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"AbstractMany state-of-the-art image retrieval systems include a re-ranking step to refine the suggested initial ranking list so as to improve the retrieval accuracy. In this paper, we present a novel 2-stage k-NN re-ranking algorithm. In stage one, we generate an expanded list of candidate database images for re-ranking so that lower ranked ground truth images will be included and re-ranked. In stage two, we re-rank the list of candidate images using a confidence score which is calculated based on, rRBO, a new proposed ranking list similarity measure. In addition, we propose the rLoCATe image feature, which captures robust color and texture information on salient image patches, and shows superior performance in the image retrieval task. We evaluate the proposed re-ranking algorithm on various initial ranking lists created using both SIFT and rLoCATe on two popular benchmark datasets along with a large-scale one million distraction dataset. The results show that our proposed algorithm is not sensitive for different parameter configurations and it outperforms existing k-NN re-ranking methods."
      }
    ]
  },
  {
    "tid":"37",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Spatial Configuration of Agile Wireless Networks with Drone-BSs and User-in-the-loop.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Agile networking can reduce over-engineering, costs, and energy waste. Toward that end, it is vital to exploit all degrees of freedom of wireless networks efficiently, so that the service quality is not sacrificed. In order to reap the benefits of flexible networking, we propose a spatial network configuration (SNC) scheme, which can result in efficient networking; both from the perspective of network capacity and profitability. First, the SNC utilizes the drone-base-stations (drone-BSs) to configure access points. Drone-BSs are shifting paradigms of heterogeneous wireless networks by providing radically flexible deployment opportunities. On the other hand, their limited endurance and potential high cost increase the importance of utilizing drone-BSs efficiently. Therefore, second, user mobility is exploited via user-in-the-loop (UIL), which aims at influencing users\u2019 mobility by offering incentives. The proposed uncoordinated SNC is a computationally efficient method, yet, it may be insufficient to exploit the synergy between the drone-BSs and UIL. Hence, we propose a joint SNC, which increases the performance gain along with the computational cost. Finally, the semi-joint SNC combines the benefits of the joint SNC with computational efficiency. The numerical results show that the semi-joint SNC is two orders of magnitude faster than the joint SNC, and a profit of more than 15% can be obtained compared to conventional systems."
      }
    ]
  },
  {
    "tid":"38",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'InStory: a system for mobile information access, storytelling and gaming activities in physical spaces' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper describes the work carried out in the InStory project. InStory has the goal of defining and implementing a platform for mobile storytelling, information access, and gaming activities. The platform has a flexible computational architecture that integrates heterogeneous devices, different media formats and computational support for different narrative modes and gaming activities. The system is driven and validated by a set of story threads and narratives that are centered on the exploration of physical spaces. This exploration is combined with the perspective of sharing information between users and providing historic context. The project also wants to explore the social aspect of shared narratives and activities, with the idea that the technology can provide new innovative approaches to social participation in different types of events."
      }
    ]
  },
  {
    "tid":"39",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Alignment-Based Recognition of Shape Outlines' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present a 2D shape recognition and classification method based on matching shape outlines. The correspondence between outlines (curves) is based on a notion of an alignment curve and on a measure of similarity between the intrinsic properties of the curve, namely, length and curvature, and is found by an efficient dynamic-programming method. The correspondence is used to find a similarity measure which is used in a recognition system. We explore the strengths and weaknesses of the outline-based representation by examining the effectiveness of the recognition system on a variety of examples."
      }
    ]
  },
  {
    "tid":"40",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Language Model Integration Based On Memory Control For Sequence To Sequence Speech Recognition' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we explore several new schemes to train a seq2seq model to integrate a pre-trained language model ( LM). Our proposed fusion methods focus on the memory cell state and the hidden state in the seq2seq decoder long short-term memory ( LSTM), and the memory cell state is updated by the LM unlike the prior studies. This means the memory retained by the main seq2seq would be adjusted by the external LM. These fusion methods have several variants depending on the architecture of this memory cell update and the use of memory cell and hidden states which directly affects the final label inference. We performed the experiments to show the effectiveness of the proposed methods in a mono-lingual ASR setup on the Librispeech corpus and in a transfer learning setup from a multilingual ASR ( MLASR) base model to a low-resourced language. In Librispeech, our best model improved WER by 3.7%, 2.4% for test clean, test other relatively to the shallow fusion baseline, with multi-level decoding. In transfer learning from an MLASR base model to the IARPA Babel Swahili model, the best scheme improved the transferred model on eval set by 9.9%, 9.8% in CER, WER relatively to the 2-stage transfer baseline."
      }
    ]
  },
  {
    "tid":"41",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Instruction-Cycle-Based Dynamic Voltage Scaling Power Management for Low-Power Digital Signal Processor With 53% Power Savings' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents and analyzes a fully digital instruction-cycle-based dynamic voltage scaling (iDVS) power management strategy for low-power processor designs. The proposed iDVS technique is fully compatible with conventional DVS scheduler algorithms. An additional computer aided design-based design flow was embedded in a standard cell library to implement the iDVS-based processor in highly integrated system-on-a-chip applications. The lattice asynchronous self-timed control digital low-dropout regulator with swift response and low quiescent current was also utilized to improve iDVS voltage transition response. Results show that the iDVS-based processor with the proposed adaptive instruction cycle control scheme can efficiently perform millions of instructions per second during iDVS transition. The iDVS-based digital signal processor chip was implemented in a HH-NEC 0.18-\u03bcm standard complementary metal-oxide semiconductor. Measurement results show that the voltage tracking speed with 11.6 V/\u03bcs saved 53% power."
      }
    ]
  },
  {
    "tid":"42",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'When stochastic rate-delay services meet stochastic network calculus' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The delay-sensitive applications has been rapidly growing in recent years, while the current Internet is not well equipped to support such delay-sensitive traffic. The proposed SRD (Stochastic Rate-Delay) services enable a user to choose between a higher transmission rate or low queuing delay at a congested network link. Delay could be allowed to spike occasionally as long as average low delay remains guaranteed. We build a model using stochastic network calculus to analyze the proposal and shed insight on its fundamental characteristics quantitatively."
      }
    ]
  },
  {
    "tid":"43",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Do Users' Perceptions of Password Security Match Reality?' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Although many users create predictable passwords, the extent to which users realize these passwords are predictable is not well understood. We investigate the relationship between users' perceptions of the strength of specific passwords and their actual strength. In this 165-participant online study, we ask participants to rate the comparative security of carefully juxtaposed pairs of passwords, as well as the security and memorability of both existing passwords and common password-creation strategies. Participants had serious misconceptions about the impact of basing passwords on common phrases and including digits and keyboard patterns in passwords. However, in most other cases, participants' perceptions of what characteristics make a password secure were consistent with the performance of current password-cracking tools. We find large variance in participants' understanding of how passwords may be attacked, potentially explaining why users nonetheless make predictable passwords. We conclude with design directions for helping users make better passwords.\n\n"
      }
    ]
  },
  {
    "tid":"44",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Stochastic nonlinear model predictive control based on progressive density simplification' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Increasing demand for Nonlinear Model Predictive Control with the ability to handle highly noise-corrupted systems has recently given rise to stochastic control approaches. Besides providing high-quality results within a noisy environment, these approaches have one problem in common, namely a high computational demand and, as a consequence, generally a short prediction horizon. In this paper, we propose to reduce the computational complexity of prediction and value function evaluation within the control horizon by simplifying the system progressively down to the deterministic case. Approximation of occurring probability densities by a specific representation, the deterministic Dirac mixture density, with a decreasing resolution (i.e., approximation quality) leads via natural decomposition to a point estimate and thus, can be treated in a deterministic manner. Hence, calculation of the remaining time steps requires considerably less computation time."
      }
    ]
  },
  {
    "tid":"45",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Comparison Between ULP and MDC With Many Descriptions for Image Transmission' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this letter, we present a performance comparison between multiple description coding (MDC) and unequal loss protection (ULP) for progressive image transmission over lossy packet networks. Two optimization criteria are considered, i.e., a multi-quality criterion, when N distinct quality levels are guaranteed at the decoder side, and the optimization of the expected quality at the receiver. We resort to both a semi-analytical approach and simulation results. To enable numerical comparisons, we address a specific MDC algorithm suitable for progressive imaging, and a state-of-the-art ULP algorithm based on Reed Solomon codes. The results, although cannot be generalized to any MDC and ULP methods, are useful to put into evidence some general features that can drive the selection of the most proper technique for the application at hand. In fact, they allow to put into evidence the main advantages and drawbacks of either technique."
      }
    ]
  },
  {
    "tid":"46",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Robustness against Byzantine Failures in Distributed Spectrum Sensing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Distributed Spectrum Sensing (DSS) enables a Cognitive Radio (CR) network to reliably detect licensed users and avoid causing interference to licensed communications. The data fusion technique is a key component of DSS. We discuss the Byzantine Failure problem in the context of data fusion, which may be caused by either malfunctioning sensing terminals or Spectrum Sensing Data Falsification (SSDF) attacks. In either case, incorrect spectrum sensing data is reported to a data collector which can lead to the distortion of data fusion outputs. We investigate various data fusion techniques, focusing on their robustness against Byzantine Failures. In contrast to existing data fusion techniques that use a fixed number of samples, we propose a new technique that uses a variable number of samples. The proposed technique, which we call Weighted Sequential Probability Ratio Test (WSPRT), introduces a reputation-based mechanism to the Sequential Probability Ratio Test (SPRT). We evaluate WSPRT by comparing it with a variety of data fusion techniques under various conditions. We also discuss practical issues that need to be considered when applying the fusion techniques to CR networks. Our simulation results indicate that WSPRT is the most robust against Byzantine Failures among the data fusion techniques that were considered."
      }
    ]
  },
  {
    "tid":"47",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An integrated compilation and performance analysis environment for data parallel programs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Supporting source-level performance analysis of programs written in data-parallel languages requires a unique degree of integration between compilers and performance analysis tools. Compilers for languages such as High Performance Fortran infer parallelism and communication from data distribution directives, thus, performance tools cannot meaningfully relate measurements about these key aspects of execution performance to source-level constructs without substantial compiler support. This paper describes an integrated system for performance analysis of data-parallel programs based on the Rice Fortran 77D compiler and the Illinois Pablo performance analysis toolkit. During code generation, the Fortran D compiler records mapping information and semantic analysis results describing the relationship between performance instrumentation and the original source program. An integrated performance analysis system based on the Pablo toolkit uses this information to correlate the program's dynamic behavior with the data parallel source code. The integrated system provides detailed source-level performance feedback to programmers via a pair of graphical interfaces. Our strategy serves as a model for integration of data-parallel compilers and performance tools."
      }
    ]
  },
  {
    "tid":"48",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Survey on Truth Discovery' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Thanks to information explosion, data for the objects of interest can be collected from increasingly more sources. However, for the same object, there usually exist conflicts among the collected multi-source information. To tackle this challenge, truth discovery, which integrates multi-source noisy information by estimating the reliability of each source, has emerged as a hot topic. Several truth discovery methods have been proposed for various scenarios, and they have been successfully applied in diverse application domains. In this survey, we focus on providing a comprehensive overview of truth discovery methods, and summarizing them from different aspects. We also discuss some future directions of truth discovery research. We hope that this survey will promote a better understanding of the current progress on truth discovery, and offer some guidelines on how to apply these approaches in application domains."
      }
    ]
  },
  {
    "tid":"49",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A mechanical model of biological cells in microinjection' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Microinjection is an effective technique to introduce foreign materials into a biological cell. Although great developments have been achieved, a full understanding of the mechanical response of biological cells to injection operation remains deficient. In this paper, a mechanical model based on membrane theory is proposed. This model utilizes the Mooney-Rivlin material to model the deformation of biomembrane. The relationship between the injection force and the deformation of biological cells is established through the quasi-static equilibrium equations, which are solved by the Runge-Kutta numerical method. To verify the mechanical model, experiments are performed on microinjection of zebrafish and medaka embryos. It is demonstrated that the modeling results agree well with the experimental data, which shows that the proposed model can be used to estimate the mechanical properties of cell biomembranes."
      }
    ]
  },
  {
    "tid":"50",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'AIMD-PQ: A path quality based TCP-friendly AIMD algorithm for multipath congestion control in heterogeneous wireless networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Multipath congestion control algorithms are supposed to be friendly to traditional single-path TCP (SP-TCP). Existing multipath Additive Increase Multiplicative Decrease algorithms (MP-AIMD) for multipath TCP (MP-TCP) are confronted with the problems: 1) they are unfair to SP-TCP when the multiple available paths have heterogeneity, e.g. different RTT; 2) they all use packet losses as congestion signals and induce spurious backoffs on a wireless link with high error rate; 3) congestion window fluctuation reduces their potential to support smooth high-quality service such as multimedia applications. This paper proposes a path quality based multipath AIMD (AIMD-PQ) to tackle the above problems. We utilize the round trip time (RTT) on each sub-path to formulate the path quality estimation. AIMD-PQ balances the loads among its sub-paths, moves traffic off the most congested sub-path, and triggers Multiplicative Decrease by both packet loss and path quality signals. Simulation results show that AIMD-PQ improves both the TCP-friendliness and window fluctuation performance of MP-TCP in a heterogeneous wireless network environment."
      }
    ]
  },
  {
    "tid":"51",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Negotiation-Based Resource Allocation Model in IaaS-Markets' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Usually, IaaS providers use the inflexible supermarket approach for trading resources: a provider offers a resource for a fixed price and consumers can buy the offered resources without negotiating with the provider (take it or leave it). Another possibility is an auction based approach. Auctions have well defined rules which are necessary to ensure fair and transparent resource allocation. However, these rules are limiting flexibility of consumers and providers. In this paper we present a negotiation based resource allocation mechanism following the offer-counteroffer negotiation protocol paradigm. On the one hand, this allocation mechanisms is similar to the supermarket approach as consumer and provider are able to communicate directly. On the other hand, the approach shows also similarities to auctions as the price is determined in a dynamic way. For justification and evaluation we developed a so called Bazaar-Extension for CloudSim which allows to run negotiations and to develop and simulate new negotiation strategies and market scenarios. Further a negotiation strategy considering basic economical principles is introduced in this paper which was used for an exemplary resource allocation scenario. The scenario shows that negotiation based resource allocation can improve the well-being of consumer and provider."
      }
    ]
  },
  {
    "tid":"52",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Asymmetrical Round Trip Based Synchronization-Free Localization in Large-Scale Underwater Sensor Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Underwater sensor networks (UWSNs) have been proposed for many location-dependent applications such as oceanographic data collection, pollution monitoring, mine reconnaissance, etc. Accurate node localization plays an important role in realizing the potential gains of these applications. Although many localization algorithms have been proposed for terrestrial sensor networks in recent years, it is not feasible to directly use these algorithms in UWSNs since UWSNs lack a fast and reliable communication channel. Further, due to their slow convergence speeds and high communication overhead, distributed localization algorithms designed for small-scale UWSNs are not practical for large-scale underwater sensor systems. To achieve accurate and energy efficient node localization in large-scale UWSNs, an asymmetrical round trip based localization (ARTL) algorithm is proposed in this paper. This algorithm has low computational complexity and excellent scalability. Without time synchronization, this algorithm can achieve highly accurate ranging in large-scale UWSNs. Simulation results demonstrate the effectiveness of our design in terms of both localization accuracy and energy consumption."
      }
    ]
  },
  {
    "tid":"53",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An audit environment for outsourcing of frequent itemset mining' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Finding frequent itemsets is the most costly task in association rule mining. Outsourcing this task to a service provider brings several benefits to the data owner such as cost relief and a less commitment to storage and computational resources. Mining results, however, can be corrupted if the service provider (i) is honest but makes mistakes in the mining process, or (ii) is lazy and reduces costly computation, returning incomplete results, or (iii) is malicious and contaminates the mining results. We address the integrity issue in the outsourcing process, i.e., how the data owner verifies the correctness of the mining results. For this purpose, we propose and develop an audit environment, which consists of a database transformation method and a result verification method. The main component of our audit environment is an artificial itemset planting (AIP) technique. We provide a theoretical foundation on our technique by proving its appropriateness and showing probabilistic guarantees about the correctness of the verification process. Through analytical and experimental studies, we show that our technique is both effective and efficient."
      }
    ]
  },
  {
    "tid":"54",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'MFSV: A Truthfulness Determination Approach for Fact Statements.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"How to determine the truthfulness of a piece of information becomes an increasingly urgent need for users. In this paper, we propose a method called MFSV, to determine the truthfulness of fact statements. We first calculate the similarity between a piece of related information and the target fact statement and capture the credibility ranking of the related information through combining importance ranking and popularity ranking. Based on these, contributions of a piece of related information to the truthfulness determination is derived. Then we propose two methods to determine the truthfulness of the target fact statement. At last, we run comprehensive experiments to show MFSV's availability and high accuracy. \u00a9 Springer-Verlag 2013."
      }
    ]
  },
  {
    "tid":"55",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Pixel-to-Model Distance for Robust Background Reconstruction' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Background information is crucial for many video surveillance applications such as object detection and scene understanding. In this paper, we present a novel Pixel-to-Model (P2M) paradigm for background modeling and restoration in surveillance scenes. In particular, the proposed approach models the background with a set of context features for each pixel, which are compressively sensed from local patches. We determine whether a pixel belongs to the background according to the minimum P2M distance, which measures the similarity between the pixel and its background model in the space of compressive local descriptors. The pixel feature descriptors of the background model are properly updated with respect to the minimum P2M distance. Meanwhile, the neighboring background model will be renewed according to the maximum P2M distance in order to handle ghost holes. The P2M distance plays an important role of background reliability in the 3D spatial-temporal domain of surveillance videos, leading to the robust background model and recovered background videos. We applied the proposed P2M distance for foreground detection and background restoration on synthetic and real-world surveillance videos. Experimental results show that the proposed P2M approach outperforms the stateof- the-art approaches both in indoor and outdoor surveillance scenes."
      }
    ]
  },
  {
    "tid":"56",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Non-Cooperating Vehicle Tracking In Vanets Using The Conditional Logit Model' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Vehicular Ad Hoc Networks (VANETs) are widely considered as indispensable elements of the future intelligent transportation systems that are aiming to apply information and communications technologies to improve transportation safety and quality of experience. We present our take on a relatively unexplored problem, exploiting VANETs for on-road surveillance. The proposal is inspired by multi-agent systems intended for surveillance, e.g., a distributed camera network. We propose a tracking system composed of three operational modules, namely, localization, tracking data collection and prediction of future locations of a target. Camera equipped onboard units (OBUs) act as remote mobile sensors. Tracking messages are communicated among the OBUs and roadside units (RSUs). These messages are also triggered in the possible locations of the target in a timely manner. Therefore, it is imperative to scope the search to limit the number of OBUs and RSUs involved in the tracking operation, thus, minimizing the number of tracking messages. To this end, a movement modeling technique utilizes the OBU-observations to classify the target's movement pattern to aid future trajectory prediction. In our previous work, we proposed a Dirichlet-multinomial (D-M) model under the Bayesian estimation framework. In this paper, we present newly identified cues towards improving the movement estimation model. The D-M model is constrained to the assumption that all the choice sets are identical across trials. We demonstrate that this is almost never the case. The improved model exploits a choice model, called the conditional logit. The conditional logit model is attractive when choice sets vary across trials. Additionally, we weight outcome of each trial according to the given choice sets to achieve higher estimation accuracy. We evaluate the new model by means of an experimental analysis and compare results with the D-M model."
      }
    ]
  },
  {
    "tid":"57",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Distributed state estimation for discrete-time sensor networks with randomly varying nonlinearities and missing measurements.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper deals with the distributed state estimation problem for a class of sensor networks described by discrete-time stochastic systems with randomly varying nonlinearities and missing measurements. In the sensor network, there is no centralized processor capable of collecting all the measurements from the sensors, and therefore each individual sensor needs to estimate the system state based not only on its own measurement but also on its neighboring sensors' measurements according to certain topology. The stochastic Brownian motions affect both the dynamical plant and the sensor measurement outputs. The randomly varying nonlinearities and missing measurements are introduced to reflect more realistic dynamical behaviors of the sensor networks that are caused by noisy environment as well as by probabilistic communication failures. Through available output measurements from each individual sensor, we aim to design distributed state estimators to approximate the states of the networked dynamic system. Sufficient conditions are presented to guarantee the convergence of the estimation error systems for all admissible stochastic disturbances, randomly varying nonlinearities, and missing measurements. Then, the explicit expressions of individual estimators are derived to facilitate the distributed computing of state estimation from each sensor. Finally, a numerical example is given to verify the theoretical results."
      }
    ]
  },
  {
    "tid":"58",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Exploiting program execution phases to trade power and performance for media workload' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Processing streaming media comprises several program phases (often distinct) that are periodic and independent of application data. In this paper we characterize execution of such programs into execution phases based on their dynamic IPC (Instruction Per Cycle) profile. We show that program execution of selected phases can be dynamically boosted by activating additional standby functional units which ae otherwise powered down for saving energy. Through simulation we show that speedup ranging from 1.1 to 1.25 can be achieved while reducing the energy-delay product (EDP) for most of the media benchmarks evaluated. Additionally we show that artificially introduced stalls during phases of processor underutilization reduces power by around 2 to 4%."
      }
    ]
  },
  {
    "tid":"59",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'zPots: a virtual pottery experience with spatial interactions using the leap motion device' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present zPots, an application for gesture-free hand-based design of virtual pottery enabled by the Leap Motion device. With zPots, a user can shape and color 3D pots by moving bare hands in the air with minimal or no training. Unlike large-space hand-and-body movements required by depth cameras such as the Kinect, the use of the Leap motion device facilitates close range 3D interactions collocated with the personal computer. We demonstrate our application as a synergistic combination of novel spatial interactions and tool metaphors that cater to engaging and realistic experiences while supporting creativity in 3D shape conceptualization and modeling."
      }
    ]
  },
  {
    "tid":"60",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Maximum weight matching dispatching scheme in buffered Clos-network packet switches' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The scalability of Clos-network switches makes them an alternative to single-stages switches for implementing large-size packet switches. This paper introduces a cell dispatching scheme, called Maximum Weight Matching Dispatching (MWMD) scheme, for buffered Clos-network switches. The MWMD scheme is based on a maximum weight matching algorithm for input-buffered switches. This paper shows that, with request queues in the buffered Clos-network architecture, the MWMD scheme is able to achieve a 100% throughput for independent admissible traffic, without allocating any buffers in the second stage and without expanding the internal bandwidth. As a practical scheme, a maximal oldest-cell-first matching dispatching (MOMD) scheme is also introduced. MOMD shows that using a finite number of iterations in the dispatching scheme, the throughout under unbalanced traffic pattern can be high."
      }
    ]
  },
  {
    "tid":"61",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Non-linear volume photon mapping' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper describes a novel extension of the photon mapping algorithm, capable of handling both volume multiple inelastic scattering and curved light paths simultaneously. The extension is based on the Full Radiative Transfer Equation (FRTE) and Fermat's law, and yields physically accurate, high-dynamic data than can be used for image generation or for other simulation purposes, such as driving simulators, underwater vision or lighting studies in architecture. Photons are traced into the participating medium with a varying index of refraction, and their curved trajectories followed (curved paths are the cause of certain atmospheric effects such as mirages or rippling desert images). Every time a photon is absorbed, a Russian roulette algorithm based on the quantum efficiency of the medium determines whether the inelastic scattering event takes place (causing volume fluorescence). The simulation of both underwater and atmospheric effects is shown, providing a global illumination solution without the restrictions of previous approaches."
      }
    ]
  },
  {
    "tid":"62",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'I/O Complexity for Range Queries on Region Data Stored Using an R-tree' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we study the node distribution of an R- tree storing region data, like for instance islands, lakes or human-inhabited areas. We will show that real re- gion datasets are packed in minimum bounding rectangles (MBRs) whose area distribution follows the same power law, named REGAL (REGion Area Law) (12), as that for the regions themselves. Moreover, these MBRs are packed in their turn into MBRs following the same law, and so on iteratively, up to the root of the R-tree. Based on this obser - vation, we are able to accurately estimate the search effort for range queries, the most prominent spatial operation, us - ing a small number of easy-to-retrieve parameters. Experiments on a variety of real datasets (islands, lakes, human-inhabited areas) show that our estimation is accu- rate, enjoying a maximum geometric average relative error within 30%."
      }
    ]
  },
  {
    "tid":"63",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'University of Hagen at GeoCLEF 2005: Using Semantic Networks for Interpreting Geographical Queries.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The IICS group at the University of Hagen employs multilayered extended semantic networks for the representation of background knowledge, queries, and documents for geographic in- formation retrieval (GIR). This paper describes our work for the participation at the GeoCLEF task of the CLEF 2005 evaluation campaign (Cross Language Evaluation Forum). In our approach, geographical concepts from the query network are expanded with concepts which are semantically connected via topological, directional, and proximity relations. We started with an existing geographical knowledge base represented as a large semantic network and expanded it with concepts automatically extracted from the GEOnet Names Server (GNS). Furthermore, we created concept hypotheses by adding a prefix with regular semantics, for example \"S\u00a8 ud\"/'South' and \"Zentral\"/'Central', and integrated the corresponding semantic relations into our geographical knowledge base. Several experiments for GIR on German documents have been performed: a baseline cor- responding to a traditional information retrieval approach; a variant expanding thematic, tem- poral, and geographic descriptors from the semantic network representation of the query; and an adaptation of a question answering (QA) algorithm based on semantic networks. The second experiment is based on a representation of the natural language description of a topic as a semantic network, which is achieved by a deep linguistic analysis. The seman- tic network is transformed into an intermediate representation of a database query explicitly representing thematic, temporal, and local restrictions. This experiment showed the best per- formance with respect to mean average precision (MAP): 10.53 percent using the topic title and description or 10.22 percent using title, description, and additional location information. The third experiment, adapting a QA algorithm, uses a modified version of the QA system InSicht. The system matches deep semantic representations (semantic networks) of queries or their equivalent or similar variants to semantic networks for document sentences. Since this approach was too much oriented towards precision, partitioning a query network was allowed when certain graph topologies exist. For example, local specifications can be split off, so that they can be matched in other sentences of the document under investigation. The geographical knowledge base developed for the other experiments improved the results of this approach, too. To keep answer time low and main memory consumption acceptable, some parameters of the InSicht system had to be adjusted. In conclusion, we provide a basic architecture for further experiments in geographic infor- mation retrieval based on semantic networks. Future research aims at improving the named entity recognition for toponyms, connecting semantic networks and databases, expanding our geographical knowledge base, and investigating the role of semantic relations in geographic queries."
      }
    ]
  },
  {
    "tid":"64",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Trade-Off Between Energy Efficiency And Spectral Efficiency In The Uplink Of A Linear Cellular System With Uniformly Distributed User Terminals' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a tight closed-form approximation of the Energy Efficiency vs. Spectral Efficiency (EE-SE) trade-off for the uplink of a linear cellular communication system with base station cooperation and uniformly distributed user terminals. We utilize the doubly-regular property of the channel to obtain a closed form approximation using the Marcenko Pasture law. We demonstrate the accuracy of our expression by comparing it with Monte-Carlo simulation and the EE-SE trade-off expression based on low-power approximation. Results show the great tightness of our expression with Monte-Carlo simulation. We utilize our closed form expression for assessing the EE performance of cooperation for both theoretical and realistic power models. The theoretical power model includes only the transmit power, whereas the realistic power model incorporates the backhaul and signal processing power in addition to the transmit power. Results indicate that for both power models, increasing the number of antennas leads to an improvement in EE performance, whereas, increasing the number of cooperating BSs results in a loss in EE when considering the realistic power model."
      }
    ]
  },
  {
    "tid":"65",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Comparisons of Circular Transmit and Linear Receive Compact Polarimetric SAR Features for Oil Slicks Discrimination.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Compact polarimetric (CP) synthetic aperture radar (SAR) has proven its potential in distinguishing oil slicks and look-alikes. Polarimetric information can be retrieved directly from scattering vector or from reconstructed pseudo-Quad-Pol covariance matrix of CP SAR data. In this paper, we analysed features from Circular Transmit and Linear Receive (CTLR) CP SAR data that are derived by taking both of these two methods. K-means clustering followed by accuracy assessment was also implemented for performance evaluation. Through experiments that were conducted based on L-band UAVSAR fully polarimetric data, it was found that optimum extraction methods varied for different features. The histogram analysis and segmentation results also demonstrated the comparable performance of CP SAR features in distinguishing different damping properties within oil slicks. This study proposed a framework of statistically analyzing polarimetric SAR (Pol-SAR) features and provided guidelines for determining optimum feature extraction methods from CP SAR data and for marine oil-spills detection and classification."
      }
    ]
  },
  {
    "tid":"66",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'TBE: Trigger-By-Example' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"TBE (Trigger-By-Example) is proposed to assist users in writing trigger rules. TBE is a graphical trigger rule specification language and system to help users understand and specify active database triggers. Since TBE borrowed its basic idea from QBE, it retained many benefits of QBE while extending the features to support triggers. Hence, TBE is a useful tool for novice users to create simple trigger rules easily. Further, since TBE is designed to insulate the details of underlying trigger systems from users, it can be used as a universal trigger interface for rule formation."
      }
    ]
  },
  {
    "tid":"67",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Locating malicious nodes for data aggregation in wireless networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Data aggregation, as a primitive communication task in wireless networks, can reduce the communication complexity. However, in-network aggregation usually brings an unavoidable security defect. Some malicious nodes may control a large percentage of the whole network data and compel the network misbehave in an arbitrary manner. Thus, locating the malicious nodes to prevent them from further disaster is a practical challenge for data aggregation schemes. Based on the grouping and localization techniques, we propose a novel integrated protocol to locate malicious nodes. The proposed protocol does not rely on any special hardware and requests only incomplete information of the network from the security schemes. We also conduct simulation study to evaluate the proposed protocol."
      }
    ]
  },
  {
    "tid":"68",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Correlation filters with controlled scale response.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Correlation filtering methods are becoming increasingly popular for image recognition and location. The recent introduction of optimal tradeoff circular harmonic function filters allowed the user to specify the response of a correlation filter to in-plane rotation distortion. In this paper we introduce a new correlation filter design that can provide a user-specified response to in-plane scale distortion. The design is based on the Mellin radial harmonic (MRH) transform and incorporates multiple harmonics into the correlation filter for improved discrimination capability. Additionally, the filter design minimizes the average correlation energy in order to achieve sharp correlation peaks, and thus we refer to these filters as minimum average correlation energy Mellin radial harmonic (MACE-MRH) filters. We present underlying theory, a MACE-MRH filter design method, and numerical simulation results."
      }
    ]
  },
  {
    "tid":"69",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On-Demand Location-Aided QoS Routing in Ad Hoc Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With the development and application of position devices, location-based routing has received growing attention. However, little study has been done on QoS routing with the aid of location information. The existing location-based routing approaches, such as flooding-based routing schemes and localized routing schemes, have their limitations. Motivated by ticket-based routing, we propose an ondemand location-aided, ticket-based QoS routing protocol (LTBR). Two special cases of LTBR, LTBR-1 and LTBR-2, are discussed in detail. LTBR-1 uses a single ticket to find a route satisfying a given QoS constraint. LTBR-2 uses multiple tickets to search valid routes in a limited area. All tickets are guided via both location and QoS information. LTBR has lower overhead compared with the original ticket-based routing, because it does not rely on an underlying routing table. On the other hand, LTBR can find routes with better QoS qualities than traditional location-based protocols. Our simulation results show that LTBR-1 can find high quality routes in relatively dense networks with high probability and very low overhead. In sparse networks, LTBR-2 can be used to enhance the probability of finding high quality routes with acceptable overhead."
      }
    ]
  },
  {
    "tid":"70",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Adopting Sp/Si-Frames In Dual-Bitstream Video Streaming With Vcr Support' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Digital video cassette recording (VCR) operations such as fast-forward and fast-reverse playbacks enable quick and user-friendly browsing of video. However, the predictive techniques adopted in current video standards severely complicate these operations. One approach to implement the fast-forward/reverse playback is to store an additional reverse-encoded bitstream into the server. Once a client requests a fast-forward/reverse operation, the server can select an appropriate frame for the client from either the forward or reverse-encoded bitstreams to reduce the network traffic and the decoder complexity. Unfortunately, the forward and reverse-encoded bitstreams are encoded separately. The frame that has previously decoded by the client may not be exactly identical to the reference of the current selected frame and the mismatch problem occurs frequently. In this paper, a novel H.264 dual-bitstream scheme aiming at providing fast-forward/reverse playback based on SP/SI-frames is proposed to eliminate mismatch errors during switching between the forward and reverse-encoded bitstreams. As a result, the proposed scheme enhances the performance of the conventional. dual-bitstream scheme."
      }
    ]
  },
  {
    "tid":"71",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Virtual reality studies outside the laboratory' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many user studies are now conducted outside laboratories to increase the number and heterogeneity of participants. These studies are conducted in diverse settings, with the potential to give research greater external validity and statistical power at a lower cost. The feasibility of conducting virtual reality (VR) studies outside laboratories remains unclear because these studies often use expensive equipment, depend critically on the physical context, and sometimes study delicate phenomena concerning body awareness and immersion. To investigate, we explore pointing, 3D tracing, and body-illusions both in-lab and out-of-lab. The in-lab study was carried out as a traditional experiment with state-of-the-art VR equipment; 31 completed the study in our laboratory. The out-of-lab study was conducted by distributing commodity cardboard VR glasses to participants; 57 completed the study anywhere they saw fit. The effects found in-lab were comparable to those found out-of-lab, with much larger variations in the settings in the out-of-lab condition. A follow-up study showed that performance metrics are mostly governed by the technology used, where more complex VR phenomena depend more critically on the internal control of the study. We argue that conducting VR studies outside the laboratory is feasible, and that certain types of VR studies may advantageously be run this way. From the results, we discuss the implications and limitations of running VR studies outside the laboratory.\n\n"
      }
    ]
  },
  {
    "tid":"72",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Prediction of Infant MRI Appearance and Anatomical Structure Evolution Using Sparse Patch-Based Metamorphosis Learning Framework.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Magnetic resonance imaging (MRI) of pediatric brain provides invaluable information for early normal and abnormal brain development. Longitudinal neuroimaging has spanned various research works on examining infant brain development patterns. However, studies on predicting postnatal brain image evolution remain scarce, which is very challenging due to the dynamic tissue contrast change and even inversion in postnatal brains. In this paper, we unprecedentedly propose a dual image intensity and anatomical structure (label) prediction framework that nicely links the geodesic image metamorphosis model with sparse patch-based image representation, thereby defining spatiotemporal metamorphic patches encoding both image photometric and geometric deformation. In the training stage, we learn the 4D metamorphosis trajectories for each training subject. In the prediction stage, we define various strategies to sparsely represent each patch in the testing image using the training metamorphosis patches; as we progressively increment the richness of the patch (from appearance-based to multimodal kinetic patches). We used the proposed framework to predict 6, 9 and 12-month brain MR image intensity and structure (white and gray matter maps) from 3 months in 10 infants. Our seminal work showed promising preliminary prediction results for the spatiotemporally complex, drastically changing brain images."
      }
    ]
  },
  {
    "tid":"73",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'KaiKai: A NAT Traversal Approach by Using Protocol Behavior Analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Typical NAT traversable applications solve the peer-to-peer communication problems by means of STUN-like NAT traversal approach. For example, Skype is a well-known NAT traversable application. The Skype developer claims that peer-to-peer voice over IP (VoIP) service can work across NATs/firewalls. Two Skype clients which reside in different NATs can connect directly with each other. Nonetheless, Skype clients can not traverse through the NAT daemon on FreeBSD system without \"Deny Incoming\" option being set. The default of this option is disabled and Skype clients must depend on third parties to relay voice instead of contacting with correspondent directly. The situation result in voice quality degradation and scalability problems. This paper proposes a novel approach \"KaiKai\" for applications to solve this problem. KaiKai can make two clients contact directly even these two clients reside in different NATs. By using protocol behavior analysis, KaiKai can cooperate with NATs with or without \"Deny Incoming\" option being set. We implemented KaiKai on Windows system and chose Skype as our demonstration application. Our experiments show that KaiKai can make two Skype clients connect directly behind different NATs. Consequently, the better quality of service and scalability can be achieved."
      }
    ]
  },
  {
    "tid":"74",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Accelerating ODE-Based Simulation of General and Heterogeneous Biophysical Models Using a GPU.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Flint is a simulator that numerically integrates heterogeneous biophysical models described by a large set of ordinary differential equations. It uses an internal bytecode representation of simulation-related expressions to handle various biophysical models built for general purposes. We propose two acceleration methods for Flint using a graphics processing unit (GPU). The first method interprets ..."
      }
    ]
  },
  {
    "tid":"75",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Convex and scalable weakly labeled SVMs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we study the problem of learning from weakly labeled data, where labels of the training examples are incomplete. This includes, for example, (i) semi-supervised learning where labels are partially known; (ii) multi-instance learning where labels are implicitly known; and (iii) clustering where labels are completely unknown. Unlike supervised learning, learning with weak labels involves a difficult Mixed-Integer Programming (MIP) problem. Therefore, it can suffer from poor scalability and may also get stuck in local minimum. In this paper, we focus on SVMs and propose the WELLSVM via a novel label generation strategy. This leads to a convex relaxation of the original MIP, which is at least as tight as existing convex Semi-Definite Programming (SDP) relaxations. Moreover, the WELLSVM can be solved via a sequence of SVM subproblems that are much more scalable than previous convex SDP relaxations. Experiments on three weakly labeled learning tasks, namely, (i) semi-supervised learning; (ii) multi-instance learning for locating regions of interest in content-based information retrieval; and (iii) clustering, clearly demonstrate improved performance, and WELLSVM is also readily applicable on large data sets."
      }
    ]
  },
  {
    "tid":"76",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Audiovisual Speech Source Separation: An overview of key methodologies' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The separation of speech signals measured at multiple microphones in noisy and reverberant environments using only the audio modality has limitations because there is generally insufficient information to fully discriminate the different sound sources. Humans mitigate this problem by exploiting the visual modality, which is insensitive to background noise and can provide contextual information about the audio scene. This advantage has inspired the creation of the new field of audiovisual (AV) speech source separation that targets exploiting visual modality alongside the microphone measurements in a machine. Success in this emerging field will expand the application of voice-based machine interfaces, such as Siri, the intelligent personal assistant on the iPhone and iPad, to much more realistic settings and thereby provide more natural human?machine interfaces."
      }
    ]
  },
  {
    "tid":"77",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'CUDASW++ 3.0: accelerating Smith-Waterman protein database search by coupling CPU and GPU SIMD instructions.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The maximal sensitivity for local alignments makes the Smith-Waterman algorithm a popular choice for protein sequence database search based on pairwise alignment. However, the algorithm is compute-intensive due to a quadratic time complexity. Corresponding runtimes are further compounded by the rapid growth of sequence databases.We present CUDASW++ 3.0, a fast Smith-Waterman protein database search algorithm, which couples CPU and GPU SIMD instructions and carries out concurrent CPU and GPU computations. For the CPU computation, this algorithm employs SSE-based vector execution units as accelerators. For the GPU computation, we have investigated for the first time a GPU SIMD parallelization, which employs CUDA PTX SIMD video instructions to gain more data parallelism beyond the SIMT execution model. Moreover, sequence alignment workloads are automatically distributed over CPUs and GPUs based on their respective compute capabilities. Evaluation on the Swiss-Prot database shows that CUDASW++ 3.0 gains a performance improvement over CUDASW++ 2.0 up to 2.9 and 3.2, with a maximum performance of 119.0 and 185.6 GCUPS, on a single-GPU GeForce GTX 680 and a dual-GPU GeForce GTX 690 graphics card, respectively. In addition, our algorithm has demonstrated significant speedups over other top-performing tools: SWIPE and BLAST+.CUDASW++ 3.0 is written in CUDA C++ and PTX assembly languages, targeting GPUs based on the Kepler architecture. This algorithm obtains significant speedups over its predecessor: CUDASW++ 2.0, by benefiting from the use of CPU and GPU SIMD instructions as well as the concurrent execution on CPUs and GPUs. The source code and the simulated data are available at http://cudasw.sourceforge.net."
      }
    ]
  },
  {
    "tid":"78",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'No-Reference Quality Metric of Contrast-Distorted Images Based on Information Maximization.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The general purpose of seeing a picture is to attain information as much as possible. With it, we in this paper devise a new no-reference/blind metric for image quality assessment (IQA) of contrast distortion. For local details, we lirst roughly remove predicted regions in an image since unpredicted remains are of much information. We then compute entropy of particular unpredicted areas of maximum..."
      }
    ]
  },
  {
    "tid":"79",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Applying genetic algorithms to optimize the power in tiled SNUCA chip multicore architectures' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose a novel technique for reducing the power consumed by the on-chip cache in SNUCA chip multicore platform. This is achieved by what we call a \"remap table\", which maps accesses to the cache banks that are as close as possible to the cores, on which the processes are scheduled. With this technique, instead of using all the available cache, we use a portion of the cache and allocate lesser cache to the application. We formulate the problem as an energy-delay (ED) minimization problem and solve it offline using a scalable genetic algorithm approach. Our experiments show up to 40% of savings in the memory sub-system power consumption and 47% savings in energy-delay product (ED)."
      }
    ]
  },
  {
    "tid":"80",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multi-Hop Bi-Directional Relay Transmission Schemes Using Amplify-and-Forward and Analog Network Coding' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we investigate two different multi-hop bi-directional relay transmission schemes based on amplify-and-forward (AF) protocol and analogue network coding (ANC). In the first scheme, referred to as the AF-ANC-Central scheme, AF is employed at each of the intermediate nodes, while ANC is only utilized at the central relay. In the second scheme, referred to as the AF-ANC-Even scheme, the even relays perform ANC and the odd relays only perform subtraction and AF. For reference purpose, the AF-No-ANC scheme is also considered, where the intermediate relays only perform AF and no ANC is employed. Bit error rate (BER) lower bounds of these three schemes are obtained, and are verified by Monte Carlo simulations to be asymptotically tight ones at high signal-to-noise ratios (SNRs). It is shown that the combination of AF and ANC is able to significantly improve system throughput when compared with the AF-No-ANC scheme. The BER performance of the AF-ANC- Central and AF-No-ANC schemes are of the same at high SNR region under the same transmission powers and system configuration, but the AF-ANC-Central scheme is able to double the system throughput. The AF-ANC-Even is able to further increase the system throughput with an SNR loss upper bounded by 1.76 dB when compared to the AF-ANC-Central and AF-No-ANC schemes."
      }
    ]
  },
  {
    "tid":"81",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A fuzzy expert system for diabetes decision support application.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"An increasing number of decision support systems based on domain knowledge are adopted to diagnose medical conditions such as diabetes and heart disease. It is widely pointed that the classical ontologies cannot sufficiently handle imprecise and vague knowledge for some real world applications, but fuzzy ontology can effectively resolve data and knowledge problems with uncertainty. This paper presents a novel fuzzy expert system for diabetes decision support application. A five-layer fuzzy ontology, including a fuzzy knowledge layer, fuzzy group relation layer, fuzzy group domain layer, fuzzy personal relation layer, and fuzzy personal domain layer, is developed in the fuzzy expert system to describe knowledge with uncertainty. By applying the novel fuzzy ontology to the diabetes domain, the structure of the fuzzy diabetes ontology (FDO) is defined to model the diabetes knowledge. Additionally, a semantic decision support agent (SDSA), including a knowledge construction mechanism, fuzzy ontology generating mechanism, and semantic fuzzy decision making mechanism, is also developed. The knowledge construction mechanism constructs the fuzzy concepts and relations based on the structure of the FDO. The instances of the FDO are generated by the fuzzy ontology generating mechanism. Finally, based on the FDO and the fuzzy ontology, the semantic fuzzy decision making mechanism simulates the semantic description of medical staff for diabetes-related application. Importantly, the proposed fuzzy expert system can work effectively for diabetes decision support application."
      }
    ]
  },
  {
    "tid":"82",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Correction to \u201cOn the Capacity Game of Private Fingerprinting Systems Under Collusion Attacks\u201d [Mar 05 884-899]' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this correspondence, we correct an error in the above paper by Somekh-Baruch and Merhav (see ibid., vol.51, no.3, p.884-99, 2005)."
      }
    ]
  },
  {
    "tid":"83",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Heterogeneous cellular network user distribution model' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In heterogeneous networks (HetNets) users are usually distributed non-uniformly. Analysis of the impact of nonuniformity of user distribution on HetNets is essential for designing efficient networks. This paper proposes a non-uniform user distribution model in which the user density depends on the distance to the associated base stations (BSs). Moreover, different tiers, such as macro cells, pico cells or femto cells, may have different user densities with different level of attraction to their associated BSs. Our proposed non-uniform user distribution model covers many possible combinations. We give an accurate performance analysis of the downlink coverage probability. The numerical evaluations show that our analysis can produce results that closely match the simulation results."
      }
    ]
  },
  {
    "tid":"84",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Complementing visual tracking of moving targets by fusion of tactile sensing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Robot control in uncertain and dynamic environments can be greatly improved using sensor-based control. Vision is a versatile low-cost sensory modality, but low sample rate, high sensor delay and uncertain measurements limit its usability, especially in strongly dynamic environments. Vision can be used to estimate a 6-DOF pose of an object by model-based pose-estimation methods, but the estimate is typically not accurate along all degrees of freedom. Force is a complementary sensory modality allowing accurate measurements of local object shape when a tooltip is in contact with the object. In multimodal sensor fusion, several sensors measuring different modalities are combined together to give a more accurate estimate of the environment. As force and vision are fundamentally different sensory modalities not sharing a common representation, combining the information from these sensors is not straightforward. We show that the fusion of tactile and visual measurements enables to estimate the pose of a moving target at high rate and accuracy. Making assumptions of the object shape and carefully modeling the uncertainties of the sensors, the measurements can be fused together in an extended Kalman filter. Experimental results show greatly improved pose estimates with the proposed sensor fusion."
      }
    ]
  },
  {
    "tid":"85",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Analysis of Joint Provision of Public Goods in Online Communities' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Online community (OC, or termed as \"virtual community\") provides a social sphere for people to share their information, knowledge and other resources. It is becoming an unprecedentedly ubiquitous online phenomenon. The contents which are jointly provided by participants are considered as public goods (Kollock, 1999). For public goods provided by collective action, there have been many papers since Olson (1965) and Hardin (1968). The major concern and conclusion are that enough public goods cannot be provided by rational individuals in collective action. This argument is also applicable to OC because OC will collapse if everyone shirks and free rides others. In the attention economy, however, information overloading becomes a major concern as well, and in response many search and recommendation technologies and services were developed in the Internet including OC. In this case, it seems that \"too much\" rather than \"not enough\" public goods are provided in OC. Surprisingly, phenomenon indicates that both \"not enough\" and \"too much\" are possible, which contradicts the conclusion of traditional literatures. Hence, this study aims to address why both possibilities exist in OC. How are different results realized by interaction behaviors among participants? Theoretically, we addressed where the optimal level of provision and community size lie. Accordingly, possible managerial strategies are discussed and suggested."
      }
    ]
  },
  {
    "tid":"86",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A cartoon-plus-texture image decomposition model for blind deconvolution' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we study a blind deconvolution problem by using an image decomposition technique. Our idea is to make use of a cartoon-plus-texture image decomposition procedure into the deconvolution problem. Because cartoon and texture components can be represented differently in images, we can adapt suitable regularization methods to restore their components. In particular, the total variational regularization is used to describe the cartoon component, and Meyer's G-norm is employed to model the texture component. In order to obtain the restored image automatically, we also use the generalized cross validation method efficiently and effectively to estimate their corresponding regularization parameters. Experimental results are reported to demonstrate that the visual quality of restored images by using the proposed method is very good, and is competitive with the other testing methods."
      }
    ]
  },
  {
    "tid":"87",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Combining classifiers with multi-representation of context in word sense disambiguation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we first argue that various ways of using context in WSD can be considered as distinct representations of a polysemous word under consideration, then all these representations are used jointly to identify the meaning of the target word. Under such a consideration, we can then straightforwardly apply the general framework for combining classifiers developed in Kittler et al. [5] to WSD problem. This results in many commonly used decision rules for WSD. The experimental result shows that the multi-representation based combination strategy of classifiers outperform individual ones as well as known techniques of classifier combination in WSD."
      }
    ]
  },
  {
    "tid":"88",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Design and Implementation of a Fuzzy-Modified Ant Colony Hardware Structure for Image Retrieval' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a hardware implementation of a fuzzy modified ant colony processor that is suitable for image retrieval is presented for the first time. The proposed method utilizes three different descriptors in a two stage fuzzy ant algorithm where the query image represents the nest and the database images represent the food. From the hardware point of view, only a small number of algorithms for hardware implementation have been reported in the image retrieval literature, since research focuses mainly on possible software solutions and the acceleration of existing algorithms. The proposed digital hardware structure is based on a sequence of pipeline stages, while parallel processing is also used in order to minimize computational times. It is capable of performing the extraction and comparison of features from (64times64)-pixel-size color images, although through a simple transformation it can be easily expanded to accommodate images of larger sizes. The architecture of the processor is generic; the units that perform the fuzzy inference can be used with different descriptors than the ones proposed here and can be utilized for other fuzzy applications. It was designed, compiled, and simulated using the Quartus Programmable Logic Development System by the Altera Corporation. The fuzzy processor exhibits a level of inference performance of 800 K fuzzy logic inferences per second with 24 rules, and can be used for real-time applications where the need for short processing times is of the utmost importance."
      }
    ]
  },
  {
    "tid":"89",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'HyperMIP: Hypervisor Controlled Mobile IP for Virtual Machine Live Migration across Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Live migration provides transparent load-balancing and fault-tolerant mechanism for applications. When a Virtual Machine migrates among hosts residing in two networks, the network attachment point of the Virtual Machine is also changed, thus the Virtual Machine will suffer from IP mobility problem after migration. This paper proposes an approach called Hypervisor controlled Mobile IP to support live migration of Virtual Machine across networks, which enables virtual machine live migration over distributed computing resources. Since Hypervisor is capable of predicting exact time and destination host of Virtual Machine migration, our approach not only can improve migration performance but also reduce the network restoration latency. Some comprehensive experiments have been conducted and the results show that the HyperMIP brings negligible overhead to network performance of Virtual Machines. The network restoration time of HyperMIP supported migration is about only 3 second. HyperMIP is a promising essential component to provide reliability and fault tolerant for network application running in Virtual Machine."
      }
    ]
  },
  {
    "tid":"90",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Supporting Learning Design via dynamic generation of learning routes in ADAPTAPlan' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The design phase on the life cycle of the eLearning process is currently one of the main bottle necks in adaptive systems. To facilitate this process and reduce the design effort our approach focuses on providing dynamic assistance to some of the author's tasks that strongly depend on data coming from users' interactions. In particular, ADAPTAPlan project is developing a system where the learning route of a student is dynamically built from the combination of user modelling and scheduling techniques making a pervasive use of educational standards (IMS). The author is requested to provide simple information about the course structure, pedagogy and restrictions and the system utilize this information together with the user model to generate a personalize IMS-LD course flow suited to that learner. Since the output is given in a standardized format, the course can be run in any standard based learning environment. This approach will be tested on a real course, \u201cHow to teach online\u201d, within the UNED's program for ongoing education and open courses."
      }
    ]
  },
  {
    "tid":"91",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Inverse consistency error in the registration of prone and supine images in CT colonography' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Robust registration between prone and supine data acquisitions for CT colonography is pivotal for medical interpretation but a challenging problem. One measure when evaluating non-rigid registration algorithms over the whole of the deformation field is the inverse consistency error, which suggests improved registration quality when the inverse deformation is consistent with the forward deformation. We show that using computed landmark displacements to initialise an intensity based registration reduces the inverse consistency error when using a state-of-the-art non-rigid b-spline registration method. This method aligns prone and supine 2D images derived from CT colonography acquisitions in a cylindrical domain. Furthermore, we demonstrate that using the same initialisation also improves registration accuracy for a set of manually identified reference points in cases exhibiting local luminal collapse."
      }
    ]
  },
  {
    "tid":"92",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Programming Support for Speculative Execution with Software Transactional Memory' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we identify the practical issues that deter the adoption of software transactional memory (STM) for speculation in real applications. These issues include dealing with excessive instrumentation added by naive identification of potential shared accesses, functions that may be called from both transactional and non-transactional contexts, and calls to functions for which the source is unavailable. We address these challenges and provide an approach for developing speculatively-executed code in C/C++ that offers superior programmability and performance. Our contributions consist of a set of programming constructs for writing speculatively-executed code and a compiler that translates code annotated with these constructs into speculatively-executable code that uses STM runtime libraries. Our approach uses annotations that simply mark the boundaries of the code that is to be executed speculatively and supports calling precompiled (e.g., C standard library) and irreversible (e.g., I/O operations and system calls) functions from within transactions. We employ a series of important optimizations for reducing the overhead of speculative execution, including: placement of read/write barriers only for accesses that actually can cause a data race; elimination of redundant read/write barriers by caching shared variables; and eliminating unnecessary search in the write buffer. We evaluate the programmability of our constructs and the performance of our compiler implementation using eight STAMP benchmarks and two real applications-the Velvet genomic assembler and the ITI decision tree constructor. Compared to Intel's STM compiler, our approach requires 91% fewer constructs to be inserted by the programmer, yet it achieves 20.8% better performance."
      }
    ]
  },
  {
    "tid":"93",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'RFID-based networks: exploiting diversity and redundancy' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this article, we outline a research agenda for developing protocols and algorithms for densely populated RFID based systems covering a wide geographic area. This will need multiple readers collaborating to read RFID tag data. We consider cases where the tag data is used for identification, or for sensing environmental parameters. We address performance issues related to 'accuracy' and 'efficiency' in such systems by exploiting 'diversity' and 'redundancy'. We discuss how tag multiplicity can be used to improve accuracy. In a similar fashion, we explore how reader diversity, achieved by using multiple readers with potentially partially overlapping coverage areas, can be exploited to improve accuracy and efficiency. Finally, we show how multiple antennas in a reader can be used to improve accuracy and access rates by utilizing antenna diversity. RFID tag/sensor data can be highly redundant for the purpose of answering a higher level query. For example, often the higher level query needs to compute a statistic or a function on the sensory data obtained by the RFID sensors, and does not need all the individual sensor readings. We outline the need for efficient tag-to-reader communication, and reader-to-reader coordination to effectively compute such functions with low overhead."
      }
    ]
  },
  {
    "tid":"94",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimal Multi-User Interference Balancing Using Transmit Beamforming' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We study the problem of non-orthogonal spatial multiplexing under hard fairness constraints, assuming that the transmitter is equipped with multiple antennas and each of the independent receivers has a single antenna. Channel state information is available at the transmitter. All users are coupled by co-channel interference, thus maintaining individual QoS requirements requires the joint optimization of beamforming and power control. In this paper we study the problem in the absence of noise. The achievable signal-to-interference-ratios (SIR) of all user are limited by the amount of mutual cross-talk. The design question: at which level can the relative SIR levels be balanced under the given channel condition, and what are the optimal beamforming weights? Our approach is analytic. By exploiting the duality between uplink and downlink beamforming, we derive an iterative optimization scheme that is globally convergent and always finds the optimally balanced level. This provides a necessary and sufficient condition for the feasibility of the given scenario. Hence, the results will prove useful for the design of new beamforming algorithms as well as for new spectrally efficient resource allocation strategies."
      }
    ]
  },
  {
    "tid":"95",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Do You Feel the Lag of Your Hadoop?' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The configuration of a Hadoop cluster is significantly important to its performance, because an improper configuration can greatly deteriorate the job execution performance. Unfortunately, systematic guidelines on how to configure a Hadoop cluster are still missing. In this paper, we undertake an empirical study on key operations and mechanisms of Hadoop job execution, including the task assignment strategy and speculative execution. Based on the experiments, we provide suggestions on the system configuration, particularly on the matching between the hardware resource partitioning scheme and the job splitting granularity."
      }
    ]
  },
  {
    "tid":"96",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'KvmSec: a security extension for Linux kernel virtual machines' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Virtualization is increasingly being used in regular desktop PCs, data centers and server farms. One of the advantages of introducing this additional architectural layer is to increase overall system security. In this paper we propose an architecture (KvmSec) that is an extension to the Linux Kernel Virtual Machine aimed at increasing the security of guest virtual machines. KvmSec can protect guest virtual machines against attacks such as viruses and kernel rootkits. KvmSec enjoys the following features: it is transparent to guest machines; it is hard to access even from a compromised virtual machine; it can collect data, analyze them, and act consequently on guest machines; it can provide secure communication between each of the guests and the host; and, it can be deployed on Linux hosts and at present supports Linux guest machines. These features are leveraged to implement a real-time monitoring and security management system. Further, differences and advantages over previous solutions are highlighted, as well as a concrete roadmap for further development."
      }
    ]
  },
  {
    "tid":"97",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'DialogWAE: Multimodal Response Generation with Conditional Wasserstein Auto-Encoder.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Variational autoencoders (VAEs) have shown a promise in data-driven conversation modeling. However, most VAE conversation models match the approximate posterior distribution over the latent variables to a simple prior such as standard normal distribution, thereby restricting the generated responses to a relatively simple (e.g., single-modal) scope. In this paper, we propose DialogWAE, a conditional Wasserstein autoencoder (WAE) specially designed for dialogue modeling. Unlike VAEs that impose a simple distribution over the latent variables, DialogWAE models the distribution of data by training a GAN within the latent variable space. Specifically, our model samples from the prior and posterior distributions over the latent variables by transforming context-dependent random noise using neural networks and minimizes the Wasserstein distance between the two distributions. We further develop a Gaussian mixture prior network to enrich the latent space. Experiments on two widely-used datasets show that DialogWAE outperforms the state-of-the-art approaches in generating more coherent, informative and diverse responses."
      }
    ]
  },
  {
    "tid":"98",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Edge caching with mobility prediction in virtualized LTE mobile networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mobile Edge Computing enables the deployment of services, applications, content storage and processing in close proximity to mobile end users. This highly distributed computing environment can be used to provide ultra-low latency, precise positional awareness and agile applications, which could significantly improve user experience. In order to achieve this, it is necessary to consider next-generation paradigms such as Information-Centric Networking and Cloud Computing, integrated with the upcoming 5th Generation networking access. A cohesive end-to-end architecture is proposed, fully exploiting Information-Centric Networking together with the Mobile Follow-Me Cloud approach, for enhancing the migration of content-caches located at the edge of cloudified mobile networks. The chosen content-relocation algorithm attains content-availability improvements of up to 500% when a mobile user performs a request and compared against other existing solutions. The performed evaluation considers a realistic core-network, with functional and non-functional measurements, including the deployment of the entire system, computation and allocation/migration of resources. The achieved results reveal that the proposed architecture is beneficial not only from the users\u2019 perspective but also from the providers point-of-view, which may be able to optimize their resources and reach significant bandwidth savings."
      }
    ]
  },
  {
    "tid":"99",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'AXIS: Generating Explanations at Scale with Learnersourcing and Machine Learning.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"While explanations may help people learn by providing information about why an answer is correct, many problems on online platforms lack high-quality explanations. This paper presents AXIS (Adaptive eXplanation Improvement System), a system for obtaining explanations. AXIS asks learners to generate, revise, and evaluate explanations as they solve a problem, and then uses machine learning to dynamically determine which explanation to present to a future learner, based on previous learners' collective input. Results from a case study deployment and a randomized experiment demonstrate that AXIS elicits and identifies explanations that learners find helpful. Providing explanations from AXIS also objectively enhanced learning, when compared to the default practice where learners solved problems and received answers without explanations. The rated quality and learning benefit of AXIS explanations did not differ from explanations generated by an experienced instructor."
      }
    ]
  },
  {
    "tid":"100",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Two Modified L2s Interface Methods For Mixed Modulation Scheme' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In current wireless system performance evaluation, separate link and system level simulators are needed due to less computation complexity. The link-to-system (L2S) interface method to interconnect two simulators is essential and need careful definition. Exponential effective signal-to-interference-and-noise ratio mapping (EESM) is a typical L2S interface method suitable for evaluation of orthogonal frequency division multiplexing systems. When it's used in simulation, all the sub. carriers for one user have to use the same modulation and coding scheme. In this paper, two modified L2S interface EESM methods are presented based on the idea that a high-level modulation symbol could be equivalently converted to several low-level modulation symbols with different reliability. As a result, usage of EESM can be extended to the situation that sub-carriers for one user use different modulation schemes."
      }
    ]
  },
  {
    "tid":"101",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'CHIP: Collaborative Host Identity Protocol with Efficient Key Establishment for Constrained Devices in Internet of Things.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The Internet of Things (IoT) is the next evolutionary paradigm of networking technologies that interconnects almost all the smart objects and intelligent sensors related to human activities, machineries, and environment. IoT technologies and Internet Protocol connectivity enable wide ranges of network devices to communicate irrespective of their resource capabilities and local networks. In order to provide seamless connectivity and interoperability, it is notable to maintain secure end-to-end (E2E) communication links in IoT. However, device constraints and the dynamic link creations make it challenging to use pre-shared keys for every secure E2E communication scenario in IoT. Variants of Host Identity Protocol (HIP) are adopted for constructing dynamic and secure E2E connections among the heterogeneous network devices with imbalanced resource profiles and less or no previous knowledge about each other. We propose a solution called collaborative HIP (CHIP) with an efficient key establishment component for the high resource-constrained devices in IoT. CHIP delegates the expensive cryptographic operations to the resource rich devices in the local networks. Finally, by providing quantitative performance evaluation and descriptive security analysis, we demonstrate the applicability of the key establishment in CHIP for the constrained IoT devices rather than the existing HIP variants."
      }
    ]
  },
  {
    "tid":"102",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Empirical study on overlapping community detection in question and answer sites' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In many social networks, people interact based on their interests. Community detection algorithms are then useful to reveal the sub-structures of a network and help us find interest groups. Identifying these social communities can bring benefit to understanding and predicting users behaviors. However, for some kind of online community sites such as question-and-answer (Q&A) sites or forums, there is no friendship based social network structure, which means people are not aware who they are in contact with. Therefore, many traditional community detection techniques do not apply directly. In this paper, we propose an empirical approach for extracting data from Q&A sites suitable to apply community detection methods. Then we compare three kinds of community detection methods we applied on a dataset extracted from the popular Q&A site StackOverflow. We analyze and comment the results of each method.\n\n"
      }
    ]
  },
  {
    "tid":"103",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On optimum selection relaying protocols in cooperative wireless networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this letter, the outage probabilities of selection relaying protocols are analyzed and compared for cooperative wireless networks. It is assumed that both source and relay use equal allocated time in transmission. Depending on the quality of the source-relay channel, the relay may choose either Decode-and-Forward (DF), Amplify-and-Forward (AF), or Direct-Transmission (DT) to forward signals. It turns out that in terms of outage probability, two selection relaying schemes are better than others: selecting between DF and AF protocols (DF-AF) or selecting between DF and DT protocols (DF-DT). It is shown that with an equal power allocation, both of the DF-AF and DF-DT selection relaying protocols have the same asymptotic outage probability. However, with an optimum power allocation strategy, the DF-AF selection scheme is in general better than the DF-DT selection scheme. Note that the optimum power allocations depend on channel variances, not on instantaneous channel gains. When the quality of the relay-destination link is much better than that of the source-relay link, observed from simulation, the outage probability of the DF-AF selection protocol with its optimum power allocation is 1.5 dB better than that of the DF-DT selection with its own optimum power allocation. Extensive simulations are presented to validate the analytical results."
      }
    ]
  },
  {
    "tid":"104",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Utility of AdaBoost to Detect Sleep Apnea-Hypopnea Syndrome From Single-Channel Airflow.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Goal: The purpose of this study is to evaluate the usefulness of the boosting algorithm AdaBoost (AB) in the context of the sleep apnea-hypopnea syndrome (SAHS) diagnosis. Methods: We characterize SAHS in single-channel airflow (AF) signals from 317 subjects by the extraction of spectral and nonlinear features. Relevancy and redundancy analyses are conducted through the fast correlation-based filt..."
      }
    ]
  },
  {
    "tid":"105",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Predicate-tree based pretty good privacy of data' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Growth of Internet has led to exponential rise in data communication over the World Wide Web. Several applications and entities such as online banking transactions, stock trading, e-commerce Web sites, etc. are at a constant risk of eavesdropping and hacking. Hence, security of data is of prime concern. Recently, vertical data have gained lot of focus because of their significant performance benefits over horizontal data in various data mining applications. In our current work, we propose a Predicate-Tree based solution for protection of data. Predicate-Trees or pTrees are compressed, data-mining-ready, vertical data structures and have been used in a plethora of data-mining research areas such as spatial association rule mining, text clustering, closed k-nearest neighbor classification, etc. We show how for data mining purposes, the scrambled pTrees would be unrevealing of the raw data to anyone except for the authorized person issuing a data mining request. In addition, we propose several techniques which come along as a benefit of using vertical pTrees. To the best of our knowledge, our approach is novel and provides sufficient speed and protection level for an effective data security."
      }
    ]
  },
  {
    "tid":"106",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Accurate buffer modeling with slew propagation in subthreshold circuits' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents an accurate and fully analytical model of delay in subthreshold inverters. The model characterizes the direct connection between the input slew and output delay. It is also capable of predicting the signal slew at the inverter output. Delay and slew prediction models are used to compute delay and understand slew propagation in an inverter chain. These models can also provide insight into skew analysis of subthreshold clock networks. Variability analyses across power supply and threshold changes ensure the accuracy of the model."
      }
    ]
  },
  {
    "tid":"107",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'App2net: A platform to transfer and configure applications on programmable virtual networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In programmable virtual networks, simple tasks, like installing software, can be extremely complex. This complexity occurs mainly because the code transfer and initial functional settings in network execution environments are not automated. In addition, the same tasks have different requirements in each service lifecycle stage. In this sense, we propose the App2net platform for enabling the transfer and configuration of network applications in programmable virtual networks that use heterogeneous execution environments. We also propose a taxonomy for grouping code transfer techniques and, based on such techniques, we develop models for code transfer. A prototype has been implemented and tested on realistic network topologies commonly found on the Internet. Results allow us to identify which models improve code transfer consuming fewer resources, regarding service lifecycle stages and network topologies."
      }
    ]
  },
  {
    "tid":"108",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Toward Human-Like Robot Learning.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present an implemented robotic system that learns elements of its semantic and episodic memory through language interaction with its human users. This human-like learning can happen because the robot can extract, represent and reason over the meaning of the user's natural language utterances. The application domain is collaborative assembly of flatpack furniture. This work facilitates a bi-directional grounding of implicit robotic skills in explicit ontological and episodic knowledge and of ontological symbols in the real-world actions by the robot. In so doing, this work provides an example of successful integration of robotic and cognitive architectures."
      }
    ]
  },
  {
    "tid":"109",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'RSPIRIT: Robust self-consistent parallel imaging reconstruction based on generalized Lasso' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a novel approach called robust iterative self-consistent parallel imaging reconstruction (RSPIRiT) in parallel magnetic resonance imaging (pMRI). Different from the smooth Tikhonov regularization used in SPIRiT, our model utilizes generalized Lasso to fix calibration errors in the reconstruction process. It results in a non-smooth optimization problem, which we introduce a new primal-dual pMRI algorithm to solve. We conduct extensive experiments to demonstrate the effectiveness of our approach, compared to state-of-the-art methods."
      }
    ]
  },
  {
    "tid":"110",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Similarity join size estimation using locality sensitive hashing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Similarity joins are important operations with a broad range of applications. In this paper, we study the problem of vector similarity join size estimation (VSJ). It is a generalization of the previously studied set similarity join size estimation (SSJ) problem and can handle more interesting cases such as TF-IDF vectors. One of the key challenges in similarity join size estimation is that the join size can change dramatically depending on the input similarity threshold. We propose a sampling based algorithm that uses Locality-Sensitive-Hashing (LSH). The proposed algorithm LSH-SS uses an LSH index to enable effective sampling even at high thresholds. We compare the proposed technique with random sampling and the state-of-the-art technique for SSJ (adapted to VSJ) and demonstrate LSH-SS offers more accurate estimates throughout the similarity threshold range and small variance using real-world data sets."
      }
    ]
  },
  {
    "tid":"111",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Slope propagation in static timing analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Static timing analysis has traditionally used the PERT method for identifying the critical path of a digital circuit. Due to the influence of the slope of a signal at a particular node on the subsequent path delay, an earlier signal with a signal slope greater than the slope of the later signal may result in a greater delay. Therefore, the traditional method for timing analysis may identify the incorrect critical path and report an optimistic delay for the circuit. We show that the circuit delay calculated using the traditional method is a discontinuous function with respect to transistor and gate sizes, posing a severe problem for circuit optimization methods. We propose a new timing analysis algorithm which resolves both these issues. The proposed algorithm selectively propagates multiple signals through each timing edge in cases where there exists ambiguity regarding which arriving signal represents the critical path. The algorithm for propagating the corresponding required times is also presented. We prove that the proposed algorithm identifies a circuit's true critical path, where the traditional timing analysis method may not. We also show that under this method circuit delay and node slack are continuous functions with respect to a circuit's transistor and gate sizes. In addition, we present a heuristic method which reduces the number of signals to be propagated at the expense of a slight loss in accuracy. Finally, we show how the proposed algorithm was efficiently implemented in an industrial static timing analysis and optimization tool, and present results for a number of industrial circuits. Our results show that the traditional timing analysis method underestimates the circuit delay by as much as 38%, while that the proposed method efficiently finds the correct circuit delay with only a slight increase in run time."
      }
    ]
  },
  {
    "tid":"112",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Evidence management for compliance of critical systems with safety standards: A survey on the state of practice.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Context: Demonstrating compliance of critical systems with safety standards involves providing convincing evidence that the requirements of a standard are adequately met. For large systems, practitioners need to be able to effectively collect, structure, and assess substantial quantities of evidence.Objective: This paper aims to provide insights into how practitioners deal with safety evidence management for critical computer-based systems. The information currently available about how this activity is performed in the industry is very limited.Method: We conducted a survey to determine practitioners' perspectives and practices on safety evidence management. A total of 52 practitioners from 15 countries and 11 application domains responded to the survey. The respondents indicated the types of information used as safety evidence, how evidence is structured and assessed, how evidence evolution is addressed, and what challenges are faced in relation to provision of safety evidence.Results: Our results indicate that (1) V&V artefacts, requirements specifications, and design specifications are the most frequently used safety evidence types, (2) evidence completeness checking and impact analysis are mostly performed manually at the moment, (3) text-based techniques are used more frequently than graphical notations for evidence structuring, (4) checklists and expert judgement are frequently used for evidence assessment, and (5) significant research effort has been spent on techniques that have seen little adoption in the industry. The main contributions of the survey are to provide an overall and upto-date understanding of how the industry addresses safety evidence management, and to identify gaps in the state of the art.Conclusion: We conclude that (1) V82V plays a major role in safety assurance, (2) the industry will clearly benefit from more tool support for collecting and manipulating safety evidence, and (3) future research on safety evidence management needs to place more emphasis on industrial applications. (C) 2014 Elsevier B.V. All rights reserved."
      }
    ]
  },
  {
    "tid":"113",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient synchronization methods for LET-based applications on a Multi-Processor System on Chip.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Distributed control applications cover a wide range of areas such as automotive, avionics, and automation. The Logical Execution Time (LET) Model of Computation (MoC) was proposed as a formal method to describe the functional and timing behavior of such applications. However, modern Multi-Processor Systems on Chip (MPSOC) do not have a shared notion of time between processors, due to their use of Globally Asynchronous Locally Synchronous (GALS) architecture. In this paper we propose two methods (based on FIFO channels and barriers) to implement time and data synchronization on a MPSOC. While a barrier synchronizes the execution flows of tasks at predefined points in their executions, a FIFO is an asynchronous data communication method between two tasks. First, they are used to implement LET applications. Next, we show how dataflow applications and mixed LET-dataflow applications are supported too. We implemented both methods on a MPSOC prototyped on a FPGA, and show that the data synchronization outperforms the related work by 67% in terms of software overhead."
      }
    ]
  },
  {
    "tid":"114",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Vessel bifurcation localization based on intraoperative three-dimensional ultrasound and catheter path for image-guided catheter intervention of oral cancers.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present a method to localize intraoperative target vessel bifurcations under bones for ultrasound (US) image-guided catheter interventions. A catheter path is recorded to acquire skeletons for the target vessel bifurcations that cannot be imaged by intraoperative US. The catheter path is combined with the centerlines of the three-dimensional (3D) US image to construct a preliminary skeleton. Based on the preliminary skeleton, the orientations of target vessels are determined by registration with the preoperative image and the bifurcations were localized by computing the vessel length. An accurate intraoperative vessel skeleton is obtained for correcting the preoperative image to compensate for vessel deformation. A reality check of the proposed method was performed in a phantom experiment. Reasonable results were obtained. The in vivo experiment verified the clinical workflow of the proposed method in an in vivo environment. The accuracy of the centerline length of the vessel for localizing the target artery bifurcation was 2.4mm. These results suggest that the proposed method can allow the catheter tip to stop at the target artery bifurcations and enter into the target arteries. This method can be applied for virtual reality-enhanced image-guided catheter intervention of oral cancers."
      }
    ]
  },
  {
    "tid":"115",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A unified linear MSE minimization MIMO beamforming design based on quadratic matrix programming' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we investigate a unified linear transceiver design with mean-square-error (MSE) as the objective function for a wide range of wireless systems. The unified design is based on an elegant mathematical programming technology namely quadratic matrix programming (QMP). It is revealed that for different wireless systems such as multi-cell coordination systems, multi-user MIMO systems, MIMO cognitive radio systems, amplify-and-forward MIMO relaying systems, the MSE minimization beamforming design problems can always be solved by solving a number of QMP problems. A comprehensive framework on how to solve QMP problems is also given."
      }
    ]
  },
  {
    "tid":"116",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Modeling information diffusion and community membership using stochastic optimization' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Communities are vehicles for efficiently disseminating news, rumors, and opinions in human social networks. Modeling information diffusion through a network can enable us to reach a superior functional understanding of the effect of network structures such as communities on information propagation. The intrinsic assumption is that form follows function---rational actors exercise social choice mechanisms to join communities that best serve their information needs. Particle Swarm Optimization (PSO) was originally designed to simulate aggregate social behavior; our proposed diffusion model, PSODM (Particle Swarm Optimization Diffusion Model) models information flow in a network by creating particle swarms for local network neighborhoods that optimize a continuous version of Holland's hyperplane-defined objective functions. In this paper, we show how our approach differs from prior modeling work in the area and demonstrate that it outperforms existing model-based community detection methods on several social network datasets."
      }
    ]
  },
  {
    "tid":"117",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Deriving User Preferences of Mobile Apps from Their Management Activities' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"App marketplaces host millions of mobile apps that are downloaded billions of times. Investigating how people manage mobile apps in their everyday lives creates a unique opportunity to understand the behavior and preferences of mobile device users, infer the quality of apps, and improve user experience. Existing literature provides very limited knowledge about app management activities, due to the lack of app usage data at scale. This article takes the initiative to analyze a very large app management log collected through a leading Android app marketplace. The dataset covers 5 months of detailed downloading, updating, and uninstallation activities, which involve 17 million anonymized users and 1 million apps. We present a surprising finding that the metrics commonly used to rank apps in app stores do not truly reflect the users\u2019 real attitudes. We then identify behavioral patterns from the app management activities that more accurately indicate user preferences of an app even when no explicit rating is available. A systematic statistical analysis is designed to evaluate machine learning models that are trained to predict user preferences using these behavioral patterns, which features an inverse probability weighting method to correct the selection biases in the training process."
      }
    ]
  },
  {
    "tid":"118",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A new hybrid method based on fuzzy-artificial immune system and k-nn algorithm for breast cancer diagnosis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The use of machine learning tools in medical diagnosis is increasing gradually. This is mainly because the effectiveness of classification and recognition systems has improved in a great deal to help medical experts in diagnosing diseases. Such a disease is breast cancer, which is a very common type of cancer among woman. As the incidence of this disease has increased significantly in the recent years, machine learning applications to this problem have also took a great attention as well as medical consideration. This study aims at diagnosing breast cancer with a new hybrid machine learning method. By hybridizing a fuzzy-artificial immune system with k-nearest neighbour algorithm, a method was obtained to solve this diagnosis problem via classifying Wisconsin Breast Cancer Dataset (WBCD). This data set is a very commonly used data set in the literature relating the use of classification systems for breast cancer diagnosis and it was used in this study to compare the classification performance of our proposed method with regard to other studies. We obtained a classification accuracy of 99.14%, which is the highest one reached so far. The classification accuracy was obtained via 10-fold cross validation. This result is for WBCD but it states that this method can be used confidently for other breast cancer diagnosis problems, too."
      }
    ]
  },
  {
    "tid":"119",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An improved model predictive control for uncertain systems with input saturation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this work, a new design method of model predictive control (MPC) is proposed for uncertain systems with input constraints. By using a new method to deal with actuator constraints, our method can reduce the conservativeness. For the design of the robust MPC controllers, a sequence of feedback control laws is used and a parameter-dependent Lyapunov function is chosen to further reduce the conservativeness. The effectiveness and performance of our MPC design method are demonstrated by an example."
      }
    ]
  },
  {
    "tid":"120",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Genetically optimizing query expansion for retrieving activities from the web' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Activities, defined as actions someone can accomplish or things one can create, are located all around the web contained in various How-To and DIY sites. In this paper we are presenting our research on a system focused on retrieving information about activities related to a simple query which is given as input. An overview of the system's design which is based on semantic query expansion is given along with detailed explanation of the optimization of the system's parameters through the use of genetic algorithms. Finally experimental results gathered from both an offline and a limited online evaluation are presented in order to test the efficiency of the system."
      }
    ]
  },
  {
    "tid":"121",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Deformable physiological atlas-based programming of deep brain stimulators: a feasibility study' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The postoperative neurological management of patients with deep brain stimulation (DBS) of the subthalamic nucleus (STN) for Parkinson's disease is a complex and dynamic process that involves optimizing the stimulation parameters and decreasing the anti-parkinsonian medication while assessing the interactions of both treatment modalities. Neurologists who manage patients undergoing DBS therapy must have expert knowledge of the electro-anatomy of the subthalamic area and be familiar with the medical treatment of motor and non-motor symptoms. In clinical practice, finding the optimal programming parameters can be a challenging and time-consuming process. We have developed a computerized system to facilitate one of the bottlenecks of DBS therapy: the IPG (Internal Pulse Generator) programming. This system consists of a deformable physiological atlas built on more than 300 intra-operative macro-stimulations acquired from 30 Parkinson's patients and of a non-rigid registration algorithm used to map these data into an atlas. By correlating the position of the quadripolar electrode implanted in the patient with the information contained in our atlas, we can determine which of four contacts has the highest probability to be the most clinically effective. Preliminary results presented in this study suggest that this approach facilitates the programming process by guiding the neurologist to the optimal contact. The system we propose was tested retrospectively on a total of 30 electrodes. In 19 of these cases, this system predicted the contact that was selected as the optimal one by the neurologist."
      }
    ]
  },
  {
    "tid":"122",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Staying safe and visible via message sharing in online social networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"an imperative channel for fast information propagation, online social networks (OSNs) also have their defects. One of them is the information leakage, i.e., information could be spread via OSNs to the users whom we are not willing to share with. Thus the problem of constructing a circle of trust to share information with as many friends as possible without further spreading it to unwanted targets has become a challenging research topic but still remained open. Our work is the first attempt to study the Maximum Circle of Trust problem seeking to share the information with the maximum expected number of poster's friends such that the information spread to the unwanted targets is brought to its knees. First, we consider a special and more practical case with the two-hop information propagation and a single unwanted target. In this case, we show that this problem is NP-hard, which denies the existence of an exact polynomial-time algorithm. We thus propose a Fully Polynomial-Time Approximation Scheme (FPTAS), which can not only adjust any allowable performance error bound but also run in polynomial time with both the input size and allowed error. FPTAS is the best approximation solution one can ever wish for an NP-hard problem. We next consider the number of unwanted targets is bounded and prove that there does not exist an FPTAS in this case. Instead, we design a Polynomial-Time Approximation Scheme (PTAS) in which the allowable error can also be controlled. When the number of unwanted targets are not bounded, we provide a randomized algorithm, along with the analytical theoretical bound and inapproximaibility result. Finally, we consider a general case with many hops information propagation and further show its #P-hardness and propose an effective Iterative Circle of Trust Detection (ICTD) algorithm based on a novel greedy function. An extensive experiment on various real-world OSNs has validated the effectiveness of our proposed approximation and ICTD algorithms. Such an extensive experiment also highlights several important observations on information leakage which help to sharpen the security of OSNs in the future."
      }
    ]
  },
  {
    "tid":"123",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'New Beamforming Techniques Based on Virtual SINR Maximization for Coordinated Multi-Cell Transmission.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose new beamforming techniques based on virtual signal-to-interference-plus-noise ratio (VSINR) for weighted sum-rate (WSR) maximization in coordinated multi-cell transmission. In earlier works based on the VSINR maximization, the parameters which control the interference power and the noise variance were set to fixed values regardless of channel realizations and the signal-t..."
      }
    ]
  },
  {
    "tid":"124",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A cost-sensitive technique for positive-example learning supporting content-based product recommendations in B-to-C e-commerce' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Existing supervised learning techniques are able to support product recommendations in business-to-consumer e-commerce but become ineffective in scenarios characterized by single-class learning, such as a training sample that consists of some examples pertaining to only one outcome class (positive or negative). To address such challenges, we develop a COst-sensitive Learning-based Positive Example Learning (COLPEL) technique, which constructs an automated classifier from a training sample comprised of positive examples and a much larger number of unlabeled examples. The proposed technique incorporates cost-proportionate rejection sampling to derive, from unlabeled examples, a subset that is likely to feature negative examples in the training sample. Our technique follows a committee machine approach and thereby constructs a set of classifiers that make joint product recommendations while mitigating the potential biases common to the use of a single classifier. We evaluate the proposed method with customers' book ratings collected from Amazon.com and include two prevalent techniques for benchmark purposes; namely, positive naive Bayes and positive example-based learning. According to our results, the proposed COLPEL technique outperforms both benchmarks, as measured by accuracy and positive and negative F1 scores."
      }
    ]
  },
  {
    "tid":"125",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Style-content separation by anisotropic part scales' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We perform co-analysis of a set of man-made 3D objects to allow the creation of novel instances derived from the set. We analyze the objects at the part level and treat the anisotropic part scales as a shape style. The co-analysis then allows style transfer to synthesize new objects. The key to co-analysis is part correspondence, where a major challenge is the handling of large style variations and diverse geometric content in the shape set. We propose style-content separation as a means to address this challenge. Specifically, we define a correspondence-free style signature for style clustering. We show that confining analysis to within a style cluster facilitates tasks such as co-segmentation, content classification, and deformation-driven part correspondence. With part correspondence between each pair of shapes in the set, style transfer can be easily performed. We demonstrate our analysis and synthesis results on several sets of man-made objects with style and content variations."
      }
    ]
  },
  {
    "tid":"126",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Ici Suppression In The Sc-Fdma Communication System With Phase Noise' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The SC-FDMA (single carrier-frequency division multiple access) is uplink standard of the 3GPP LTE (3rd generation partnership project long term evolution) mobile system. SC-FDMA has very low PAPR (peak to average power ratio) but is sensitive to the ICI (inter carrier interference) by phase noise. In this paper, we analyze the effect of phase noise considering the back-off amount of HPA (high power amplifier). And we propose the equalizer of advanced PNS (phase noise suppressing) algorithm to remove the ICI component effectively. The adaptive equalizer has similar form of SD-FDE (single carrier-frequency domain equalization) and the operation process based on PNS algorithm to remove the ICI component even in the IQ imbalance environment, which will be shown in the simulation results."
      }
    ]
  },
  {
    "tid":"127",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Effective and Efficient Memory Protection Using Dynamic Tainting' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Programs written in languages allowing direct access to memory through pointers often contain memory-related faults, which cause nondeterministic failures and security vulnerabilities. We present a new dynamic tainting technique to detect illegal memory accesses. When memory is allocated, at runtime, we taint both the memory and the corresponding pointer using the same taint mark. Taint marks are then propagated and checked every time a memory address m is accessed through a pointer p; if the associated taint marks differ, an illegal access is reported. To allow always-on checking using a low overhead, hardware-assisted implementation, we make several key technical decisions. We use a configurable, low number of reusable taint marks instead of a unique mark for each allocated area of memory, reducing the performance overhead without losing the ability to target most memory-related faults. We also define the technique at the binary level, which helps handle applications using third-party libraries whose source code is unavailable. We created a software-only prototype of our technique and simulated a hardware-assisted implementation. Our results show that 1) it identifies a large class of memory-related faults, even when using only two unique taint marks, and 2) a hardware-assisted implementation can achieve performance overheads in single-digit percentages."
      }
    ]
  },
  {
    "tid":"128",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Model Checking Bluespec Specified Hardware Designs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Using RTL (Register Transfer Level) models for the verification of complex hardware designs involves reducing the state space of designs using various abstraction techniques. In this paper, we propose faster and earlier verification of hardware designs at a level of abstraction above RTL. We consider a high-level (above RTL) hardware model that uses atomic rules to describe the behavior of a design, which can then be synthesized to RTL code. Bluespec System Verilog (BSV) is an example of such a high-level specification language. We propose a methodology for verification of BSV models using Spin, which is a Model Checking tool. Verification of high-level BSV models may avoid the need for using abstraction techniques since such models already ignore various low-level details that are irrelevant for verifying a design's behavioral properties. Moreover, using our proposed methodology different behaviors of BSV models can be efficiently verified at high-level aiding in faster verification."
      }
    ]
  },
  {
    "tid":"129",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Deep Structures of Collaboration: Physiological Correlates of Collective Intelligence and Group Satisfaction.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Collective intelligence (CI), a group's capacity to perform a wide variety of tasks, is a key factor in successful collaboration. Group composition, particularly diversity and member social perceptiveness, are consistent predictors of CI, but we have limited knowledge about the mechanisms underlying their effects. To address this gap, we examine how physiological synchrony, as an indicator of coordination and rapport, relates to CI in computer-mediated teams, and if synchrony might serve as a mechanism explaining the effect of group composition on CI. We present results from a laboratory experiment where 60 dyads completed the Test of Collective Intelligence (TCI) together online and rated their group satisfaction, while wearing physiological sensors. We find that synchrony in facial expressions (indicative of shared experience) was associated with CI and synchrony in electrodermal activity (indicative of shared arousal) with group satisfaction. Furthermore, various forms of synchrony mediated the effect of member diversity and social perceptiveness on CI and group satisfaction. Our results have important implications for online collaborations and distributed teams.\n\n"
      }
    ]
  },
  {
    "tid":"130",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Comparison Framework for MSSs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Satellite networks can provide the possibility to access communication services on wide areas of the earth; this characteristic coupled with the support of user mobility allows satellite networks having unique features well suited for different scenarios, like: land mobile, aeronautical, maritime, transports, rescue and disaster relief. Mobile satellite networks, however, entail crucial technological challenges, such as: link budget, antenna technologies for both the satellite and the mobile terminal, and efficient utilization of the radio spectrum. This paper investigates these aspects and provides an analytical framework that can be used to evaluate the characteristics of satellite networks in the design phase as well as for their operational phase. In particular, quantitative comparisons are presented here for different systems in terms of supported user density, bandwidth efficiency, and mobility aspects."
      }
    ]
  },
  {
    "tid":"131",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'D2: Anomaly Detection and Diagnosis in Networked Embedded Systems by Program Profiling and Symptom Mining' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Detecting and diagnosing anomalies in networked embedded systems like sensor networks is a very difficult task, due to the variable workloads and severe resource constraints. We notice that most node-level debugging tools can provide detailed program information inside the node but fail to detect when and where a problem occurs in the network. On the other hand, most network-level diagnosis tools can effectively detect a problem from the network but fail to narrow down the problem within the node because they lack detailed program information. To close the gap, we propose D2, a new anomaly detection and diagnosis method by combining program profiling and symptom mining. D2 employs binary instrumentation to perform lightweight function count profiling. Based on the statistics, D2 uses PCA (Principal Component Analysis) based approach for automatically detecting network anomalies. Compared to previous methods, D2 is able to point programmers closer to the most likely causes by a novel approach combining statistical tests and program call graph analysis. We implement our method based on TinyOS 2.1.1 and evaluate its effectiveness by case studies in the development of a working sensor network. Results show that our method is effective for detecting and diagnosing problems in real-world sensor network systems, and at the same time, incurs an acceptable overhead."
      }
    ]
  },
  {
    "tid":"132",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A real-time publish/subscribe driver alert system for accident avoidance due to red light running' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Traffic signals provide the key mechanism to regulate traffic, and promote safe and efficient traffic flow at intersections. However, incidents of red light running can compromise the safety of the system. These incidents stem mostly due to the well known dilemma zone problem where the drivers cannot decide whether slowing down the vehicle would bring their vehicle to a stop or should they increase the speed and cross the intersection. Other factors, such as wet road conditions, make the problem worse."
      }
    ]
  },
  {
    "tid":"133",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Enhancing Video Delivery in the LTE Wireless Access Using Cross-Layer Mechanisms.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The current evolution of the global Internet data traffic shows an increasing demand of video transmissions, which potentially leads to the saturation of mobile networks. To cope with this issue, this paper describes techniques to handle the video traffic load in the last hop, of the communication network, i.e., the wireless access. The general idea is to benefit from a cross-layer architecture for efficient video transport, where multiple wireless access technologies, represented by Wi-Fi and next generation cellular technologies (4G and beyond), interact with the upper layers through an abstract interface. This architecture enables the introduction of enhancements in the LTE-A wireless access: evolved Multimedia Broadcast and Multicast Services (eMBMS) extended with dynamic groupcast communications, video relay at the Packet Data Convergence Protocol (PDCP) level and a smart video frame dropping mechanism to provide mobile users with a satisfactory level of Quality of Experience (QoE). These video-aware mechanisms leverage the abstract interface and allow mobile operators to fine-tune their networks while coping with the upcoming mobile video traffic increase."
      }
    ]
  },
  {
    "tid":"134",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Kernel-based reinforcement learning for traffic signal control with adaptive feature selection' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Reinforcement learning in a large-scale system is computationally challenging due to the curse of the dimensionality. One approach is to approximate the Q-function as a function of a state-action related feature vector, then learn the parameters instead. Although assumptions from the priori knowledge can potentially explore an appropriate feature vector, selecting a biased one that insufficiently represents the system usually leads to the poor learning performance. To avoid this disadvantage, this paper introduces kernel methods to implicitly propose a learnable feature vector instead of a pre-selected one. More specifically, the feature vector is estimated from a reference set which contains all critical state-action pairs observed so far, and it can be updated by either adding a new pair or replace an existing one in the reference set. Thus the approximate Q-function keeps adjusting itself as the knowledge about the system accumulates via observations. Our algorithm is designed in both batch mode and online mode in the context of the traffic signal control. In addition, the convergence of this algorithm is experimentally supported. Furthermore, some regularization methods are proposed to avoid overfitting of Q-function on the noisy observations. Finally, A simulation on the traffic signal control in a single intersection is provided, and the performance of this algorithm is compared with Q-learning, in which the Q-function is numerically estimated for each state-action pair without approximation."
      }
    ]
  },
  {
    "tid":"135",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Sudden Noise Reduction Based On Gmm With Noise Power Estimation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper describes a method for reducing sudden noise using noise detection and classification methods, and noise power estimation. Sudden noise detection and classification have been dealt with in our previous study. In this paper, noise classification is improved to classify more kinds of noises based on k-means clustering, and GMM-based noise reduction is performed using the detection and classification results. As a result of classification, we can determine the kind of noise we arc dealing with, but the power is unknown. in this paper, this problem is solved by combining an estimation of noise power with the noise reduction method. In our experiments, the proposed method achieved good performance for recognition of utterances overlapped by sudden noises."
      }
    ]
  },
  {
    "tid":"136",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Impacts of Task Re-Execution Policy on MapReduce Jobs.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"MapReduce is a popular distributed programming framework for large-scale data processing. To prevent MapReduce jobs from being interrupted by node failures that occur frequently in a MapReduce cluster consisting of a set of commodity machines/nodes, the most well-known MapReduce implementation, i.e. Hadoop, adopts a task re-execution policy (TR policy). When a map/reduce task of a job crashes, the..."
      }
    ]
  },
  {
    "tid":"137",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The implementation of business process reengineering' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"As more organizations undertake business process reengineering (BPR), issues in implementing BPR projects become a major concern. This field research seeks empirically to explore the problems of implementing reengineering projects and how the severity of these problems relates to BPR project success. Based on past theories and research related to the implementation of organizational change as well as field experience of reengineering experts, a comprehensive list of sixty-four BPR implementation problems was identified. The severity of each problem was then rated by those who have participated in reengineering in 105 organizations. Analysis of the results clearly demonstrates the central importance of change management in BPR implementation success. Resolutions of problems in other areas such as technological competence and project planning were also determined to be necessary, but not sufficient, conditions for reengineering success. Further, problems that are more directly related to the conduct of a project such as process delineation, project management, and tactical planning were perceived as less difficult, yet highly related to project success. This situation was also true for human resource problems such as training personnel for the redesigned process. These findings suggest that reengineering project implementation is complex, involving many factors. To succeed, it is essential that change be managed and that balanced attention be paid to all identified factors, including those that are more contextual (e.g., management support and technological competence) as well as factors that pertain directly to the conduct of the project (e.g., project management and process delineation). As one of the first pieces of empirical evidence based on a field study, this research emphasizes the importance of addressing BPR implementation within the broader context of organizational change in a complex sociotechnical environment."
      }
    ]
  },
  {
    "tid":"138",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Learning to Predict Severity of Software Vulnerability Using Only Vulnerability Description' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Software vulnerabilities pose significant security risks to the host computing system. Faced with continuous disclosure of software vulnerabilities, system administrators must prioritize their efforts, triaging the most critical vulnerabilities to address first. Many vulnerability scoring systems have been proposed, but they all require expert knowledge to determine intricate vulnerability metrics. In this paper, we propose a deep learning approach to predict multi-class severity level of software vulnerability using only vulnerability description. Compared with intricate vulnerability metrics, vulnerability description is the \"surface level\" information about how a vulnerability works. To exploit vulnerability description for predicting vulnerability severity, discriminative features of vulnerability description have to be defined. This is a challenging task due to the diversity of software vulnerabilities and the richness of vulnerability descriptions. Instead of relying on manual feature engineering, our approach uses word embeddings and a one-layer shallow Convolutional Neural Network (CNN) to automatically capture discriminative word and sentence features of vulnerability descriptions for predicting vulnerability severity. We exploit large amounts of vulnerability data from the Common Vulnerabilities and Exposures (CVE) database to train and test our approach."
      }
    ]
  },
  {
    "tid":"139",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Graphical views of the behavior of parallel programs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"There is often an increasing gap between a programmer\u2032s mental conception of a problem and the realization of a parallel program which solves that problem. To bridge this gap, information about program execution must be displayed. Since a parallel program may be executed on several processors simultaneously, activities may be distributed across both space and time, and therefore simple sequential displays of a program\u2032s execution are inadequate. Intuitively, pictures are much more meaningful than text, particularly with parallel programs where large quantities of information may be generated. In this paper, we describe how the behavior of parallel programs may be graphically interpreted, and discuss current approaches and difficulties in parallel program visualization. A graphical interface designed for languages based on message-passing is also presented."
      }
    ]
  },
  {
    "tid":"140",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On The Centrality Of Vertices Of Molecular Graphs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"For acyclic systems the center of a graph has been known to be either a single vertex of two adjacent vertices, that is, an edge. It has not been quite clear how to extend the concept of graph center to polycyclic systems. Several approaches to the graph center of molecular graphs of polycyclic graphs have been proposed in the literature. In most cases alternative approaches, however, while being apparently equally plausible, gave the same results for many molecules, but occasionally they differ in their characterization of molecular center. In order to reduce the number of vertices that would qualify as forming the center of the graph, a hierarchy of rules have been considered in the search for graph centers. We reconsidered the problem of the center of a graph by using a novel concept of graph theory, the vertex weights, defined by counting the number of pairs of vertices at the same distance from the vertex considered. This approach gives often the same results for graph centers of acyclic graphs as the standard definition of graph center based on vertex eccentricities. However, in some cases when two nonequivalent vertices have been found as graph center, the novel approach can discriminate between the two. The same approach applies to cyclic graphs without additional rules to locate the vertex or vertices forming the center of polycyclic graphs, vertices referred to as central vertices of a graph. In addition, the novel vertex weights, in the case of acyclic, cyclic, and polycyclic graphs can be interpreted as vertex centralities, a measure for how close or distant vertices are from the center or central vertices of the graph. Besides illustrating the centralities of a number of smaller polycyclic graphs, we also report on several acyclic graphs showing the same centrality values of their vertices. (c) 2013 Wiley Periodicals, Inc."
      }
    ]
  },
  {
    "tid":"141",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Metallic-CNT and Non-uniform CNTs Tolerant Design of CNFET-based Circuits Using Independent N2-Transistor Structures' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Carbon Nanotube Field Effect Transistors (CNFETs), consisting of semi conducting Carbon Nanotubes (CNTs), show great promise as extensions to silicon CMOS. However, imperfections related to CNT growth process result in metallic and non-uniform CNTs leading to CNFET failures. This paper investigates a defect tolerant technique for CNFETs which adds redundancy at the transistor level to provide a built-in immunity to metallic and non-uniform CNTs. The proposed technique is based on replacing each transistor by independent N2 series-parallel/parallel-series transistor structures. The technique is adopted to use single directional independence inherent in CNTs to guarantee immunity to both open and short defects as validated by theoretical analysis and experimental results. As demonstrated by extensive simulation results, the proposed technique achieves 1-part-per-bilion failure rate in a typical CNFET fabrication process."
      }
    ]
  },
  {
    "tid":"142",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'From problems to protocols: Towards a negotiation handbook' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Automated negotiation protocols represent a potentially powerful tool for problem solving in decision support systems involving participants with conflicting interests. However, the effectiveness of negotiation approaches depends greatly on the negotiation problem under consideration. Since there is no one negotiation protocol that clearly outperforms all others in all scenarios, we need to be able to decide which protocol is most suited for each particular problem. The goal of our work is to meet this challenge by defining a ''negotiation handbook'', that is, a collection of design rules which allow us, given a particular negotiation problem, to choose the most appropriate protocol to address it. This paper describes our progress towards this goal, including a tool for generating a wide range of negotiation scenarios, a set of high-level metrics for characterizing how negotiation scenarios differ, a testbed environment for evaluating protocol performance with different scenarios, and a community repository which allows us to systematically record and analyze protocol performance data."
      }
    ]
  },
  {
    "tid":"143",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Improving accuracy of microarray classification by a simple multi-task feature selection filter.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Leveraging information from the publicly accessible data repositories can be very useful when training a classifier from a small-sample microarray data. To achieve this, we proposed a multi-task feature selection filter that borrows strength from auxiliary microarray data. It uses Kruskal-Wallis test on auxiliary data and ranks genes based on their aggregated p-values. The top-ranked genes are selected as features for the target task classifier. The multi-task filter was evaluated on microarray data related to nine different types of cancers. The results showed that the multi-task feature selection is very successful when applied in conjunction with both single-task and multi-task classifiers."
      }
    ]
  },
  {
    "tid":"144",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Differential Distributed Space-Frequency Coding for Broadband Non-Regenerative Wireless Relaying Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Exploiting the desirable properties of cyclic group coding, we develop a full-rate differential distributed space- frequency coding (DSFC) in the frequency domain for broadband non-regenerative wireless relaying systems. The pairwise error probability analysis demonstrates that our scheme guarantees a diversity order of Gd les min{ML1, ML2}, where M is the number of the relay nodes, and L1 and L2 are the number of taps of source-to-relay and relay-to-destination channels, respectively. A novel subcarrier allocation method without channel state information (CSI) at any node is proposed to improve the diversity product. Moreover, for an arbitrary number of relays, a competitive non-regenerative DSFC proposed by Zhang et. aL, is extended to differential transmission incorporated with the proposed subcarrier allocation method for fair comparison. Both theoretical analysis and simulation results confirm that the proposed DSFC outperforms Zhang's scheme significantly in the differential case."
      }
    ]
  },
  {
    "tid":"145",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Dynamic programming prediction errors of recurrent neural fuzzy networks for speech recognition' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes Mandarin phrase recognition using dynamic programming (DP) prediction errors of singleton-type recurrent neural fuzzy networks (SRNFNs). This method is called DP-SRNFN. The recurrent property of SRNFN makes it suitable for processing temporal speech patterns. A Mandarin phrase comprises monosyllabic words. SRNFN training is based on the word unit. There are N\"w SRNFNs for modeling N\"w words, and each SRNFN receives the current frame feature and predicts the next one of its modeling word. In recognizing N\"P phrases, the prediction error of each trained SRNFN is computed, and DP is used to find the optimal path that maps the input frames to the best matched SRNFNs (words) for each of the N\"P phrases. The accumulated error of each phrase model is computed from its optimal path and the one with the minimum error is the recognition result. To verify DP-SRNFN performance, this study conducted experiments on recognizing 30 Mandarin phrases. SRNFN training with noisy features for phrase recognition under different noisy environments was also conducted. DP-SRNFN performance is compared with the hidden Markov models (HMMs). Results show that DP-SRNFN achieves higher recognition rates than HMM in both clean and noisy environments."
      }
    ]
  },
  {
    "tid":"146",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Measuring Forces In Liver Cutting For Reality-Based Haptic Display' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Reality-based modeling of deformable tissues is critical for providing accurate haptic feedback to the surgeon in common surgical tasks such as grasping and cutting organs/tissues. In reality-based modeling, we are interested in modeling tissues as accurately as possible by determining the mechanical properties experimentally and developing a predictive model that is self consistent with the experimentally-determined properties. In this paper, we present the newly developed hardware and software to characterize the mechanical response of pig liver during (ex-vivo) cutting. The macroscopic cutting force-displacement curve shows repeating self-similar units of localized linear loading followed by sudden unloading. The sudden unloading coincides with onset of localized crack growth. This experimental data was used to determine the self-consistent local effective Young's modulus of the specimens to be used in finite element models. Results from plane-stress and plane-strain finite element analyses reveal that the magnitude of the self-consistent local effective Young's modulus varies within close bounds."
      }
    ]
  },
  {
    "tid":"147",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Performance Consequences Of Inconsistent Client-Side Membership, Information In The Open Group Model' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In a distributed fault-tolerant server system realized according to the open group model, inconsistency will (temporarily) arise between the dynamic membership of the replicated service and its client-side representation in the event of server failures and recoveries. The paper proposes techniques for maintaining this consistency and discuss their performance implications in failure/recovery scenarios where clients load balance requests on the servers. Comparative performance measurements is carried out for two of the proposed techniques. The results indicate that the performance impact of lacking consistency is easily kept small, and that the cost of the technique is small."
      }
    ]
  },
  {
    "tid":"148",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Dynamic Access Control and Trust Management for Blockchain-Empowered IoT' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The Internet of Things (IoT), while providing comprehensive interconnection and ubiquitous services, poses security issues by enabling resources sharing among various devices from different untrusted authorities. Blockchain, as a distributed ledger, provides a traceable and verifiable platform to ensure the secure access control in IoT. The existing works based on blockchain may bring up intolerable computing overhead and delay to the lightweight IoT devices. In this article, we propose a dynamic and lightweight attribute-based access control framework for blockchain-empowered IoT, to achieve secure and fine-grained authorization. The proposed scheme allows access to resources by evaluating attributes, operations, and the environment relevant to a request. The access policy is executed through smart contract in blockchain for security and flexibility. To further adapt to IoT device constraints, we design a access control framework based on decentralized application (DApp), which can maintain tamper proof in a timely manner and be adapt to the delay-intolerant application. When delay-intolerant access is required, access can be allowed according to local replica of the blockchain, without a consensus of blockchain network. Considering the time-varying attributes of IoT devices, a trust management scheme is proposed based on the Markov chain to resist the security fluctuation caused by the vulnerability of IoT devices. In the experiments, we deploy our system prototype on Ethereum to evaluate the feasibility and effectiveness of the scheme. The results show the proposed scheme can achieve secure, high throughput, and flexible access control in IoT."
      }
    ]
  },
  {
    "tid":"149",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Flexible, Direct Interactions between CoAP-enabled IoT Devices' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The wireless communication capability of sensors and actuators made them suitable for several automation solutions which involve sensing physical properties and acting upon them. These days, gateway or cloud based sensor/actuator interaction models are widely used. In this model, every sensor/actuator interaction goes through the gateway or via the cloud. In order to realize the true Internet of Things philosophy where everything is interconnected, direct interactions between sensors and actuators, also called bindings, are important. In this paper, we introduce a CoAP based sensor/actuator binding solution where a 3rd party is responsible for setting up the binding, but is not involved in any of the further interactions between the sensor and actuator. As binding creation and execution is fully based on Restful CoAP interactions, very flexible bindings between any two devices can be created. We implemented this solution in Contiki and evaluated the implementation by taking different measures such as delay, memory footprint, and packet size."
      }
    ]
  },
  {
    "tid":"150",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The Equivalence of Semidefinite Relaxation MIMO Detectors for Higher-Order QAM' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In multi-input multi-output (MIMO) detection, semidefinite relaxation (SDR) has been shown to be an efficient high-performance approach. For BPSK and QPSK, it has been found that SDR can provide near-optimal bit error probability performance. This has stimulated a number of recent research endeavors that aim to apply SDR to the high-order QAM cases. These independently developed SDRs are different in concept, structure and complexity, and presently no serious analysis has been given to compare these methods. This paper analyzes the relationship of three such SDR methods, namely the polynomial-inspired SDR (PI-SDR) by Wiesel , the bound-constrained SDR (BC-SDR) by Sidiropoulos and Luo, and the virtually-antipodal SDR (VA-SDR) by Mao Rather unexpectedly, we prove that the three SDRs are equivalent in the following sense: The three SDRs yield the same optimal objective values, and their optimal solutions have strong correspondences. Specifically, we establish this solution equivalence between BC-SDR and VA-SDR for any 4q -QAM constellations, and that between BC-SDR and PI-SDR for 16-QAM and 64-QAM. Moreover, the equivalence result holds for any channel, problem size, and signal-to-noise ratio. Our theoretical findings are confirmed by simulations, where the three SDRs offer identical symbol error probabilities. Additional simulation results are also provided to demonstrate the effectiveness of SDR compared to some other MIMO detectors, in terms of complexity and symbol error performance."
      }
    ]
  },
  {
    "tid":"151",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fine-grained partitioning for aggressive data skipping' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Modern query engines are increasingly being required to process enormous datasets in near real-time. While much can be done to speed up the data access, a promising technique is to reduce the need to access data through data skipping. By maintaining some metadata for each block of tuples, a query may skip a data block if the metadata indicates that the block does not contain relevant data. The effectiveness of data skipping, however, depends on how well the blocking scheme matches the query filters. In this paper, we propose a fine-grained blocking technique that reorganizes the data tuples into blocks with a goal of enabling queries to skip blocks aggressively. We first extract representative filters in a workload as features using frequent itemset mining. Based on these features, each data tuple can be represented as a feature vector. We then formulate the blocking problem as a optimization problem on the feature vectors, called Balanced MaxSkip Partitioning, which we prove is NP-hard. To find an approximate solution efficiently, we adopt the bottom-up clustering framework. We prototyped our blocking techniques on Shark, an open-source data warehouse system. Our experiments on TPC-H and a real-world workload show that our blocking technique leads to 2-5x improvement in query response time over traditional range-based blocking techniques."
      }
    ]
  },
  {
    "tid":"152",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Tour generation for exploration of 3D virtual environments' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Navigation in complex and large-scale 3D virtual environments has been shown to be a difficult task, imposing a high cognitive load on the user. In this paper, we present a comprehensive method for assisting users in exploring and understanding such 3D worlds. The method consists of two distinct phases: an off-line computation step deriving a grand tour using the world geometry and any semantic target information as input, and an on-line interactive navigation step providing guided exploration and improved spatial perception for the user. The former phase is based on a voxelized version of the geometrical dataset that is used to compute a connectivity graph for use in a TSP-like formulation of the problem. The latter phase takes the output tour from the off-line step as input for guiding 3D navigation through the environment."
      }
    ]
  },
  {
    "tid":"153",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Distributed Shared Memory System Based on the Maestro2 High Performance Cluster Network' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cluster computing has become a valid alternative for high performance computing. To fully exploit the computing power of these environments, one must utilize high performance network and protocol technologies, since parallel applications running on clusters require low latency and high throughput. To address this issue, Maestro2 high performance network technology has been developed. Parallel applications running on clusters usually use two different known alternatives to share information; message passing or distributed shared memory. Maestro2 already supports a high performance message passing communication system, MMP. This paper describes a first version of a distributed shared memory system based on Maestro2, and a first set of experimental results is presented."
      }
    ]
  },
  {
    "tid":"154",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'New challenges in CSCL: towards adaptive script support' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Scaffolding learners, i.e. helping learners to attain tasks they could not accomplish without support, entails the notion of fading, i.e. reducing the scaffolding for learners to become more and more self-regulated. Fading implies to tailor support for collaboration, such as collaboration scripts, to the particular needs of the specific collaborators. In computer supported collaborative learning (CSCL) settings, support can be designed in a very restrictive and inflexible fashion; at the same time computerized settings open new possibilities for the realization of adaptive support as they enable automation of analysis and feedback mechanisms. In this symposium we present new technical approaches and latest empirical research on possibilities and limitations of adaptive support for learners in CSCL settings."
      }
    ]
  },
  {
    "tid":"155",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Exit Chart Aided Design Of Ds-Cdma Ultrawideband Systems Using Iterative Decoding' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a novel UltraWideBand (UWB), Direct Sequence Code Division Multiple Access (DS-CDMA) aided system designed for the IEEE 802.15.3a UWB channel specifications. Substantial performance improvements can be attained by serially concatenated channel encoding combined with a Unity Rate Code (URC). We compare the performance of the iterative aided decoding Correlation (Corr) and Minimum Mean Square Error (MMSE) detectors exchanging extrinsic information between the URC's decoder as well as the outer Recursive Systematic Convolutional (RSC) code's decoder. Moreover, the iterative decoding convergence analysis of the proposed system is carried out with the aid of Extrinsic Information Transfer (EXIT) charts. As expected, the iteratively decoded fully-loaded system employing MMSE detection outperforms its counterpart employing the equivalent Corr detection. Explicitly, UWB DS-CDMA using the MMSE detector attains a BER of 10(-5) at E-b/N-o = 2dB when supporting U = 32 users employing i = 12 decoding iterations, while the same system using the Corr detector operates at BER-10(-1)."
      }
    ]
  },
  {
    "tid":"156",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Speed-optimized free-viewpoint rendering based on depth layering' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper free-viewpoint rendering is addressed and a new fast approach for virtual views synthesis from view-plus-depth 3D representation is proposed. Depth layering in disparity domain is employed in order to optimally approximate the scene geometry by a set of constant depth layers. This approximation facilitates the use of connectivity information for segment-based forward warping of the reference layer map, producing a complete virtual view layer map containing no cracks or holes. The warped layer map is used to guide the disocclusions inpainting process of the synthesized texture map. For this purpose, a speed-optimized patch-based inpainting approach is proposed. In contrast to the existing methods, patch similarity function is based on local binary patterns descriptors. Such binary representation allows for efficient processing and comparison of patches, as well as compact storage and reuse of previously calculated binary descriptors. The experimental results demonstrate realtime capability of the proposed method even for CPU-based implementation, while the quality is comparable with other view synthesis approaches."
      }
    ]
  },
  {
    "tid":"157",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Learning high-dimensional Mixture Models for fast collision detection in Rapidly-Exploring Random Trees' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a new approach for fast collision detection in high dimensional configuration spaces for Rapidly-exploring Random Trees (RRT) motion planning. The proposed method is based upon Gaussian Mixture Models (GMM) that are learned using an incremental Expectation Maximization clustering algorithm trained online using exemplars provided by a slow, conventional kinematic-based collision detection routine. The number of collision checks needed can be drastically reduced using a biased random sampling from the learned GMM distribution, and the learned models are continually refined and improved as the RRT planning algorithm proceeds. Our proposed method is demonstrated on several example applications and experimental results show marked improvement in computational efficiency over previous approaches."
      }
    ]
  },
  {
    "tid":"158",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Architecture and Guiding Framework for the Social Enterprise' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With the widespread adoption of social (aka Web 2.0) technologies like social networks, blogs, and wikis, there is a growing interest in looking into how enterprises should capitalize on these technologies. The social enterprise is the one that on top of having an online presence on the Internet, strives to open up new communication channels with stakeholders using social technologies. Building upon previous works on social business processes and on practical experiences on adopting such technologies in different Brazilian companies, this paper suggests an architecture and guiding framework for the social enterprise in terms of how to bridge the gap between the business world and social world and how to integrate social technologies into the enterprise's day-to-day operations."
      }
    ]
  },
  {
    "tid":"159",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Graph Classification Based on Dissimilarity Space Embedding' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recently, an emerging trend of representing objects by graphs can be observed. In fact, graphs offer a powerful alternative to feature vectors in pattern recognition, machine learning, and related fields. However, the domain of graphs contains very little mathematical structure, and consequently, there is only a limited amount of classification algorithms available. In this paper we survey recent work on graph embedding using dissimilarity representations. Once a population of graphs has been mapped to a vector space by means of this embedding procedure, all classification methods developed in statistical pattern recognition become directly available. In an experimental evaluation we show that the proposedmethodology of first embedding graphs in vector spaces and then applying a statistical classifier has significant potential to outperform classifiers that directly operate in the graph domain. Additionally, the proposed framework can be considered a contribution towards unifying the domains of structural and statistical pattern recognition."
      }
    ]
  },
  {
    "tid":"160",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'MIP-Nets: Enabling Information Sharing in Loosely-Coupled Teamwork.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"People collaborate in carrying out such complex activities as treating patients, co-authoring documents and developing software. While technologies such as Dropbox and Github enable groups to work in a distributed manner, coordinating team members' individual activities poses significant challenges. In this paper, we formalize the problem of \"information sharing in loosely-coupled extended-duration teamwork\". We develop a new representation, Mutual Influence Potential Networks (MIP-Nets), to model collaboration patterns and dependencies among activities, and an algorithm, MIP-DOI, that uses this representation to reason about information sharing."
      }
    ]
  },
  {
    "tid":"161",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Adaptive load distribution algorithms for heterogeneous distributed systems with multiple task classes' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A load distribution (LD) algorithm achieves better system performance by smoothing out any workload imbalance that may exist in a distributed system. This is done by relocating application tasks from busy nodes to lightly loaded (or idle) nodes. Previous studies on LD algorithms allow only one single task to be transferred for each sender-receiver negotiation session. While this approach is effective for a homogeneous system, it is too conservative to be applied to a heterogeneous system where nodes may have drastically different processing speeds. In this paper, we propose a class of LD algorithms that allow a batch of tasks to be transferred during each negotiation session. The core of the algorithms is a protocol that ensures a sender-receiver pair to negotiate and arrive at a suitable batch size. The protocol takes into consideration the processing speeds of the sender and receiver, as well as their relative workload, thus ensuring the maximal benefit for each negotiation session. An additional advantage of the algorithms is that a task batch can be composed according to different performance objectives."
      }
    ]
  },
  {
    "tid":"162",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Distributed Degree-Based Link Scheduling for Collision Avoidance in Wireless Sensor Networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Wireless sensor networks (WSNs) consist of multiple sensor nodes, which communicate with each other under the constrained energy resources. Retransmissions caused by collision and interference during the communication among sensor nodes increase overall network delay. Since the network delay increases as the node's waiting time increases, the network performance is reduced. Thus, the link scheduling scheme is needed to communicate without collision and interference. In the distributed WSNs environment, a sensor node has limited information about its neighboring nodes. Therefore, a comprehensive link scheduling scheme is required for distributed WSNs. Many schemes in the literature prevent collision and interference through time division multiple access (TDMA) protocol. However, considering the collision and interference in TDMA-based schedule increases the delay time and decreases the communication efficiency. This paper proposes the distributed degree-based link scheduling (DDLS) scheme, based on the TDMA. The DDLS scheme achieves the link scheduling more efficiently than the existing schemes and has the low delay and the duty cycle in the distributed environment. Communication between sensor nodes in the proposed DDLS schemes is based on collision avoidance maximal independent link set, which enables to assign collision-free timeslots to sensor nodes, and meanwhile decreases the number of timeslots needed and has low delay time and the duty cycle. Simulation results show that the proposed DDLS scheme reduces the scheduling length by average 81%, the transmission delay by 82%, and duty cycle by over 85% in comparison with distributed collision-free low-latency scheduling scheme."
      }
    ]
  },
  {
    "tid":"163",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A logarithmic scheduling algorithm for high speed input-queued switches' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Throughput of input queued switches using FIFOs is limited due to the Head of Line blocking (HoL) problem. Using VOQs at inputs and a proper scheduling algorithm, near 100% throughput can be achieved. Scaling in terms of number of ports and line rates is a crucial factor for switches deployed in very high speed networks. We propose a novel scheduler called DHL, Decomposed Hierarchical Logarithmic, which is faster than schedulers like iSLIP while performing better for bursty and IP traffics. DHL supports priority in a manner better than other input queue switches especially for bursty traffics. We also propose an optimal version of DHL which is fairer while having half of DHL's area. By pipelining, it is also possible to increase its speed 2 times. DHL scales well in terms of performance, area, and delay. Its area scales linearly with the number of input ports and its delay grows logarithmically while its performance remains almost constant. Our scheduler can be implemented with a low complexity, while working with different service policies like Round Robin, RR, and Earliest Deadline First, EDF. We report our simulation results regarding the average delay, throughput, burst reduction, fairness and priority with respect to the switch sizes."
      }
    ]
  },
  {
    "tid":"164",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A monitoring and advisory system for diabetes patient management using a rule-based method and KNN.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Diabetes is difficult to control and it is important to manage the diabetic's blood sugar level and prevent the associated complications by appropriate diabetic treatment. This paper proposes a system that can provide appropriate management for diabetes patients, according to their blood sugar level. The system is designed to send the information about the blood sugar levels, blood pressure, food consumption, exercise, etc., of diabetes patients, and manage the treatment by recommending and monitoring food consumption, physical activity, insulin dosage, etc., so that the patient can better manage their condition. The system is based on rules and the K Nearest Neighbor (KNN) classifier algorithm, to obtain the optimum treatment recommendation. Also, a monitoring system for diabetes patients is implemented using Web Services and Personal Digital Assistant (PDA) programming."
      }
    ]
  },
  {
    "tid":"165",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Collision Detection and Resolution in Hierarchical Peer-to-Peer Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We consider the problem of anonymous file exchange in wireless ad-hoc networks. We propose a new protocol, dubbed RASH (Roam And SHare), to address the problem. RASH is a peer-to-peer (P2P) protocol which fulfills three objectives: (a) protection of ..."
      }
    ]
  },
  {
    "tid":"166",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'BiB+-tree: an efficient multiversion access method for bitemporal databases' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"By supporting all three dimensions (i.e., key, valid and transaction time) efficiently, bitemporal databases can be applied to various application domains. Although there exist a number of previous research works, none of proposed access methods supports bitemporal databases efficiently. Concretely, update operations dramatically increase the storage cost and querying operations still bear high costs. Specially, now-related data exacerbate these problems in bitemporal databases. Recently, some notable access methods based on B-/R-tree families have been proposed to address such problems. Among them, multiversion approaches have become very promising for bitemporal databases. However, the query performance is still far away from the maturity, and none of proposed solutions have dealt with it radically. In this paper, we introduce a novel and hybrid multiversion access method, called bitemporal B+-tree (BiB+-tree), to overcome weaknesses of the previous solutions and efficiently support the storage and retrieval of bitemporal data. The BiB+-tree indexes data based on the B+-tree access method, clusters data based on both the transaction time and key (multiversion and B+-tree approach, respectively), and manages valid time dimensions based on the R*-tree. Experimental results with large bitemporal datasets establish theoretical analyses as well as the practical value of our proposed BiB+-tree."
      }
    ]
  },
  {
    "tid":"167",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Rate-Based watermark traceback: a new approach' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Network based intrusions have become a serious threat to Internet users. Despite many sophisticated defense techniques, attacks continue to increase. At present, in order to hide the source of the attack, many attackers prefer a stepping stone to launch their attack due to the anonymous nature of the Internet. The size, header and content of an IP packet will be changed because of the stepping stone and all these changes make it more difficult to trace the source of attacks. Currently, researchers study the time interval between IP packets and embed the watermark into the packet stream by adjusting the time interval between IP packets to trace the source of attacks. In this paper we study the previous watermarking schemes based on inter packet delay and propose a novel watermark scheme based on the rate of packets. For the first time we used weak signal detection model and cluster technology to resume the watermark, so as to avoid the present schemes which are based on precision time synchronization or packet number. Simulation tests show that the novel watermark is robust and can countermine the time perturbation, packet losing perturbation and packet padding perturbation caused by an attacker on purpose."
      }
    ]
  },
  {
    "tid":"168",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Dynamic data clustering using stochastic approximation driven multi-dimensional particle swarm optimization' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With an ever-growing attention Particle Swarm Optimization (PSO) has found many application areas for many challenging optimization problems. It is, however, a known fact that PSO has a severe drawback in the update of its global best (gbest) particle, which has a crucial role of guiding the rest of the swarm. In this paper, we propose two efficient solutions to remedy this problem using a stochastic approximation (SA) technique. For this purpose we use simultaneous perturbation stochastic approximation (SPSA), which is applied only to the gbest (not to the entire swarm) for a low-cost solution. Since the problem of poor gbest update persists in the recently proposed extension of PSO, called multi-dimensional PSO (MD-PSO), two distinct SA approaches are then integrated into MD-PSO and tested over a set of unsupervised data clustering applications. Experimental results show that the proposed approaches significantly improved the quality of the MD-PSO clustering as measured by a validity index function. Furthermore, the proposed approaches are generic as they can be used with other PSO variants and applicable to a wide range of problems."
      }
    ]
  },
  {
    "tid":"169",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Discovery and regeneration of hidden emails' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The popularity of email has triggered researchers to look for ways to help users better organize the enormous amount of information stored in their email folders. One challenge that has not been studied extensively in text mining is the reconstruction of hidden emails. A hidden email is an original email that has been quoted in subsequent emails but is not itself present in the folder; it may have been deleted or may never have been received. This paper proposes a method for reconstructing hidden emails using the embedded quotations found in messages further down the thread hierarchy. To do so, we model all the quoted fragments in a precedence graph, from which hidden emails are regenerated as bulletized documents. The bulletized model is our solution to the situation when a total ordering of fragment is not possible. We give a necessary and sufficient condition for each component of the precedence graph to be captured in a single bulletized email, and we develop heuristics that minimize the number of regenerated emails when the condition is not met. Finally, we present empirical results showing the scalability of our approach."
      }
    ]
  },
  {
    "tid":"170",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Decentralized Robust Control Of Uncertain Robots With Backlash And Flexibility At Joints' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes a design method of decentralized robust controllers for robots with joint backlash, flexibility and damping characteristics. For each joint subsystem, a robust tracking controller is designed in two steps: first, a nominal controller is designed for the nominal plant to get desired tracking performance, then a robust compensator is added to restrain the influence of the perturbation, that is the difference of the real plant from the nominal plant. The controller designed by the proposed method is a linear time-invariant one. It is shown that by applying the controller with a sufficiently wide frequency bandwidth robust output tracking property can be achieved in the contact phase while a new contact of the motor with the load in the correct direction is ensured in the backlash phase. An important feature of the method is that the controller parameters can be tuned on-line easily."
      }
    ]
  },
  {
    "tid":"171",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A distributed decomposition policy for computational grid resource allocation optimization based on utility functions' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a market-oriented resource allocation strategy for grid resource. The proposed model uses the utility functions for calculating the utility of a resource allocation. This allows the integration of different optimization objectives into allocation process. This paper is targeted to solve the above issues by using utility-based optimization scheme. We decompose the optimization problem into two levels of sub problems so that the computational complexity is reduced. Two market levels converge to its optimal points; a globally optimal point is achieved. Total user benefit of the computational grid is maximized when the equilibrium prices are obtained through the service market level optimization and resource market level optimization. The economic model is the basis of an iterative algorithm that, given a finite set of requests, is used to perform optimal resource allocation. The experiments show that scheduling based on pricing directed resource allocation involves less overhead and leads to more efficient resource allocation than conventional Round-Robin scheduling."
      }
    ]
  },
  {
    "tid":"172",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Learning A Predictable And Generative Vector Representation For Objects' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"What is a good vector representation of an object? We believe that it should be generative in 3D, in the sense that it can produce new 3D objects; as well as be predictable from 2D, in the sense that it can be perceived from 2D images. We propose a novel architecture, called the TL-embedding network, to learn an embedding space with these properties. The network consists of two components: (a) an autoencoder that ensures the representation is generative; and (b) a convolutional network that ensures the representation is predictable. This enables tackling a number of tasks including voxel prediction from 2D images and 3D model retrieval. Extensive experimental analysis demonstrates the usefulness and versatility of this embedding."
      }
    ]
  },
  {
    "tid":"173",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Tele-Operated High Speed Anthropomorphic Dextrous Hands With Object Shape And Texture Identification' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper reports on the development of two Tele-Operated High Speed Anthropomorphic Dextrous robotic hands. The aim of developing these hands was to achieve a system that seamlessly interfaced between humans and robots.To provide sensory feedback, to a remote operator tactile sensors were developed to be mounted on the robotic hands. Two systems were developed, the first, being a skin sensor capable of shape reconstruction placed on the palm of the hand to feed back the shape of objects grasped and the second is a highly sensitive tactile array for surface texture identification."
      }
    ]
  },
  {
    "tid":"174",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Implications of the ability to learn simple actions on the efficiency of the evolution of social behavior of agents' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We investigate the effect of the ability to learn simple actions on the performance characteristics of the evolution of social behavior in agents situated in an inherently cooperative environment. Using a continuous predator\u2013prey pursuit problem, we verified that relatively complex social behavior emerges from simple, implicit, locally defined, and thus robust and scalable interactions between the predator agents. Considering a distinct aspect of the phenomenon of emergence, we hypothesize that the ability of agents to learn how to perform simple, atomic acts of implicit interaction might facilitate the evolution of more complex behavior. The empirical results indicate that incorporation of the proposed approach to learning in genetic programming (employed as an algorithmic paradigm to evolve the social behavior of the agents) is associated with about a two-fold decrease in the computational effort of evolution."
      }
    ]
  },
  {
    "tid":"175",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Supernumerary Robotic Limbs for Human Body Support.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A robot attached to a human body can support the human when performing tasks in dangerous environments or when taking postures that are fatiguing and uncomfortable. This paper presents a new type of supernumerary robotic limbs (SRL) that supports the human body against floors, walls, and surrounding structures so that the human can perform a task safely, comfortably, and stably. First, the use of ..."
      }
    ]
  },
  {
    "tid":"176",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On Secure Two-Party Computation in Three Rounds.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We revisit the exact round complexity of secure two-party computation. While four rounds are known to be sufficient for securely computing general functions that provide output to one party [Katz-Ostrovsky, CRYPTO\u201904], Goldreich-Krawczyk [SIAM J. Computing\u201996] proved that three rounds are insufficient for this task w.r.t. black-box simulation."
      }
    ]
  },
  {
    "tid":"177",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Robust face recognition after plastic surgery using region-based approaches.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper advances the use of region-based strategies for addressing the problem of face recognition after plastic surgery. The proposed methods implement the region-based approach in several ways. FARO (FAce Recognition against Occlusions and Expression Variations) divides the face into relevant regions (left eye, right eye, nose and mouth) and then codes them independently using Partitioned Iterated Function System (PIFS) processing. FACE (Face Analysis for Commercial Entities) applies a localized version of image correlation index. Finally, the Split Face Architecture (SFA), adaptive and integrative in nature, can leverage any known recognition method, from PCA to most recent ones (including FARO and FACE), provided that it is possible to divide the face into regions. Experimental results, compared with those available from recent experiments reported in literature, show that our methods yield much better performance than state-of-the art algorithms, both holistic and region based. We compare region-based approaches in addressing face recognition after plastic surgery.We exploit a split face architecture to address face recognition as a multibiometric problem.We propose collaboration among component modules of a multibiometric system.We implement a supervisor module to coordinate feedback from any response."
      }
    ]
  },
  {
    "tid":"178",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Privacy-preserving logistic regression' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper addresses the important tradeoff between privacy and learnability, when designing algorithms for learning from private databases. We focus on privacy-preserving logistic regression. First we apply an idea of Dwork et al. (6) to design a privacy-preserving logistic regression algorithm. This involves bound- ing the sensitivity of regularized logistic regression, and perturbing the learned classifier with noise proportional to the sensitivity. We then provide a privacy-preserving regularized logistic regression algorithm based on a new privacy-preserving technique: solving a perturbed optimization problem. We prove that our algorithm preserves privacy in the model due to (6). We provide learning guarantees for both algorithms, which are tighter for our new algorithm, in cases in which one would typically apply logistic regression. Ex- periments demonstrate improved learning performance of our method, versus the sensitivity method. Our privacy-preserving technique does not depend on the sen- sitivity of the function, and extends easily to a class of convex loss functions. Our work also reveals an interesting connection between regularization and privacy."
      }
    ]
  },
  {
    "tid":"179",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient density estimation via piecewise polynomial approximation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We give a computationally efficient semi-agnostic algorithm for learning univariate probability distributions that are well approximated by piecewise polynomial density functions. Let p be an arbitrary distribution over an interval I, and suppose that p is \u03c4-close (in total variation distance) to an unknown probability distribution q that is defined by an unknown partition of I into t intervals and t unknown degree d polynomials specifying q over each of the intervals. We give an algorithm that draws \u00d5(t(d + 1)/\u03b52) samples from p, runs in time poly(t, d + 1, 1/\u03b5), and with high probability outputs a piecewise polynomial hypothesis distribution h that is (14\u03c4 + \u03b5)-close to p in total variation distance. Our algorithm combines tools from real approximation theory, uniform convergence, linear programming, and dynamic programming. Its sample complexity is simultaneously near optimal in all three parameters t, d and \u03b5; we show that even for \u03c4 = 0, any algorithm that learns an unknown t-piecewise degree-d probability distribution over I to accuracy \u03b5 must use [EQUATION] samples from the distribution, regardless of its running time. We apply this general algorithm to obtain a wide range of results for many natural density estimation problems over both continuous and discrete domains. These include state-of-the-art results for learning mixtures of log-concave distributions; mixtures of t-modal distributions; mixtures of Monotone Hazard Rate distributions; mixtures of Poisson Binomial Distributions; mixtures of Gaussians; and mixtures of k-monotone densities. Our general technique gives improved results, with provably optimal sample complexities (up to logarithmic factors) in all parameters in most cases, for all these problems via a single unified algorithm."
      }
    ]
  },
  {
    "tid":"180",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Empirical performance evaluation of IETF mobile IPv6 and proxy mobile IPv6' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With the rapid growth in the number of mobile subscribers and mobile devices, the demand high-speed Internet access is becoming a primary concern in our lives. Not long ago, the most stable and well known solution of IP-based mobility management is Mobile IPv6 (MIPv6). Even if MIPv6 is a well-known mature standard for IPv6 mobility management support, however, it has revealed some problems such as handover latency, packet loss, and signaling overhead. Thus, a new IPv6 mobility management protocol called Proxy Mobile IPv6 (PMIPv6) is being actively standardized by the IETF NETLMM Working Group. Unlike the various host-based protocols such as MIPv6, a network-based approach such as PMIPv6 has salient features and is expected to expedite the real deployment of IP-based mobility management. In this paper, we present an experimental evaluation of MIPv6 and PMIPv6, and give qualitative and quantitative analysis which highlights the main desirable features and key strengths of PMIPv6."
      }
    ]
  },
  {
    "tid":"181",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Evaluating performance in continuous context recognition using event-driven error characterisation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Evaluating the performance of a continuous activity recognition system can be a challenging problem. To-date there is no widely accepted standard for dealing with this, and in general methods and measures are adapted from related fields such as speech and vision. Much of the problem stems from the often imprecise and ambiguous nature of the real-world events that an activity recognition system has to deal with. A recognised event might have variable duration, or be shifted in time from the corresponding real-world event. Equally it might be broken up into smaller pieces, or joined together to form larger events. Most evaluation attempts tend to smooth over these issues, using \u201cfuzzy\u201d boundaries, or some other parameter based error decision, so as to make possible the use of standard performance measures (such as insertions and deletions.) However, we argue that reducing the various facets of a activity system into limited error categories \u2013 that were originally intended for different problem domains \u2013 can be overly restrictive. In this paper we attempt to identify and characterise the errors typical to continuous activity recognition, and develop a method for quantifying them in an unambiguous manner. By way of an initial investigation, we apply the method to an example taken from previous work, and discuss the advantages that this provides over two of the most commonly used methods."
      }
    ]
  },
  {
    "tid":"182",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Weighted Queue-Based Model for Correlated Rayleigh and Rician Fading Channels.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A new channel model for binary additive noise communication channel with memory, called weighted queue-based channel (WQBC), is introduced. The proposed WQBC generalizes the conventional queue-based channel (QBC) such that each queue cell has a different contribution to the noise process, i.e. the queue cells are selected with different probabilities. Suitably selecting the modeling function, the ..."
      }
    ]
  },
  {
    "tid":"183",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Inducing and tracking confusion with contradictions during critical thinking and scientific reasoning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cognitive disequilibrium and its affiliated affective state of confusion have been found to be beneficial to learning due to the effortful cognitive activities that accompany their experience. Although confusion naturally occurs during learning, it can be induced and scaffolded to increase learning opportunities. We addressed the possibility of induction in a study where learners engaged in trialogues on critical thinking and scientific reasoning topics with animated tutor and student agents. Confusion was induced by staging disagreements and contradictions between the animated agents, and the (human) learners were invited to provide their opinions. Self-reports of confusion and learner responses to embedded forced-choice questions indicated that the contradictions were successful at inducing confusion in the minds of the learners. The contradictions also resulted in enhanced learning gains under certain conditions."
      }
    ]
  },
  {
    "tid":"184",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Query completion in community-based Question Answering search.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Query completion has long been proved useful to help a user explore and express his information need. In general search, such completions can be generated from a large scale query log and other accessory information. However, without query log, how to generate query completion for community-based Question Answering (cQA) search remains a challenging problem."
      }
    ]
  },
  {
    "tid":"185",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Context-aware and priority-based user association and resource allocation in heterogeneous wireless networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Heterogeneous wireless networks (HWNs) are usually characterized by the integration of cellular networks and wireless local area networks (WLANs) to meet user requirements and enhance system capacity. This paper proposes a user association and downlink resource allocation algorithm in HWNs with users having different priorities. The proposed solution employs contextual information related to the preferences of the users, their requested data rate, and the characteristics of networks. The user preference is translated through a profit function that is based on the received signal quality and the power consumption at mobile terminals (MTs). Accordingly, an optimization problem is formulated to maximize the overall user satisfaction for each priority level. The formulated problem throws firm restrictions to prevent low-priority users from allocating resources utilized by other users with higher priorities. We then propose a novel heuristic approach with polynomial-time complexity to approximate the formulated problem. Furthermore, the system architecture is discussed and a new solution management strategy is proposed to limit the complexity of the algorithm. Simulation results show that the proposed approximation method maintains the nearest performance to the optimal solution."
      }
    ]
  },
  {
    "tid":"186",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Community network clouds as a case for the IEEE Intercloud standardization' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The IEEE P2302 Intercloud WG conducts work since 2011 on the project Standard for Intercloud Interoperability and Federation with the goal to define a standard architecture and building components for large-scale interoperability of independent cloud providers. While the standardization process has achieved fine-grained definitions of several Intercloud components, a deployment of the Intercloud to demonstrate the architectural feasibility is not yet operational. In this paper, we describe a deployed community network cloud and we show how it matches in several aspects the vision of the Intercloud. Similar to the Intercloud, the community network cloud consists of many small cloud providers, which for interoperability use a set of common services. In this sense, the community network cloud is a real use case for elements that the Intercloud standardization WG envisions, and can feed back to and even become part of the Intercloud. In fact, a study on Small or Medium Enterprise (SME) provided commercial services in the community network cloud indicates the importance of the success of the Intercloud standardization initiative for SMEs."
      }
    ]
  },
  {
    "tid":"187",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Taming Existence in RDF Querying' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We introduce the recursive, rule-based RDF query language RDFLog. RDFLog extends previous RDF query languages by arbitrary quantifier alternation: blank nodes may occur in the scope of all, some, or none of the universal variables of a rule. In addition RDFLog is aware of important RDF features such as the distinction between blank nodes, literals and URIs or the RDFS vocabulary. The semantics of RDFLog is closed (every answer is an RDF graph), but lifts RDF's restrictions on literal and blank node occurrences for intermediary data. We show how to define a sound and complete operational semantics that can be implemented using existing logic programming techniques. Using RDFLog we classify previous approaches to RDF querying along their support for blank node construction and show equivalence between languages with full quantifier alternation and languages with only *** *** rules."
      }
    ]
  },
  {
    "tid":"188",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A protocol for piggy-backing on Markov based wavelength reservation in WDM optical networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"During lightpath establishment in WDM optical networks, two important steps, other than routing, are: wavelength selection and wavelength reservation. If two or more lightpaths select the same wavelength, wavelength collision occurs. The basic reason for collision is the non-availability of the updated wavelength usage information. Markov based wavelength selection guesses a unique wavelength in advance, where each node periodically broadcasts its adjoining link usage information at a regular interval T. But, at any intermediate time t (0"
      }
    ]
  },
  {
    "tid":"189",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Secure Dynamic Identity Based Authentication Protocol with Smart Cards for Multi-Server Architecture.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Due to the rapid growth of computer networks and service providing servers, many network environments have been becoming multi-server architecture and various multi-server authentication protocols have been proposed. In such an environment, a user can obtain different network services from multiple network servers without repeating registration to each server. Recently, Li et al. proposed a secure dynamic ID based authentication protocol for multi-server architecture using smart cards. They claimed that their protocol preserves mutual authentication and protected from several attacks. However, in this paper, we find that Li et al.'s protocol cannot provide the protection against leak-of-verifier attack, impersonation attack, session key disclosure attack and many logged-in users' attack. To remedy these security flaws, we propose an improved version of dynamic ID based authentication protocol, which covers all the identified weaknesses of Li et al.'s protocol and is more secure and efficient for practical multi-server environments."
      }
    ]
  },
  {
    "tid":"190",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Throughput and Delay in Heterogeneous Cognitive Radio Networks with Cooperative Secondary Users' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper1, we investigate the throughput and delay in heterogeneous cognitive radio networks (HCRN), where the data source and the destination (S-D) is heterogeneously distributed following a rank based model and secondary users (SUs) provide relay service for primary users (PUs). We consider two scenarios: 1) PUs and SUs are both static; and 2) PUs are static and SUs are mobile. For scenario 1, we show that the primary network throughput is the same for different heterogeneous extents of S-D distribution owing to the flexible assistance of SUs, while the throughput of secondary networks is proven to be changing with the S-D heterogeneity exponent , which depicts the variation of different heterogeneous extents of S-D distribution. In addition, the delay of both primary and secondary networks are shown to be altering with . Further, we reveal that the number of SUs required to assist PUs can be dramatically reduced when considering the S-D heterogeneity, while achieving the same primary network throughput. For scenario 2, we utilize a modified uniform mobility (MUM) model to depict the motion of SUs and mainly focus on the analysis of throughput and delay for primary networks. It shows that the primary network throughput is also free of the heterogeneous extent of S-D distribution, while the delay changes with . Due to the mobility of SUs, a better delay-throughput tradeoff of primary networks is achieved compared with that in scenario 1."
      }
    ]
  },
  {
    "tid":"191",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Market-Equilibrium, Competitive, and Cooperative Pricing for Spectrum Sharing in Cognitive Radio Networks: Analysis and Comparison' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In a cognitive radio network, frequency spectrum can be shared between primary (or licensed) users and secondary (or unlicensed) users, where the secondary users pay the primary users (or primary service provider) for radio resource usage. This is referred to as spectrum trading. In spectrum trading, pricing is a key issue of interest to primary service providers (i.e., spectrum sellers) as well as to secondary service providers (i.e., spectrum buyers). In a cognitive radio network, pricing model for spectrum sharing depends on the objective of spectrum trading, and therefore, the behaviors of spectrum sellers and spectrum buyers. In this paper, we investigate three different pricing models, namely, market-equilibrium, competitive, and cooperative pricing models for spectrum trading in a cognitive radio environment. In these pricing models, the primary service providers have different behaviors (i.e., competitive and cooperative behaviors) to achieve different objectives of spectrum trading. Specifically, in marketequilibrium pricing model, the objective of spectrum trading is to satisfy spectrum demand from the secondary users, and there is neither competition nor cooperation among primary service providers. In the competitive pricing, the objective is to maximize the individual profit, and there is competition among primary service providers. In cooperative pricing, the objective of spectrum trading is to maximize the total profit, and cooperation exists among primary service providers. We propose distributed algorithms to achieve the pricing solutions of these different pricing models and analyze stability of these distributed algorithms. We perform extensive performance analysis of these pricing algorithms considering different aspects such as profit of the primary service providers, stability region, and impact of number of primary service providers, which reveals interesting insights into the spectrum trading problem."
      }
    ]
  },
  {
    "tid":"192",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Smart data pricing models for the internet of things: a bundling strategy approach.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The Internet of Things (IoT) has emerged as a new paradigm for the future Internet. In IoT, devices are connected to the Internet and thus are a huge data source for numerous applications. In this article, we focus on addressing data management in IoT through using a smart data pricing (SDP) approach. With SDP, data can be managed flexibly and efficiently through intelligent and adaptive incentive mechanisms. Moreover, data is a major source of revenue for providers and partners. We propose a new pricing scheme for IoT service providers to determine the sensing data buying price and IoT service subscription fee offered to sensor owners and service users, respectively. Additionally, we adopt the bundling strategy that allows multiple providers to form a coalition and offer their services as a bundle, attracting more users and achieving higher revenue. Finally, we outline some important open research issues for SDP and IoT."
      }
    ]
  },
  {
    "tid":"193",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Characterizing key stakeholders in an online black-hat marketplace' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Over the past few years, many black-hat marketplaces have emerged that facilitate access to reputation manipulation services such as fake Facebook likes, fraudulent search engine optimization (SEO), or bogus Amazon reviews. In order to deploy effective technical and legal countermeasures, it is important to understand how these black-hat marketplaces operate, shedding light on the services they offer, who is selling, who is buying, what are they buying, who is more successful, why are they successful, etc. Toward this goal, in this paper, we present a detailed micro-economic analysis of a popular online black-hat marketplace, namely, SEOClerks.com. As the site provides non-anonymized transaction information, we set to analyze selling and buying behavior of individual users, propose a strategy to identify key users, and study their tactics as compared to other (non-key) users. We find that key users: (1) are mostly located in Asian countries, (2) are focused more on selling black-hat SEO services, (3) tend to list more lower priced services, and (4) sometimes buy services from other sellers and then sell at higher prices. Finally, we discuss the implications of our analysis with respect to devising effective economic and legal intervention strategies against marketplace operators and key users."
      }
    ]
  },
  {
    "tid":"194",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Performance analysis of rate-adaptive modulation with antenna selection in multiuser MIMO system' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The performance analysis of multiuser multiple-input multiple-output (MIMO) system with rate-adaptive modulation and antenna selection over Rayleigh fading channels is presented. By means of exact expression of bit error rate (BER), the suboptimal switching thresholds (ST) of fading gain subject to a target BER constraint are derived. It is shown that this suboptimal ST are very close to the optimal ST, and is more accurate than the conventional ST based on approximate BER expression. By the ST, we obtain exact closed-form expressions of the spectrum efficiency (SE) and average BER. Moreover, a tightly approximate BER is derived to simplify the calculation of exact BER, which has more accuracy than the existing approximate BER. Simulation results for SE and BER are in good agreement with the derived theoretical expressions. The results show that the multiuser MIMO with adaptive modulation can provide better SE than the single user counterpart, and the system SE using suboptimal ST is also higher than that using conventional ST."
      }
    ]
  },
  {
    "tid":"195",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Discretizing stochastic tractography: A fast implementation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Probabilistic tractography has emerged as an alternative to classical deterministic methods to overcome their lack of connectivity information between different brain regions. However, it relies on statistical sampling, which is computationally taxing. In this study, a well-known, random walk based stochastic tractography method is discretized by limiting the set of directions that a sampling pabstract can follow. This sets up to a framework based on a Markov chain that can accommodate all the desirable features of stochastic tractography, principally trajectory regularization through p abstract deflection. The system produces results that are comparable to those by the stochastic algorithm it is based on (\u03c1 = 0.79), though 60 times faster."
      }
    ]
  },
  {
    "tid":"196",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Verifying design modularity, hierarchy, and interaction locality using data clustering techniques' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Modularity, hierarchy, and interaction locality are general approaches to reducing the complexity of any large system. A widely used principle in achieving these goals in designing software systems is striving for high cohesion within a module and low coupling between modules. However, this principle has difficulties in practice. Because a hierarchical system structure often consists of several layers, it is difficult to decide at what layer an interaction should be considered as cohesion, and at what layer an interaction should be considered as coupling. In this paper, we do not differentiate cohesion and coupling, but use a general term interaction to represent the dependencies between software modules. We propose a method to verify the design modularity, hierarchy, and interaction locality of a software system. This approach is based on the component interactions gathered from certain design level artifacts, such as UML diagrams. Data clustering technique is then used to group software components according to the degree of interactions between them. To show how to use this approach, we apply it to Parna's KWIC object-oriented design example, in which sequence diagram is used to derive the degree of component interactions."
      }
    ]
  },
  {
    "tid":"197",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient Long-term Mapping in Dynamic Environments' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"As autonomous robots are increasingly being introduced in real-world environments operating for long periods of time, the difficulties of long-term mapping are attracting the attention of the robotics research community. This paper proposes a full SLAM system capable of handling the dynamics of the environment across a single or multiple mapping sessions. Using the pose graph SLAM paradigm, the system works on local maps in the form of 2D point cloud data which are updated over time to store the most up-to-date state of the environment. The core of our system is an efficient ICP-based alignment and merging procedure working on the clouds that copes with non-static entities of the environment. Furthermore, the system retains the graph complexity by removing out-dated nodes upon robust inter- and intra-session loop closure detections while graph coherency is preserved by using condensed measurements. Experiments conducted with real data from longterm SLAM datasets demonstrate the efficiency, accuracy and effectiveness of our system in the management of the mapping problem during long-term robot operation."
      }
    ]
  },
  {
    "tid":"198",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Learning and Integrating Semantics for Image Indexing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose learning and integration frameworks that extract and combine local and global semantics for image indexing and retrieval. In the supervised learning version, support vector detectors are trained on semantic support regions without image segmentation. The reconciled and aggregated detection-based indexes then serve as an input for support vector learning of image classifiers to generate class-relative image indexes. In the unsupervised learning approach, image classifiers are first trained on local image blocks from a small number of labeled images. Then local semantic patterns are discovered from clustering the image blocks with high classification output. Training samples are induced from cluster memberships for support vector learning to form local semantic pattern detectors. During retrieval, similarities based on both local and global indexes are combined to rank images. Query-by-example experiments on 2400 unconstrained consumer photos with 16 semantic queries show that the proposed approaches outperformed the fusion of color and texture features significantly in average precisions by 55% and 37% respectively."
      }
    ]
  },
  {
    "tid":"199",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Random forest dissimilarity based multi-view learning for Radiomics application.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"\u2022Propose a Random forest dissimilarity based method for multi-view learning.\u2022Study the effect of hyperparameters on the quality of random forest dissimilarity.\u2022Compare the proposed method to the state of art Radiomics solutions.\u2022Compare the proposed method to multi-view learning approaches\u2022Show that the proposed approach outperforms the state-of-the-art methods."
      }
    ]
  },
  {
    "tid":"200",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Dissecting Urban Noises from Heterogeneous Geo-Social Media and Sensor Data' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Geo-social media services, such as Foursquare and Flickr, provide rich data that sensors various urban activities of human beings from geographical, mobility, visual, and social aspects. While noise pollution in modern cities is getting worse and sound sensors are sparse and costly, it is highly demanded to infer and analyze the noise at any region in urban areas. In this paper, we aim to leverage heterogeneous geo-social sensor data on Foursquare, Flickr, and Gowalla, to dissect urban noises for every regions in a city. Using NYC 311 noise complaint records as the approximation of urban noises generated by regions, we propose a novel unsupervised framework that integrates the extracted geographical, mobility, visual, and social features to infer the noise composition for regions and time intervals of interest in NYC. Experimental results show that our system can achieve promising results with substantially few training data, compared to state-of-the-art methods."
      }
    ]
  },
  {
    "tid":"201",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Agents Jumping in the Air: Dream or Reality?' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mobile agent technology has traditionally been recognized as a very useful approach to build applications for mobile computing and wireless environments. However, only a few studies report practical experiences with mobile agents in a wireless medium. This leads us to the following question: is mobile agent technology ready to be used in this environment? In this paper, we study existing mobile agent platforms by analyzing if they could be used effectively in a wireless medium. We identify some key missing features in the platforms and highlight the requirements and challenges that lie ahead. With this work, we expose existing problems and hope to motivate further research in the area."
      }
    ]
  },
  {
    "tid":"202",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Diffeomorphic shape trajectories for improved longitudinal segmentation and statistics.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Longitudinal imaging studies involve tracking changes in individuals by repeated image acquisition over time. The goal of these studies is to quantify biological shape variability within and across individuals, and also to distinguish between normal and disease populations. However, data variability is influenced by outside sources such as image acquisition, image calibration, human expert judgment, and limited robustness of segmentation and registration algorithms. In this paper, we propose a two-stage method for the statistical analysis of longitudinal shape. In the first stage, we estimate diffeomorphic shape trajectories for each individual that minimize inconsistencies in segmented shapes across time. This is followed by a longitudinal mixed-effects statistical model in the second stage for testing differences in shape trajectories between groups. We apply our method to a longitudinal database from PREDICT-HD and demonstrate our approach reduces unwanted variability for both shape and derived measures, such as volume. This leads to greater statistical power to distinguish differences in shape trajectory between healthy subjects and subjects with a genetic biomarker for Huntington's disease (HD)."
      }
    ]
  },
  {
    "tid":"203",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Joint Rate Control Scheme in a Hybrid Stereoscopic Video Codec System for 3DTV Broadcasting' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a frame-level joint rate control scheme for both intra- and inter-predictive coding is first introduced for a hybrid video codec system that independently encodes the two-view sequences of a stereoscopic video. The proposed rate control scheme jointly works on the left view encoder of MPEG-2 Video and the right view encoder of an H.264/AVC for stereoscopic video input, and is formulated as optimization problems that allow control of the output bit rates of both encoders in the sense that: 1) the sum of the two decoded video qualities is maximized, and 2) the quality difference is maintained around a desired level for a given target bit budget at the same time. In order to verify the effectiveness of the proposed joint rate scheme, it is compared with an independent rate control scheme. The experimental results show that the joint rate control scheme effectively regulates the output bitstreams of the hybrid encoders based on MPEG-2 Video and H.264/AVC. The joint rate control scheme achieves average 0.71 dB gain in PSNR and 67.07% reduction in the MSE of PSNR difference for I-frame only, average 0.54dB gain and 91.08% reduction for IPPP coding, and average 0.32dB gain and 79.01% reduction for IBBP coding."
      }
    ]
  },
  {
    "tid":"204",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Secure and Privacy Preserving Protocol for Cloud-based Vehicular DTNs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cloud-assisted vehicular delay tolerant networks (CV-DTNs) have been utilized in wide-raging applications where a continuous end-to-end connection is unavailable, the message transmission is fulfilled by the cooperation among vehicular nodes and follows a store-carry-and-forward manner, and the complex computational work can be delegated to the disengaged vehicles in the parking lots which constitute the potential vehicular cloud. Nevertheless, the existing incentive schemes as well as the packet forwarding protocols cannot well model continuous vehicle collaboration, resist vehicle compromise attacks and collusion attacks, leaving the privacy preservation issues untouched. In this paper, a novel threshold credit-based incentive mechanism (TCBI) is proposed based on the modified model of population dynamics to efficiently resist the node compromise attacks, stimulate the cooperation among intermediate nodes, maximize vehicular nodes\u2019 interest and realize the fairness of possessing the same opportunity of transmitting packets for credits. Then, a TCBI-based privacy-preserving packet forwarding protocol is proposed to solve the open problem of resisting layer-adding attack by outsourcing the privacy-preserving aggregated transmission evidence generation for multiple resourceconstrained vehicles to the cloud side from performing any one-way trapdoor function only once. The vehicle privacy is well protected from both the cloud and transportation manager. Finally, formal security proof and the extensive simulation show the effectiveness of our proposed TCBI in resisting the sophisticated attacks and the efficiency in terms of high reliability, high delivery ratio and low average delay in cloud-assisted vehicular DTNs."
      }
    ]
  },
  {
    "tid":"205",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Knowledge Sanitization on the Web' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The widespread use of the Internet caused the rapid growth of data on the Web. But as data on the Web grew larger in numbers, so did the perils due to the applications of data mining. Privacy preserving data mining (PPDM) is the field that investigates techniques to preserve the privacy of data and patterns. Knowledge Hiding, a subfield of PPDM, aims at preserving the sensitive patterns included in the data, which are going to be published. A wide variety of techniques fall under the umbrella of Knowledge Hiding, such as frequent pattern hiding, sequence hiding, classification rule hiding and so on. In this tutorial we create a taxonomy for the frequent itemset hiding techniques. We also provide as examples for each category representative works that appeared recently and fall into each one of these categories. Then, we focus on the detailed overview of a specific category, the so called linear programming-based techniques. Finally, we make a quantitative and qualitative comparison among some of the existing techniques that are classified into this category."
      }
    ]
  },
  {
    "tid":"206",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Face liveness detection with component dependent descriptor' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Spoofing attacks mainly include printing artifacts, electronic screens and ultra-realistic face masks or models. In this paper, we propose a component-based face coding approach for liveness detection. The proposed method consists of four steps: (1) locating the components of face; (2) coding the low-level features respectively for all the components; (3) deriving the high-level face representation by pooling the codes with weights derived from Fisher criterion; (4) concatenating the histograms from all components into a classifier for identification. The proposed framework makes good use of micro differences between genuine faces and fake faces. Meanwhile, the inherent appearance differences among different components are retained. Extensive experiments on three published standard databases demonstrate that the method can achieve the best liveness detection performance in three databases."
      }
    ]
  },
  {
    "tid":"207",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A direct variational method for planning monotonically optimal paths for redundant manipulators in constrained workspaces' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes a path planner for serial manipulators with a large number of degrees of freedom, working in cluttered workspaces. Based on the variational principles, this approach involves formulating the path planning problem as constrained minimization of a functional representing the total joint movement over the complete path. We use modified boundary conditions at both ends of the trajectory to find more suitable start and end configurations. The concept of monotonic optimality is introduced in order to optimize the manipulator paths between the resulting end configurations. For obstacle avoidance, volume and proximity based penalizing schemes are developed and used. The presented planner uses a global approach to search for feasible paths and at the same time involves no pre-processing task. A variety of test cases have been presented to establish the efficacy of the presented scheme in providing good quality paths. The extent of advantage accruing out of the measures of free end-configurations and monotonic optimality are also analyzed quantitatively."
      }
    ]
  },
  {
    "tid":"208",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Co-Salient Object Detection From Multiple Images' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a novel method to discover co-salient objects from a group of images, which is modeled as a linear fusion of an intra-image saliency (IaIS) map and an inter-image saliency (IrIS) map. The first term is to measure the salient objects from each image using multiscale segmentation voting. The second term is designed to detect the co-salient objects from a group of images. To compute the IrIS map, we perform the pairwise similarity ranking based on an image pyramid representation. A minimum spanning tree is then constructed to determine the image matching order. For each region in an image, we design three types of visual descriptors, which are extracted from the local appearance, e.g., color, color co-occurrence and shape properties. The final region matching problem between the images is formulated as an assignment problem that can be optimized by linear programming. Experimental evaluation on a number of images demonstrates the good performance of the proposed method on co-salient object detection."
      }
    ]
  },
  {
    "tid":"209",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Reward-based voltage scheduling for dynamic-priority hard real-time systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Reward-based scheduling has been investigated for flexible applications in which an approximate but timely result is acceptable. Meanwhile, significant research efforts have been made on voltage scheduling which exploits the tradeoff between the processor speed and the energy consumption. In this paper, we address the combined scheduling problem of maximizing the total reward of hard real-time systems with a given energy budget. We present an optimal off-line algorithm and an efficient on-line algorithm for jobs with their own release-times/deadlines under Earliest-Deadline-First (EDF) scheduling. Experimental results show that the solution computed by the on-line algorithm is no more than 14% worse than the theoretical optimal solution obtained by the optimal off-line algorithm."
      }
    ]
  },
  {
    "tid":"210",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A midas plugin to enable construction of reproducible web-based image processing pipelines.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Image processing is an important quantitative technique for neuroscience researchers, but difficult for those who lack experience in the field. In this paper we present a web-based platform that allows an expert to create a brain image processing pipeline, enabling execution of that pipeline even by those biomedical researchers with limited image processing knowledge. These tools are implemented as a plugin for Midas, an open-source toolkit for creating web based scientific data storage and processing platforms. Using this plugin, an image processing expert can construct a pipeline, create a web-based User Interface, manage jobs, and visualize intermediate results. Pipelines are executed on a grid computing platform using BatchMake and HTCondor. This represents a new capability for biomedical researchers and offers an innovative platform for scientific collaboration. Current tools work well, but can be inaccessible for those lacking image processing expertise. Using this plugin, researchers in collaboration with image processing experts can create workflows with reasonable default settings and streamlined user interfaces, and data can be processed easily from a lab environment without the need for a powerful desktop computer. This platform allows simplified troubleshooting, centralized maintenance, and easy data sharing with collaborators. These capabilities enable reproducible science by sharing datasets and processing pipelines between collaborators. In this paper, we present a description of this innovative Midas plugin, along with results obtained from building and executing several ITK based image processing workflows for diffusion weighted MRI (DW MRI) of rodent brain images, as well as recommendations for building automated image processing pipelines. Although the particular image processing pipelines developed were focused on rodent brain MRI, the presented plugin can be used to support any executable or script-based pipeline."
      }
    ]
  },
  {
    "tid":"211",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Security Framework for Cloud-Based Web Crawling System' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In the face of large amounts of complicated web information, a professional and individualized web crawling system is required for users to acquire information effectively. In this context, a cloud-based web crawling system is proposed, which can improve the efficiency of application development and reduce its maintenance costs. However, it also poses security risks for application developers. To address the problem of security and privacy protection in a cloud based web crawling system, this paper proposes a framework based on the SSL connection and simHash to ensure the security of storage and transmission. Moreover, a case study demonstrates the effectiveness and efficiency of the security framework."
      }
    ]
  },
  {
    "tid":"212",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Local semi-supervised regression for single-image super-resolution' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a local semi-supervised learning-based algorithm for single-image super-resolution. Different from most of example-based algorithms, the information of test patches is considered during learning local regression functions which map a low-resolution patch to a high-resolution patch. Localization strategy is generally adopted in single-image super-resolution with nearest neighbor-based algorithms. However, the poor generalization of the nearest neighbor estimation decreases the performance of such algorithms. Though the problem can be fixed by local regression algorithms, the sizes of local training sets are always too small to improve the performance of nearest neighbor-based algorithms significantly. To overcome the difficulty, the semi-supervised regression algorithm is used here. Unlike supervised regression, the information about test samples is considered in semi-supervised regression algorithms, which makes the semi-supervised regression more powerful. Noticing that numerous test patches exist, the performance of nearest neighbor-based algorithms can be further improved by employing a semi-supervised regression algorithm. Experiments verify the effectiveness of the proposed algorithm."
      }
    ]
  },
  {
    "tid":"213",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Longitudinal, End-To-End View Of The Dnssec Ecosystem' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The Domain Name System's Security Extensions (DNSSEC) allow clients and resolvers to verify that DNS responses have not been forged or modified in-flight. DNSSEC uses a public key infrastructure (PKI) to achieve this integrity, without which users can be subject to a wide range of attacks. However, DNS SEC can operate only if each of the principals in its PM properly performs its management tasks: authoritative name servers must generate and publish their keys and signatures correctly, child zones that support DNSSEC must be correctly signed with their parent's keys, and resolvers must actually validate the chain of signatures.This paper performs the first large-scale, longitudinal measurement study into how well DNSSEC's PKI is managed. We use data from all DNSSEC-enabled sub domains under the .com, .org, and .net TLDs over a period of 21 months to analyze DNS SEC deployment and management by domains; we supplement this with active measurements of more than 59K DNS resolvers worldwide to evaluate resolver-side validation.Our investigation reveals pervasive mismanagement of the DNSSEC infrastructure. For example, we found that 31% of domains that support DNS SEC fail to publish all relevant records required for validation; 39% of the domains use insufficiently strong key-signing keys; and although 82% of resolvers in our study request DNSSEC records, only 12% of them actually attempt to validate them. These results highlight systemic problems, which motivate improved automation and auditing of DNS SEC management."
      }
    ]
  },
  {
    "tid":"214",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Distributed tracking of second-order multi-agent systems with measurement noise' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper is concerned with the leader-following consensus problem of second-order agents under fixed and directed topologies. The control input of each agent can only use its local state and the states of its neighbors corrupted by white noises. By using Hurwitz stability criteria, Ito formula and graph theory, some necessary and sufficient conditions are given for mean square bounded tracking. Furthermore, if the leader's acceleration has a limit when the time goes to infinity, we give an explicit expression of the tracking error in relation to the network and control parameters. Finally, the effectiveness of the distributed tracking controller is demonstrated by a simulation example."
      }
    ]
  },
  {
    "tid":"215",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Radar backscattering model for multilayer mixed-species forests' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A multilayer canopy scattering model is developed for mixed-species forests. The multilayer model provides a significantly enhanced representation of actual complex forest structures compared to the conventional canopy-trunk layer models. Multilayer Michigan Microwave Canopy Scattering model (Multi-MIMICS) allows overlapping layer configuration and a tapered trunk model applicable to forests of mixed species and/or mixed growth stages. The model is the first-order solution to a set of radiative transfer equations and includes layer interactions between overlapping layers. It simulates SAR backscattering coefficients based on input dimensional, geometrical, and dielectric variables of forest canopies. The Multi-MIMICS is an efficient realization of actual forest structures and can be shaped for specific interest of forest parameters. We present the model's application and validation in the paper. The model is parameterized using data collected from a 220,000-ha area of forests in central Queensland, Australia. Fifteen 50\u00d750 m test sites representing the general forest diversity and growth stages are chosen as ground truth. Polarimetric backscattering airborne SAR (AIRSAR) data of the same area are acquired to validate the model simulations. The model predicts SAR backscattering coefficients of the test areas. Simulation results show a good agreement with AIRSAR data at most frequencies and polarizations. The simulated backscattering coefficient from the multilayer model and the standard MIMICS are also compared and significant improvements are observed."
      }
    ]
  },
  {
    "tid":"216",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Quality and robustness improvement for real world industrial systems using a fuzzy particle swarm optimization' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a novel fuzzy particle swarm optimization with cross-mutated (FPSOCM) operation, where a fuzzy logic system developed based on the knowledge of swarm intelligence is proposed to determine the inertia weight for the swarm movement of particle swarm optimization (PSO) and the control parameter of a newly introduced cross-mutated operation. Hence, the inertia weight of the PSO can be adaptive with respect to the search progress. The new cross-mutated operation intends to drive the solution to escape from local optima. A suite of benchmark test functions are employed to evaluate the performance of the proposed FPSOCM. Experimental results show empirically that the FPSOCM performs better than the existing hybrid PSO methods in terms of solution quality, robustness, and convergence rate. The proposed FPSOCM is evaluated by improving the quality and robustness of two real world industrial systems namely economic load dispatch system and self-provisioning systems for communication network services. These two systems are employed to evaluate the effectiveness of the proposed FPSOCM as they are multi-optima and non-convex problems. The performance of FPSOCM is found to be significantly better than that of the existing hybrid PSO methods in a statistical sense. These results demonstrate that the proposed FPSOCM is a good candidate for solving product or service engineering problems which have multi-optima or non-convex natures."
      }
    ]
  },
  {
    "tid":"217",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Traceability and modeling of requirements in enterprise architecture from a design rationale perspective' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Our work aims to rationalize Enterprise Architectures (EA) by providing the reasoning behind the designs, in terms of selection criteria, design alternatives and more. Its major contribution is a formal metamodel that captures the reasoning and the inter-relationships of design decisions. This paper extends our approach in order to provide an explicit bridging between the Problem space that is defined by the different requirements and the Solution space that is described by specific design decisions. In doing so, EA Anamnesis also supports traceability from specific design decisions to the given requirements."
      }
    ]
  },
  {
    "tid":"218",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Parallelizing video transcoding using Map-Reduce-based cloud computing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Due to the complexity of video coding, fast transcoding is still a challenge. Various parallel coding methods have been proposed. In this paper, we present a parallel transcoding system over Map/Reduce cloud computing architecture. Input video sequences are divided into segments, and mapped to multiple computers. The sub-tasks are launched in parallel with processing results concatenated to the final output sequences. For heterogeneous clips, computing capacity, and task-launching overhead, the task scheduling over cloud is an NP-hard problem. We propose a low-complexity heuristic algorithm, Max-MCT, to find out the optimal solutions for task scheduling. By estimating the low-bound of finish time, we transform the problem into a virtual knapsack problem. But it is not an optimal solution for the original problem therefore we use a minimal complete time (MCT) algorithm to minimize the entire finish time. We carry out extensive experiments on numerical simulations. The results verified that our algorithm outperforms the existing algorithms."
      }
    ]
  },
  {
    "tid":"219",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Event-based sensor data exchange and fusion in the Internet of Things environments.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Internet of Things (IoT) is a promising technology for improving our lives and society by integrating smart devices in our environment and paving the way for novel ICT application, spanning from smart cities to energy efficiency and home automation. However, such a vision encompasses the availability of thousands of smart devices, or even more, that continuously exchange a huge volume of data among each other and with cloud-based services, raising a big data problem. Such a problem can be approached by properly applying data fusion practices within an IoT infrastructure. Due to the characteristics and peculiarities of the communications among smart devices within the IoT, an event-based data fusion is needed, where devices exchange notifications of events among each others. Such data fusion should be focused on special devices where notification heterogeneity, and data source trust issues have to be faced with. Accordingly, the contribution of this work is proposing (i) a novel broker-less event-based communication protocol specifically tailored to sensors with constrained resources, (ii) a solution for flexible event-based communications among heterogeneous data sources, and (iii) an approach based on the theory of evidence for data fusion processes that depend on the matching and trust degree of the data to be fused."
      }
    ]
  },
  {
    "tid":"220",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Selecting distinctive features to improve performances of multidimensional fuzzy vault scheme' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Fuzzy vault scheme is one of the most popular biometric cryptosystems. However, the scheme is designed for set differences while Euclidean distance is often used in biometric techniques. Multidimensional fuzzy vault scheme (MDFVS) is a modified version that can be easily implemented based on biometric feature data. In MDFVS, every point is a vector, and Euclidean distance measure is used for genuine points filtering. To get better performances, the step of feature selection in the MDFVS algorithms is very important and should be well designed. In this paper we propose applying recognition rate to measure discrimination of features and selecting strong distinctive features into genuine points. Some principles of selecting strong distinctive features to compose genuine points are discussed. An implementation of MDFVS with feature selection is also presented. Experimental results based on palmprint show that the proposed feature selection approach improves the performances of MDFVS."
      }
    ]
  },
  {
    "tid":"221",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Method of Honey Plant Classification Based on IR Spectrum: Extract Feature Wavelength Using Genetic Algorithm and Classify Using Linear Discriminate Analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Bayesian linear classifier is the basic scheme to solve model classification basing on statistics. Face with the classification of three different nectar plant, the near infrared spectrum data was acquired. The character of the near infrared spectrums is known as litter sample and higher dimension. In this paper, the method has developed to acquire the feature wavelength based on genetic algorithm. It can solve the problem of the effective information extraction from the high-dimensional data matrix. The fitness function of genetic algorithm is been set to minimize the error rate of classification. The K-S algorithm was used to construct the calibration set and validation set. There are 132 samples in the calibration set and 42 samples in the validation set. The feature wavelengths were acquired respectively basing on different preprocessing. The result indicates using the 10 feature wavelengths based on raw data can obtain best resolution compare with the principal component analysis -linear discriminate analysis model. The result indicated that the GA-LDA classifier can made the model to be simplified and the correction rate can be increased evidently after using the feature wavelength."
      }
    ]
  },
  {
    "tid":"222",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Periodic step-size adaptation in second-order gradient descent for single-pass on-line structured learning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"It has been established that the second-order stochastic gradient descent (SGD) method can potentially achieve generalization performance as well as empirical optimum in a single pass through the training examples. However, second-order SGD requires computing the inverse of the Hessian matrix of the loss function, which is prohibitively expensive for structured prediction problems that usually involve a very high dimensional feature space. This paper presents a new second-order SGD method, called Periodic Step-size Adaptation (PSA). PSA approximates the Jacobian matrix of the mapping function and explores a linear relation between the Jacobian and Hessian to approximate the Hessian, which is proved to be simpler and more effective than directly approximating Hessian in an on-line setting. We tested PSA on a wide variety of models and tasks, including large scale sequence labeling tasks using conditional random fields and large scale classification tasks using linear support vector machines and convolutional neural networks. Experimental results show that single-pass performance of PSA is always very close to empirical optimum."
      }
    ]
  },
  {
    "tid":"223",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'ParallelPC: an R package for efficient constraint based causal exploration.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Discovering causal relationships from data is the ultimate goal of many research areas. Constraint based causal exploration algorithms, such as PC, FCI, RFCI, PC-simple, IDA and Joint-IDA have achieved significant progress and have many applications. A common problem with these methods is the high computational complexity, which hinders their applications in real world high dimensional datasets, e.g gene expression datasets. In this paper, we present an R package, ParallelPC, that includes the parallelised versions of these causal exploration algorithms. The parallelised algorithms help speed up the procedure of experimenting big datasets and reduce the memory used when running the algorithms. The package is not only suitable for super-computers or clusters, but also convenient for researchers using personal computers with multi core CPUs. Our experiment results on real world datasets show that using the parallelised algorithms it is now practical to explore causal relationships in high dimensional datasets with thousands of variables in a single multicore computer. ParallelPC is available in CRAN repository at this https URL"
      }
    ]
  },
  {
    "tid":"224",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An analysis of services for the mobile tourist' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mobile tourism services have become an essential tool for supporting tourists around the world. Our research aims to identify desirable and undesirable features for mobile tourism services from the tourists' perspective. Focus group discussions and surveys were conducted to gather tourists' needs, exploring the problems faced. This paper presents our findings highlighting desirable design features being maps and transportation information, and unpopular features such as gaming activities. The results also reflect differences in preferences between business/leisure travelers and frequent/infrequent travelers. We believe results of our work are useful in providing design insights for mobile service providers when deciding on features to be included in tourist-friendly information-rich systems, and the types of services customized for specific tourist groups."
      }
    ]
  },
  {
    "tid":"225",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Circumstantial-Evidence-Based Judgment for Software Effort Estimation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Expert judgment for software effort estimation is oriented toward direct evidences that refer to actual effort of similar projects or activities through experts' experiences. However, the availability of direct evidences implies the requirement of suitable experts together with past data. The circpmstantialevidence-based judgment proposed in this paper focuses on the development experiences deposited in human knowledge, and can then be used to qualitatively estimate implementation effort of different proposals of a new project by rational inference. To demonstrate the process of circumstantial-evidence-based judgment, this paper adopts propositional learning theory based diagnostic reasoning to infer and compare different effort estimates when implementing a Web service composition project with some different techniques and contexts. The exemplar shows our proposed work can help determine effort tradeoff before project implementation. Overall, circumstantial-evidence-based judgment is not an alternative but complementary to expert judgment so as to facilitate and improve software effort estimation."
      }
    ]
  },
  {
    "tid":"226",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Delay asymptotics for a priority queueing system' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we study discrete-time priority queueing systems fed by a large number of arrival streams. We first provide bounds on the actual delay asymptote in terms of the virtual delay asymptote. Then, under suitable assumptions on the arrival process to the queue, we show that these asymptotes are the same. We then consider a priority queueing system with two queues. Using the earlier result, we derive an upper bound on the tail probability of the delay. Under certain assumptions on the rate function of the arrival process, we show that the upper bound is tight. We then consider a system with Markovian arrivals and numerically evaluate the delay tail probability and validate these results with simulations."
      }
    ]
  },
  {
    "tid":"227",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'MalHunter: Performing a Timely Detection on Malicious Domains via a Single DNS Query.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Domain names have been abused for illicit online activities for decades. A wealth of effort has been devoted to detect malicious domains in the past. However, these works primarily identify suspicious DNS behaviors (e.g., lookup patterns, resolution graphs) to distinguish legitimate domains from malicious ones. Whereas, these behaviors can only be observed after malicious activity is already underway, thus are often too late to prevent miscreants from reaping benefits of the attacks, delaying detection. In this paper, we propose MalHunter, a timely detection technique that determines a domain\u2019s reputation via only a single DNS query. We base it on the insight that miscreants need to host malicious domains on IPs that they control, which makes different malicious domains are commonly hosted on the same IPs and creates intrinsic associations. To capture these inherent associations, we employ a deep neural network architecture based method, thus making it possible for detecting malicious domains via only a single DNS query. We evaluate MalHunter using real-world DNS traffic collected from three large ISP networks in China over two months. Compared to previous approaches, our method significantly reduces the time delay of detection from days or weeks to approximate ten microseconds while maintaining as high detection accuracy."
      }
    ]
  },
  {
    "tid":"228",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Toward an Optimal Online Checkpoint Solution under a Two-Level HPC Checkpoint Model.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The traditional single-level checkpointing method suffers from significant overhead on large-scale platforms. Hence, multilevel checkpointing protocols have been studied extensively in recent years. The multilevel checkpoint approach allows different levels of checkpoints to be set (each with different checkpoint overheads and recovery abilities), in order to further improve the fault tolerance performance of extreme-scale HPC applications. How to optimize the checkpoint intervals for each level, however, is an extremely difficult problem. In this paper, we construct an easy-to-use two-level checkpoint model. Checkpoint level 1 deals with errors with low checkpoint/recovery overheads such as transient memory errors, while checkpoint level 2 deals with hardware crashes such as node failures. Compared with previous optimization work, our new optimal checkpoint solution offers two improvements: (1) it is an online solution without requiring knowledge of the job length in advance, and (2) it shows that periodic patterns are optimal and determines the best pattern. We evaluate the proposed solution and compare it with the most up-to-date related approaches on an extreme-scale simulation testbed constructed based on a real HPC application execution. Simulation results show that our proposed solution outperforms other optimized solutions and can improve the performance significantly in some cases. Specifically, with the new solution the wall-clock time can be reduced by up to 25.3 percent over that of other state-of-the-art approaches. Finally, a brute-force comparison with all possible patterns shows that our solution is always within $1$ percent of the best pattern in the experiments."
      }
    ]
  },
  {
    "tid":"229",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Promoting efficient communications for high-speed railway using smart collaborative networking' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The rapid development of HSR has brought great convenience to people, especially those on long-distance journeys. Existing communication technologies are quite limited in satisfying the challenging demands of HSR networks, providing poor quality of experience for HSR passengers (e.g., frequent link failures and long network delays). 5G mobile communications are expected to overcome these challengi..."
      }
    ]
  },
  {
    "tid":"230",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Data trace cache: an application specific cache architecture' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Benefits of advances in processor technology have long been held hostage to the widening processor-memory gap. Off-chip memory access latency is one of the most critical parameters limiting system performance. Caches have been used as a way of alleviating this problem by reducing the average memory access latency. The memory bottleneck assumes greater significance for high performance computer architectures with high data throughput requirements such as network processors.This paper addresses the memory bottleneck with the goal of minimizing off-chip memory demand and average memory access latency by proposing the use of small application specific compiler-visible data trace caches. We focus on tree data structures which are responsible for a significant component of the memory traffic in several applications. We have observed that tree accesses create a simple to characterize trace of memory references and propose a data trace cache design to exploit the locality of reference in these data traces.Our study reveals that data trace caches can reduce the total number of misses from 7% to 53% for accesses to rooted tree data structures as compared to a conventional cache for a variety of applications for small cache sizes (256 - 1024 bytes). Such caches are in keeping with the philosophy of victim caches, stream buffers, and pre-fetch buffers in that relatively small investments in silicon can realize substantive reduction in off-chip memory bandwidth demand.\n\n"
      }
    ]
  },
  {
    "tid":"231",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Boosting tuple propagation in multi-relational classification' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Multi-relational classification is a mining method aiming at building classifiers for the tuples in some target relation based on its own data as well as on the data possibly dispersed over other non-target relations, by exploiting the relationships among them formalized via foreign key constraints. While improving on the efficacy of the resulting classifiers, propagating data via the foreign key constraints deteriorates the scalability of the underlying algorithm. In the paper, various techniques are discussed to efficiently implement this propagation task, and hence to boost performances of current multi-relational classification algorithms. These techniques are based on suitable adaptations of state-of-the-art query optimization methods, and are conceived to be coupled with database management systems. A system prototype integrating all the techniques is illustrated, and results of experimental activity conducted on top of it are eventually discussed."
      }
    ]
  },
  {
    "tid":"232",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Character Recognition in Natural Scenes Using Convolutional Co-occurrence HOG' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recognition of characters in natural images is a challenging task due to the complex background, variations of text size and perspective distortion, etc. Traditional optical character recognition (OCR) engine cannot perform well on those unconstrained text images. A novel technique is proposed in this paper that makes use of convolutional co occurrence histogram of oriented gradient (ConvCoHOG), which is more robust and discriminative than both the histogram of oriented gradient (HOG) and the co-occurrence histogram of oriented gradients (CoHOG). In the proposed technique, a more informative feature is constructed by exhaustively extracting features from every possible image patches within character images. Experiments on two public datasets including the ICDAr 2003 Robust Reading character dataset and the Street View Text (SVT) dataset, show that our proposed character recognition technique obtains superior performance compared with state-of-the-art techniques."
      }
    ]
  },
  {
    "tid":"233",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Transductive Zero-Shot Recognition via Shared Model Space Learning.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Zero-shot Recognition (ZSR) is to learn recognition models for novel classes without labeled data. It is a challenging task and has drawn considerable attention in recent years. The basic idea is to transfer knowledge from seen classes via the shared attributes. This paper focus on the transductive ZSR, i.e., we have unlabeled data for novel classes. Instead of learning models for seen and novel classes separately as in existing works, we put forward a novel joint learning approach which learns the shared model space (SMS) for models such that the knowledge can be effectively transferred between classes using the attributes. An effective algorithm is proposed for optimization. We conduct comprehensive experiments on three benchmark datasets for ZSR. The results demonstrates that the proposed SMS can significantly outperform the state-of-the-art related approaches which validates its efficacy for the ZSR task."
      }
    ]
  },
  {
    "tid":"234",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'PRec-I-DCM3: A Parallel Framework for Fast and Accurate Large Scale Phylogeny Reconstruction' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Accurate reconstruction of phylogenetic trees very often involves solving hard optimization problems, particularly the maximum parsimony (MP) and maximum likelihood (ML) problems. Various heuristics have been devised for solving these two problems; however, they obtain good results within reasonable time only on small datasets. This has been a major impediment for large-scale phylogeny reconstruction, particularly for the effort to assemble the Tree of Life - the evolutionary relationship of all organisms on earth. Roshan et al. recently introduced Rec-I-DCM3, an efficient and accurate meta-method for solving the MP problem on large datasets of up to 14,000 taxa. Nonetheless, a drastic improvement in Rec-I-DCM3's performance is still needed in order to achieve similar (or better) accuracy on datasets at the scale of the Tree of Life. In this paper, we improve the performance of Rec-I-DCM3 via parallelization. Experimental results demonstrate that our parallel method, PRec-I-DCM3, achieves significant improvements, both in speed and accuracy, over its sequential counterpart"
      }
    ]
  },
  {
    "tid":"235",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Towards Utilization of Neurofuzzy Systems for Taxonomic Identification Using Psittacines as a Case Study.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Demonstration of the neurofuzzy application to the task of psittacine (parrot) taxonomic identification is presented in this paper. In this work, NEFCLASS-J neurofuzzy system is utilized for classification of parrot data for 141 and 183 groupings, using 68 feature points or qualities. The reported results display classification accuracies of above 95%, which is strongly tied to the setting of certain parameters of the neurofuzzy system. Rule base sizes were in the range of 1,750 to 1,950 rules."
      }
    ]
  },
  {
    "tid":"236",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Adaptive Fuzzy Control via Observer Design for Uncertain Nonlinear Systems With Unmodeled Dynamics' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, the problems of stability and tracking control for a class of large-scale nonlinear systems with unmodeled dynamics are addressed by designing the decentralized adaptive fuzzy output feedback approach. Because the dynamic surface control technique is introduced, the designed controllers can avoid the issue of \u201cexplosion of complexity,\u201d which comes from the traditional backstepping design procedure that deals with large-scale nonlinear systems with unmodeled dynamics. In addition, a reduced-order observer is designed to estimate those immeasurable states. Based on the Lyapunov stability method, it is proven that all the signals in the closed-loop system are bounded, and the system outputs track the reference signals to a small neighborhood of the origin by choosing the design parameters appropriately. The simulation examples are given to verify the effectiveness of the proposed techniques."
      }
    ]
  },
  {
    "tid":"237",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A cell outage compensation scheme based on immune algorithm in LTE networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cell outage is the total loss of radio services in the coverage area of a cell. Aiming at mitigating the coverage problems caused by a sudden outage, this paper proposed a concrete cell outage compensation scheme based on immune algorithm. Taking into account the two objectives, coverage and quality, the scheme constructs a mathematical model which adjusts the uplink target received power in surrounding cells, and generates a specific solution by adopting immune algorithm. The reported simulation results show that the proposed algorithm is able to recover a large proportion of unserved users and mitigate the performance degradation."
      }
    ]
  },
  {
    "tid":"238",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Radio Access Network Sharing in 5G: Strategies and Benefits.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In a shared radio access network, a user can be served through the network of his home operator or the network of another service operator in the sharing system. Consequently, when the home operator is unable to serve its user, and there is more than one available service operator, a selection decision must be made. The decision must consider the satisfaction of three main agents: the user, his home operator and the service operator. This paper presents a strategic algorithm for the access selection decision in a multi-operator wireless network. It is based on a cost function that combines the requirements of the user, its home operator profit and the offered QoS of the service operator. This cost function takes into account the operators\u2019 strategies for cooperation. This work focuses on the service operator strategy and proposes two strategies: a pricing strategy that consists of increasing the service cost, and a sharing strategy that consists of limiting the amount of shared resources. Simulation results prove the efficiency of the proposed algorithm and show how sharing between operators brings benefits in terms of user acceptance and profits as well. In addition, results show that the service operator strategy affects the access selection decision and the cooperation benefits; a pricing strategy can guarantee high profits for the service operator and can improve its client acceptance."
      }
    ]
  },
  {
    "tid":"239",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Approximate Analysis of Multi-Class Synchronized Closed Queueing Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Queueing networks with synchronization mechanisms are encountered in the modeling and analysis of parallel computer systems and computer/communication networks. Exact analytical solutions of such networks are not available. In particular, due to the synchronization mechanisms. These networks do not have a product-form solution. As a result approximation methods that can provide accurate estimates of the performance parameters are of high interest. The purpose of this paper is to propose such an approximation method that applies to a fairly general class of multi-class closed queueing networks with synchronization mechanisms. The method is based on the use of a product-form approximation technique. Numerical results that show that the accuracy of the method is fairly good are provided."
      }
    ]
  },
  {
    "tid":"240",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'P2P Web Search: Make It Light, Make It Fly (Demo)' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose a live demonstration of MinervaLight, a P2P Web search engine. MinervaLight combines the (previously separate) focused crawler BINGO! (to harvest Web data), the local search engine TopX, and our P2P Web search system MINERVA under one common user interface. The crawler unattendedly downloads and indexes Web data, where the scope of the focused crawl can be tailored to the thematic interest profile of the user. The result of this process is a local search index, which is used by TopX to evaluate user queries. In the background, MinervaLight continuously computes compact statistical synopses that describe a user's local search index and publishes that information to a conceptually global, but physically fully decentralized directory. MinervaLight o! ers a search interface where users can submit queries to MINERVA. Sophisticated query routing strategies are used to identify the most promising peers for each query based on the statistical synopses in the directory. The query is forwarded to those judiciously chosen peers and evaluated based on their local indexes. These results are sent back to the query initiator and merged into a single result list. We give a live demonstration of the fully functional system."
      }
    ]
  },
  {
    "tid":"241",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Structural Kernel Learning For Large Scale Multiclass Object Co-Detection' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Exploiting contextual relationships across images has recently proven key to improve object detection. The resulting object co-detection algorithms, however, fail to exploit the correlations between multiple classes and, for scalability reasons are limited to modeling object instance similarity with relatively low-dimensional hand-crafted features. Here, we address the problem of multiclass object co detection for large scale datasets. To this end, we formulate co-detection as the joint multiclass labeling of object candidates obtained in a class-independent manner. To exploit the correlations between objects, we build a fully-connected CRF on the candidates, which explicitly incorporates both geometric layout relations across object classes and similarity relations across multiple images. We then introduce a structural boosting algorithm that lets us exploit rich, high dimensional deep network features to learn object similarity within our fully-connected CRP'. Our experiments on PASCAL VOC 2007 and 2012 evidence the benefits of our approach over object detection with RCNN, single-image CRF methods and state-of-the-art co-detection algorithms."
      }
    ]
  },
  {
    "tid":"242",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Modeling Time-Varying Aggregate Interference in Cognitive Radio Systems, and Application to Primary Exclusive Zone Design' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Accurately characterizing the time-varying interference caused to the primary users is essential in ensuring a successful deployment of cognitive radios (CR). We show that the aggregate interference at the primary receiver (PU-Rx) from multiple, randomly located cognitive users (CUs) is well modeled as a shifted lognormal random process, which is more accurate than the lognormal and the Gaussian process models considered in the literature, even for a relatively dense deployment of CUs. It also compares favorably with the asymptotically exact stable and symmetric truncated stable distribution models, except at high CU densities. Our model accounts for the effect of imperfect spectrum sensing, which depends on path-loss, shadowing, and small-scale fading of the link from the primary transmitter to the CU; the interweave and underlay modes of CR operation, which determine the transmit powers of the CUs; and time-correlated shadowing and fading of the links from the CUs to the PU-Rx. It leads to expressions for the probability distribution function, level crossing rate, and average exceedance duration. The impact of cooperative spectrum sensing is also characterized. We validate the model by applying it to redesign the primary exclusive zone to account for the time-varying nature of interference."
      }
    ]
  },
  {
    "tid":"243",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Study on wearable system for daily life support using McKibben pneumatic artificial muscle' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes the basic technologies in order to develop a wearable hand assistive system for daily life support. Current prosthetics have some problems for wearability. We focus on the load caused by weight of wearable system. The actuator is one of the heaviest parts in wearable systems. Therefore we propose to use the McKibben pneumatic artificial muscle which is lightweight and compact in size. At first, we propose a new method to control air pressure of artificial muscle without pressure sensor in order to reduce the size of system. Second, we investigate variable stiffness of human finger to perform human finger dexterity and simulate it by using our proposed polyarticular tendon drive system."
      }
    ]
  },
  {
    "tid":"244",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A supervisory fuzzy neural network control system for tracking periodic inputs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A supervisory fuzzy neural network (FNN) control system is designed to track periodic reference inputs in this study. The control system is composed of a permanent magnet (PM) synchronous servo motor drive with a supervisory FNN position controller. The supervisory FNN controller comprises a supervisory controller, which is designed to stabilize the system states around a defined bound region and an FNN sliding-mode controller, which combines the advantages of the sliding-mode control with robust characteristics and the FNN with online learning ability. The theoretical and stability analyses of the supervisory FNN controller are discussed in detail. Simulation and experimental results show that the proposed control system is robust with regard to plant parameter variations and external load disturbance. Moreover, the advantages of the proposed control system are indicated in comparison with the sliding-mode control system"
      }
    ]
  },
  {
    "tid":"245",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'DXCS: an XCS system for distributed data mining' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"XCS is a flexible system for data mining due to its ability to deal with environmental changes, learn online with little prior knowledge and evolve accurate and maximally general classifiers. In this paper, we propose DXCS which is an XCS-based distributed data mining system. A MDL metric is proposed to quantify and analyze network load, and study the balance between network load and classifier accuracy in the presence of noise. The DXCS system shows promising results."
      }
    ]
  },
  {
    "tid":"246",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Non-dictionary-based Thai word segmentation using decision trees' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"For languages without word boundary delimiters, dictionaries are needed for segmenting running texts. This figure makes segmentation accuracy depend significantly on the quality of the dictionary used for analysis. If the dictionary is not sufficiently good, it will lead to a great number of unknown or unrecognized words. These unrecognized words certainly reduce segmentation accuracy. To solve such problem, we propose a method based on decision tree models. Without use of a dictionary, specific information, called syntactic attribute, is applied to identify the structure of Thai words. C4.5 is used as a tool for this purpose. Using a Thai corpus, experiment results show that our method outperforms some well-known dictionary-dependent techniques, maximum and longest matching methods, in case of no dictionary."
      }
    ]
  },
  {
    "tid":"247",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Principles For Learning Controllable Tts From Annotated And Latent Variation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"For building flexible and appealing high-quality speech synthesisers, it is desirable to be able to accommodate and reproduce fine variations in vocal expression present in natural speech. Synthesisers can enable control over such output properties by adding adjustable control parameters in parallel to their text input. If not annotated in training data, the values of these control inputs can be optimised jointly with the model parameters. We describe how this established method can be seen as approximate maximum likelihood and MAP inference in a latent variable model. This puts previous ideas of (learned) synthesiser inputs such as sentence-level control vectors on a more solid theoretical footing. We furthermore extend the method by restricting the latent variables to orthogonal subspaces via a sparse prior. This enables us to learn dimensions of variation present also within classes in coarsely annotated speech. As an example. we train an LSTM-based TTS system to learn nuances in emotional expression from a speech database annotated with seven different acted emotions. Listening tests show that our proposal successfully can synthesise speech with discernible differences in expression within each emotion, without compromising the recognisability of synthesised emotions compared to an identical system without learned nuances."
      }
    ]
  },
  {
    "tid":"248",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A low-complexity power allocation scheme for distributed wireless links in rayleigh fading channels with capacity optimization' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a distributed power allocation scheme for wireless links in Rayleigh fast fading environments is proposed. An optimization model is formulated to minimize total transmit powers with constraint conditions that outage probability of each link should be satisfied. A closed-form near optimal solution to the model is presented based on the exploration of power ratio factor at the boundary of system capacity. In terms of the scheme, each link can determine its power ratio factor independently. The inequity problem of the power allocation algorithms in incremental fashion is avoided. Meanwhile the scheme is in low computational complexity due to closed-form solution and it is more time-saving than those schemes in iterative manner. By performance evaluation, we verify our analysis on the optimization model and validate a better result of our scheme in comparison with the counterpart. That is, a larger system capacity is admissible by our method."
      }
    ]
  },
  {
    "tid":"249",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimizing i/o costs of multi-dimensional queries using bitmap indices' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Bitmap indices are efficient data structures for processing complex, multi-dimensional queries in data warehouse applications and scientific data analysis. For high-cardinality attributes, a common approach is to build bitmap indices with binning. This technique partitions the attribute values into a number of ranges, called bins, and uses bitmap vectors to represent bins (attribute ranges) rather than distinct values. In order to yield exact query answers, parts of the original data values have to be read from disk for checking against the query constraint. This process is referred to as candidate check and usually dominates the total query processing time. In this paper we study several strategies for optimizing the candidate check cost for multi-dimensional queries. We present an efficient candidate check algorithm based on attribute value distribution, query distribution as well as query selectivity with respect to each dimension. We also show that re-ordering the dimensions during query evaluation can be used to reduce I/O costs. We tested our algorithm on data with various attribute value distributions and query distributions. Our approach shows a significant improvement over traditional binning strategies for bitmap indices."
      }
    ]
  },
  {
    "tid":"250",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Joint relative position and velocity estimation for an anchorless network of mobile nodes' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Localization is a fundamental challenge for any network of nodes, in particular when the nodes are in motion and no reference nodes are available. Traditionally, the Multidimensional scaling (MDS) algorithm is employed at discrete time instances using pairwise distance measurements to find the relative node positions (with arbitrary rotation). In this paper, we present a novel framework to localize an anchorless network of mobile nodes given only time-varying inter-nodal distances. The time derivatives of the pairwise distances are used to jointly estimate the initial relative position and relative velocity of the nodes. Under linear velocity assumption for a small time duration, we show that the combination of the initial relative positions and relative velocity beget the relative motion of the nodes at discrete time instances. The proposed approach can be seen as an extension of the classical MDS, wherein Doppler measurements, if available, can be readily incorporated. We derive Cram\u00e9r Rao bounds and perform simulations to evaluate the performance of the proposed estimators. Furthermore, the computational complexity and the benefits of the proposed algorithms are also presented."
      }
    ]
  },
  {
    "tid":"251",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On adaptive modulation and power control for cognitive radios with primary user outage constraint' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We study power and rate adaptation for cognitive radios in fading channels, with the objective to maximize the average spectral efficiency of the secondary user (SU) transmitter. The average transmit power and bit error rate (BER) constraints of the SU is considered, together with either the short-term or the long-term outage constraint over the channel state of the primary user (PU). In each case, the optimal power and rate solutions are derived. Unlike previous works on power and rate adaptation for cognitive radios, in this work the cochannel interferences between the SU and PU are explicitly modeled and related to their respective average BER and outage requirements, as well as their respective power and rate variables. Numerical results are presented to provide insights into the dynamics of the power and rate solutions."
      }
    ]
  },
  {
    "tid":"252",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Acoustic features for estimation of perceptional similarity' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper describes an examination of acoustic features for the estimation of perceptional similarity between speeches. We firstly extract some acoustic features including personality from speeches of 36 persons. Secondly, we calculate each distance between extracted features using Gaussian Mixture Model (GMM) or Dynamic Time Warping (DTW), and then we sort speeches based on the physical similarity. On the other hand, there is the permutation based on the perceptional similarity which is sorted according to the subject. We evaluate the physical features by the Spearman's rank correlation coefficient with two permutations. Consequently, the results show that DTW distance with high STRAIGHT Cepstrum is an optimum feature for estimation of perceptional similarity."
      }
    ]
  },
  {
    "tid":"253",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Assessing an architecture's ability to support feature evolution.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Enabling rapid feature delivery is essential for product success and is therefore a goal of software architecture design. But how can we determine if and to what extent an architecture is \"good enough\" to support feature addition and evolution, or determine if a refactoring effort is successful in that features can be added more easily? In this paper, we contribute a concept called the Feature Space, and a formal definition of Feature Dependency, derived from a software project's revision history. We capture the dependency relations among the features of a system in a feature dependency structure matrix (FDSM), using features as first-class design elements. We also propose a Feature Decoupling Level (FDL) metric that can be used to measure the level of independence among features. Our investigation of 17 open source projects shows that files within each feature space are much more likely to be changed together, hence each feature space forms a meaningful maintainable unit that should be treated separately. The data also show that the history-based FDL is highly correlated a structure-based maintainability metric: Decoupling Level (DL). When we examine a project's evolution history, we see that if a system is well-modularized, it is more likely that features can be added independently. For shorter periods of time, however, FDL and DL may not be consistent, e.g., when the addition of new features deviates from the designed architecture or does not involve parts of the system that have architecture flaws. In such cases, FDL and FDSM can be used to monitor potential architecture degradation caused by improper feature addition.\n\n"
      }
    ]
  },
  {
    "tid":"254",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Formal Development of a Complex Information System' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we present a new model to formally represent complex information systems. This new formalism is very suitable to specify time requirements (both time consumption due to the performance of tasks and timeouts) as well as to represent data communication among different components of a system. In addition to provide the theoretical framework, we show how this formalism, based on finite state machines, has been used as the basis to develop a real information system. This exercise convinced us that a formal approach to develop complex systems can facilitate some of the development phases. In particular, the testing phase, more precisely, how to choose which tests to apply, is simplified since tests are automatically extracted from the specification."
      }
    ]
  },
  {
    "tid":"255",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Boundary image matching supporting partial denoising using time-series matching techniques.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we deal with the problem of boundary image matching which finds similar boundary images regardless of partial noise exploiting time-series matching techniques. Time-seris matching techniques make it easier to compute distances for similarity identification, and therefore it is feasible to perform boundary image matching even on a large image database. To solve this problem, we first convert all boundary images into times-series and derive partial denoising time-series. The partial denoising time-series is generated from an original time-series by removing partial noise; that is, it is obtained by changing a position of partial denoising from original time-series. We then introduce the partial denoising distance, which is the minimum distance from a query time-series to all possible partial denoising time-series generated from a data time-series, and propose partial denoising boundary image matching using the partial denoising distance as a similarity measure. Computing the partial denoising distance, however, incurs a severe computational overhead since there are a large number of partial denoising time-series to be considered. Thus, in order to improve its performance, we present a tight lower bound of the partial denoising distance and also optimize the computation of the partial denoising distance. We finally propose range and k-NN query algorithms according to a query processing method for partial denoising boundary image matching. Through extensive experiments, we show that our lower bound-based approach and the optimization method of the partial denoising distance improve search performance by up to an order of magnitude."
      }
    ]
  },
  {
    "tid":"256",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A guided tour in random intersection graphs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Random graphs, introduced by P. Erd\u0151s and A. R\u00e9nyi in 1959, still attract a huge amount of research in the communities of Theoretical Computer Science, Algorithms, Graph Theory, Discrete Mathematics and Statistical Physics. This continuing interest is due to the fact that, besides their mathematical beauty, such graphs are very important, since they can model interactions and faults in networks and also serve as typical inputs for an average case analysis of algorithms. The modeling effort concerning random graphs has to show a plethora of random graph models; some of them have quite elaborate definitions and are quite general, in the sense that they can simulate many other known distributions on graphs by carefully tuning their parameters."
      }
    ]
  },
  {
    "tid":"257",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A generic security API for symmetric key management on cryptographic devices.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Security APIs are used to define the boundary between trusted and untrusted code. The security properties of existing APIs are not always clear. In this paper, we give a new generic API for managing symmetric keys on a trusted cryptographic device. We state and prove security properties for our API. In particular, our API offers a high level of security even when the host machine is controlled by an attacker. Our API is generic in the sense that it can implement a wide variety of (symmetric key) protocols. As a proof of concept, we give an algorithm for automatically instantiating the API commands for a given key management protocol. We demonstrate the algorithm on a set of key establishment protocols from the Clark-Jacob suite."
      }
    ]
  },
  {
    "tid":"258",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Cooperation mode selection for maximizing throughput in wireless networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many works have been conducted on cooperation communication in wireless networks for recent years. Two main cooperative modes amplify-and-forward (AF) and decode-and-forward (DF) are proposed in literatures. In this paper, we present a novel cooperation mode selection method, Maximum Throughput Cooperation Mode (MTCM). MTCM selects optimal communication modes, including direct transmission, AF and DF to maximize the system throughput. In MTCM, the transmission errors are taken into consideration. The simulations are performed to validate the theoretical results. \u00a9 2011 IEEE."
      }
    ]
  },
  {
    "tid":"259",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Non-Linear Spectral Mixture Analysis Of Landsat Imagery By Means Of Neural Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Urban surfaces are highly inhomogeneous because of the high spatial and spectral diversity of man-made structures. Spectral unmixing techniques although developed to be used with hyperspectral data, are useful for assessing sub-pixel information on multispectral data as well. The large spectral variability imposes the use of multiple endmember spectral mixture analysis techniques, in which many possible mixture models are considered to produce the best fit. The use of many endmembers and mixture models result in prohibitive computational time. In this study, an artificial neural network is used to inverse the pixel spectral mixture in Landsat imagery. Endmember spectra, collected from the image were used to train the network and capture the spectral variability of man-made structures."
      }
    ]
  },
  {
    "tid":"260",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Boolean Computation Using Self-Sustaining Nonlinear Oscillators.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Self-sustaining nonlinear oscillators of practically any type can function as latches and registers if Boolean logic states are represented physically as the phase of oscillatory signals. Combinational operations on such phase-encoded logic signals can be implemented using arithmetic negation and addition followed by amplitude limiting. With these, general-purpose Boolean computation using a wide ..."
      }
    ]
  },
  {
    "tid":"261",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Comparative analysis of top-down and bottom-up methodologies for multi-agent system design' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Traditionally, top-down and bottom-up design approaches have competed with each other in Algorithmics and Software Engineering. In the top-down approach, design process starts with specifying the global system state and assuming that each component has global knowledge of the system, as in a centralized approach. The solution is then decentralized by replacing global knowledge with communication. In the bottom-up approach, on the other hand, the design starts with specifying requirements and capabilities of individual components, and the global behavior is said to emerge out of interactions among constituent components and between components and the environment. In this paper we present a comparative study of both approaches with particular emphasis on applications to multi-agent system engineering."
      }
    ]
  },
  {
    "tid":"262",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Distributed Channel Selection In Crahns With Heterogeneous Spectrum Opportunities: A Local Congestion Game Approach' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This letter investigates the problem of distributed channel selection in cognitive radio ad hoc networks (CRAHNs) with heterogeneous spectrum opportunities. Firstly, we formulate this problem as a local congestion game, which is proved to be an exact potential game. Then, we propose a spatial best response dynamic (SBRD) to rapidly achieve Nash equilibrium via local information exchange. Moreover, the potential function of the game reflects the network collision level and can be used to achieve higher throughput."
      }
    ]
  },
  {
    "tid":"263",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An empirical study of the global behavior of a structured overlay network' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Distributed applications built on top of Structured Overlay Networks (SONs) operate based on certain assurances from the underlying Peer-to-Peer network. Such applications continue to increase in scale, becoming more complex and difficult to manage. The situation becomes worse if the behavior of underlying SON is non-deterministic or even unknown for a given scenario. This implies non-trivial questions: what behavior should the application layer expect from the underlying SON in a given scenario? Under what conditions should a SON exhibit resiliency against an extremely hostile environment? Ideally, the behavior of a complex system such as a SON needs to be defined for every possible operating condition. Existing literature lacks a systematic and in-depth study of the global behavior of a SON. This work is a step towards answering those questions, which starts by proposing an organization of the global operating space of a SON, also defines the term \u201cbehavior\u201d, with respect to a SON. In order to conduct the experimental study, an existing ring-based SON, namely Beernet, is chosen as a representative. As the entire operating space of a SON is extremely large, due to space limitation, this paper presents the first results of our investigation, the behavior of Beernet along the dimension of Churn. The study assesses behaviors like key availability, updates, replica management, and failed transactions, as a function of churn up to 100% node turnover per time unit. The result shows that, continuous injection of extremely high churn causes the ring to be dissolved, creating isolation of peers. However, at such a high node turnover of 100% per 5s, there are instances, where the % of failed transactions didn't reach 100%, especially in cases, when the join events dominate failures during initial period."
      }
    ]
  },
  {
    "tid":"264",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Curvature manipulation techniques in redirection using haptic cues' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes a method for improving the effects of redirected walking (RDW) by using haptic cues, particularly for the discrimination of path curvature. Some research has shown that users can be redirected on a circular arc with a radius of at least 22 m without being able to detect the inconsistency by showing a straight path in the virtual world. However, this is still too large to enable the presentation of a demonstration in a restricted space. We develop an RDW system, which displays a visual representation of a flat wall and users virtually walk straight along it although, in reality, users walk along a convex surface wall with touching it. Using this system, we conduct an experiment, and the results show that our method reduced the amount of perceived curvature in RDW down to 62% as compared to an only visual method."
      }
    ]
  },
  {
    "tid":"265",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Trapdoors for lattices: simpler, tighter, faster, smaller' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We give new methods for generating and using \"strong trapdoors\" in cryptographic lattices, which are simultaneously simple, efficient, easy to implement (even in parallel), and asymptotically optimal with very small hidden constants. Our methods involve a new kind of trapdoor, and include specialized algorithms for inverting LWE, randomly sampling SIS preimages, and securely delegating trapdoors. These tasks were previously the main bottleneck for a wide range of cryptographic schemes, and our techniques substantially improve upon the prior ones, both in terms of practical performance and quality of the produced outputs. Moreover, the simple structure of the new trapdoor and associated algorithms can be exposed in applications, leading to further simplifications and efficiency improvements. We exemplify the applicability of our methods with new digital signature schemes and CCA-secure encryption schemes, which have better efficiency and security than the previously known lattice-based constructions."
      }
    ]
  },
  {
    "tid":"266",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Capstone Design Project for a Ubiquitous Sensor Network' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Ubiquitous sensor networks (USNs) are one of the most important areas, and thus range of relevant research results have been introduced. Hands-on experiments of a ubiquitous sensor network prototype thus give a great opportunity for students to understand necessary theory and technology, covering embedded system design, network theory, RF (radio frequency) signal processing, and so on, which well exceed the scope of undergraduate study. However, implementation of only a part of functions using well-designed evaluation platforms is not generally appropriate to fulfill students' motivation and achievement. In this paper, we introduce a successful example of an undergraduate capstone design project that designs and implements ubiquitous sensor network base station controller and monitoring program. Our prerequisite courses provide substantial background for embedded system hardware design and implementation, wireless network, and embedded software, so that the students do not suffer from lack of implementation techniques rather than their creativity and challenges for the design goal. In addition, the capstone design project course has one-to-one mentor-mentee relation with companies, specifically with Samsung Data System for this project. As a result, the USN monitoring system is awarded Golden Prize from Korean Ministry of Commerce, Industry and Energy in 2006, through a national- wide competition."
      }
    ]
  },
  {
    "tid":"267",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Designing DPA Resistant Circuits Using BDD Architecture and Bottom Pre-charge Logic' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Differential power analysis (DPA) attacks are the most powerful side channel attacks against cryptographic systems. In this work, a reduced ordered binary decision diagram (ROBDD) based dual rail circuit for a basic DPA resistant cell has been designed. The specialty of this cell is that the overall input current of the cell is invariant to the input combinations of data bits applied to the cell. For the first time, bottom pre-charge logic is used in the design of such a cell. The ROBDD based design minimizes both area and early propagation effect. A number of logic functions including AND, OR, XOR, NOT, NAND, NOR and also an adder, all based on the basic cell, have then been designed in a hierarchical manner. Experimental results demonstrate DPA resistance of the circuits (for example an adder) developed using this cell, outperforming other competing design with respect to peak power variance."
      }
    ]
  },
  {
    "tid":"268",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Performance Analysis of Network I/O Workloads in Virtualized Data Centers' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Server consolidation and application consolidation through virtualization are key performance optimizations in cloud-based service delivery industry. In this paper, we argue that it is important for both cloud consumers and cloud providers to understand the various factors that may have significant impact on the performance of applications running in a virtualized cloud. This paper presents an extensive performance study of network I/O workloads in a virtualized cloud environment. We first show that current implementation of virtual machine monitor (VMM) does not provide sufficient performance isolation to guarantee the effectiveness of resource sharing across multiple virtual machine instances (VMs) running on a single physical host machine, especially when applications running on neighboring VMs are competing for computing and communication resources. Then we study a set of representative workloads in cloud-based data centers, which compete for either CPU or network I/O resources, and present the detailed analysis on different factors that can impact the throughput performance and resource sharing effectiveness. For example, we analyze the cost and the benefit of running idle VM instances on a physical host where some applications are hosted concurrently. We also present an in-depth discussion on the performance impact of colocating applications that compete for either CPU or network I/O resources. Finally, we analyze the impact of different CPU resource scheduling strategies and different workload rates on the performance of applications running on different VMs hosted by the same physical machine."
      }
    ]
  },
  {
    "tid":"269",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multi-source Information Gain for Random Forest: An Application to CT Image Prediction from MRI Data' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Random forest has been widely recognized as one of the most powerful learning-based predictors in literature, with a broad range of applications in medical imaging. Notable efforts have been focused on enhancing the algorithm in multiple facets. In this paper, we present an original concept of multi-source information gain that escapes from the conventional notion inherent to random forest. We propose the idea of characterizing information gain in the training process by utilizing multiple beneficial sources of information, instead of the sole governing of prediction targets as conventionally known. We suggest the use of location and input image patches as the secondary sources of information for guiding the splitting process in random forest, and experiment on the challenging task of predicting CT images from MRI data. The experimentation is thoroughly analyzed in two datasets, i.e., human brain and prostate, with its performance further validated with the integration of auto-context model. Results prove that the multi-source information gain concept effectively helps better guide the training process with consistent improvement in prediction accuracy."
      }
    ]
  },
  {
    "tid":"270",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Survey Of Visual Traffic Surveillance Using Spatio-Temporal Analysis And Mining' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The focus of this survey is on spatio-temporal data mining and database retrieval for visual traffic surveillance systems. In many traffic surveillance applications, such as incident detection, abnormal events detection, vehicle speed estimation, and traffic volume estimation, the data used for reasoning is really in the form of spatio-temporal data (e.g. vehicle trajectories). How to effectively analyze these spatio-temporal data to automatically find its inherent characteristics for different visual traffic surveillance applications has been of great interest. Examples of spatio-temporal patterns extracted from traffic surveillance videos include, but are not limited to, sudden stops, harsh turns, speeding, and collisions. To meet the different needs of various traffic surveillance applications, several application- or event-specific models have been proposed in the literature. This paper provides a survey of different models and data mining algorithms to cover state of the art in spatio-temporal modelling, spatio-temporal data mining, and spatio-temporal retrieval for traffic surveillance video databases. In addition, the database model issues and challenges for traffic surveillance videos are also discussed in this survey."
      }
    ]
  },
  {
    "tid":"271",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'GPUTENSOR: Efficient tensor factorization for context-aware recommendations.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recommendation systems play an important role in many practical applications that help users manage information and provide personalized recommendations. The context in which a choice is made is an important factor for recommendation systems. Recently, researchers extended the classical matrix factorization to enable generic integration of contextual information by modeling the relevant data as a tensor. However, current tensor factorization methods have two important limitations: (1) computing costs can be very high in practice and (2) they can only be applied to static conditions. In this paper, we propose the GPUTENSOR, a parallel tensor factorization algorithm, to accelerate the tensor factorization of large datasets to support efficient context-aware recommendations. The basic idea of this algorithm is to partition a tensor into smaller blocks and then exploit the inherent parallelism and high memory bandwidth of Graphics Processing Units (GPU) to perform tensor related operations in parallel. Furthermore, based on the observation that contextual information changes dynamically and frequently, we present GPUTENSOR+, an effective incremental method that models the apparent changes of a tensor by adaptively updating its previously factorized components instead of recomputing them on the whole dataset every time data are changed. Experiments indicate that the proposed methods can achieve significant timesavings without a significant loss in accuracy. As an important data mining tool, these methods can be used a potential basis for many other interesting tensor applications, such as clustering, trend detection, social network analysis, and latent concept discovery."
      }
    ]
  },
  {
    "tid":"272",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Understanding and Mitigating Packet Corruption in Data Center Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We take a comprehensive look at packet corruption in data center networks, which leads to packet losses and application performance degradation. By studying 350K links across 15 production data centers, we find that the extent of corruption losses is significant and that its characteristics differ markedly from congestion losses. Corruption impacts fewer links than congestion, but imposes a heavier loss rate; and unlike congestion, corruption rate on a link is stable over time and is not correlated with its utilization. Based on these observations, we developed CorrOpt, a system to mitigate corruption. To minimize corruption losses, it intelligently selects which corrupting links can be safely disabled, while ensuring that each top-of-rack switch has a minimum number of paths to reach other switches. CorrOpt also recommends specific actions (e.g., replace cables, clean connectors) to repair disabled links, based on our analysis of common symptoms of different root causes of corruption. Our recommendation engine has been deployed in over seventy data centers of a large cloud provider. Our analysis shows that, compared to current state of the art, CorrOpt can reduce corruption losses by three to six orders of magnitude and improve repair accuracy by 60%."
      }
    ]
  },
  {
    "tid":"273",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Benefits Of Topology Aware Mapping For Mesh Interconnects' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The fastest supercomputers today such as Blue Gene/L, Blue Gene/P, Cray XT3 and XT4 are connected by a three-dimensional torus/mesh interconnect. Applications running on these machines can benefit from topology-awareness while mapping tasks to processors at runtime. By co-locating communicating tasks on nearby processors, the distance traveled by messages and hence the communication traffic can be minimized, thereby reducing communication latency and contention on the network. This paper describes preliminary work utilizing this technique and performance improvements resulting from it in the context of a n-dimensional k-point stencil program. It shows that even for simple benchmarks, topology-aware mapping can have a significant impact on performance. Automated topology-aware mapping by the runtime using similar ideas can relieve the application writer from this burden and result in better performance. Preliminary work towards achieving this for a molecular dynamics application, NAMD, is also presented. Results on up to 32,768 processors of IBM's Blue Gene/L, 4,096 processors of IBM's Blue Gene/P and 2,048 processors of Cray's XT3 support the ideas discussed in the paper."
      }
    ]
  },
  {
    "tid":"274",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Planar Batting Under Shape, Pose, And Impact Uncertainty' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper explores the planning and control of a manipulation task accomplished in conditions of high uncertainty. Statistical techniques, like particle filters, provide a framework for expressing the uncertainty and partial observability of the real world and taking actions to reduce them. We explore a classic manipulation problem of planar batting, but with a new twist of shape, pose and impact uncertainty. We demonstrate a technique for characterizing and reducing this uncertainty using a particle filter coupled with a lookahead planner that maximizes information gain. We show that a twostep planner that first acts for information gain and then acts to maximize the expectation of achieving a desired goal is effective at managing shape, pose and impact uncertainty."
      }
    ]
  },
  {
    "tid":"275",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Quality factory and quality notification service in data warehouse' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many modern organizations operate in a dynamic and large-scaled web-based environment that involves critical and frequent interactions with external organizations. Within this context, the associated data warehousing environment is often dynamic and evolutionary, with sometimes frequent changes in the quality of various resources (e.g. source systems, processes, metadata), and also in the users' quality requirements. There is thus the need to not only capture quality changes on-the-fly, but also provide automated quality notifications to relevant end users. Managing data warehousing systems in such environment imposes many new challenges for DW design, management and maintenance. In this paper, we propose an extended data warehousing systems architecture that incorporates and extends the concepts of the Quality Factory (QF) and the Quality Notification Service (QNS) that were previously presented in the Cooperative Information Systems (CIS) literature."
      }
    ]
  },
  {
    "tid":"276",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Using structural similarity for clustering XML documents' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we describe a method for clustering XML documents. Its goal is to group documents sharing similar structures. Our approach is two-step. We first automatically extract the structure from each XML document to be classified. This extracted structure is then used as a representation model to classify the corresponding XML document. The idea behind the clustering is that if XML documents share similar structures, they are more likely to correspond to the structural part of the same query. Finally, for the experimentation purpose, we tested our algorithms on both real (ACM SIGMOD Record corpus) and synthetic data. The results clearly demonstrate the interest of our approach."
      }
    ]
  },
  {
    "tid":"277",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Be Appropriate And Funny: Automatic Entity Morph Encoding' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Internet users are keen on creating different kinds of morphs to avoid censorship, express strong sentiment or humor. For example, in Chinese social media, users often use the entity morph \"(sic) (Instant Noodles)\" to refer to \"(sic) (Zhou Yongkang)\" because it shares one character \"(sic) (Kang)\" with the well-known brand of instant noodles \"(sic) (Master Kang)\". We developed a wide variety of novel approaches to automatically encode proper and interesting morphs, which can effectively pass decoding tests(1)."
      }
    ]
  },
  {
    "tid":"278",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Unraveling the BitTorrent Ecosystem' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"BitTorrent is the most successful open Internet application for content distribution. Despite its importance, both in terms of its footprint in the Internet and the influence it has on emerging P2P applications, the BitTorrent Ecosystem is only partially understood. We seek to provide a nearly complete picture of the entire public BitTorrent Ecosystem. To this end, we crawl five of the most popular torrent-discovery sites over a ine-month period, identifying all of 4.6 million and 38,996 trackers that the sites reference. We also develop a high-performance tracker crawler, and over a narrow window of 12 hours, crawl essentially all of the public Ecosystem's trackers, obtaining peer lists for all referenced torrents. Complementing the torrent-discovery site and tracker crawling, we further crawl Azureus and Mainline DHTs for a random sample of torrents. Our resulting measurement data are more than an order of magnitude larger (in terms of number of torrents, trackers, or peers) than any earlier study. Using this extensive data set, we study in-depth the Ecosystem's torrent-discovery, tracker, peer, user behavior, and content landscapes. For peer statistics, the analysis is based on one typical snapshot obtained over 12 hours. We further analyze the fragility of the Ecosystem upon the removal of its most important tracker service."
      }
    ]
  },
  {
    "tid":"279",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Realization of FIR Digital Filters Based on Stochastic/Binary Hybrid Computation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recently, some attempts have been made to apply stochastic computation to realization of Finite Impulse Response (FIR) digital filters. Such new FIR filter realizations lead to significant reduction of hardware complexity over the conventional filter realizations based on binary computation. However, the stochastic FIR filters suffer from lower computational accuracy than the FIR filters based on binary computation. This paper presents a new method for realization of stochastic FIR filters to improve computational accuracy. In the proposed method, multipliers are realized using stochastic computation but adders are realized using binary computation. Evaluation results demonstrate that our method achieves a 7dB improvement in stopband attenuation."
      }
    ]
  },
  {
    "tid":"280",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A multi-objective hyper-heuristic based on choice function' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Hyper-heuristics are emerging methodologies that perform a search over the space of heuristics in an attempt to solve difficult computational optimization problems. We present a learning selection choice function based hyper-heuristic to solve multi-objective optimization problems. This high level approach controls and combines the strengths of three well-known multi-objective evolutionary algorithms (i.e. NSGAII, SPEA2 and MOGA), utilizing them as the low level heuristics. The performance of the proposed learning hyper-heuristic is investigated on the Walking Fish Group test suite which is a common benchmark for multi-objective optimization. Additionally, the proposed hyper-heuristic is applied to the vehicle crashworthiness design problem as a real-world multi-objective problem. The experimental results demonstrate the effectiveness of the hyper-heuristic approach when compared to the performance of each low level heuristic run on its own, as well as being compared to other approaches including an adaptive multi-method search, namely AMALGAM."
      }
    ]
  },
  {
    "tid":"281",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Metadata update performance in file systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Structural changes, such as file creation and block allocation, have consistently been identified as file system performance problems in many user environments. We compare several implementations that maintain metadata integrity in the event of a system failure but do not require changes to the on-disk structures. In one set of schemes, the file system uses asynchronous writes and passes ordering requirements to the disk scheduler. These scheduler-enforced ordering schemes outperform the conventional approach (synchronous writes) by more than 30 percent for metadata update intensive benchmarks, but are suboptimal mainly due to their inability to safely use delayed writes when ordering is required. We therefore introduce soft updates, an implementation that asymptotically approaches memory-based file system performance (within 5 percent) while providing stronger integrity and security guarantees than most UNIX file systems. For metadata update in-tensive benchmarks, this improves performance by more than a factor of two when compared to the conventional approach."
      }
    ]
  },
  {
    "tid":"282",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fast Video Stitching For Aerially Captured Hd Videos' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Videos captured from the air by flying devices like Unmanned Aerial Vehicles (UAVs) have great application prospects in many fields such as journalism, art, military and public security. Due to the dificulties such as vibration, needing for speed and high resolution and so on, it is non-trivial to apply traditional static image stitching algorithms to flying cameras. To this end, we propose a real-time video stitching system which is capable to stitch high definition (HD) videos captured by mobile aerial devices. In our work, we use scale invariant information to speed up the feature point extraction."
      }
    ]
  },
  {
    "tid":"283",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient delivery of MBMS multicast traffic over HSDPA' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Multimedia Broadcast/Multicast Service (MBMS) and High-Speed Downlink Packet Access (HSDPA) are two key technologies that constitute a significant step towards the Mobile Broadband. MBMS was introduced by the third Generation Partnership Project (3GPP) in order to support broadcast and multicast communication over wireless networks and in particular over Universal Mobile Telecommunications System (UMTS). Concurrently, HSDPA aims to ensure the transmission of high peak data rates and increase system capacity. In this paper we evaluate the performance of MBMS multicast transmission over the premier transport channel used in HSDPA, named High Speed-Downlink Shared Channel (HS-DSCH). Due to the fact that downlink transmission power is the scarcest resource in UMTS networks, the evaluation is performed through an analytical dimensioning of the MBMS downlink transmission power. Furthermore, the impacts of power allocation on MBMS capacity are investigated."
      }
    ]
  },
  {
    "tid":"284",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Privacy-Preserving Image Denoising From External Cloud Databases' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Along with the rapid advancement of digital image processing technology, image denoising remains a fundamental task, which aims to recover the original image from its noisy observation. With the explosive growth of images on the Internet, one recent trend is to seek high quality similar patches at cloud image databases and harness rich redundancy therein for promising denoising performance. Despite the well-understood benefits, such a cloud-based denoising paradigm would undesirably raise security and privacy issues, especially for privacy-sensitive image data sets. In this paper, we initiate the first endeavor toward privacy-preserving image denoising from external cloud databases. Our design enables the cloud hosting encrypted databases to provide secure query-based image denoising services. Considering that image denoising intrinsically demands high quality similar image patches, our design builds upon recent advancements on secure similarity search, Yao's garbled circuits, and image denoising operations, where each is used at a different phase of the design for the best performance. We formally analyze the security strengths. Extensive experiments over real-world data sets demonstrate that our design achieves the denoising quality close to the optimal performance in plaintext."
      }
    ]
  },
  {
    "tid":"285",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Revocable and Decentralized Attribute-Based Encryption.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a revocable and decentralized attribute-based encryption (ABE) system that splits the task of decryption key generation across multiple attribute authorities (AAs) without requiring any central party such that it achieves attribute revocation by simply stopping updating of the corresponding private key. In our system, a party can easily behave as an AA by creating a publi..."
      }
    ]
  },
  {
    "tid":"286",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Making recommendations by integrating information from multiple social networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"It is becoming a common practice to use recommendation systems to serve users of web-based platforms such as social networking platforms, review web-sites, and e-commerce web-sites. Each platform produces recommendations by capturing, maintaining and analyzing data related to its users and their behavior. However, people generally use different web-based platforms for different purposes. Thus, each platform captures its own data which may reflect certain aspects related to its users. Integrating data from multiple platforms may widen the perspective of the analysis and may help in modeling users more effectively. Motivated by this, we developed a recommendation framework which integrates data collected from multiple platforms. For this purpose, we collected and anonymized datasets which contain information from several social networking and social media platforms, namely BlogCatalog, Twitter, Flickr, Facebook, YouTube and LastFm. The collected and integrated data forms a consolidated repository that may become a valuable source for researchers and practitioners. We implemented a number of recommendation methodologies to observe their performance for various cases which involve using single versus multiple features from a single source versus multiple sources. The conducted experiments have shown that using multiple features from multiple sources is expected to produce a more concrete and wider perspective of user\u2019s behavior and preferences. This leads to improved recommendation outcome."
      }
    ]
  },
  {
    "tid":"287",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Exploiting Unintended Feature Leakage in Collaborative Learning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Collaborative machine learning and related techniques such as federated learning allow multiple participants, each with his own training dataset, to build a joint model by training locally and periodically exchanging model updates. We demonstrate that these updates leak unintended information about participants' training data and develop passive and active inference attacks to exploit this leakage. First, we show that an adversarial participant can infer the presence of exact data points -- for example, specific locations -- in others' training data (i.e., membership inference). Then, we show how this adversary can infer properties that hold only for a subset of the training data and are independent of the properties that the joint model aims to capture. For example, he can infer when a specific person first appears in the photos used to train a binary gender classifier. We evaluate our attacks on a variety of tasks, datasets, and learning configurations, analyze their limitations, and discuss possible defenses."
      }
    ]
  },
  {
    "tid":"288",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'DCT Based blind audio watermarking scheme' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"During these recent years, different digital audio media are available due to internet and audio processing techniques. This is beneficial to our daily life but it brings the problem of ownership protection. As a solution, audio watermarking technique has been quickly developed. It consists of embedding information into digital audio data. This paper proposes an audio watermarking scheme operating in frequency domain using Discrete Cosine transforms (DCT). We take the advantage of hiding the mark in the frequency domain to guarantee robustness. To increase robustness of our schemes, the watermark is refined by the Hamming error correcting code. We study the band used to hide the watermark bits according essentially to the effects of the MP3 compression on the watermarked audio signal. To assure watermark embedding and extraction, we use neural network architecture. Experimental results show that the watermark is imperceptible and the algorithm is robust to many attacks like MP3 compression with several compression rates and various Stirmark attacks. Furthermore, the watermark can be blind extracted without needing the original audio."
      }
    ]
  },
  {
    "tid":"289",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Transfer Learning and Representation Discovery in Intelligent Tutoring Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We describe a novel framework developed for transfer learning within reinforcement learning (RL) problems. Then we exhibit how this framework can be extended to intelligent tutoring systems (ITS). We compose an algorithm that automatically constructs a graphical representation based on the transfer framework. We evaluate this on a real-world ITS example and show that the model constructed by our approach performs better than previously published results. We propose that transfer learning is a useful and related area to explore for furthering intelligent tutoring systems."
      }
    ]
  },
  {
    "tid":"290",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Development of an optimization model for water resources systemsplanning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A number of inexact fuzzy programming methods have been developed for the planning of water-resources-management systems under uncertainty. However, most of them do not allow the parameters in the objective and constraints of a programming problem to be functional intervals (i.e., the lower and upper bounds of the intervals are functions of impact factors). In this study, an interval fuzzy bi-infinite De Novo programming (IFBDP) method is developed in response to the above concern. A case study is also conducted; the solutions are then compared with those obtained from inexact De Novo programming (IDNP) and interval-fuzzy De Novo programming (IFDNP) that takes no account of bi-infinite programming. It is indicated that the IFBDP method can generate more reliable solutions with a lower risk of system failure due to the possible constraints violation and provide a more flexible management planning since the budgets availability can be adjusted with the variations in water price. These solutions are more flexible than those identified through IFDNP since the tolerance intervals are introduced to measure the level of constraints satisfaction. Moreover, it can be used for analyzing various scenarios that are associated with different levels of economic consequences under uncertainty."
      }
    ]
  },
  {
    "tid":"291",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Detecting malicious packet dropping in the presence of collisions and channel errors in wireless ad hoc networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Detecting malicious packet dropping is important in ad hoc networks to combat a variety of security attacks such as blackhole, greyhole, and wormhole attacks. We consider the detection of malicious packet drops in the presence of collisions and channel errors and describe a method to distinguish between these types. We present a simple analytical model for packet loss that helps a monitoring node to detect malicious packet dropping attacks. The model is analyzed and evaluated using simulations. The results show that it is possible to detect malicious packet drops in the presence of collisions and channel errors."
      }
    ]
  },
  {
    "tid":"292",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Performance and scalability of self-organizing hierarchical ad hoc wireless networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A novel self-organizing hierarchical architecture is proposed for improving the performance and scalability properties of ad hoc wireless networks. This paper presents the results of a simulation study of the performance and throughput capacity of a specific three-tier hierarchical ad hoc network with 802.11 radios, forwarding nodes and access points. The performance of the proposed hierarchical network is evaluated for two well-known classes of ad hoc routing protocols: dynamic source routing (DSR) and ad hoc on-demand distance vector (AODV), and compared with that of a conventional \"flat\" ad hoc network. The results for an example sensor network scenario have shown significant capacity increases with the hierarchical architecture for both DSR and AODV cases. Modifications to ad hoc routing metrics for energy efficiency are also considered. The scalability properties of the three-tier hierarchy are studied further in terms of the achievable system capacity as a function of the relative densities of sensor nodes, forwarding nodes and access points. It is shown that the capacity of the three-tier hierarchical network scales well when the number of forwarding nodes and access points are increased in the right proportions."
      }
    ]
  },
  {
    "tid":"293",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Collaboration technology in teams and organizations: Introduction to the special issue.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Information technology advances in recent years have facilitated new forms of information and knowledge sharing. Teams and organizations are increasingly exploring and adopting new technologies to support collaborative work. Such technologies range from collaboration and communication technologies that connect members of virtual teams across national and international boundaries to social media technologies that allow teams and organizations to disseminate and gather information from within and outside their institutional boundaries. The widespread availability of smart phones has given whole societies opportunities to participate in large-scale sensemaking, problem solving, and efforts to organize collaborative action. Some of the more popular modern collaboration technologies that have been widely employed in organizations include wikis, social networks, crowdsourcing, tagging plugins, and mashups (Andriole 2010; Van Osch et al. 2015; Go and You 2016). Awiki is a web-based application that allows users to collaboratively develop and modify web pages. The most wellknown example of a wiki is Wikipedia, the online encyclopedia that has been developed and maintained by thousands of volunteers for over 15 years. A wiki system can facilitate knowledge acquisition and support collaboration and communication among members of an organization. Specifically, wikis have the potential to serve as dynamically evolving organizational knowledge repositories by harvesting fragmented knowledge contributed by users. Compared to traditional knowledge management systems (KMSs), wiki systems put less emphasis on centralized control, strict discipline, and extensive monitoring, while users do not need to possess significant technical expertise to participate in the contribution process (Hasan and Pfaff 2006). Experiences with the organizational application of wikis have shown that although wikis require constant contributions to ensure sustainability, they provide a low-cost solution to knowledge management in terms of the creation and deployment of knowledge repositories (Almeida and Rocha 2011; Standing and Kiniti 2011; Arazy and Gellatly 2012). Furthermore, studies suggest that wikis are more effective in facilitating tacit and informational knowledge sharing among organization members than traditional KMSs (Cress and Kimmerle 2008; Standing and Kiniti 2011). Organizations can employ social network technologies both internally and externally. Using internal social networks, individuals can shorten their learning cycle by exploring shared experiences of colleagues (Lin and Hsueh 2006). * Kewen Wu kew259@mail.usask.ca"
      }
    ]
  },
  {
    "tid":"294",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'RIO: A denial of service experimentation platform in a Future Internet Testbed' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Denial of service attacks (DoS) present a serious threat to the security of communication network infrastructure. Whereas the dissemination of new DoS attacks is fast and dynamic, the development of countermeasures to these attacks requires the construction and validation of a test environment that accurately reproduces Internet behavioral patterns. The construction of this environment often requires more time investment from security professionals than the time spent with the development of associated attack countermeasures. This work introduces RIO (Resource Orchestration Infrastructure) platform. RIO platform enables security professional a realistic and flexible approach to DoS experimentation creation and setup. This platform automates the creation and setup of virtual machines, virtual switches, network links, attackers, experimental data collection and experiment execution, based on a single document defining the experiment in a descriptive language. The proposal efficacy is supported by RIO platform ease of use and time reduction towards the acquisition of experimental results."
      }
    ]
  },
  {
    "tid":"295",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Recurrent Spatial Transformer Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"  We integrate the recently proposed spatial transformer network (SPN) [Jaderberg et. al 2015] into a recurrent neural network (RNN) to form an RNN-SPN model. We use the RNN-SPN to classify digits in cluttered MNIST sequences. The proposed model achieves a single digit error of 1.5% compared to 2.9% for a convolutional networks and 2.0% for convolutional networks with SPN layers. The SPN outputs a zoomed, rotated and skewed version of the input image. We investigate different down-sampling factors (ratio of pixel in input and output) for the SPN and show that the RNN-SPN model is able to down-sample the input images without deteriorating performance. The down-sampling in RNN-SPN can be thought of as adaptive down-sampling that minimizes the information loss in the regions of interest. We attribute the superior performance of the RNN-SPN to the fact that it can attend to a sequence of regions of interest. "
      }
    ]
  },
  {
    "tid":"296",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Further progress in meeting recognition: the ICSI-SRI spring 2005 speech-to-text evaluation system' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We describe the development of our speech recognition system for the National Institute of Standards and Technology (NIST) Spring 2005 Meeting Rich Transcription (RT-05S) evaluation, highlighting improvements made since last year [1]. The system is based on the SRI-ICSI-UW RT-04F conversational telephone speech (CTS) recognition system, with meeting-adapted models and various audio preprocessing steps. This year's system features better delay-sum processing of distant microphone channels and energy-based crosstalk suppression for close-talking microphones. Acoustic modeling is improved by virtue of various enhancements to the background (CTS) models, including added training data, decision-tree based state tying, and the inclusion of discriminatively trained phone posterior features estimated by multilayer perceptrons. In particular, we make use of adaptation of both acoustic models and MLP features to the meeting domain. For distant microphone recognition we obtained considerable gains by combining and cross-adapting narrow-band (telephone) acoustic models with broadband (broadcast news) models. Language models (LMs) were improved with the inclusion of new meeting and web data. In spite of a lack of training data, we created effective LMs for the CHIL lecture domain. Results are reported on RT-04S and RT-05S meeting data. Measured on RT-04S conference data, we achieved an overall improvement of 17% relative in both MDM and IHM conditions compared to last year's evaluation system. Results on lecture data are comparable to the best reported results for that task."
      }
    ]
  },
  {
    "tid":"297",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On the SIR Meta Distribution for Poisson Networks With Interference Cancellation.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This letter presents a theoretical framework for the analysis of the meta distribution of the SIR for Poisson networks with interference cancellation (IC) enabled at the receivers, which gives deep insight into the network performance on a link-wise basis. A simple but insightful IC model named C-IC is studied for which the exact bth moment of the meta distribution and its beta distribution approximation and some useful bounds are validated. The conditions for the mean local delay to be finite are also stated. The results show that IC improves the performance not only in terms of the mean but also in terms of the variance of the meta distribution."
      }
    ]
  },
  {
    "tid":"298",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Welfare of Sequential Allocation Mechanisms for Indivisible Goods.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Sequential allocation is a simple and attractive mechanism for the allocation of indivisible goods used in a number of real world settings. In sequential allocation, agents pick items according to a policy, the order in which agents take turns. Sequential allocation will return an allocation which is Pareto efficient - no agent can do better without others doing worse. However, sequential allocation may not return the outcome that optimizes the social welfare. We consider therefore the relationship between the welfare and the efficiency of the allocations returned by sequential allocation mechanisms. We then study some simple computational questions about what welfare is possible or necessary depending on the choice of policy. Over half the problems we study turn out to be tractable, and we give polynomial time algorithms to compute them. We also consider a novel control problem in which the Chair chooses a policy to improve social welfare. Again, many of the control problems we study turn out to be tractable, and our results give polynomial time algorithms. In this case, tractability is a good thing so that the Chair can improve the social welfare of the allocation."
      }
    ]
  },
  {
    "tid":"299",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A multi-scale learning framework for visual categorization' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Spatial pyramid matching has recently become a promising technique for image classification. Despite its success and popularity, no prior work has tackled the problem of learning the optimal spatial pyramid representation for the given image data and the associated object category. We propose a Multiple Scale Learning (MSL) framework to learn the best weights for each scale in the pyramid. Our MSL algorithm would produce class-specific spatial pyramid image representations and thus provide improved recognition performance. We approach the MSL problem as solving a multiple kernel learning (MKL) task, which defines the optimal combination of base kernels constructed at different pyramid levels. A wide range of experiments on Oxford flower and Caltech- 101 datasets are conducted, including the use of state-of-the-art feature encoding and pooling strategies. Finally, excellent empirical results reported on both datasets validate the feasibility of our proposed method."
      }
    ]
  },
  {
    "tid":"300",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Tracking endocardial boundary and motion via graph cut distribution matching and multiple model filtering' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Tracking the left ventricular (LV) endocardial boundary and motion from cardiac magnetic resonance (MR) images is difficult because of low contrast and photometric similarities between the heart wall and papillary muscles within the LV cavity This study investigates the problem via Graph Cut Distribution Matching (GCDM) and Interacting Multiple Model (IMM) smoothing GCDM yields initial frame segmentations by keeping the same photometric/geometric distribution of the cavity over cardiac cycles, whereas IMM constrains the results with prior knowledge of temporal consistency Incorporation of prior knowledge that characterizes the dynamic behavior of the LV enhances the accuracy of both motion estimation and segmentation However, accurately characterizing the behavior using a single Markovian model is not sufficient due to substantial variability in heart motion Moreover, dynamic behaviors of normal and abnormal hearts are very different This study introduces multiple models, each corresponding to a different phase of the LV dynamics The IMM, an effective estimation algorithm for Markovian switching systems, yields the state estimate of endocardial points as well as the model probability that indicates the most-likely model The proposed method is evaluated quantitatively by comparison with independent manual segmentations over 2280 images acquired from 20 subjects, which demonstrated competitive results in comparisons with a recent method."
      }
    ]
  },
  {
    "tid":"301",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Rtoc: A Rt-Linux Based Open Robot Controller' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"An open robot control system pursues easy extension, flexible reconfiguration, facile portability and jointless interoperation. Therefore, the system elements from multidisciplinary areas can be integrated and reconfigured easily in such a system. Also the system modules can be ported flexibly. In this paper, a Rt-linux based open robot controller (RTOC) is investigated. A reference model for robot controlling is proposed, in which hardware platform, operating system module and application modules are included. Then for the implementation of RTOC, two critical implementation problems-layered architecture and the intra-layer interfaces are discussed on the base of its reference model. The RTOC openness is also analyzed. Consequently, the proposed RTOC is applied to an industrial arc welding robot."
      }
    ]
  },
  {
    "tid":"302",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Web-Based Motivation-Supporting Model for Effective Teaching-Learning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Web-based Instruction (WBI) has been adopted in many educational systems. However, due to WBI's lack of face-to-face communication existing in a traditional classroom, their effectiveness is minimal. One way to lessen this problem is for these systems to implement a model that provides motivation to students. In this paper, we present a model to support motivation based on constructivism, which is known to be very suitable for WBI. The model is developed according to the four types of interactions: students-teachers, students-students, students-contents, and students-experts. We then describe the implementation of our model and its application to a third-grade science course for demonstration purposes."
      }
    ]
  },
  {
    "tid":"303",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Simultaneous deblocking and error concealment for decoded visual signal' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose a unified deblocking/error-concealment algorithm for simultaneously alleviating blockiness and restoring lost blocks. Blockiness and block loss are two of the most frequently encountered artifacts for block-DCT based visual signal coding and transmission. The problem of error-concealment and deblocking is formulated as block-wise expectation estimation processes conditioned on available pixels in a local neighborhood. A nonparametric kernel regression approach is used for approximating the conditional probability and thus the conditional expectation of each image block. Missing blocks are restored and blockiness are suppressed with a single pass of the algorithm. The algorithm is highly efficient in that only block-wise additions and multiplications in pixel domain within a local neighborhood are involved. Experimental results are provided to justify the effectiveness of the proposed unified error-concealment/deblocking algorithm."
      }
    ]
  },
  {
    "tid":"304",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Guaranteed performance routing of unpredictable traffic with fast path restoration' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Two-phase routing, where traffic is first distributed to intermediate nodes before being routed to the final destination, has been recently proposed for handling widely fluctuating traffic without the need to adapt network routing to changing traffic. Preconfiguring the network in a traffic-independent manner using two-phase routing simplifies network operation considerably. In this paper, we extend this routing scheme by providing resiliency against link failures through fast path restoration along disjoint end-to-end backup paths. We view this as important progress toward adding carrier-class reliability to the robustness of the scheme so as to facilitate its future deployment in Internet service provider (ISP) networks. On the theoretical side, the main contribution of the paper is the development of linear-programming-based and fast combinatorial algorithms for two-phase routing with fast path restoration so as to minimize the maximum utilization of any link in the network, or equivalently, maximize the throughput. The algorithms developed are fully polynomial time approximation schemes (FPTAS)-for any given \u03b5 0, an FPTAS guarantees a solution that is within a (1 + \u03b5)-factor of the optimum and runs in time polynomial in the input size and 1/\u03b5. To the best of our knowledge, this is the first work in the literature that considers making the scheme resilient to link failures through preprovisioned fast restoration mechanisms. We evaluate the performance of fast path restoration (in terms of throughput) and compare it to that of unprotected routing. For our experiments, we use actual ISP network topologies collected for the Rocketfuel project and three research network topologies."
      }
    ]
  },
  {
    "tid":"305",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On Achieving Maximum Network Lifetime Through Optimal Placement of Cluster-heads in Wireless Sensor Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In a wireless sensor network, the network lifetime is an important issue when the size of the network is large. In order to make the network scalable, it is divided into a number of clusters. In each cluster, a set of nodes is grouped and coordinated by a cluster-head. Due to high computations and data forwarding tasks at the cluster-heads, cluster-heads get drained out sooner. Hence, high power nodes are used as cluster- heads. Cluster-heads that are far away from the sink transmit their data packets via intermediate cluster-heads in a multi-hop fashion to the sink. Thus the cluster-heads that are near to the sink get overloaded and drained out sooner due to which the network lifetime decreases. In order to balance the lifetime of the cluster-heads, the existing works try to find either the optimum density or the optimum transmission range for the cluster-heads. The cluster-head placement algorithms should consider the fact that the node density and their transmission ranges are related parameters. In order to balance the lifetime of the cluster- head nodes and achieve the maximum network lifetime with the minimum network cost, we propose a novel optimal placement of cluster-heads (OPC) algorithm. We formulate an optimization problem and find the optimum number of cluster-heads to be deployed and their optimum transmission ranges for the given network parameters. In addition, we show that our algorithm performs better than the existing algorithm, in terms of network lifetime and network cost through the simulation results."
      }
    ]
  },
  {
    "tid":"306",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Lost in translation: understanding the possession of digital things in the cloud' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"People are amassing larger and more diverse collections of digital things. The emergence of Cloud computing has enabled people to move their personal files to online places, and create new digital things through online services. However, little is known about how this shift might shape people's orientations toward their digital things. To investigate, we conducted in depth interviews with 13 people comparing and contrasting how they think about their possessions, moving from physical ones, to locally kept digital materials, to the online world. Findings are interpreted to detail design and research opportunities in this emerging space."
      }
    ]
  },
  {
    "tid":"307",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Robust kernel discriminant analysis and its application to feature extraction and recognition' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Subspace analysis is an effective technique for dimensionality reduction, which aims at finding a low-dimensional space of high-dimensional data. In this paper, a novel subspace method called robust kernel discriminant analysis is proposed for dimensionality reduction. An optimization function is firstly defined in terms of the distance between similar elements and the distance between dissimilar elements, which can preserve the structure of the data in the mapping space. Then the optimization function is transformed into an eigenvalue problem and the projection vectors are obtained by solving the eigenvalue problem. Finally, experimental results on face images and handwritten numerical characters demonstrate the effectiveness and feasibility of the proposed method."
      }
    ]
  },
  {
    "tid":"308",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimising small-world properties in VANETs: Centralised and distributed overlay approaches.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Advantages of bringing small-world properties in mobile ad hoc networks (MANETs) in terms of quality of service has been studied and outlined in the past years. In this work, we focus on the specific class of vehicular ad hoc networks (VANETs) and propose to un-partition such networks and improve their small-world properties. To this end, a subset of nodes, called injection points, is chosen to provide backend connectivity and compose a fully-connected overlay network. The optimisation problem we consider is to find the minimal set of injection points to constitute the overlay that will optimise the small-world properties of the resulting network, i.e., (1) maximising the clustering coefficient (CC) so that it approaches the CC of a corresponding regular graph and (2) minimising the difference between the average path length (APL) of the considered graph and the APL of corresponding random graphs. Two accurate evolutionary algorithms (namely, NSGAII and MOCHC) are used to find an upper-bound of high quality solutions to this new multi-objective optimisation problem, on realistic instances in the city-centre of Luxembourg. The obtained sets of solutions are then used to assess the performance of five novel heuristics proposed to solve the problem, i.e., two centralised and three decentralised. The results provided by these heuristics turned out to be extremely accurate with respect to the solutions found by the evolutionary algorithms. (C) 2014 Elsevier B.V. All rights reserved."
      }
    ]
  },
  {
    "tid":"309",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Modeling Components and Component-Based Systems in KobrA' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this chapter we present a version of the Trading System case study modeled according to the KobrA approach. KobrA is a UML-based method for describing components and component-based systems developed at the Fraunhofer Institute for Experimental Software Engineering at the beginning of the decade. The acronym stands for the term \"Komponenten basierte Anwendungsentwicklung\" --- German for \"Component-based Application Development\". KobrA has been successfully used by a number of companies in industrial settings and has given rise to numerous specializations and offshoots (e.g. MARMOT [1] and MORABIT [2]). The original version of the method [3] was developed for the UML 1.x flavor of the UML, but in this chapter we introduce an updated version optimized for use with the 2.x versions of the UML [4] and its related standards such as OCL [5]. KobrA also provides support for other advanced software engineering approaches such as product-lines, but these are beyond the scope of this chapter. Here we focus on the component-modeling aspects of the method."
      }
    ]
  },
  {
    "tid":"310",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'DTIA: An Architecture for Inter-Domain Routing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes an architecture for inter-domain routing, called DTIA - Dynamic Topological Information Architecture. DTIA separates the issues of reachability and routing, and this paper addresses the first one. One major requirement has been not to change IP packets and the commercial relations in the Internet. DTIA is based on the knowledge of a static network formed by the Autonomous Systems (AS) and an algorithm to manage link failures. We use the concept of a region as a mechanism to sustain scale. DTIA supports the most important functionalities of BGP: some of them are built in and others can be implemented on top of the reachability level or the routing level. The main concerns we aim to solve are taking advantage of multihoming, increase the robustness in terms of convergence, reduce the churn rate and range of routing events, and due to forwarding packets by AS identifiers and topologic links (as opposed to prefix policy defined) reduce the growth of the routing table."
      }
    ]
  },
  {
    "tid":"311",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'DESIGNi: a workbench for supporting interaction design' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"New devices expand design possibilities, but also lead to more challenges in the creation of interaction forms. This article introduces DESIGNi, a workbench that supports designers in exploring interaction forms and their attributes in a structured and systematic way. We present the components of DESIGNi and its use in creating a business application. Moreover, a comparison of the interaction forms specified in the design process with DESIGNi and the perceived interaction characteristics in user studies revealed interesting insights and points for improvement in the interaction design itself."
      }
    ]
  },
  {
    "tid":"312",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Effective image retrieval based on hidden concept discovery in image database.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper addresses content-based image retrieval in general, and in particular, focuses on developing a hidden semantic concept discovery methodology to address effective semantics-intensive image retrieval. In our approach, each image in the database is segmented into regions associated with homogenous color, texture, and shape features. By exploiting regional statistical information in each image and employing a vector quantization method, a uniform and sparse region-based representation is achieved. With this representation, a probabilistic model based on statistical-hidden-class assumptions of the image database is obtained, to which the expectation-maximization technique is applied to analyze semantic concepts hidden in the database. An elaborated retrieval algorithm is designed to support the probabilistic model. The semantic similarity is measured through integrating the posterior probabilities of the transformed query image, as well as a constructed negative example, to the discovered semantic concepts. The proposed approach has a solid statistical foundation; the experimental evaluations on a database of 10000 general-purposed images demonstrate its promise and effectiveness."
      }
    ]
  },
  {
    "tid":"313",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Solving dense symmetric indefinite systems using GPUs.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper studies the performance of different algorithms for solving a dense symmetric indefinite linear system of equations on multicore CPUs with a Graphics Processing Unit (GPU). To ensure the numerical stability of the factorization, pivoting is required. Obtaining high performance of such algorithms on the GPU is difficult because all the existing pivoting strategies lead to frequent synchronizations and irregular data accesses. Until recently, there has not been any implementation of these algorithms on a hybrid CPU/GPU architecture. To improve their performance on the hybrid architecture, we explore different techniques to reduce the expensive data transfer and synchronization between the CPU and GPU, or on the GPU (e.g., factorizing the matrix entirely on the GPU or in a communication-avoiding fashion). We also study the performance of the solver using iterative refinements along with the factorization without pivoting combined with the preprocessing technique based on random butterfly transformations, or with the mixed-precision algorithm where the matrix is factorized in single precision. This randomization algorithm only has a probabilistic proof on the numerical stability, and for this paper, we only focused on the mixed-precision algorithm without pivoting. However, they demonstrate that we can obtain good performance on the GPU by avoiding the pivoting and using the lower precision arithmetics, respectively. As illustrated with the application in acoustics studied in this paper, in many practical cases, the matrices can be factorized without pivoting. Because the componentwise backward error computed in the iterative refinement signals when the algorithm failed to obtain the desired accuracy, the user can use these potentially unstable but efficient algorithms in most of the cases and fall back to a more stable algorithm with pivoting only in the case of the failure. Copyright (c) 2017 John Wiley & Sons, Ltd."
      }
    ]
  },
  {
    "tid":"314",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Cross species analysis of microarray expression data.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many biological systems operate in a similar manner across a large number of species or conditions. Cross-species analysis of sequence and interaction data is often applied to determine the function of new genes. In contrast to these static measurements, microarrays measure the dynamic, condition-specific response of complex biological systems. The recent exponential growth in microarray expression datasets allows researchers to combine expression experiments from multiple species to identify genes that are not only conserved in sequence but also operated in a similar way in the different species studied.In this review we discuss the computational and technical challenges associated with these studies, the approaches that have been developed to address these challenges and the advantages of cross-species analysis of microarray data. We show how successful application of these methods lead to insights that cannot be obtained when analyzing data from a single species. We also highlight current open problems and discuss possible ways to address them."
      }
    ]
  },
  {
    "tid":"315",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Partial Evaluation of Order-sorted Equational Programs modulo Axioms.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Partial evaluation (PE) is a powerful and general program optimization technique with many successful applications. However, it has never been investigated in the context of expressive rule-based languages like Maude, CafeOBJ, OBJ, ASF+SDF, and ELAN, which support: rich type structures with sorts, subsorts and overloading; and equational rewriting modulo axioms such as commutativity, associativity-commutativity, and associativity-commutativity-identity. In this paper, we illustrate the key concepts by showing how they apply to partial evaluation of expressive rule-based programs written in Maude. Our partial evaluation scheme is based on an automatic unfolding algorithm that computes term variants and relies on equational least general generalization for ensuring global termination. We demonstrate the use of the resulting partial evaluator for program optimization on several examples where it shows significant speed-ups."
      }
    ]
  },
  {
    "tid":"316",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'EPCBIR: An efficient and privacy-preserving content-based image retrieval scheme in cloud computing.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Abstract   The content-based image retrieval (CBIR) has been widely studied along with the increasing importance of images in our daily life. Compared with the text documents, images consume much more storage and thus are very suitable to be stored on the cloud servers. The outsourcing of CBIR to the cloud servers can be a very typical service in cloud computing. For the privacy-preserving purposes, sensitive images, such as medical and personal images, need to be encrypted before being outsourced, which will cause the CBIR technologies in plaintext domain unusable. In this paper, we propose a scheme that supports CBIR over the encrypted images without revealing the sensitive information to the cloud server. Firstly, the feature vectors are extracted to represent the corresponding images. Then, the pre-filter tables are constructed with the locality-sensitive hashing to increase the search efficiency. Next, the feature vectors are protected by the secure k-nearest neighbor (kNN) algorithm. The security analysis and experiments show the security and efficiency of the proposed scheme."
      }
    ]
  },
  {
    "tid":"317",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Cost-Effective provisioning and scheduling of deadline-constrained applications in hybrid clouds' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In order to meet distributed application deadlines, Resource Management Systems (RMSs) have to utilize additional resources from public Cloud providers when in-house resources cannot cope with the demand of the applications. As a means to enable this feature, called Cloud Bursting, the RMS has to be able to determine when, how many, and for how long such resources are required and provision them dynamically. The RMS has also to determine which tasks will be executed on them and in which order they will be submitted (scheduling). Current approaches for dynamic provisioning of Cloud resources operate at a per-job level, ignoring characteristics of the whole organization workload, which leads to inefficient utilization of Cloud resources. This paper presents an architecture for coordinated dynamic provisioning and scheduling that is able to cost-effectively complete applications within their deadlines by considering the whole organization workload at individual tasks level when making decisions and an accounting mechanism to determine the share of the cost of utilization of public Cloud resources to be assigned to each user. Experimental results show that the proposed strategy can reduce the total utilization of public Cloud services by up to 20% without any impact in the capacity of meeting application deadlines."
      }
    ]
  },
  {
    "tid":"318",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Communication in Shared Memory: Concepts, Definitions, and Efficient Detection' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Optimizing the communication behavior of parallel applications has emerged as an important topic in parallel processing. In shared memory architectures, threads communicate implicitly through memory accesses to shared memory areas. The communication behavior can be improved by mapping threads that communicate a lot to processing units that are close to each other in the memory hierarchy, such that they can benefit from shared caches and faster interconnections. An important aspect of such a communication-aware thread mapping is the accurate and efficient detection of communication in shared memory. Previous work used impromptu definitions, without an evaluation of the complexities of different communication types. In this paper, we perform an in-depth, systematic evaluation of communication in shared memory, focusing on its architectural effects. We present an efficient way to detect communication, which is orders of magnitude faster than a cache simulator, while maintaining a high accuracy."
      }
    ]
  },
  {
    "tid":"319",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'How They Did It: An Analysis of Emission Defeat Devices in Modern Automobiles' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Modern vehicles are required to comply with a range of environmental regulations limiting the level of emissions for various greenhouse gases, toxins and particulate matter. To ensure compliance, regulators test vehicles in controlled settings and empirically measure their emissions at the tailpipe. However, the black box nature of this testing and the standardization of its forms have created an opportunity for evasion. Using modern electronic engine controllers, manufacturers can programmatically infer when a car is undergoing an emission test and alter the behavior of the vehicle to comply with emission standards, while exceeding them during normal driving in favor of improved performance. While the use of such a defeat device by Volkswagen has brought the issue of emissions cheating to the public's attention, there have been few details about the precise nature of the defeat device, how it came to be, and its effect on vehicle behavior. In this paper, we present our analysis of two families of software defeat devices for diesel engines: one used by the Volkswagen Group to pass emissions tests in the US and Europe, and a second that we have found in Fiat Chrysler Automobiles. To carry out this analysis, we developed new static analysis firmware forensics techniques necessary to automatically identify known defeat devices and confirm their function. We tested about 900 firmware images and were able to detect a potential defeat device in more than 400 firmware images spanning eight years. We describe the precise conditions used by the firmware to detect a test cycle and how it affects engine behavior. This work frames the technical challenges faced by regulators going forward and highlights the important research agenda in providing focused software assurance in the presence of adversarial manufacturers."
      }
    ]
  },
  {
    "tid":"320",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Software assisted non-volatile register reduction for energy harvesting based cyber-physical system' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Wearable devices are important components as information collector in many cyber-physical systems. Energy harvesting instead of battery is a better power source for these wearable devices due to many advantages. However, harvested energy is naturally unstable and program execution will be interrupted frequently. Non-volatile processors demonstrate promising advantages to back up volatile state before the system energy is depleted. However, it also introduces non-negligible energy and area overhead. Since the chip size is a vital factor for wearable devices, in this work, we target non-volatile register reduction for application-specific systems. We propose to analyze the application program and determine efficient backup positions, by which the necessary non-volatile register file size can be significantly reduced. The evaluation results deliver an average of 62.9% reduction on non-volatile register file size for stack backup, with negligible storage overheads."
      }
    ]
  },
  {
    "tid":"321",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Novel Classification Algorithm Based on Fuzzy Kernel Multiple Hyperspheres' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper a novel classification algorithm based on fuzzy kernel multiple hyperspheres is presented. In the training process all training samples of each class are covered by the constructed multiple hyperspheres. Each hypersphere encompasses as many samples with the same class as possible via the greedy method. A fuzzy member- ship function is defined to label the testing samples in the classification process. Moreover, the kernel function is used instead of Euclidean inner product. Finally, Experiments on three artificial datasets and five real datasets show that our approach is valid and has encouraging pattern classification performance."
      }
    ]
  },
  {
    "tid":"322",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A dynamical consensus method based on exit-delegation mechanism for large group emergency decision making.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Aiming at the lower consensus and the urgency of large group emergency decision making, a dynamical consensus method based on an exit-delegation mechanism is proposed and investigated. Firstly, the method is initiated by transferring a large group into small groups via the preference clustering method. Then, the consistency and consensus measures are calculated and two different criteria are used to guide the consensus reaching process. In addition, considering the urgency of emergency decision making, an exit-delegation mechanism is introduced to deal with clusters. When the consistency/consensus level is low, the proximity index of each cluster is computed. For the cluster whose proximity degree is lower than the threshold, it is advised to exit the decision-making process and a delegation mechanism is employed to reserve his influence by giving trust weights to other clusters. Meantime, a feedback mechanism is developed to give advice to clusters whose preferences should be subject to change, and to obtain a solution which satisfies the consistency and consensus criteria simultaneously. Finally, a case is taken to verify the rationality and feasibility of the method."
      }
    ]
  },
  {
    "tid":"323",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Robust Tree-based Causal Inference for Complex Ad Effectiveness Analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"As the online advertising industry has evolved into an age of diverse ad formats and delivery channels, users are exposed to complex ad treatments involving various ad characteristics. The diversity and generality of ad treatments call for accurate and causal measurement of ad effectiveness, i.e., how the ad treatment causes the changes in outcomes without the confounding effect by user characteristics. Various causal inference approaches have been proposed to measure the causal effect of ad treatments. However, most existing causal inference methods focus on univariate and binary treatment and are not well suited for complex ad treatments. Moreover, to be practical in the data-rich online environment, the measurement needs to be highly general and efficient, which is not addressed in conventional causal inference approaches. In this paper we propose a novel causal inference framework for assessing the impact of general advertising treatments. Our new framework enables analysis on uni- or multi-dimensional ad treatments, where each dimension (ad treatment factor) could be discrete or continuous. We prove that our approach is able to provide an unbiased estimation of the ad effectiveness by controlling the confounding effect of user characteristics. The framework is computationally efficient by employing a tree structure that specifies the relationship between user characteristics and the corresponding ad treatment. This tree-based framework is robust to model misspecification and highly flexible with minimal manual tuning. To demonstrate the efficacy of our approach, we apply it to two advertising campaigns. In the first campaign we evaluate the impact of different ad frequencies, and in the second one we consider the synthetic ad effectiveness across TV and online platforms. Our framework successfully provides the causal impact of ads with different frequencies in both campaigns. Moreover, it shows that the ad frequency usually has a treatment effect cap, which is usually over-estimated by naive estimation."
      }
    ]
  },
  {
    "tid":"324",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Finding Tiny Faces in the Wild with Generative Adversarial Network' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Face detection techniques have been developed for decades, and one of remaining open challenges is detecting small faces in unconstrained conditions. The reason is that tiny faces are often lacking detailed information and blurring. In this paper, we proposed an algorithm to directly generate a clear high-resolution face from a blurry small one by adopting a generative adversarial network (GAN). Toward this end, the basic GAN formulation achieves it by super-resolving and refining sequentially (e.g. SR-GAN and cycle-GAN). However, we design a novel network to address the problem of super-resolving and refining jointly. We also introduce new training losses to guide the generator network to recover fine details and to promote the discriminator network to distinguish real vs. fake and face vs. non-face simultaneously. Extensive experiments on the challenging dataset WIDER FACE demonstrate the effectiveness of our proposed method in restoring a clear high-resolution face from a blurry small one, and show that the detection performance outperforms other state-of-the-art methods."
      }
    ]
  },
  {
    "tid":"325",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A peer-to-peer approach to resource location in grid environments' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Computational grids provide mechanisms for sharing and accessing large and heterogeneous collections of re-mote resources such as computers, online instruments, stor-age space, data, and applications. Resources are requested by specifying a set of desired attributes. Resource attributes have various degrees of dynamism, from mostly static at-tributes, such as operating system version, to highly dy-namic ones, such as available network bandwidth or CPU load. Another dimension of dynamism is introduced by variable and highly diverse sharing policies: resources are made available to the grid community based on locally de-fined and potentially changing policies. In such a dynamic environment it is often more effi-cient to forward requests than to disseminate resource in-formation that soon becomes stale. We investigate a set of request forwarding strategies in a peer-to-peer like ar-chitecture: each peer acts independently, based on possi-bly inaccurate information about the rest of the system and forwards the requests it cannot solve to another peer. We investigate these strategies in emulated environments with different sharing characteristics, different request distribu-tions, and on networks of up to 32,768 (\t) peers. The results allow us to characterize the correlation between re-source discovery performance based on request forwarding, resource sharing characteristics, and user request patterns. We modeled the following parameters that define a grid environment: (a) resource information distribution and den-sity refers to sharing fairness, some nodes will share infor-mation about more resources than others; (b) requests dis-tribution encapsulates information on users'behavior. We analyze four request forwarding strategies under two ex-treme resource distributions: perfectly balanced, where all nodes have information about the same number of resources and unbalanced, where a small number of nodes have most of the resources in the system. We considered two request distributions: a uniform distribution, with all requests hav-ing the same probability of being submitted, and Condor traces, that follow a distribution close to Zipf (hence, a small number of requests are frequently asked). In these experi-"
      }
    ]
  },
  {
    "tid":"326",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A linear time algorithm for high quality mesh simplification' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"High resolution 3D range scanning as well as iso-surface extraction have introduced densely and uniformly sampled models that are difficult to render at an interactive rate. To remove excessive details and produce meshes of various resolutions for different kinds of applications, the study of fast and high quality polygonal mesh simplification algorithms has become important. In this paper, we propose a new linear time algorithm that can achieve fast and high quality mesh simplification. In the new algorithm, we pipeline the cost computation, optimization, and edge collapse, and use a small constant-sized replacement selection min-heap instead of a large greedy queue to effectively reduce the runtime complexity to linear complexity. Compared to previous works, our new algorithm has at least three advantages. First, the new algorithm is runtime efficient. Second, the new algorithm is memory efficient. Third, the algorithm is capable of generating competitive high quality outputs."
      }
    ]
  },
  {
    "tid":"327",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mapping pipelined applications onto heterogeneous embedded systems: a bayesian optimization algorithm based approach' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we propose a flow based on the Bayesian Optimization Algorithm (BOA) for mapping pipelined applications on a heterogeneous multiprocessor platform on Field Programmable Gate Array (FPGA) with customizable processors. BOA is a Probabilistic Model Building Genetic Algorithm (PMBGA) that, substituting the classical mutation and crossover operators with the construction and the sampling of a Bayesian network, is able to identify correlated sub-structures within the problem to be maintained while generating new solutions. The paper introduces the model adopted for pipelined applications and then shows why BOA fits the problem better than other search algorithms, like Genetic Algorithm (GA), Simulated Annealing (SA) and Tabu Search (TS). We also show that our algorithm is able to cope with data parallel pipelined algorithms. We finally validate our flow on realistic applications like JPEG and ADPCM coding by executing the resulting mapping on our platform."
      }
    ]
  },
  {
    "tid":"328",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Enhanced biclustering on expression data' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Microarrays are one of the latest breakthroughs in experimental molecular biology, which provide a powerful tool by which the expression patterns of thousands of genes can be monitored simultaneously and are already producing huge amount of valuable data. The concept of bicluster was introduced by Cheng and Church (2000) to capture the coherence of a subset of genes and a subset of conditions. A set of heuristic algorithms were also designed to either find one bicluster or a set of biclusters, which consist of iterations of masking values and discovered biclusters, coarse and fine node deletion, node addition, and the inclusion of inverted data. These heuristics inevitably suffer from some serious drawback. The masking of values and discovered biclusters with random numbers may result in the phenomenon of random interference which in turn impacts the discovery of high quality biclusters. To address this issue and to further accelerate the biclustering process, we generalize the model of bicluster to incorporate values and propose a probabilistic algorithm (FLOC) that can discover a set of k possibly overlapping biclusters simultaneously. Furthermore, this algorithm can easily be extended to support additional features that suit different requirements at virtually little cost. Experimental study on the yeast gene expression data shows that the FLOC algorithm can offer substantial improvements over the previously proposed algorithm."
      }
    ]
  },
  {
    "tid":"329",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Sex Differences in the Human Connectome' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The human brain and the neuronal networks comprising it are of immense interest to the scientific community. In this work, we focus on the structural connectivity of human brains, investigating sex differences across male and female connectomes (brain-graphs) for the knowledge discovery problem \\\"Which brain regions exert differences in connectivity across the two sexes?\\\". One of our main findings discloses the statistical difference at the pars orbitalis of the connectome between sexes, which has been shown to function in language production. Moreover, we use these discriminative regions for the related learning problem \\\"Can we classify a given human connectome to belong to one of the sexes just by analyzing its connectivity structure?\\\" . We show that we can learn decision tree as well as support vector machine classification models for this task. We show that our models achieve up to 79% prediction accuracy with only a handful of brain regions as discriminating factors. Importantly, our results are consistent across two data sets, collected at two different centers, with two different scanning sequences, and two different age groups (children and elderly). This is highly suggestive that we have discovered scientifically meaningful sex differences."
      }
    ]
  },
  {
    "tid":"330",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Routing on demand: toward the energy-aware traffic engineering with OSPF' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Energy consumption has already become a major challenge to the current Internet. Most researches aim at lowering energy consumption under certain fixed performance constraints. Since trade-offs exist between network performance and energy saving, Internet Service Providers (ISPs) may desire to achieve different Traffic Engineering (TE) goals corresponding to changeable requirements. The major contributions of this paper are twofold: 1) we present an OSPF-based routing mechanism, Routing On Demand (ROD), that considers both performance and energy saving, and 2) we theoretically prove that a set of link weights always exists for each trade-off variant of the TE objective, under which solutions (i.e., routes) derived from ROD can be converted into shortest paths and realized through OSPF. Extensive evaluation results show that ROD can achieve various trade-offs between energy saving and performance in terms of Maximum Link Utilization, while maintaining better packet delay than that of the energy-agnostic TE."
      }
    ]
  },
  {
    "tid":"331",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Lurking Malice in the Cloud: Understanding and Detecting Cloud Repository as a Malicious Service.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The popularity of cloud hosting services also brings in new security challenges: it has been reported that these services are increasingly utilized by miscreants for their malicious online activities. Mitigating this emerging threat, posed by such \\\"bad repositories\\\" (simply Bar), is challenging due to the different hosting strategy to traditional hosting service, the lack of direct observations of the repositories by those outside the cloud, the reluctance of the cloud provider to scan its customers' repositories without their consent, and the unique evasion strategies employed by the adversary. In this paper, we took the first step toward understanding and detecting this emerging threat. Using a small set of \\\"seeds\\\" (i.e., confirmed Bars), we identified a set of collective features from the websites they serve (e.g., attempts to hide Bars), which uniquely characterize the Bars. These features were utilized to build a scanner that detected over 600 Bars on leading cloud platforms like Amazon, Google, and 150K sites, including popular ones like groupon.com, using them. Highlights of our study include the pivotal roles played by these repositories on malicious infrastructures and other important discoveries include how the adversary exploited legitimate cloud repositories and why the adversary uses Bars in the first place that has never been reported. These findings bring such malicious services to the spotlight and contribute to a better understanding and ultimately eliminating this new threat."
      }
    ]
  },
  {
    "tid":"332",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Leveraging Social Media Sources to Generate Personalized Music Playlists.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents MyMusic, a system that exploits social media sources for generating personalized music playlists. This work is based on the idea that information extracted from social networks, such as Facebook and Last. fm, might be effectively exploited for personalization tasks. Indeed, information related to music preferences of users can be easily gathered from social platforms and used to define a model of user interests. The use of social media is a very cheap and effective way to overcome the classical cold start problem of recommender systems. In this work we enriched social media-based playlists with new artists related to those the user already likes. Specifically, we compare two different enrichment techniques: the first leverages the knowledge stored on DBpedia, the structured version of Wikipedia, while the second is based on the content-based similarity between descriptions of artists. The final playlist is ranked and finally presented to the user that can listen to the songs and express her feedbacks. A prototype version of MyMusic was made available online in order to carry out a preliminary user study to evaluate the best enrichment strategy. The preliminary results encouraged keeping on this research."
      }
    ]
  },
  {
    "tid":"333",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Reinforcement Learning Controller Design for Affine Nonlinear Discrete-Time Systems using Online Approximators' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, reinforcement learning state- and output-feedback-based adaptive critic controller designs are proposed by using the online approximators (OLAs) for a general multi-input and multioutput affine unknown nonlinear discretetime systems in the presence of bounded disturbances. The proposed controller design has two entities, an action network that is designed to produce optimal signal and a critic network that evaluates the performance of the action network. The critic estimates the cost-to-go function which is tuned online using recursive equations derived from heuristic dynamic programming. Here, neural networks (NNs) are used both for the action and critic whereas any OLAs, such as radial basis functions, splines, fuzzy logic, etc., can be utilized. For the output-feedback counterpart, an additional NN is designated as the observer to estimate the unavailable system states, and thus, separation principle is not required. The NN weight tuning laws for the controller schemes are also derived while ensuring uniform ultimate boundedness of the closed-loop system using Lyapunov theory. Finally, the effectiveness of the two controllers is tested in simulation on a pendulum balancing system and a two-link robotic arm system."
      }
    ]
  },
  {
    "tid":"334",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Selection of relevant genes in cancer diagnosis based on their prediction accuracy.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"One of the main problems in cancer diagnosis by using DNA microarray data is selecting genes relevant for the pathology by analyzing their expression profiles in tissues in two different phenotypical conditions. The question we pose is the following: how do we measure the relevance of a single gene in a given pathology?A gene is relevant for a particular disease if we are able to correctly predict the occurrence of the pathology in new patients on the basis of its expression level only. In other words, a gene is informative for the disease if its expression levels are useful for training a classifier able to generalize, that is, able to correctly predict the status of new patients. In this paper we present a selection bias free, statistically well founded method for finding relevant genes on the basis of their classification ability.We applied the method on a colon cancer data set and produced a list of relevant genes, ranked on the basis of their prediction accuracy. We found, out of more than 6500 available genes, 54 overexpressed in normal tissues and 77 overexpressed in tumor tissues having prediction accuracy greater than 70% with p-value <or=0.05.The relevance of the selected genes was assessed (a) statistically, evaluating the p-value of the estimate prediction accuracy of each gene; (b) biologically, confirming the involvement of many genes in generic carcinogenic processes and in particular for the colon; (c) comparatively, verifying the presence of these genes in other studies on the same data-set."
      }
    ]
  },
  {
    "tid":"335",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Equitable Solution to the Stable Marriage Problem' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A stable marriage problem (SMP) of size n involves n men and n women, each of whom has ordered members of the opposite gender by descending preferability. A solution is a perfect matching among men and women, such that there exists no pair who prefer each other to their current spouses. The problem was formulated in 1962 by Gale and Shapley, who showed that any instance can be solved in polynomial time, and has attracted interest due to its application to any two-sided market. Still, the solution obtained by the Gale-Shapley algorithm is favorable to one side. Gusfield and Irving introduced the equitable stable marriage problem (ESMP), which calls for finding a stable matching that minimizes the distance between men's and women's sum-of-rankings of their spouses. Unfortunately, ESMP is strongly NP-hard, approximation algorithms therefor are impractical, while even proposed heuristics may run for an unpredictable number of iterations. We propose a novel, deterministic approach that treats both genders equally, while eschewing an exhaustive exploration of the space of all stable matchings. Our thorough experimental study shows that, in contrast to previous proposals, our method not only achieves high-quality solutions, but also terminates efficiently and repeatably on all tested large problem instances."
      }
    ]
  },
  {
    "tid":"336",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Data Detection in Cooperative STBC-OFDM Systems With Multiple Frequency Offsets' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper addresses the problem of data detection in cooperative space-time block coded (STBC) orthogonal frequency division multiplexing (OFDM) systems in the presence of multiple carrier frequency offsets (CFO). An enhanced iterative maximum-likelihood detector (EIMLD) is proposed. This method consists of first removing inter-carrier interference (ICI), and then performing iterative symbol detection and inter-symbol interference reduction. Simulation results show that EIMLD significantly outperforms existing iterative methods. Comparisons with the zero-forcing and minimum-mean square error detectors, which require complex matrix inversion, are also carried out."
      }
    ]
  },
  {
    "tid":"337",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'FIDS: an intelligent financial Web news articles digest system' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we present a system called FIDS (Financial Information Digest System), which can digest online financial news automatically. Compared to previous approaches, FIDS is unique in the way that it can understand news articles in different domains simultaneously. These domains are all concerned with financial news. The system is able to integrate the information from different articles by conducting automatic content-based classification and information item extraction. Moreover, it allows one to perform cross-validation on their contents. As a result, users can have access to more complete information which otherwise would be scattered in different articles"
      }
    ]
  },
  {
    "tid":"338",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A reflective practice of automated and manual code reviews for a studio project' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, the target of code review is project management system (PMS), developed by a studio project in a software engineering master's program, and the focus is on finding defects not only in view of development standards, i.e., design rule and naming rule, but also in view of quality attributes of PMS, i.e., performance and security. From the review results, a few lessons are learned. First, defects which had not been found in the test stage of PMS development could be detected in this code review. These are hidden defects that affect system quality and that are difficult to find in the test. If the defects found in this code review had been fixed before the test stage of PMS development, productivity and quality enhancement of the project would have been improved. Second, manual review takes much longer than an automated one. In this code review, general check items were checked by automation tool, while project-specific ones were checked by manual method. If project-specific check items could also be checked by automation tool, code review and verification work after fixing the defects would be conducted very efficiently. Reflecting on this idea, an evolution model of code review is studied, which eventually seeks fully automated review as an optimized code review."
      }
    ]
  },
  {
    "tid":"339",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Consistency-Based Secure Localization Scheme against Wormhole Attacks in WSNs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Wormhole attacks can negatively affect the localization in wireless sensor networks. A typical wormhole attack can be launched by two colluding external attackers, one of which sniffs packets at one point in the network, tunnels them through a wired or wireless link to another point, and the other of which relays them within its vicinity. In this paper, we investigate the impact of the wormhole attack on the localization process and propose a novel consistency-based secure localization scheme against wormhole attacks, which includes wormhole attack detection, valid locators identification and self-localization. We also conduct the simulations to demonstrate the effectiveness of our proposed scheme."
      }
    ]
  },
  {
    "tid":"340",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Asymptotic attitude tracking of the rotorcraft-based UAV via RISE feedback and NN feedforward' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents an asymptotic attitude tracking controller for a rotorcraft-based unmanned aerial vehicle (RUAV) using the robust integral of the signum of the error(RISE) feedback and neural network (NN) feedforward terms. Usually, the typical NN-based attitude controller guarantees the uniformly ultimately bounded stability. In this study, semi-global asymptotic tracking of the RUAV is guaranteed by the RISE feedback term and NN feedforward term adapted by the projection method. The controller is basically designed by the linear dynamic model inversion method whose model is obtained by the linearization of the nonlinear RUAV model at the hover flight. Then, the uncertainty generated in the linearization is removed by the RISE feedback and NN feedforward terms. The asymptotic tracking of the attitude states is proven with the Lyapunov stability analysis, and a numerical simulation using the nonlinear RUAV model is performed to validate the effectiveness of the proposed controller."
      }
    ]
  },
  {
    "tid":"341",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Backpacking: Deployment of Heterogeneous Radios in High Data Rate Sensor Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The early success of wireless sensor networks has led to a new generation of increasingly sophisticated sensor network applications, such as HP's CeNSE. These applications demand high network throughput that easily exceeds the capability of low-power 802.15.4 radios that are most commonly used in today's sensor nodes. To address this issue, this paper investigates an energy-efficient approach to supplementing an 802.15.4 based sensor network with high bandwidth, high power, longer range radios such as 802.11. Exploiting a key observation that the high bandwidth radio achieves low energy consumption per transmitted bit of data due to its inherent transmission efficiency, we propose a hybrid network architecture that utilizes an optimal density of dual-radio (802.15.4 and 802.11) nodes to augment a sensor network having only 802.15.4 radios. We present a cross-layer mathematical model to calculate this optimal density, which strikes a balance between the low energy per bit of the high-bandwidth radio and the low sleep power of 802.15.4 radio. Experimental results obtained using a wireless testbed reveal that our architecture improves the average energy per bit, the time elapsed before half of the nodes drain their battery, and the end-to-end delay by 62%, 106%, and 73% respectively, compared to a network that uses only 802.15.4 radios."
      }
    ]
  },
  {
    "tid":"342",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'PATCH: Process-Variation-Resilient Space Allocation for Open-Channel SSD with 3D Flash' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Advanced three-dimensional (3D) flash memory adopts charge-trap technology that can effectively improve the hit density and reduce the coupling effect. Despite these advantages, 3D charge-trap flash brings a number of new challenges. First, current etching process is unable to manufacture perfect channels with identical feature size. Second, the cell current in 3D charge-trap flash is only 20% compared to planar flash memory, making it difficult to give a reliable sensing margin. These issues are affected by process variation, and they pose threats to the integrity of data stored in 3D charge-trap flash. This paper presents PATCH, a process-variation-resilient space allocation scheme for open-channel SSD with 3D charge-trap flash memory. PATCH is a novel hardware and file system interface that can transparently allocate physical space in the presence of process variation. PATCH utilizes the rich functionalities provided by the system infrastructure of open-channel SSD to reduce the uncorrectable bit errors. We demonstrate the viability of the proposed technique using a set of extensive experiments. Experimental results show that PATCH can effectively enhance the reliability with negligible extra erase operations in comparison with representative schemes."
      }
    ]
  },
  {
    "tid":"343",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Self-Checkpoint: An In-Memory Checkpoint Method Using Less Space and Its Practice on Fault-Tolerant HPL.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Fault tolerance is increasingly important in high performance computing due to the substantial growth of system scale and decreasing system reliability. In-memory/diskless checkpoint has gained extensive attention as a solution to avoid the IO bottleneck of traditional disk-based checkpoint methods. However, applications using previous in-memory checkpoint suffer from little available memory space. To provide high reliability, previous in-memory checkpoint methods either need to keep two copies of checkpoints to tolerate failures while updating old checkpoints or trade performance for space by flushing in-memory checkpoints into disk. In this paper, we propose a novel in-memory checkpoint method, called self-checkpoint, which can not only achieve the same reliability of previous in-memory checkpoint methods, but also increase the available memory space for applications by almost 50%. To validate our method, we apply the self-checkpoint to an important problem, fault tolerant HPL. We implement a scalable and fault tolerant HPL based on this new method, called SKT-HPL, and validate it on two large-scale systems. Experimental results with 24,576 processes show that SKT-HPL achieves over 95% of the performance of the original HPL. Compared to the state-of-the-art in-memory checkpoint method, it improves the available memory size by 47% and the performance by 5%."
      }
    ]
  },
  {
    "tid":"344",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mission planning for UAV-based opportunistic disaster recovery networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"After severe natural disasters, being able to communicate is key to effective disaster relief. When survivors are injured and trapped in remote places, for instance, they need to inform rescue workers about their situation in order to get help. The necessary infrastructure can be badly damaged, making the delivery of that information impossible. Even the disaster countermeasures of mobile operators might still leave coverage holes where there is no connection to any cellular network. In this work, we propose the use of Unpiloted Aerial Vehicles (UAVs) as data mules to provide means of communication to survivors in those coverage holes. Having a fleet of UAVs, the generation of suitable flight paths is the first step required, followed by their assignment to the UAVs. This problem has been modeled as an integer linear program. Due to the computational complexity, a heuristic solving algorithm is proposed to solve the formulated problem. The optimal and the heuristic solution both have been implemented and evaluated against each other."
      }
    ]
  },
  {
    "tid":"345",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Situationally-Aware Voice-Commandable Robotic Forklift Working Alongside People in Unstructured Outdoor Environments' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"One long-standing challenge in robotics is the realization of mobile autonomous robots able to operate safely in human workplaces, and be accepted by the human occupants. We describe the development of a multiton robotic forklift intended to operate alongside people and vehicles, handling palletized materials within existing, active outdoor storage facilities. The system has four novel characteristics. The first is a multimodal interface that allows users to efficiently convey task-level commands to the robot using a combination of pen-based gestures and natural language speech. These tasks include the manipulation, transport, and placement of palletized cargo within dynamic, human-occupied warehouses. The second is the robot's ability to learn the visual identity of an object from a single user-provided example and use the learned model to reliably and persistently detect objects despite significant spatial and temporal excursions. The third is a reliance on local sensing that allows the robot to handle variable palletized cargo and navigate within dynamic, minimally prepared environments without a global positioning system. The fourth concerns the robot's operation in close proximity to people, including its human supervisor, pedestrians who may cross or block its path, moving vehicles, and forklift operators who may climb inside the robot and operate it manually. This is made possible by interaction mechanisms that facilitate safe, effective operation around people. This paper provides a comprehensive description of the system's architecture and implementation, indicating how real-world operational requirements motivated key design choices. We offer qualitative and quantitative analyses of the robot operating in real settings and discuss the lessons learned from our effort."
      }
    ]
  },
  {
    "tid":"346",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient Matrix Multiplication on Wireless Sensor Networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Wireless Sensor Networks (WSN for short) have been widely used for data collecting and processing in various applications. This paper studies the problem of how to efficiently realize matrix multiplication on a WSN. Based on a widely accepted model of WSNs, we show the lower bound on the number of parallel steps needed by any algorithm for n\u8133n matrix multiplication on a WSN of n2 sensors with transmission radius r. We then present two parallel algorithms for matrix multiplication on a WSN which have the same time complexity as the lower bound. For both algorithms we analyze their main performance parameters including time cost, memory cost and energy-efficiency in detail."
      }
    ]
  },
  {
    "tid":"347",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An optimal control approach for communication and motion co-optimization in realistic fading environments' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We consider an energy co-optimization problem of minimizing the total communication and motion energy of a robot tasked with transmitting a given number of information bits while moving along a fixed path. The data is transmitted to a remote station over a fading channel, which changes along the trajectory of the robot. While a previous approach to the problem used a speed-based motion-energy model, this paper uses acceleration both as an input to the system and as a basis for the motion energy which is more realistic. Furthermore, while previous approaches posed the problem in discrete time, we formulate it in continuous time. This enables us to pose the problem in an optimal control framework amenable to the use of maximum principle. We then compute the optimal control input via an effective algorithm recently developed by us that converges very fast. We use practical models for channel fading and energy consumption: the channel quality is predicted based on actual measurements, and the energy models are based on physical principles. Simulation is used to solve a specific problem and demonstrate the efficacy of our proposed approach."
      }
    ]
  },
  {
    "tid":"348",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'OASIS natural language call steering trial' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A recent trial of natural language call steering on live UK calls to the operator is described along with its results. The characteristics of the problem are described along with the acoustic, language, semantic and dialogue modelling approaches employed. Natural language call steering is found to be viable, with recognition and semantic accuracy the current limiting factors."
      }
    ]
  },
  {
    "tid":"349",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A new interactive semi-supervised clustering model for large image database indexing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Indexing methods play a very important role in finding information in large image databases. They organize indexed images in order to facilitate, accelerate and improve the results for later retrieval. Alternatively, clustering may be used for structuring the feature space so as to organize the dataset into groups of similar objects without prior knowledge (unsupervised clustering) or with a limited amount of prior knowledge (semi-supervised clustering). In this paper, we introduce a new interactive semi-supervised clustering model where prior information is integrated via pairwise constraints between images. The proposed method allows users to provide feedback in order to improve the clustering results according to their wishes. Different strategies for deducing pairwise constraints from user feedback were investigated. Our experiments on different image databases (Wang, PascalVoc2006, Caltech101) show that the proposed method outperforms semi-supervised HMRF-kmeans (Basu et al., 2004)."
      }
    ]
  },
  {
    "tid":"350",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'OHCS: A database for handwritten atomic Odia Character Recognition' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a complete database of handwritten atomic Odia characters is suggested. The first version of the database has been modeled and named OHCSv1.0 (Odia handwritten character set). The database comprises of 17,100 transcribed characters, each collected twice from 150 unique people at different point of time. Each character has 300 number of occurrences. The character images are standardized to a size of 64 \u00d7 64 pixels. A novel framework for perceiving transcribed Odia characters from this database has also been proposed. The character images are gathered into various groups in view of their shape components utilizing an incremental spectral clustering algorithm. During testing, affinity of probe character to a cluster is first decided. Subsequently, the trained classifier recognizes the character inside the cluster. Suitable simulation has been carried out to validate the scheme."
      }
    ]
  },
  {
    "tid":"351",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multiple Facial Image Editing Using Edge-Aware PDE Learning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper introduces a novel facial editing tool, called edge-aware mask, to achieve multiple photo-realistic rendering effects in a unified framework. The edge-aware masks facilitate three basic operations for adaptive facial editing, including region selection, edit setting and region blending. Inspired by the state-of-the-art edit propagation and partial differential equation PDE learning method, we propose an adaptive PDE model with facial priors for masks generation through edge-aware diffusion. The edge-aware masks can automatically fit the complex region boundary with great accuracy and produce smooth transition between different regions, which significantly improves the visual consistence of face editing and reduce the human intervention. Then, a unified and flexible facial editing framework is constructed, which consists of layer decomposition, edge-aware masks generation, and layer/mask composition. The combinations of multiple facial layers and edge-aware masks can achieve various facial effects simultaneously, including face enhancement, relighting, makeup and face blending etc. Qualitative and quantitative evaluations were performed using different datasets for different facial editing tasks. Experiments demonstrate the effectiveness and flexibility of our methods, and the comparisons with the previous methods indicate that improved results are obtained using the combination of multiple edge-aware masks."
      }
    ]
  },
  {
    "tid":"352",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Word-of-Mouth Understanding: Entity-Centric Multimodal Aspect-Opinion Mining in Social Media' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Most existing approaches on aspect-opinion mining focus on the text domain and cannot be applied to social media where the aspects are essentially multimodal and the opinions depend on the specific aspects. To address the problem of multimodal aspect-opinion mining for entities by leveraging multiple cross-collection sources in social media, in this paper we propose a multimodal aspect-opinion mod..."
      }
    ]
  },
  {
    "tid":"353",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Explaining an IT professional's preferred employment duration: empirical tests of a causal model of antecedents' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Prior research, drawing upon Rousseau's (1995) conception of psychological contracts, proposes that a critical antecedent of staying behavior is an IT professional's preferred employment duration. In previous work we further examined the antecedents of this construct, and argued that preferred employment duration is jointly determined by career anchor, life stage, and competencies of the IT professional, with the type of the employing organization serving as a potentially moderating factor. This paper presents empirical tests of the proposed conceptual model. An experiment with IT professionals at the entry stage of their careers shows that preferred employment duration is significantly influenced by the career anchors of job security and technical competence. We also find that the characteristics of the employing organization, viz., risk and variety, influence preferred employment duration. Perceived variety has a direct effect, while perceived risk negatively moderates the effects of the career anchor of job security on preferred employment duration."
      }
    ]
  },
  {
    "tid":"354",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Class of Random Fuzzy Programming and Its Hybrid PSO Algorithm' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a new class of two-stage random fuzzy programming with recourse (RFPR) problems. Since the RFPR problem usually includes random fuzzy parameters with infinite supports, it is inherently an infinite dimensional optimization problem that can rarely be solved directly by the conventional optimization algorithms. To overcome this difficulty, this paper developed an approximation method for the original RFPR problem, and turn it into a finite-dimensional one. We also establish a convergence relation between the objective values of the original problem and its approximating problem. To solve a general RFPR problem, we design a hybrid algorithm by integrating the approximation method, neural network (NN) and particle swarm optimization (PSO) algorithm. Finally, one numerical example is presented to demonstrate the effectiveness of the designed algorithm."
      }
    ]
  },
  {
    "tid":"355",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Growing Grapes in Your Computer to Defend Against Malware' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Behavior-based detection is promising to resolve the pressing security problem of malware. However, the great challenge lies in how to detect malware in a both accurate and light-weight manner. In this paper, we propose a novel behavior-based detection method, named growing grapes, aiming to enable accurate online detection. It consists of a clustering engine and detection engine. The clustering engine groups the objects, e.g., processes and files, of a suspicious program together into a cluster, just like growing grapes. The detection engine recognizes the cluster as malicious if the behaviors of the cluster match a predefined behavior template formed by a set of discrete behaviors. The approach is accurate since it identifies a malware based on multiple behaviors and the source of the processes requesting the behaviors. The approach is also light-weight as it uses OS-level information flows instead of data flows that generally impose significant performance impact on the system. To further improve the performance, a novel method of organizing the behavior template and template database is proposed, which not only makes the template matching process very quick, but also makes the storage space small and fixed. Furthermore, the detection accuracy and performance are optimized to the best degree using a combinatorial optimization algorithm, which properly selects and combines multiple behaviors to form a template for malware detection. Finally, the approach novelly identifies malicious OS objects in a cluster fashion rather than one by one as done in traditional methods, which help users to thoroughly eliminate the changes of a malware without malware family knowledge. Compared with commercial antimalware tools, extensive experiments show that our approach can detect new malware samples with higher detection rate and lower false positive rate while imposing low overhead on the system."
      }
    ]
  },
  {
    "tid":"356",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Study of Sourcing Channels for Electronic Business Transactions' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"There are two popular forms of business-to-business (B2B) marketplaces: public marketplaces and private channels. We study why firms choose either or both of these sourcing channels. Using a framework of decision making under uncertainty, we explain firms' choice of B2B channels as a hedging strategy and as a method of obtaining greater managerial flexibility for the future. We show that greater uncertainty can lead to higher investment with firms more likely to invest in both public and private channels. We find that the level of information technology (IT) capability and spending is an important factor in firms' decision making. When a firm chooses its level of IT investment simultaneously with the decision about which sourcing channels to use, the firm choosing both channels selects the highest level of IT capability and the firm implementing only one channel selects lower levels of IT capability."
      }
    ]
  },
  {
    "tid":"357",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A self-adaptive bandwidth reservation scheme for sectored cellular communications' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many mechanisms based on bandwidth reservation have been proposed in the literature to decrease connection dropping probability for handoffs in cellular communications. The handoff events occur at a much higher rate in sectored cellular networks than in traditional cellular systems. An efficient bandwidth reservation mechanism for the neighboring cells is therefore critical in the process of handoff during the connection of multimedia calls to avoid the unwillingly forced termination and waste of limited bandwidth in the sectored cellular communications, particularly when the handoff traffic is heavy. In this paper, a self-adaptive bandwidth reservation scheme, which employs a neural fuzzy bandwidth-reserving estimator, is proposed to reduce the forced termination probability. Meanwhile, a channel borrowing technique is used to decrease the new call blocking probability of real-time traffic. The simulation results show that our scheme can achieve superior performance than traditional fixed bandwidth-reserving scheme in sectored cellular networks when performance metrics are measured in terms of the forced termination probability and the new call blocking probability."
      }
    ]
  },
  {
    "tid":"358",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Finding all sorting tandem duplication random loss operations' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A tandem duplication random loss (TDRL) operation duplicates a contiguous segment of genes, followed by the random loss of one copy of each of the duplicated genes. Although the importance of this operation is founded by several recent biological studies, it has been investigated only rarely from a theoretical point of view. Of particular interest are sorting TDRLs which are TDRLs that, when applied to a permutation representing a genome, reduce the distance towards another given permutation. The identification of sorting genome rearrangement operations in general is a key ingredient of many algorithms for reconstructing the evolutionary history of a set of species. In this paper we present methods to compute all sorting TDRLs for two given gene orders. In addition, a closed formula for the number of sorting TDRLs is derived and further properties of sorting TDRLs are investigated. It is also shown that the theoretical findings are useful for identifying unique sorting TDRL scenarios for mitochondrial gene orders."
      }
    ]
  },
  {
    "tid":"359",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'MuMMI: Multiple Metrics Modeling Infrastructure' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The MuMMI (Multiple Metrics Modeling Infrastructure) project is an infrastructure that facilitates systematic measurement, modeling, and prediction of performance, power consumption and performance-power tradeoffs for parallel systems. In this paper, we present the MuMMI framework, which consists of an Instrument or, Databases and Analyzer. The MuMMI instrument or provides for automatic performance and power data collection and storage with low overhead. The MuMMI Databases store performance, power and energy consumption and hardware performance counters' data. The MuMMI Analyzer entails performance and power modeling and performance-power tradeoff and optimizations. As part of the MuMMI project, we mainly focus on discussing the design and development of a MuMMI Instrument or to provide automatic performance and power data collection and storage with low overhead on multicore systems in detail, then utilize the MuMMI Instrument or to collect performance and power data for a hybrid MPI/OpenMP earthquake application to discuss application performance-power trade-off and optimizations. Our experimental results show that we reduce up to 8.5% the application execution time and lower up to 18.35% the energy consumption by applying Dynamic Voltage and Frequency Scaling (DVFS), Dynamic Concurrency Throttling (DCT) and loop optimizations."
      }
    ]
  },
  {
    "tid":"360",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Performance Analysis of Primary User RSS/DoA Estimation and Localization in Cognitive Radio Networks Using Sectorized Antennas.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this letter we present analytical performance evaluation of primary user (PU) RSS/DoA estimation and localization through cooperating cognitive radios (CRs). We assume that the CRs are equipped with sectorized antennas, an antenna model we use to describe different types of directional antennas with a low hardware overhead (e.g. only a single RF front-end). We first derive the bias and mean square error (MSE) of RSS and DoA estimates based on the simplified least square (SLS) algorithm. We then proceed to derive the theoretical MSE of PU localization using the Stansfield algorithm that fuses SLS estimates from multiple nodes. Simulation results studying the impact of various system parameters on the RSS/DoA estimation and localization accuracy are presented to verify theoretical derivations, as well as provide design guidelines for localization systems based on sectorized antennas."
      }
    ]
  },
  {
    "tid":"361",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A note on group decision-making based on concepts of ideal and anti-ideal points in a fuzzy environment' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In a very recent paper by Kuo et al. [M.S. Kuo, G.H. Tzeng, W.C. Huang, Group decision-making based on concepts of ideal and anti-ideal points in a fuzzy environment, Mathematical and Computer Modelling 45 (3-4) (2007) 324-339], a fuzzy multicriteria decision analysis method based on the concepts of ideal and anti-ideal points was presented. This note illustrates with a numerical example that the method presented by Kuo et al. is not correct and can evaluate more than one decision alternative as the best even if they are not Pareto optimal at all. The fundamental reason for this is because the closeness coefficient values adopted by the authors do not reflect the superiority or inferiority of decision alternatives and cannot be used for ranking purpose. Corrections to their method are therefore suggested."
      }
    ]
  },
  {
    "tid":"362",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'HYDRA: large-scale social identity linkage via heterogeneous behavior modeling' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We study the problem of large-scale social identity linkage across different social media platforms, which is of critical importance to business intelligence by gaining from social data a deeper understanding and more accurate profiling of users. This paper proposes HYDRA, a solution framework which consists of three key steps: (I) modeling heterogeneous behavior by long-term behavior distribution analysis and multi-resolution temporal information matching; (II) constructing structural consistency graph to measure the high-order structure consistency on users' core social structures across different platforms; and (III) learning the mapping function by multi-objective optimization composed of both the supervised learning on pair-wise ID linkage information and the cross-platform structure consistency maximization. Extensive experiments on 10 million users across seven popular social network platforms demonstrate that HYDRA correctly identifies real user linkage across different platforms, and outperforms existing state-of-the-art algorithms by at least 20% under different settings, and 4 times better in most settings."
      }
    ]
  },
  {
    "tid":"363",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Practical applications of the R2P embedded framework for robot rapid development' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Rapid Robot Prototyping (R2P) is an open source HW/SW framework providing components for the rapid development of robotic applications. R2P framework components reuse and easy integration is obtained through an embedded real-time publish/subscribe middleware which allows distributed control loops to be set up in a flexible way. R2P aims at increasing hardware and software reuse while reducing integration time. In this paper we present the R2P framework detailing its architectural design and an example of robot platform developed with R2P. We also briefly discuss the flexibility of the approach and the easiness of reuse of its components."
      }
    ]
  },
  {
    "tid":"364",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On-demand Construction of Personalized Learning Experiences Using Semantic Web and Web 2.0 Techniques' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Nowadays, the Semantic Web technologies are exploited also in the e-learning domain in order to provide personalized and adaptive learning experiences, semantic annotation of learning contents and learner profiling. The approaches of the Web 2.0, instead, are used to implement and deploy knowledge exchange services based on the concept of social collaboration. In this work, we propose an approach resulted from the convergence between Semantic Web and Social Web to manage, agilely and easily, the contingent learning needs of workers within organizations. Our intention is to support the use of natural languages to express the learning needs for either driving the automatic generation of learning units or effectively adapting learning pathways."
      }
    ]
  },
  {
    "tid":"365",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Modified Algorithm for Generalized Discriminant Analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Generalized discriminant analysis (GDA) is an extension of the classical linear discriminant analysis (LDA) from linear domain to a nonlinear domain via the kernel trick. However, in the previous algorithm of GDA, the solutions may suffer from the degenerate eigenvalue problem (i.e., several eigenvectors with the same eigenvalue), which makes them not optimal in terms of the discriminant ability. In this letter, we propose a modified algorithm for GDA (MGDA) to solve this problem. The MGDA method aims to remove the degeneracy of GDA and find the optimal discriminant solutions, which maximize the between-class scatter in the subspace spanned by the degenerate eigenvectors of GDA. Theoretical analysis and experimental results on the ORL face database show that the MGDA method achieves better performance than the GDA method."
      }
    ]
  },
  {
    "tid":"366",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Building the Bridges -- A Proposal for Merging Different Paradigms in Mobile NFC Ecosystem' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In late 1990s. the multi-application initiative was put forward to have multiple applications on a single smart card. This would have enabled a cardholder to accumulate all of her smart card based applications (e.g. banking, telecom, and transport etc.) on a single device. However, despite the initial fervour for the multi-application smart card initiative, there were no wide spread adoption of this model. Nevertheless, the Near Field Communication (NFC) has reinvigorated the multi-application initiative again. In this paper, we will analyse why the multi-application smart card initiative failed to materialise a decade ago and whether this time around it will succeed as a viable model or not. The NFC trials being conducted basically rely on the existing ownership architectures, which can create market segregation and thus reducing the potential revenue generation capability. We propose a possible approach that avoids market segregation, increase revenue generation, and provide flexibility, robustness and scalability to existing ownership architecture."
      }
    ]
  },
  {
    "tid":"367",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Layout-Aware Switching Activity Localization to Enhance Hardware Trojan Detection' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Government agencies and the semiconductor industry have raised serious concerns about malicious modifications to the integrated circuits. The added functionality known as hardware Trojan poses major detection and isolation challenges. This paper presents a new hardware trust architecture to magnify functional Trojans activity. Trojan detection resolution depends on Trojan activity directly and circuit activity reversely. The proposed architecture reorders scan cells based on their placement during physical design to reduce circuit switching activity by limiting it into a specific region. This helps magnify Trojan contribution to the total circuit transient power by increasing Trojan-to-circuit switching activity (TCA) and Trojan-to-circuit power consumption (TCP). The proposed technique aims to improve the efficiency of power-based side-channel signal analysis techniques for detecting hardware Trojans. Our simulation results demonstrate the efficiency of the method in significantly increasing TCA and TCP."
      }
    ]
  },
  {
    "tid":"368",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multi-objective optimization of water-using systems.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Industrial water systems often allow efficient water uses via water reuse and/or recirculation. The design of the network layout connecting water-using processes is a complex problem which involves several criteria to optimize. Most of the time, this design is achieved using Water Pinch technology, optimizing the freshwater flow rate entering the system. This paper describes an approach that considers two criteria: (i) the minimization of freshwater consumption and (ii) the minimization of the infrastructure cost required to build the network. The optimization model considers water reuse between operations and wastewater treatment as the main mechanisms to reduce freshwater consumption. The model is solved using multi-objective distributed Q-learning (MDQL), a heuristic approach based on the exploitation of knowledge acquired during the search process. MDQL has been previously tested on several multi-objective optimization benchmark problems with promising results [C. Mariano, Reinforcement learning in multi-objective optimization, Ph.D. thesis in Computer Science, Instituto Tecnol\u00f3gico y de Estudios Superiores de Monterrey, Campus Cuernavaca, March, 2002, Cuernavaca, Mor., M\u00e9xico, 2001]. In order to compare the quality of the results obtained with MDQL, the reduced gradient method was applied to solve a weighted combination of the two objective functions used in the model. The proposed approach was tested on three cases: (i) a single contaminant four unitary operations problem where freshwater consumption is reduced via water reuse, (ii) a four contaminants real-world case with ten unitary operations, also with water reuse, and (iii) the water distribution network operation of Cuernavaca, Mexico, considering reduction of water leaks, operation of existing treatment plants at their design capacity, and design and construction of new treatment infrastructure to treat 100% of the wastewater produced. It is shown that the proposed approach can solved highly constrained real-world multi-objective optimization problems."
      }
    ]
  },
  {
    "tid":"369",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multipath Key Exchange on P2P Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Distributed-hash-table (DHT) has been proposed to solve the problem of scaling for P2P networks. However, there are some problems of security in P2P networks. One of the problem is that key exchange is not performed preventing against endangering security. This reason may be the fact that P2P networks have no trusted server. Although certification and authentication are able to protect key exchange from spoofing and man-in-the-middle attacks, these cannot be applied because of that. Therefore, an attacker can easily compromise key exchange since P2P networks should accept any node whether it is malicious or not, and every node of P2P networks plays a role of router. Until today, for the above reasons, there is no way to exchange the secret key on P2P networks protecting against encountering security. In this paper, we will propose a key exchange method on P2P networks protecting against spoofing and man-in-the-middle attacks. For the purpose of protecting key exchange from encountering security, we will present the enhanced routings which are directional and probabilistic routings."
      }
    ]
  },
  {
    "tid":"370",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Epidemic Model of Computer Worms with Time Delay and Variable Infection Rate.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With rapid development of Internet, network security issues become increasingly serious. Temporary patches have been put on the infectious hosts, which may lose efficacy on occasions. This leads to a time delay when vaccinated hosts change to susceptible hosts. On the other hand, the worm infection is usually a nonlinear process. Considering the actual situation, a variable infection rate is introduced to describe the spread process of worms. According to above aspects, we propose a time-delayed worm propagation model with variable infection rate. Then the existence condition and the stability of the positive equilibrium are derived. Due to the existence of time delay, the worm propagation system may be unstable and out of control. Moreover, the threshold tau(0) of Hopf bifurcation is obtained. The worm propagation system is stable if time delay is less than tau(0). When time delay is over tau(0), the system will be unstable. In addition, numerical experiments have been performed, which can match the conclusions we deduce. The numerical experiments also show that there exists a threshold in the parameter.., which implies that we should choose appropriate infection rate beta(iota) to constrain worm prevalence. Finally, simulation experiments are carried out to prove the validity of our conclusions."
      }
    ]
  },
  {
    "tid":"371",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Automatically wrapping legacy software into services: A grid case study' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The computational grid is rapidly evolving into a service-oriented computing infrastructure that facilitates resource sharing\n for solving large-scale data and computationally intensive problems. Peer-to-peer (P2P) systems have emerged as an infrastructure\n enabling technologies for enhanced scalability and reliability in file sharing and content distribution. It is envisioned\n that P2P enabled service-oriented grid systems would virtualize various resources as services with high scalability and reliability.\n Many legacy software resources exist nowadays, but making them grid aware services for effective resource sharing has become\n an issue of vital importance. This paper presents GSLab, a toolkit for automatically wrapping legacy software into services\n that can be published, discovered and reused in grid environments. GSLab employs Sun Grid Engine (SGE) to enhance its performance\n in execution of wrapped services. Using GSLab, we have automatically wrapped a legacy computer animation rendering code written\n in C as a service that can be discovered and accessed in a SGE environment. The evaluation results show that the performance\n of GSLab improves with an increase in the number of computing nodes involved."
      }
    ]
  },
  {
    "tid":"372",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fighting the Semantic Gap on CBIR Systems through New Relevance Feedback Techniques' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper introduces two novel relevance feedback techniques that integrate a new way to implement the query center movement with a suitable weighting on the similarity function. These techniques integrated to a content-based image retrieval (CBIR) system, improves the precision of the results when using texture features up to 42%, and employing at most 5 iterations. Thus, the user satisfaction with the system is increased as our experiments demonstrated. Besides being effective, the new RF techniques are very fast as they take less than one second to reprocess the queries at each iteration. The experiments also show that with three iterations the users are satisfied with the query results, and the major gain in precision happens in the first iteration, achieving improvements of up to 30%, what lessens the user efforts and anxiety."
      }
    ]
  },
  {
    "tid":"373",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Teaching humanoid robotics by means of human teleoperation through RGB-D sensors' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a graduate course project on humanoid robotics offered by the University of Padova. The target is to safely lift an object by teleoperating a small humanoid. Students have to map human limbs into robot joints, guarantee the robot stability during the motion, and teleoperate the robot to perform the correct movement."
      }
    ]
  },
  {
    "tid":"374",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Telling stories to computers for document retrieval' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The documents users must handle are growing both in number and diversity. However, the ways of organizing and retrieving them remain largely unchanged. Given the innate human ability to tell stories, the use of narratives can be a natural and effective way to retrieve documents. To better understand how narratives can be used in this context, a thorough characterization of their contents and structure was obtained from several interviews. Then, the results were validated by the evaluation of low fidelity prototypes for story-capture interfaces, allowing us to verify that stories are valid as document-retrieval tools, and the shape those interfaces should take (structured text entry)."
      }
    ]
  },
  {
    "tid":"375",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Exploring students learning behavior with an interactive etextbook in computer science courses.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present empirical findings from using an interactive electronic textbook (eTextbook) system named OpenDSA to teach sophomore- and junior-level Computer Science courses. The web-based eTextbook infrastructure allows us to collect large amounts of data that can provide detailed information about students\u2019 study behavior. In particular we were interested in seeing if the students will attempt to manipulate the electronic resources so as to receive credit without deeply going through the materials. We found that a majority of students do not read the text. On the other hand, we found evidence that students voluntarily complete additional exercises (after obtaining credit for completion) as a study aid prior to exams. We determined that visualization use was fairly high (even when credit for their completion was not offered). Skipping to the end of slideshows was more common when credit for their completion was offered, but also occurred when it was not. We measured the level of use of mobile devices for learning by CS students. Almost all students did not associate their mobile devices with studying. The only time they accessed OpenDSA from a mobile device was for a quick look up, and never for in depth study."
      }
    ]
  },
  {
    "tid":"376",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'M-CLOCK: migration-optimized page replacement algorithm for hybrid DRAM and PCM memory architecture' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Phase Change Memory (PCM) has drawn great attention as a main memory due to its attractive characteristics such as non-volatility, byte-addressability, and in-place update. However, since the capacity of PCM is not fully mature yet, hybrid memory architecture that consists of DRAM and PCM has been suggested. In addition, page replacement algorithm based on hybrid memory architecture is actively being studied because existing page replacement algorithms cannot be used on hybrid memory architecture in that they do not consider the two weaknesses of PCM: high write latency and low endurance. In this paper, to mitigate the above hardware limitations of PCM, we revisit the page cache layer for the hybrid memory architecture. We also propose a novel page replacement algorithm, called M-CLOCK, to improve the performance of hybrid memory architecture and the lifespan of PCM. In particular, M-CLOCK aims to reduce the number of PCM writes that negatively affect the performance of hybrid memory architecture. Experimental results clearly show that M-CLOCK outperforms the state-of-the-art page replacement algorithms in terms of the number of PCM writes and effective memory access time by up to 98% and 34%, respectively.\n\n"
      }
    ]
  },
  {
    "tid":"377",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Predictive Task Monitoring for Business Processes.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Information sources providing real-time status of physical objects have drastically increased in recent times. So far, research in business process monitoring has mainly focused on checking the completion of tasks. However, the availability of real-time information allows for a more detailed tracking of individual business tasks. This paper describes a framework for controlling the safe execution of tasks and signalling possible misbehaviours at runtime. It outlines a real use case on smart logistics and the preliminary results of its application."
      }
    ]
  },
  {
    "tid":"378",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Energy-efficient non-cooperative cognitive radio networks: micro, meso, and macro views' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cognitive radio technology can significantly improve spectrum utilization efficiency via enabling secondary users to access licensed spectrum dynamically without harmful interference to primary users. Most existing works on cognitive radio networks were focused on enhancing spectrum efficiency using spectrum sensing, spectrum sharing, and network deployment schemes. However, the energy efficiency issue was largely ignored. Thus, the dilemma between high energy consumption by fast-growing wireless communications and fewer global energy resources has motivated research on energy-efficient cognitive radio networks for enhancing spectrum efficiency and energy efficiency simultaneously. In this article, we overview energy-efficient non-cooperative cognitive radio networks from the micro, meso, and macro perspectives, where the micro view means how to design energy-efficient spectrum sensing algorithms for each individual secondary user, the meso view means how to coordinate non-cooperative secondary users to share spectrum efficiently, and the macro view means how to deploy cognitive radio networks in an energy-efficient approach."
      }
    ]
  },
  {
    "tid":"379",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Combining rhythm-based and pitch-based methods for background and melody separation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Musical works are often composed of two characteristic components: the background (typically the musical accompaniment), which generally exhibits a strong rhythmic structure with distinctive repeating time elements, and the melody (typically the singing voice or a solo instrument), which generally exhibits a strong harmonic structure with a distinctive predominant pitch contour. Drawing from findings in cognitive psychology, we propose to investigate the simple combination of two dedicated approaches for separating those two components: a rhythm-based method that focuses on extracting the background via a rhythmic mask derived from identifying the repeating time elements in the mixture and a pitch-based method that focuses on extracting the melody via a harmonic mask derived from identifying the predominant pitch contour in the mixture. Evaluation on a data set of song clips showed that combining such two contrasting yet complementary methods can help to improve separation performance--from the point of view of both components--compared with using only one of those methods, and also compared with two other state-of-the-art approaches."
      }
    ]
  },
  {
    "tid":"380",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'CHARMY: A Framework for Designing and Verifying Architectural Specifications' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Introduced in the early stages of software development, the Charmy framework assists the software architect in making and evaluating architectural choices. Rarely, the software architecture of a system can be established once and forever. Most likely poorly defined and understood architectural constraints and requirements force the software architect to accept ambiguities and move forward to the construction of a suboptimal software architecture. Charmy aims to provide an easy and practical tool for supporting the iterative modeling and evaluation of software architectures. From an UML-based architectural design, an executable prototype is automatically created. Charmy simulation and model checking features help in understanding the functioning of the system and discovering potential inconsistencies of the design. When a satisfactory and stable software architecture is reached, Java code conforming to structural software architecture constraints is automatically generated through suitable transformations. The overall approach is tool supported."
      }
    ]
  },
  {
    "tid":"381",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mobile crowd sensing management with the ParticipAct living lab.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The widespread availability of smartphones have enabled the blossom of Mobile Crowd Sensing (MCS) projects, whose goal is to involve users in participatory tasks aimed at building large real-world datasets. In this framework, we present the large-scale experience of the ParticipAct Living Lab, an ongoing experiment at the University of Bologna, which involves about 170 students in MCS campaigns. Specifically, we originally present the analysis of the large set of ParticipAct collected results against some primary datasets in the literature; we present the evaluation and assessment of the original participatory sensing campaign management aspects of ParticipAct; and we report the lessons learned from this wide-scale deployment experience."
      }
    ]
  },
  {
    "tid":"382",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Modeling And Detecting Anomalous Topic Access' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"There has been considerable success in developing strategies to detect insider threats in information systems based on what one might call the random object access model or ROA. This approach models illegitimate users as ones who randomly access records. The goal is to use statistics, machine learning, knowledge of workflows and other techniques to support an anomaly detection framework that finds such users. In this paper we introduce and study a random topic access model or RTA aimed at users whose access may be illegitimate but is not fully random because it is focused on common semantic themes. We argue that this model is appropriate for a meaningful range of attacks and develop a system based on topic summarization that is able to formalize the model and provide anomalous user detection effectively for it. To this end, we use healthcare as an example and propose a framework for evaluating the ability to recognize various types of random users called random topic access detection or RTAD. Specifically, we utilize a combination of Latent Dirichlet Allocation (LDA), for feature extraction, a k-nearest neighbor (k-NN) algorithm for outlier detection and evaluate the ability to identify different adversarial types. We validate the technique in the context of hospital audit logs where we show varying degrees of success based on user roles and the anticipated characteristics of attackers. In particular, it was found that RTAD exhibits strong performance for roles are described by a few topics, but weaker performance when users are more topic-agnostic."
      }
    ]
  },
  {
    "tid":"383",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Robust blind adaptive beamforming under double constraints.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In practical complex communication environments, the performance of linearly constrained constant modulus algorithm (CMA) is known to degrade severely in the presence of even slight signal steering vector mismatches. To overcome the mismatches, a novel robust CMA is proposed for blind adaptive beamforming via the worst-case performance optimization and the oblique projection of signal steering vector, which is based on double constraints on explicit modeling of uncertainties in the desired signal array in this paper. To improve robustness, the weight vector is optimized to involve minimization of a constant modulus algorithm objective function with penalty for the worst-case signal steering vector. The theoretical analysis for our proposed algorithm in terms of SINR and convergence performance is presented in this paper. The proposed robust constrained CMA resolves the problem of interference capture, provides excellent robustness against the signal steering vector mismatches, and improves array output performance. Simulation results are presented to show the excellence of this technique and the main parameters of concern to evaluate the performance are analyzed. \u00a9 2011 Springer-Verlag London Limited."
      }
    ]
  },
  {
    "tid":"384",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Exploiting Bilingual Translation for Question Retrieval in Community-Based Question Answering.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Community-based question answering (CQA) has become an important issue due to the popularity of CQA archives on the web. This paper is concerned with the problem of question retrieval. Question retrieval in CQA archives aims to find historical questions that are semantically equivalent or relevant to the queried questions. However, question retrieval is challenging partly due to the word ambiguity and lexical gap between the queried questions and the historical questions in the archives. To deal with these problems, we propose the use of translated words to enrich the question representation, going beyond the words in the original language to represent a question. In this paper, each original language question (e.g., English) is automatically translated into an foreign language (e.g., Chinese) by machine translation services, and the resulting translated questions serves as a semantically enhanced representation for supplementing the original bag of words. Experiments conducted on real CQA data set demonstrate that our proposed approach significantly outperforms several baseline methods and achieves the state-of-the-art performance. \u00a9 2012 The COLING."
      }
    ]
  },
  {
    "tid":"385",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Select your SNPs (SYSNPs): a web tool for automatic and massive selection of SNPs.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Association studies are the choice approach in the discovery of the genomic basis of complex traits. To carry out such analysis, researchers frequently need to (1) select optimally informative sets of Single Nucleotide Polymorphisms (SNPs) in candidate regions and (2) annotate the results of associations found by means of genome-wide SNP arrays. These are complex tasks, since many criteria have to be considered, including the SNPs' functional properties, technological information and haplotype frequencies in given populations. SYSNPs implements algorithms that allow for efficient and simultaneous consideration of all the relevant criteria to obtain sets of SNPs that properly cover arbitrarily large lists of genes or genomic regions. Complementarily, SYSNPs allows for comprehensive functional annotation of SNPs linked to any given marker SNP. SYSNPs dramatically reduces the effort needed for SNP selection from days of searching various databases to a few minutes using a simple browser."
      }
    ]
  },
  {
    "tid":"386",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Construction of indoor floor plan and localization' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Indoor positioning and tracking services are garnering more attention. Recently, several state-of-the-art localization techniques have been proposed that use radio maps or the sensors readily available on smartphones. This paper presents a localization system called construction of indoor floor plan and localization (CALL), which is based on a floor plan, access points (APs), and smartphone sensors. The public access points provide a number of reference points that can be used to build the floor plan automatically, and acquire the user initial position. Moreover, The APs are used to calibrate and adjust the user\u2019s position, distance, and direction in real time. Smartphone sensors are used to detect the number of steps and the direction. The proposed method is implemented on a smartphone and tested in real indoor environments. Our experiment with CALL demonstrates that using a static floor plan instead of radio map will avoid the costly database updates and searches that are usually required in other approaches due to signal attenuation. It shows that CALL outperforms the existing works in terms of accuracy and effectiveness in indoor localization.\n"
      }
    ]
  },
  {
    "tid":"387",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'New media for teaching applied cryptography and network security' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Considering that security education needs to train students to deal with security problems in real environments, we developed new media for teaching applied cryptography and network security. They permit students to gain real-life security experience from virtual laboratories on CD, DVD, or online. The application of those media is helpful to reduce investment and administration costs as well as effectively support security experiments which previously had to be done in conventional security laboratories."
      }
    ]
  },
  {
    "tid":"388",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Gated cardiac SPECT using different motion models' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Gated cardiac SPECT imaging suffers from several degrading factors ranging from limited data counts to motion blur. In this work we investigate the use of different optical flow estimation methods for motion-compensated temporal processing in gated SPECT, of which the goal is to improve the accuracy of image reconstruction. In particular, we examine how much room is still left in optical flow estimation toward improving the reconstructed image quality. In our experiments we conducted a quantitative evaluation using simulated imaging study with multiple noise realizations from the NCAT phantom. As an upper bound, the known motion in NCAT was used for reconstruction. The results show that 4D reconstruction with a periodic optical flow model can achieve results almost matching that from the known motion."
      }
    ]
  },
  {
    "tid":"389",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'CUBIC: Search for Binding Sites' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The regulation of gene transcription is achieved through specific interactions between transcription factors and their binding sites in the upstream region of the gene being regulated. Correct identification of these binding sites represents a key challenging problem in computational biology. Our approach to the problem is to find a \"clear\" cluster in the space of all k-mers from the upstream regulatory regions of a set of genes that potentially share similar binding sites. The cluster identification is performed by using Minimal Spanning Tree (MST) technique with a special distance between k-mers based on the chosen Profile. It's shown that widely used \"conservation\" characteristic in position is a result of a \"common sense\" requirement for \"conservation\". The local convergence of algorithm for \"conservation\" maximization of Profile has been proved and the method for statistical significance evaluation of results is presented. All ideas have been implemented in a form of software CUBIC."
      }
    ]
  },
  {
    "tid":"390",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Unmixing-Based Bayesian Model for Spatio-Temporal Satellite Image Fusion in Heterogeneous Landscapes.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Studies of land surface dynamics in heterogeneous landscapes often require satellite images with a high resolution, both in time and space. However, the design of satellite sensors often inherently limits the availability of such images. Images with high spatial resolution tend to have relatively low temporal resolution, and vice versa. Therefore, fusion of the two types of images provides a useful way to generate data high in both spatial and temporal resolutions. A Bayesian data fusion framework can produce the target high-resolution image based on a rigorous statistical foundation. However, existing Bayesian data fusion algorithms, such as STBDF (spatio-temporal Bayesian data fusion) -I and -II, do not fully incorporate the mixed information contained in low-spatial-resolution pixels, which in turn might limit their fusion ability in heterogeneous landscapes. To enhance the capability of existing STBDF models in handling heterogeneous areas, this study proposes two improved Bayesian data fusion approaches, coined ISTBDF-I and ISTBDF-II, which incorporate an unmixing-based algorithm into the existing STBDF framework. The performance of the proposed algorithms is visually and quantitatively compared with STBDF-II using simulated data and real satellite images. Experimental results show that the proposed algorithms generate improved spatio-temporal-resolution images over STBDF-II, especially in heterogeneous areas. They shed light on the way to further enhance our fusion capability."
      }
    ]
  },
  {
    "tid":"391",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'GUITAR: Piecing Together Android App GUIs from Memory Images' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"An Android app's graphical user interface (GUI) displays rich semantic and contextual information about the smartphone's owner and app's execution. Such information provides vital clues to the investigation of crimes in both cyber and physical spaces. In real-world digital forensics however, once an electronic device becomes evidence most manual interactions with it are prohibited by criminal investigation protocols. Hence investigators must resort to \\\"image-and-analyze\\\" memory forensics (instead of browsing through the subject phone) to recover the apps' GUIs. Unfortunately, GUI reconstruction is still largely impossible with state-of-the-art memory forensics techniques, which tend to focus only on individual in-memory data structures. An Android GUI, however, displays diverse visual elements each built from numerous data structure instances. Furthermore, whenever an app is sent to the background, its GUI structure will be explicitly deallocated and disintegrated by the Android framework. In this paper, we present GUITAR, an app-independent technique which automatically reassembles and redraws all apps' GUIs from the multitude of GUI data elements found in a smartphone's memory image. To do so, GUITAR involves the reconstruction of (1) GUI tree topology, (2) drawing operation mapping, and (3) runtime environment for redrawing. Our evaluation shows that GUITAR is highly accurate (80-95% similar to original screenshots) at reconstructing GUIs from memory images taken from a variety of Android apps on popular phones. Moreover, GUITAR is robust in reconstructing meaningful GUIs even when facing GUI data loss."
      }
    ]
  },
  {
    "tid":"392",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Recent research development in flip-chip routing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The flip-chip package is introduced for modern IC designs with higher integration density, larger I/O counts, faster speed, better signal integrity, etc. To ease design changes, an extra metal layer is introduced to redistribute nets between wire-bonding (I/O) pads in a die and bump pads in a package carrier. Flipchip routing is performed by redistributing and interconnecting nets between the I/O and bump pads. As the design complexity grows, routing has played a pivotal role in flip-chip design. In this paper, we first introduce popular flip-chip structures, their routing-region modeling, and induced routing problems, survey key published techniques for flip-chip routing with respect to specific structures and pad assignment methods, and provide some future research directions for the modern flip-chip routing problem."
      }
    ]
  },
  {
    "tid":"393",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Discovering and Exploiting Statistical Properties for Query Optimization in Relational Databases: A Survey' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Discovering and exploiting statistical features in relational datasets is key to query optimization in a relational database management system (RDBMS ), and is also needed for database design, cleaning, and integration. This paper surveys a variety of methods for automatically discovering important statistical features such as correlations, functional dependencies, keys, and algebraic constraints. We discuss proactive approaches in which the data is scanned or sampled (periodically, at optimization time or at query time), or in which exploratory queries are executed. Also discussed are reactive approaches that monitor the results of the query processing. Finally, we discuss methods for dealing with the practical challenges of maintaining statistical information in the face of heavy system utilization, and of dealing with inconsistencies that arise from incomplete cardinality models, use of multiple discovery methods, or changes in the underlying data over time. \u00a9 2009 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 1: 000-000, 2008"
      }
    ]
  },
  {
    "tid":"394",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Empowering the Creative User : Personalized HTTP-based Adaptive Streaming of Multi-path Nonlinear Video' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents the design, implementation, and validation of a novel system that supports streaming and playout of personalized, multi-path, nonlinear video. In contrast to regular video, in which the file content is played sequentially, our design allows multiple nonlinear video sequences of the underlying (linear) video to be stitched together and played in any personalized order, and clients can be provided multiple path choices. The design combines the ideas of HTTP-based adaptive streaming (HAS) and multi-path nonlinear video. Personalization of the content is achieved with the use of a customized metafile, which is downloaded separately from the underlying media and the manifest file that defines the HAS structure. An extension to the user interface allows path choices to be presented to and made by the user. Novel buffer management and prefetching policies are used to ensure seamless uninterrupted playback regardless of client path choices, even under scenarios in which clients defer their choices until the last possible moment. Our solution allows creative home users to easily create their own multi-path nonlinear video, opening the door to an endless possibility of new opportunities and media forms.\n\n"
      }
    ]
  },
  {
    "tid":"395",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Classified quadtree-based adaptive loop filter' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a classified quadtree-based adaptive loop filter (CQALF) in video coding. Pixels in a picture are classified into two categories by considering the impact of the deblocking filter, the pixels that are modified and the pixels that are not modified by the deblocking filter. A wiener filter is carefully designed for each category and the filter coefficients are transmitted to decoder. For the pixels that are modified by the deblocking filter, the filter is estimated at encoder by minimizing the mean square error between the original input frame and a combined frame which is a weighted average of the reconstructed frames before and after the deblocking filter. For pixels that the deblocking filter does not modify, the filter is estimated by minimizing the mean square error between the original frame and the reconstructed frame. The proposed algorithm is implemented on top of KTA software and compatible with the quadtree-based adaptive loop filter. Compared with kta2.6rl anchor, the proposed CQALF achieves 10.05%, 7.55%, and 6.19% BD bitrate reduction in average for intra only, IPPP, and HB coding structures respectively."
      }
    ]
  },
  {
    "tid":"396",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Target market selection and marketing effort under uncertainty: The selective newsvendor' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We consider a firm that markets, procures, and delivers a good with a single selling season in a number of different markets. The price for the good is market-dependent, and each market has an associated demand distribution, with parameters that depend on the amount of marketing effort applied. Given long procurement lead-times, the firm must decide which markets it will serve prior to procuring the good. We develop a profit maximizing model to address the firm\u2019s integrated market selection, marketing effort, and procurement decisions. The model implicitly accounts for inventory pooling across markets, which reduces safety stock costs but increases model complexity. The resulting model is a nonlinear integer optimization problem, for which we develop specialized solution methods. For the case in which budget constraints exist, we provide a novel solution approach that uses a tailored branch-and-bound algorithm. Our approach solves a broad range of 3000 test instances in an average of less than 2 seconds, significantly outperforming a leading commercial global optimization solver."
      }
    ]
  },
  {
    "tid":"397",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Expandable and Cost-Effective Network Structures for Data Centers Using Dual-Port Servers' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A fundamental goal of data center networking is to efficiently interconnect a large number of servers with the low equipment cost. Several server-centric network structures for data centers have been proposed. They, however, are not truly expandable and suffer a low degree of regularity and symmetry. Inspired by the commodity servers in today's data centers that come with dual port, we consider how to build expandable and cost-effective structures without expensive high-end switches and additional hardware on servers except the two NIC ports. In this paper, two such network structures, called HCN and BCN, are designed, both of which are of server degree 2. We also develop the low overhead and robust routing mechanisms for HCN and BCN. Although the server degree is only 2, HCN can be expanded very easily to encompass hundreds of thousands servers with the low diameter and high bisection width. Additionally, HCN offers a high degree of regularity, scalability, and symmetry, which conform to the modular designs of data centers. BCN is the largest known network structure for data centers with the server degree 2 and network diameter 7. Furthermore, BCN has many attractive features, including the low diameter, high bisection width, large number of node-disjoint paths for the one-to-one traffic, and good fault-tolerant ability. Mathematical analysis and comprehensive simulations show that HCN and BCN possess excellent topological properties and are viable network structures for data centers."
      }
    ]
  },
  {
    "tid":"398",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'What Can Be Verified Locally?.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In the framework of distributed network computing, it is known that not all Turing-decidable predicates on labeled networks can be decided locally whenever the computing entities are Turing machines (TM). This holds even if nodes are running non-deterministic Turing machines (NTM). In contrast, we show that every Turing-decidable predicate on labeled networks can be decided locally if nodes are running alternating Turing machines (ATM). More specifically, we show that, for every such predicate, there is a local algorithm for ATMs, with at most two alternations, that decides whether the actual labeled network satisfies that predicate. To this aim, we define a hierarchy of classes of decision tasks, where the lowest level contains tasks solvable with TMs, the first level those solvable with NTMs, and the level k>1 contains those tasks solvable with ATMs with k\u22121 alternations. We characterize the entire hierarchy, and show that it collapses in the second level."
      }
    ]
  },
  {
    "tid":"399",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'From natural language definitions to knowledge bases axioms' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we propose to assist ontology development in the case of simple dictionary-like definitions discussing a system which translates natural language definitions into logical formulas and providing a partial translation from first-order formulas into Description Logics formulas. We compared our approach with other existing state-of-the-art tools using different sets of dictionarylike sentences, obtaining some preliminary but encouraging results."
      }
    ]
  },
  {
    "tid":"400",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Networks of interactions between feed-forward loop transcriptional motifs in gene-regulatory networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Transcriptional motifs are smaller subnetworks found within the gene-regulatory networks of many organisms in larger abundance than can be explained by chance alone. The feed-forward loop is one such three-node motif, wherein one top-level protein regulates the expression of a target gene either directly, or indirectly through an intermediate regulator protein. However, no systematic effort has yet been made to understand how individual feed-forward loops interconnect. Here, we address this problem by examining embedded transcriptional motifs that interact topologically by sharing one (vertex-share graphs), two (edge-share graphs), or three (triad-share graphs) nodes. Using transcriptional networks of the bacterium Escherichia coli and the yeast Saccharomyces cerevisiae, we constructed networks of feed-forward loops based on these interaction patterns, and termed them \\\"motif networks.\\\" In view of these motif networks, we show that, on average, feed-forward loops connect primarily to others similarly connected--a phenomenon termed assortativity or homophily and often attributed to social networks. We fit these correlations to a power-law equation, which exhibits a sublinear exponent indicative of an \\\"economy of scale\\\" in the FFL connectivity. We show that connectivity distributions of the motif networks (similar to degree distributions in complex networks) appear approximately uniform, but with a large variance. Although assortative mixing may arise from a scale-free degree distribution, we conclude that assortativity observed here arises by alternative means."
      }
    ]
  },
  {
    "tid":"401",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multilevel global placement with congestion control' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Abstract, In this paper, we develop a multilevel global place-ment algorithm (MGP) integrated with fast incremental global routing for directly updating and optimizing congestion cost during physical hierarchy generation. Fast global routing is achieved using a fast two-bend routing and incremental A-tree algorithm. The routing congestion is modeled by the wire usage estimated by the fast global router. A hierarchical area density control is developed for placing objects with significant size vari-ations. Experimental results show that, compared to GORDIAN-L, the wire length-driven MGP is 4-6.7 times faster and generates slightly better wire length for test circuits larger than 100 000 cells. Moreover, the congestion-driven MGP improves wiring overflow by 45%-74% with 5% larger bounding box wire length but 3%-7% shorter routing wire length measured by a graph-based A-tree global router. Index Terms, Congestion, deep submicrometer, interconnect, physical hierarchy, placement, routing. "
      }
    ]
  },
  {
    "tid":"402",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Partial Sorting Problem on Evolving Data.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we investigate the top--selection problem, i.e. to determine and sort the top  elements, in the dynamic data model. Here  means that the underlying total order evolves over time, and that the order can only be probed by pair-wise comparisons. It is assumed that at each time step, only one pair of elements can be compared. This assumption of restricted access is reasonable in the dynamic model, especially for massive data sets where it is impossible to access all the data before the next change occurs. Previously only two special cases were studied (Anagnostopoulos et\u00a0al. in 36th international colloquium on automata, languages and programming (ICALP). LNCS, vol 5566, pp 339\u2013350, ) in this model: selecting the element of a given rank, and sorting all elements. This paper systematically deals with . Specifically, we identify the critical point  such that the top--selection problem can be solved error-free with probability  if and only if . A lower bound of the error when  is also determined, which actually is tight under some conditions. In contrast, we show that the top--set problem, which means finding the top  elements without sorting them, can be solved error-free with probability  for all . Additionally, we consider some extensions of the dynamic data model and show that most of these results still hold."
      }
    ]
  },
  {
    "tid":"403",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Reduce to the Max: A Simple Approach for Massive-Scale Privacy-Preserving Collaborative Network Measurements (Short Paper)' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Privacy-preserving techniques for distributed computation have been proposed recently as a promising framework in collaborative inter-domain network monitoring. Several different approaches exist to solve such class of problems, e.g., Homomorphic Encryption (HE) and Secure Multiparty Computation (SMC) based on Shamir's Secret Sharing algorithm (SSS). Such techniques are complete from a computation-theoretic perspective: given a set of private inputs, it is possible to perform arbitrary computation tasks without revealing any of the intermediate results. In this paper we advocate the use of \"elementary\" (as opposite to \"complete\") Secure Multiparty Computation (E-SMC) procedures for traffic monitoring. E-SMC supports only simple computations with private input and public output, i.e., they can not handle secret input nor secret (intermediate) output. The proposed simplification brings a dramatic reduction in complexity and enables massive-scale implementation with acceptable delay and overhead. Notwithstanding their simplicity, we claim that a simple additive E-SMC scheme is sufficient to perform many computation tasks of practical relevance to collaborative network monitoring, such as anonymous publishing and set operations."
      }
    ]
  },
  {
    "tid":"404",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An interface system based on multimodal principle for cardiological diagnosis assistance' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A pilot interface system is developed for computer-aided diagnostic and interventional assistance of cardiologists based on clinical trials. The system includes optimized methods for processing and presentation of cardiological knowledge and data. The multimodal principle of the interface is based on know-how tests of a new magnetic microsensor for detection of catheter navigation in images from angiograms and intra-vascular ultrasound and on new investigations of the principle of operation of the electrocardiograph. The user interface includes interactive access to the clinical database and a cognitive approach to disease visualization."
      }
    ]
  },
  {
    "tid":"405",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Wave and Wait Protocol (WWP): An Energy-Saving Transport Protocol for Mobile IP-Devices' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This work involves the development of an experimental transport-level protocol running on top of IP for small, mobile, wireless stations. The protocol does not aim to handle real-time traffic. Its central concern is to conserve battery-powered energy used for transmission, even at the expense of slower data throughput. It attempts to conserve energy expenditure by adjusting the amount of data transmitted, trying to keep it below perceived network congestion. The higher the detected congestion risk-level in the network the less it attempts to transmit, thereby minimizing the need for duplicate data retransmission due to congested routers losing packets and so on. We outline the protocol specification, mechanisms and implementation, as well as preliminary evaluation results that clearly demonstrate the energy-saving capabilities of the protocol."
      }
    ]
  },
  {
    "tid":"406",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Future research directions in design of reliable communication systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this position paper on reliable networks, we discuss new trends in the design of reliable communication systems. We focus on a wide range of research directions including protection against software failures as well as failures of communication systems equipment. In particular, we outline future research trends in software failure mitigation, reliability of wireless communications, robust optimization and network design, multilevel and multirealm network resilience, multiple criteria routing approaches in multilayer networks, resilience options of the fixed IP backbone network in the interplay with the optical layer survivability, reliability of cloud computing networks, and resiliency of software-defined networks. Described research directions are frequently enhanced with examples."
      }
    ]
  },
  {
    "tid":"407",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Distributed Rate Allocation in Inter-Session Network Coding' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this work, we propose a distributed rate allocation algorithm that minimizes the average decoding delay for multimedia clients in inter-session network coding systems. We consider a scenario where the users are organized in a mesh network and each user requests the content of one of the available sources. We propose a novel distributed algorithm where network users determine the coding operations and the packet rates to be requested from the parent nodes, such that the decoding delay is minimized for all clients. A rate allocation problem is solved by every user, which seeks the rates that minimize the average decoding delay for its children and for itself. Since this optimization problem is a priori non-convex, we introduce the concept of equivalent packet flows, which permits to estimate the expected number of packets that every user needs to collect for decoding. We then decompose our original rate allocation problem into a set of convex subproblems, which are eventually combined to obtain an effective approximate solution to the delay minimization problem. The results demonstrate that the proposed scheme eliminates the bottlenecks and reduces the decoding delay experienced by users with limited bandwidth resources. We validate the performance of our distributed rate allocation algorithm in different video streaming scenarios using the NS-3 network simulator. We show that our system is able to take benefit of inter-session network coding for simultaneous delivery of video sessions in networks with path diversity."
      }
    ]
  },
  {
    "tid":"408",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Increasing Spatial Spectrum Utilization Through Opportunistic User-to-User Communications.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The requirements for ubiquitous and highly reliable wireless services, combined with the low utilization of licensed spectrum, call for flexible and efficient spectrum management schemes. To this end, a lot of attention is paid in the literature on allowing secondary-external users to opportunistically access the licensed spectrum. In parallel to these efforts, the question is whether the licensed users could further improve their own spectrum utilization. In this paper, we focus on increasing the spatial spectrum utilization of an infrastructure-based wireless system by adding autonomous functionality to the primary (system) users. An opportunistic operation mode for the uplink (UL) period, totally transparent to the base station (BS) of the system, is introduced. Users operating in this mode identify spatial spectrum UL opportunities by interpreting BS broadcast messages, and exploit these opportunities by establishing direct connections. It is shown that multiple direct connections can take place in parallel with a single standard UL transmission. Moreover, significant additional throughput is achieved, and in most of the cases, the energy consumption for the direct connections is lower than that of the conventional ones (using the standard mode)."
      }
    ]
  },
  {
    "tid":"409",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Distributed Storage Control Algorithms for Dynamic Networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recent technological advances have rendered storage a readily available resource, yet there exist few examples that use it for enhancing network performance. We revisit in-network storage and we evaluate its usage as an additional degree of freedom in network optimization. We consider the network design problem of maximizing the volume of end-to-end transferred data and we derive storage allocatio..."
      }
    ]
  },
  {
    "tid":"410",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Learning Assumptions for CompositionalVerification of Timed Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Compositional techniques such as assume-guarantee reasoning (AGR) can help to alleviate the state space explosion problem associated with model checking. However, compositional verification is difficult to be automated, especially for timed systems, because constructing appropriate assumptions for AGR usually requires human creativity and experience. To automate compositional verification of timed systems, we propose a compositional verification framework using a learning algorithm for automatic construction of timed assumptions for AGR. We prove the correctness and termination of the proposed learning-based framework, and experimental results show that our method performs significantly better than traditional monolithic timed model checking."
      }
    ]
  },
  {
    "tid":"411",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Speaker verification based on g.729 and g.723.1 coder parameters and handset mismatch compensation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A novel technique for speaker verification over a communica- tion network is proposed. The technique employs cepstral coef- ficients (LPCCs) derived from G.729 and G.723.1 coder param- eters as feature vectors. Based on the LP coefficients derived from the coder parameters, LP residuals are reconstructed, and the verification performance is improved by taking account of the additional speaker-dependent information contained in the reconstructed residuals. This is achieved by adding the LPCCs of the LP residuals to the LPCCs derived from the coder param- eters. To reduce the acoustic mismatch between different hand- sets, a technique combining a handset selector with stochastic feature transformation is employed. Experimental results based on 150 speakers show that the proposed technique outperforms the approaches that only utilize the coder-derived LPCCs."
      }
    ]
  },
  {
    "tid":"412",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Variation in the correlation of G + C composition with synonymous codon usage bias among bacteria.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"G + C composition at the third codon position (GC3) is widely reported to be correlated with synonymous codon usage bias. However, no quantitative attempt has been made to compare the extent of this correlation among different genomes. Here, we applied Shannon entropy from information theory to measure the degree of GC3 bias and that of synonymous codon usage bias of each gene. The strength of the correlation of GC3 with synonymous codon usage bias, quantified by a correlation coefficient, varied widely among bacterial genomes, ranging from -0.07 to 0.95. Previous analyses suggesting that the relationship between GC3 and synonymous codon usage bias is independent of species are thus inconsistent with the more detailed analyses obtained here for individual species."
      }
    ]
  },
  {
    "tid":"413",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Understanding the occurrence of errors in process models based on metrics' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Business process models play an important role for the management, design, and improvement of process organizations and process-aware information systems. Despite the extensive application of process modeling in practice, there are hardly empirical results available on quality aspects of process models. This paper aims to advance the understanding of this matter by analyzing the connection between formal errors (such as deadlocks) and a set of metrics that capture various structural and behavioral aspects of a process model. In particular, we discuss the theoretical connection between errors and metrics, and provide a comprehensive validation based on an extensive sample of EPC process models from practice. Furthermore, we investigate the capability of the metrics to predict errors in a second independent sample of models. The high explanatory power of the metrics has considerable consequences for the design of future modeling guidelines and modeling tools."
      }
    ]
  },
  {
    "tid":"414",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Keyword Retrieval Using Scale-Space Pyramid' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose a pyramid-based method for keyword spotting in historical document images. The documents are represented by a scale-space pyramid of their features. The search for a query keyword begins at the highest level of the pyramid, where the initial candidates for matching are located. The candidates are further refined at each level of the pyramid. The number of levels is adaptive and depends on the length of the query word. The results from all the document images are combined and ranked. We compare two feature representations, grid-based and continuous, and show that continuous feature representation outperforms the grid-based representation. In order to reduce the memory used to store the scale-space pyramid of features, we discuss and compare two compressing approaches. The proposed method was evaluated on four different collections of historical documents achieving state-of-the-art results."
      }
    ]
  },
  {
    "tid":"415",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Measurement-Based Self Organization Of Interfering 802.11 Wireless Access Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The popularity of IEEE 802.11 WLANs has led to dense deployments in urban areas. High density leads to suboptimal performance unless the interfering networks learn how to optimally use and share the spectrum. This paper proposes two fully distributed algorithms that allow (i) multiple interfering 802.11 Access Points to select their operating frequency in order to minimize interference, and (ii) users to choose the Access Point they attach to, in order to get their fair share of the whole network bandwidth. The proposed algorithms rely on Gibbs sampler, and do not require explicit coordination among the wireless devices. They only require the participating wireless nodes to measure local quantities such as interference and transmission delay. The algorithms are shown to lead to optimal bandwidth sharing, where optimality is defined according to the minimal potential delay. We analytically prove the convergence of the proposed algorithms, and study their performance by simulation."
      }
    ]
  },
  {
    "tid":"416",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multiple views gait recognition using View Transformation Model based on optimized Gait Energy Image' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Gait is one of well recognized biometrics that has been widely used for human identification. However, the current gait recognition might have difficulties due to viewing angle being changed. This is because the viewing angle under which the gait signature database was generated may not be the same as the viewing angle when the probe data are obtained. This paper proposes a new multi-view gait recognition approach which tackles the problems mentioned above. Being different from other approaches of same category, this new method creates a so called View Transformation Model (VTM) based on spatial-domain Gait Energy Image (GEI) by adopting Singular Value Decomposition (SVD) technique. To further improve the performance of the proposed VTM, Linear Discriminant Analysis (LDA) is used to optimize the obtained GEI feature vectors. When implementing SVD there are a few practical problems such as large matrix size and over-fitting. In this paper, reduced SVD is introduced to alleviate the effects caused by these problems. Using the generated VTM, the viewing angles of gallery gait data and probe gait data can be transformed into the same direction. Thus, gait signatures can be measured without difficulties. The extensive experiments show that the proposed algorithm can significantly improve the multiple view gait recognition performance when being compared to the similar methods in literature."
      }
    ]
  },
  {
    "tid":"417",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Roofline Model Toolkit: A Practical Tool For Architectural And Program Analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present preliminary results of the Roofline Toolkit for multicore, manycore, and accelerated architectures. This paper focuses on the processor architecture characterization engine, a collection of portable instrumented micro benchmarks implemented with Message Passing Interface (MPI), and OpenMP used to express thread-level parallelism. These benchmarks are specialized to quantify the behavior of different architectural features. Compared to previous work on performance characterization, these microbenchmarks focus on capturing the performance of each level of the memory hierarchy, along with thread-level parallelism, instruction-level parallelism and explicit SIMD parallelism, measured in the context of the compilers and run-time environments. We also measure sustained PCIe throughput with four GPU memory managed mechanisms. By combining results from the architecture characterization with the Roofline model based solely on architectural specifications, this work offers insights for performance prediction of current and future architectures and their software systems. To that end, we instrument three applications and plot their resultant performance on the corresponding Roofline model when run on a Blue Gene/Q architecture."
      }
    ]
  },
  {
    "tid":"418",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Challenges and techniques for effective and efficient similarity search in large video databases' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Searching relevant visual information based on content features in large databases is an interesting and changeling topic that has drawn lots of attention from both the research community and industry. This paper gives an overview of our investigations on effective and efficient video similarity search. We briefly introduce some novel techniques developed for two specific tasks studied in this PhD project: video retrieval in a large collection of segmented video clips, and video subsequence identification from a long unsegmented stream. The proposed methods for processing these two types of similarity queries have shown encouraging performance and are being incorporated into our prototype system of video search named UQLIPS, which has demonstrated some marketing potentials for commercialisation."
      }
    ]
  },
  {
    "tid":"419",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Power allocation for cognitive radio with hybrid energy supplies' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we investigate the power allocation problem for a point-to-point cognitive radio (CR) with the secondary user (SU) powered by both the renewable energy and the conventional energy. In particular, the transmission of the SU is assumed to be divided into time slots, and a transmission outage constraint is proposed to provide a satisfactory quality of service (QoS) to the SU by restricting the number of slots whose rate is lower than the target rate. Meanwhile, an interference outage constraint is proposed to guarantee the QoS of the primary user (PU) by restricting the number of slots whose interference power is higher than the predefined interference power threshold. Under the proposed transmission outage and interference outage constraints together with the energy causality constraint, the power allocation problem to minimize the total conventional energy consumption of the SU is investigated and a heuristic scheme is proposed. We show that the proposed power allocation scheme greatly outperforms the reference random scheme. We also show that relaxing the transmission outage constraint is more effective in reducing the conventional energy consumption of the SU than relaxing the interference outage constraint."
      }
    ]
  },
  {
    "tid":"420",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'It is not for everyone in China' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"China is expected to surpass South Korea to become one of the three largest IT service markets in the Asia Pacific region by 2008. According to a 2004 China Computer World (CCW) research survey the sale of enterprise resource planning software amounted to 1.19 billion yuan in the first half of 2004, up 29 % over the same period a year earlier. Low cost of labor make inexpensive labor more advantageous than IT for a many industries in China. IT is not for every business in China as IT can provide value for many enterprises there but actual value of it depends on many things such as firm size, market orientation, and industry characteristics."
      }
    ]
  },
  {
    "tid":"421",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Enhancing relation extraction by eliciting selectional constraint features from wikipedia' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Selectional Constraints are usually checked for detecting semantic relations. Previous work usually defined the constraints manually based on handcrafted concept taxonomy, which is time-consuming and impractical for large scale relation extraction. Further, the determination of entity type (e.g. NER) based on the taxonomy cannot achieve sufficiently high accuracy. In this paper, we propose a novel approach to extracting relation instances using the features elicited from Wikipedia, a free online encyclopedia. The features are represented as selectional constraints and further employed to enhance the extraction of relations. We conduct case studies on the validation of the extracted instances for two common relations hasArtist(album, artist) and hasDirector(film, director). Substantially high extraction precision (around 0.95) and validation accuracy (near 0.90) are obtained."
      }
    ]
  },
  {
    "tid":"422",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A cooperative coevolutionary algorithm for multiobjective particle swarm optimization' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Coevolutionary architectures have been shown to be effective ways to improve the performance of multiobjective (MO) optimization problems. This paper presents a cooperative coevolutionary algorithm for multiobjective particle swarm optimization (COMOPSO), which applies the divide-and-conquer approach to decompose decision vectors into smaller components and evolves multiple solutions in the form of cooperative subswarms. Representatives from each evolving subswarm are combined to form the solution to the whole system. The fitness of each individual is related to its ability to collaborate with individuals from other species, thereby encouraging the development of cooperative strategies. An adaptive niche sharing algorithm is introduced to handle the selection of the niche radius in a dynamic manner. Coupled with the adaptive niche sharing algorithm COMOPSO demonstrates its effectiveness and efficiency in evolving highly competitive solution sets against various MO algorithms on benchmark problems characterized by different difficulties with consistent results."
      }
    ]
  },
  {
    "tid":"423",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Towards a variable size sliding window model for frequent itemset mining over data streams' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Sliding window is a widely used model for data stream mining due to its emphasis on recent data and its bounded memory requirement. The main idea behind a transactional sliding window is to keep a fixed size window over a data stream. The window size is kept constant by removing old transactions from the window, when new transactions arrive. Older transactions of window are removed irrespective to whether a significant change has occurred or not. Another challenge of sliding window model is determining window size. The classic approach for determining the window size is to obtain it from the user. In order to determine the precise size of the window, the user must have prior knowledge about the time and scale of changes within the data stream. However, due to the unpredictable changing nature of data streams, this prior knowledge cannot be easily determined. Moreover, by using a fixed window size during a data stream mining, the performance of this model is degraded in terms of reflecting recent changes. Based on these observations, this study relaxes the notion of window size and proposes a new algorithm named VSW (Variable Size sliding Window frequent itemset mining) which is suitable for observing recent changes in the set of frequent itemsets over data streams. The window size is determined dynamically based on amounts of concept change that occurs within the arriving data stream. The window expands as the concept becomes stable and shrinks when a concept change occurs. In this study, it is shown that if stale transactions are removed from the window after a concept change, updated frequent itemsets always belong to the most recent concept. Experimental evaluations on both synthetic and real data show that our algorithm effectively detects the concept change, adjust the window size, and adapts itself to the new concepts along the data stream."
      }
    ]
  },
  {
    "tid":"424",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Single machine batch scheduling with deadlines and resource dependent processing times' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We consider the problem of scheduling n jobs on a single machine where each job has a deadline and a processing time that is a linear decreasing function of the amount of a common discrete resource allocated to the job. Jobs may be combined to form batches containing contiguously scheduled jobs. For each batch, a constant set-up time is needed before the first job of the batch is processed. The completion time of each job in a batch coincides with the completion time of the last job in the batch. A schedule specifies the sequence of jobs and the size of each batch, i.e. the number of jobs it contains. The objective is to find simultaneously a resource allocation and a schedule which is feasible with respect to the deadlines so as to minimize the total weighted resource consumption. The problem is shown to be NP-hard even for the special case of common parameters. Two dynamic programming algorithms are presented for the general problem, as well as a fully polynomial approximation scheme."
      }
    ]
  },
  {
    "tid":"425",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Independent Markov chain occupancy grid maps for representation of dynamic environment' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we propose a new grid based approach to model a dynamic environment. Each grid cell is assumed to be an independent Markov chain (iMac) with two states. The state transition parameters are learned online and modeled as two Poisson processes. As a result, our representation not only encodes the expected occupancy of the cell, but also models the expected dynamics within the cell. The paper also presents a strategy based on recency weighting to learn the model parameters from observations that is able to deal with non-stationary cell dynamics. Moreover, an interpretation of the model parameters with discussion about the convergence rates of the cells is presented. The proposed model is experimentally validated using offline data recorded with a Laser Guided Vehicle (LGV) system running in production use."
      }
    ]
  },
  {
    "tid":"426",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Learning Utility Models For Decentralised Coordinated Target Tracking' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In decentralised target tracking, a set of sensors observes moving targets. When the sensors are static but steerable, each sensor must dynamically choose which target to observe in a decentralised manner. We show that the information exchanged by the sensors to synchronise their beliefs can be exploited to learn a model of the utility function that drives each others' decisions. Instead of communicating utilities to enable negotiation, each sensor regresses on the learnt model to predict the utilities of other team members. This approach bridges the gap between coordinating implicitly, a locally-greedy solution, and negotiating explicitly. We validated our approach in both hardware and simulations, and found that it out-performed implicit coordination by a statistically significant margin with both ideal and limited communications."
      }
    ]
  },
  {
    "tid":"427",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Semi-Supervised Cross-Media Feature Learning with Unified Patch Graph Regularization' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With the rapid growth of multimedia data such as text, image, video, audio and 3D model, cross-media retrieval has become increasingly important, because users can retrieve the results with various types of media by submitting a query of any media type. Comparing with single-media retrieval such as image retrieval and text retrieval, cross-media retrieval is better because it provides the retrieval results with all kinds of media at the same time. In this paper, we focus on how to learn cross-media features for different media types, which is a key challenge for cross-media retrieval. Existing methods either model different media types separately or only exploit the labeled multimedia data. Actually, data from different media types with the same semantic category are complementary to each other, and jointly modeling them is able to improve the accuracy of cross-media retrieval. In addition, although the labeled data are accurate, they require a lot of human labor and thus are very scarce. To address the above problems, we propose a semi-supervised crossmedia feature learning algorithm with unified patch graph regularization (S2UPG). Our motivation and contributions mainly lie in the following three aspects: (1) Existing methods only model different media types in different graphs, while we employ one joint graph to simultaneously model all the media types. The joint graph is able to fully exploit the semantic correlations among various media types, which are complementary to provide the rich hint for cross-media correlation. (2) Existing methods only consider the original media instances (such as images, videos, texts, audios, and 3D models) but ignore their patches, while we make full use of both the media instances and their patches in one graph. Cross-media patches could emphasize the important parts and make cross-media correlations more precise. (3) Traditional semi-supervised learning methods only exploit single-media unlabeled instances, while our approach f- lly exploits cross-media unlabeled instances and their patches, which can increase the diversity of training data and boost the accuracy of cross-media retrieval. Comparing with the current state-of-theart methods on 3 datasets, including the challenging XMedia dataset with 5 media types, the comprehensive experimental results show that our proposed approach performs better."
      }
    ]
  },
  {
    "tid":"428",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'High-Order Energies for Stereo Segmentation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a novel segmentation approach for stereo images using the high-order energy optimization, which utilizes the disparity maps and statistical information of stereo images to enrich the high-order potential functions. To the best of our knowledge, our approach is the first one to formulate the problem of stereo segmentation as a high-order energy optimization problem, which simultaneously segments the foreground objects in left and right images using the proposed high-order potential function. A new method for designing the penalty function in our high-order term is proposed by the corresponding pixels and their neighboring pixels between left and right images. The relationships of stereo correspondence by disparity maps are further employed to enhance the connections between the left and right stereo images. Experimental results demonstrate that the proposed approach can effectively improve the performance of two kinds of stereo segmentation, including the automatic saliency-aware stereocut and the interactive stereo segmentation with user scribbles."
      }
    ]
  },
  {
    "tid":"429",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On entropy approximation for Gaussian mixture random vectors' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"For many practical probability density representa- tions such as for the widely used Gaussian mixture densities, an analytic evaluation of the differential entropy is not possible and thus, approximate calculations are inevitable. For this purpose, the first contribution of this paper deals with a novel entropy approximation method for Gaussian mixture random vectors, which is based on a component-wise Taylor-series expansion of the logarithm of a Gaussian mixture and on a splitting method of Gaussian mixture components. The employed order of the Taylor-series expansion and the number of components used for splitting allows balancing between accuracy and computational demand. The second contribution is the determination of mean- ingful and efficiently to calculate lower and upper bounds of the entropy, which can be also used for approximation purposes. In addition, a refinement method for the more important upper bound is proposed in order to approach the true entropy value."
      }
    ]
  },
  {
    "tid":"430",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Structural Fault Based Specification Reduction for Testing Analog Circuits' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Anti-random testing has proved useful in a series of empirical evaluations. The basic premise of anti-random testing is to chose new test vectors that are as far away from existing test inputs as possible. The distance measure is either Hamming distance ..."
      }
    ]
  },
  {
    "tid":"431",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Prediction-based cluster management for target tracking in wireless sensor networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The key impediments to a successful wireless sensor network (WSN) application are the energy and the longevity constraints of sensor nodes. Therefore, two signal processing oriented cluster management strategies, the proactive and the reactive cluster management, are proposed to efficiently deal with these constraints. The former strategy is designed for heterogeneous WSNs, where sensors are organized in a static clustering architecture. A non-myopic cluster activation rule is realized to reduce the number of hand-off operations between clusters, while maintaining desired estimation accuracy. The proactive strategy minimizes the hardware expenditure and the total energy consumption. On the other hand, the main concern of the reactive strategy is to maximize the network longevity of homogeneous WSNs. A Dijkstra-like algorithm is proposed to dynamically form active cluster based on the relation between the predictive target distribution and the candidate sensors, considering both the energy efficiency and the data relevance. By evenly distributing the energy expenditure over the whole network, the objective of maximizing the network longevity is achieved. The simulations evaluate and compare the two proposed strategies in terms of tracking accuracy, energy consumption and execution time. Copyright \u00a9 2010 John Wiley & Sons, Ltd."
      }
    ]
  },
  {
    "tid":"432",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Inter-greedy technique for fusion of different segmentation strategies leading to high-performance carotid IMT measurement in ultrasound images.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"User-based estimation of intima-media thickness (IMT) of carotid arteries leads to subjectivity in its decision support systems, while being used as a cardiovascular risk marker. During automated computer-based decision support, we had developed segmentation strategies that follow three main courses of our contributions: (a) signal processing approach combined with snakes and fuzzy K-means (CULEXsa), (b) integrated approach based on seed and line detection followed by probability based connectivity and classification (CALEXsa), and (c) morphological approach with watershed transform and fitting (WS). These grayscale segmentation algorithms yielding carotid wall boundaries has certain bias along with their own merits. We recently developed a fusion technique that was helpful in removing bias which combines two carotid wall boundaries using ground truth as an ideal marker. Here we have extended this fusion concept by taking merits of these multiple boundaries, so called, Inter-Greedy (IG) approach. Further we estimate IMT from these fused boundaries from multiple sources. Starting from the technique with the overall least system error (the snake-based one), we iteratively swapped the vertices of the profiles until we minimized its overall distance with respect to ground truth. The fusion boundary was the Inter-Greedy boundary. We used the polyline distance metric for performance evaluation and error minimization. We ran the segmentation protocol over the database of 200 carotid longitudinal B-mode ultrasound images and compared the performance of all the four techniques (CALEXia, CULEXsa, WS, IG). The mean error of Inter-Greedy technique yielded 0.32\u2009\u00b1\u20090.44 pixel (20.0\u2009\u00b1\u200927.5 \u00b5m) for the LI boundary (a 33.3%\u2009\u00b1\u20095.6% improvement over initial best performing technique) and 0.21\u2009\u00b1\u20090.34 pixel (13.1\u2009\u00b1\u200921.3 \u00b5m) for MA boundary (a 32.3%\u2009\u00b1\u20096.7% improvement). IMT measurement error for Greedy method was 0.74\u2009\u00b1\u20090.75 pixel (46.3\u2009\u00b1\u200946.9 \u00b5m), a 43.5%\u2009\u00b1\u20092.4% improvement."
      }
    ]
  },
  {
    "tid":"433",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A New Memory-Disk Integrated System with HW Optimizer' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Current high-performance computer systems utilize a memory hierarchy of on-chip cache, main memory, and secondary storage due to differences in device characteristics. Limiting the amount of main memory causes page swap operations and duplicates data between the main memory and the storage device. The characteristics of next-generation memory, such as nonvolatility, byte addressability, and scaling to greater capacity, can be used to solve these problems. Simple replacement of secondary storage with new forms of nonvolatile memory in a traditional memory hierarchy still causes typical problems, such as memory bottleneck, page swaps, and write overhead. Thus, we suggest a single architecture that merges the main memory and secondary storage into a system called a Memory-Disk Integrated System (MDIS). The MDIS architecture is composed of a virtually decoupled NVRAM and a nonvolatile memory performance optimizer combining hardware and software to support this system. The virtually decoupled NVRAM module can support conventional main memory and disk storage operations logically without data duplication and can reduce write operations to the NVRAM. To increase the lifetime and optimize the performance of this NVRAM, another hardware module called a Nonvolatile Performance Optimizer (NVPO) is used that is composed of four small buffers. The NVPO exploits spatial and temporal characteristics of static&sol;dynamic data based on program execution characteristics. Enhanced virtual memory management and address translation modules in the operating system can support these hardware components to achieve a seamless memory-storage environment. Our experimental results show that the proposed architecture can improve execution time by about 89&percnt; over a conventional DRAM main memory&sol;HDD storage system, and 77&percnt; over a state-of-the-art PRAM main memory&sol;HDD disk system with DRAM buffer. Also, the lifetime of the virtually decoupled NVRAM is estimated to be 40&percnt; longer than that of a traditional hierarchy based on the same device technology."
      }
    ]
  },
  {
    "tid":"434",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Modeling under-resourced languages for speech recognition.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"One particular problem in large vocabulary continuous speech recognition for low-resourced languages is finding relevant training data for the statistical language models. Large amount of data is required, because models should estimate the probability for all possible word sequences. For Finnish, Estonian and the other fenno-ugric languages a special problem with the data is the huge amount of different word forms that are common in normal speech. The same problem exists also in other language technology applications such as machine translation, information retrieval, and in some extent also in other morphologically rich languages. In this paper we present methods and evaluations in four recent language modeling topics: selecting conversational data from the Internet, adapting models for foreign words, multi-domain and adapted neural network language modeling, and decoding with subword units. Our evaluations show that the same methods work in more than one language and that they scale down to smaller data resources."
      }
    ]
  },
  {
    "tid":"435",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Release Enforcement In Resource-Oriented Partitioned Scheduling For Multiprocessor Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"When partitioned scheduling is used in real-time multiprocessor systems, access to shared resources can jeopardize the schedulability if the task partition is not done carefully. To tackle this problem we change our view angle from focusing on the computing tasks to focusing on the shared resources by applying resource-oriented partitioned scheduling. We use a release enforcement technique to shape the interference from the higher-priority jobs to be sporadic, analyze the schedulability, and provide strategies for partitioning both the critical and the non-critical sections of tasks onto processors individually. Our approaches are shown to be effective, both in the evaluations and from a theoretical point of view by providing a speedup factor of 6, improving previously known results."
      }
    ]
  },
  {
    "tid":"436",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Obstacle Avoidance Method in Real Space for Virtual Reality Immersion' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Typical Head-Mounted Displays (HMDs) that provide a highly immersive Virtual Reality (VR) experience make any interaction between a user and real space difficult by occluding the user's entire field of view. Video see-through type HMDs can solve this problem by superimposing real-space information on the VR environment. The existing method of supporting interactions with the real space is superimposition of boundary lines of the real space on the virtual space in the HMD. However, overlaying the boundary lines on the entire field of view may reduce the user's immersive feeling. In this paper, we propose two methods to support interactions with the real world while playing immersive VR games without reducing the user's immersive feeling as much as possible, even when the user wanders. The first method is to superimpose a 3D point cloud of real space around the user on the virtual space in the HMD. The second method is to deploy familiar objects (e.g., furniture in his/her room) in the virtual space in the HMD. The user traces the familiar objects as subgoals to reach the goal. We implement the two methods and conduct a user study to compare interaction performance. As a result of the user study, we find that the second method provides better spatial information about the real space without reducing the user's immersive feeling, compared to the existing method."
      }
    ]
  },
  {
    "tid":"437",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'LeadFlow4LD: Learning and Data Flow Composition-Based Solution for Learning Design in CSCL' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"IMS-LD is the de facto standard for learning design (LD) specification which typically comprises an activity flow and a data flow. Nevertheless, the specification of the data flow between tools is an open issue in IMS-LD, especially in collaborative learning. In such case, handling shared data derived from individual and collaborative tools is error-prone for learners who suffer an extra cognitive load. Additionally, problems in the collaborative data flow specification affect the reusability of the whole learning design. In this paper, we present LeadFlow4LD, a solution of specification and enactment for LD in CSCL in order to address the aforementioned issues in an interoperable and standard way. Such a solution is based on approaches for the composition of the activity flow specified in IMS-LD and the data flow specified in a standard workflow language, such as BPEL. An architecture and a prototype for validating the propose solution through a case study based on a significant CSCL situation are also presented."
      }
    ]
  },
  {
    "tid":"438",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On incentive-based inter-domain caching for content delivery in future internet architectures' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Leveraging in-network caching of proposed future network architectures such as content-centric networking (CCN) encourages users and ISPs to enjoy performance improvements of networks. However, we raise an argument against a rule of thumb stating that more content storage is better since business relationships in inter-domain networks are based on profit negotiations rather than an efficiently deployed network architecture. The objective of this paper is to identify the optimal cache allowance for a caching mechanism in CCN in inter-domain networks. We propose an incentive model for ISPs and analyze their maximum profits by studying the effects of cache allowance in their routers. The results confirmed from our analysis indicate that the minimum expected round-trip time can be achieved by deploying the maximum cache allowance in all routers. In contrast, the maximum cache allowance in the router connected to clients and the minimum cache allowance in the gateway router of an ISP that connects to the clients provides the maximum profit to that ISP. On the other hand, the maximum cache allowance in the gateway router of an ISP that is connected to clients and the minimum cache allowance in the router close to the server provides the highest profit to an ISP connected to the server."
      }
    ]
  },
  {
    "tid":"439",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A model for drosophila melanogaster development from a single cell to stripe pattern formation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The development of multicellular organisms, from the early forms of zygote, involves a range of phenomena that control cell growth and differentiation, making the overall process of morphogenesis highly complex. A well studied example of such a huge phenomenon is given by Drosophila Melanogaster morphogenesis that has been object of several models, whose main goal was to investigate the mechanisms involved in the spatial and temporal evolution of the patterning process, namely gene regulatory network, mor-phogen diffusion, synthesis and degradation. In this paper we present a model of Drosophila development that considers also nuclear division and movements as basic morphogenetic mechanisms. The model is run on top of a prototype simulator which is based on a variation of an existing SSA (Stochastic Simulation Algorithm), tailored to the specific features of embryo development, including dynamicity in the topology of compartment network."
      }
    ]
  },
  {
    "tid":"440",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Acceptable costs of minimax regret equilibrium: A Solution to security games with surveillance-driven probabilistic information.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"\u2022We extend the application of security game to online security resource allocation.\u2022A regret and loss aversion based method for online security game is given.\u2022An epistemic characterization is given for the justification of the new method.\u2022We show the linear programming formulation and robustness analysis of the method.\u2022The viability of the method is demonstrated with an experimental evaluation."
      }
    ]
  },
  {
    "tid":"441",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Extracting Variability-Safe Feature Models from Source Code Dependencies in System Variants' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"To effectively cope with increasing customization demands, companies that have developed variants of software systems are faced with the challenge of consolidating all the variants into a Software Product Line, a proven development paradigm capable of handling such demands. A crucial step in this challenge is to reverse engineer feature models that capture all the required feature combinations of each system variant. Current research has explored this task using propositional logic, natural language, and search-based techniques. However, using knowledge from the implementation artifacts for the reverse engineering task has not been studied. We propose a multi-objective approach that not only uses standard precision and recall metrics for the combinations of features but that also considers variability-safety, i.e. the property that, based on structural dependencies among elements of implementation artifacts, asserts whether all feature combinations of a feature model are in fact well-formed software systems. We evaluate our approach with five case studies and highlight its benefits for the software engineer."
      }
    ]
  },
  {
    "tid":"442",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Acquiring Lexical Knowledge from Query Logs for Query Expansion in Patent Searching' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Query expansion is a crucial step in recall-oriented domains such as Patent Searching. Currently, automatic query expansion in patent search is mostly based on statistical measures. Additional query terms are extracted from the query documents based on entropy measures. To automate query expansion in patent searching, we acquire lexical knowledge from Query Logs of USPTO Patent Examiners. Results show good performance in query expansion and patent searching using the lexical database. This will help improving (semi-) automated query expansion in patent searching."
      }
    ]
  },
  {
    "tid":"443",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Prediction method to maintain QoS in weather impacted wireless and satellite networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Rain and snow can have a distorting effect on Ku and Ka bands signal fidelity resulting in excessive digital transmission errors. This loss of signal attenuation is commonly referred to as rain fade. Rain fade impacts the Quality of Service in wireless and satellite networks. A decision support system is therefore necessary for service providers to accurately predict rain fade and establish mitigation planning by adaptively selecting appropriate power level, coding and modulation schemes. This paper establishes an important component of the decision support system by adaptively computing rainfall rate and rain attenuation at any location on earth using ITU-R models combined with bi-linear interpolation and frequency extrapolation. We also introduce a novel method for accurately determining rain attenuation as a function of frequency based on proceeding results from prediction weather database, since rain attenuation is considered a dominant impairment for wireless signals [1]\u2013[6]. Finally, a three dimensional relationship is proposed for rain attenuation with both frequency and probability to provide decision support system with an accurate view of satellite\u2019s parameters. The derived parameter values, being fed through neural networks and fuzzy systems to improve estimate Signal to Noise Ratio, will enable the decision support system to maintain Quality of Service and Service Level Agreements by adaptively adjusting satellite signal power, modulation, coding, and data rate at unpredictable weather conditions."
      }
    ]
  },
  {
    "tid":"444",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Decentralized Implementation of Centralized Controllers for Interconnected Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Given a centralized controller associated with a linear time-invariant (LTI) interconnected system, this paper is concerned with designing a parameterized decentralized controller in such a way that the state and input of the system under the obtained decentralized controller can become arbitrarily close to those of the system under the given centralized controller, by tuning the controller's parameters. To this end, a two-level decentralized controller is designed, where the upper level captures the dynamics of the centralized closed-loop system, and the lower level is an observed-based subcontroller designed based on the new notion of structural initial value observability. The proposed method can decentralize every generic centralized controller, provided the interconnected system satisfies very mild conditions."
      }
    ]
  },
  {
    "tid":"445",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A novel method for the 3-D reconstruction of scoliotic ribs from frontal and lateral radiographs.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Among the external manifestations of scoliosis, the rib hump, which is associated with the ribs' deformities and rotations, constitutes the most disturbing aspect of the scoliotic deformity for patients. A personalized 3-D model of the rib cage is important for a better evaluation of the deformity, and hence, a better treatment planning. A novel method for the 3-D reconstruction of the rib cage, based only on two standard radiographs, is proposed in this paper. For each rib, two points are extrapolated from the reconstructed spine, and three points are reconstructed by stereo radiography. The reconstruction is then refined using a surface approximation. The method was evaluated using clinical data of 13 patients with scoliosis. A comparison was conducted between the reconstructions obtained with the proposed method and those obtained by using a previous reconstruction method based on two frontal radiographs. A first comparison criterion was the distances between the reconstructed ribs and the surface topography of the trunk, considered as the reference modality. The correlation between ribs axial rotation and back surface rotation was also evaluated. The proposed method successfully reconstructed the ribs of the 6th-12th thoracic levels. The evaluation results showed that the 3-D configuration of the new rib reconstructions is more consistent with the surface topography and provides more accurate measurements of ribs axial rotation."
      }
    ]
  },
  {
    "tid":"446",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mechanical design and control of the new 7-DOF CyberMotion simulator' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper describes the mechanical and control design of the new 7-DOF CyberMotion Simulator, a redundant industrial manipulator arm consisting of a standard 6 DOF anthropomorphic manipulator plus an actuated cabin attached to the end-effector. Contrarily to Stewart platforms, an industrial manipulator offers several advantages when used as motion simulator: larger motion envelope, higher dexterity, and possibility to realize any end-effector posture within the workspace. In addition to this, the new actuated cabin acts as an additional joint and provides the needed kinematic redundancy to cope with the robot actuator and joint range constraints, which in general can significantly deteriorate the desired motion cues the robot is reproducing. In particular, we will show that, by suitably exploiting the redundancy better results can be obtained in reproducing sustained acceleration cues, a relevant problem when implementing vehicle simulators."
      }
    ]
  },
  {
    "tid":"447",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Robust Text Line, Word And Character Extraction from Telugu Document Image' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Designing an OCR system for Indian languages in general is more complex than those of European languages due the linguistic complexity. Efforts are on the way for the development of efficient OCR systems for Indian languages, especially for Telugu, a popular South Indian language. In this paper, we proposed a method for reliable extraction of text line, word and character from document images of Telugu scripts. In the text line segmentation, first we establish the relationship between the connected components and then cluster the connected components of a line using vertical spatial relation and nearest neighbor algorithm. In word segmentation, the space between two adjacent characters is computed and clustered into word space and character space. Consonant and vowel modifiers are segregated from the word image and segment the characters."
      }
    ]
  },
  {
    "tid":"448",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Genetic Search for Object Identification' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":" . We attack the problem of recognizing real, planar objectsfrom two-dimensional, intensity images taken from arbitrary viewpointsusing genetic algorithms. More specifically, we use genetic algorithms tosearch for a geometric mapping that brings subsets of points comprisingthe model and subsets of points comprising the scene into alignment.The genetic algorithm searches the image space and we compare differentencodings and operators on a set of three increasingly complex scenes.Our ... "
      }
    ]
  },
  {
    "tid":"449",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A case for application aware channel access in wireless networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The increasing reliance of users on wireless networks for Internet connectivity has posed two significant challenges for mobile networking research. The first challenge is to provide high quality of service for interactive real-time applications such as VoIP and video conferencing. The second challenge is to reduce the energy consumption of mobile devices and improve battery life. Past research has focused on separately addressing these seemingly conflicting goals in distributed medium access based wireless networks. Contrary to the traditional tiered networking approach, we argue that an application aware approach to medium access and power saving has the potential to significantly improve the performance of real-time applications and conserve battery power on mobile devices. As a proof of concept, we present the design and implementation of Rendezvous - an application aware MAC protocol. Rendezvous uses short term dynamic channel reservations to achieve higher quality of service and increased power saving opportunities for mobile devices running VoIP and real-time video applications. A preliminary evaluation from our testbed implementation reveals promising results, motivating the need and opportunity for future research in the direction of application awareness at lower layers of the networking stack."
      }
    ]
  },
  {
    "tid":"450",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Spark-Ignition-Engine Idle Speed Control: An Adaptive Control Approach.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The paper presents an application of a recently developed adaptive posicast controller (APC) for time-delay systems to the idle speed control (ISC) problem in spark ignition (SI) internal combustion (IC) engines. The objective is to regulate the engine speed to a prescribed set-point in the presence of accessory load torque disturbances such as those due to air conditioning and power steering. The adaptive controller, integrated with the existing proportional spark controller, is used to drive the electronic throttle actuator. We present both simulation and experimental results demonstrating the performance improvement by employing the adaptive controller. We also present the modifications and improvements to the controller structure which were developed during the course of experimentation to solve specific problems. In addition, the potential for the reduction in calibration time and effort which can be achieved with our approach is discussed."
      }
    ]
  },
  {
    "tid":"451",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Robust multi-object tracking with semantic color correlation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Multi-object tracking is an important computer vision task with wide variety of real-life applications from surveillance and monitoring to biomedical video analysis. Multi-object tracking is a challenging problem due to complications such as partial or full occlusions, factors affecting object appearance, object interaction dynamics, etc. and computational cost. In this paper, we propose a detection-based multi-object tracking system that uses a two-step data association scheme to ensure time efficiency while preserving tracking accuracy; a robust but discriminative object appearance model that compares object color attributes using a novel color correlation cost matrix; and a framework that handles occlusions through prediction. Our experiments on UA-DETRAC multi-object tracking benchmark dataset consisting of challenging real-world traffic videos show promising results against state-of-the-art trackers."
      }
    ]
  },
  {
    "tid":"452",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Speaker diarization using bottom-up clustering based on a parameter-derived distance between adapted GMMs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we present an approach for speaker diarization based on segmentation followed by bottom-up clustering, where clusters are modeled using adapted Gaussian mixture models. We propose a novel inter-cluster distance in the model param- eter space which is easily computable and which can both be used as the dissimilarity measure in the clustering scheme and as a stop criterion. Using adapted Gaussian mixture models en- ables a good description of the feature vector distribution with- in a cluster while adaptation prevents over-training for clusters with few data. Experiments carried out on broadcast news data in French demonstrate the potential of the proposed approach which exhibits performance similar to BIC clustering. How- ever, our clustering method appeared to be more sensitive to segmentation errors than the BIC approach."
      }
    ]
  },
  {
    "tid":"453",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Probabilistic document length priors for language models' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper addresses the issue of devising a new document prior for the language modeling (LM) approach for Information Retrieval. The prior is based on term statistics, derived in a probabilistic fashion and portrays a novel way of considering document length. Furthermore, we developed a new way of combining document length priors with the query likelihood estimation based on the risk of accepting the latter as a score. This prior has been combined with a document retrieval language model that uses Jelinek-Mercer (JM), a smoothing technique which does not take into account document length. The combination of the prior boosts the retrieval performance, so that it outperforms a LM with a document length dependent smoothing component (Dirichlet prior) and other state of the art high-performing scoring function (BM25). Improvements are significant, robust across different collections and query sizes."
      }
    ]
  },
  {
    "tid":"454",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'How much management is management enough? providing monitoring processes with online adaptation and learning capability' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recent investigations of management traffic patterns in production networks suggest that just a small and static set of management data tends to be used, the flow of management data is relatively constant, and the operations in use for manager-agent communication are reduced to a few, sometimes obsolete set. This is an indication of lack of progress of monitoring processes, taking into account their strategic role and potential, for example, to anticipate and prevent faults, performance bottlenecks, and security problems. One of the main reasons for such limitation relies on the fact that operators, who still are a fundamental element of the monitoring control loop, can no longer handle the rapidly increasing size and heterogeneity of both hardware and software components that comprise modern networked computing systems. This form of human-in-the-loop management certainly hampers timely adaptation of monitoring processes. To tackle this issue, this paper presents a model, inspired by the reinforcement learning theory, for adaptive network, service and application monitoring. The model is instantiated through a prototypical implementation of an autonomic element, which, based on historical and even unexpected values retrieved for management objects, dynamically widens or restricts the set of management objects to be monitored."
      }
    ]
  },
  {
    "tid":"455",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Benchmarking of Distributed Computing Engines Spark and GraphLab for Big Data Analytics' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we evaluate and compare two representativeand popular distributed processing engines for large scalebig data analytics, Spark and graph based engine GraphLab. Wedesign a benchmark suite including representative algorithmsand datasets to compare the performances of the computingengines, from performance aspects of running time, memory andCPU usage, network and I/O overhead. The benchmark suite istested on both local computer cluster and virtual machines oncloud. By varying the number of computers and memory weexamine the scalability of the computing engines with increasingcomputing resources (such as CPU and memory). We also runcross-evaluation of generic and graph based analytic algorithmsover graph processing and generic platforms to identify thepotential performance degradation if only one processing engineis available. It is observed that both computing engines showgood scalability with increase of computing resources. WhileGraphLab largely outperforms Spark for graph algorithms, ithas close running time performance as Spark for non-graphalgorithms. Additionally the running time with Spark for graphalgorithms over cloud virtual machines is observed to increaseby almost 100% compared to over local computer clusters."
      }
    ]
  },
  {
    "tid":"456",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Utility-Based Resource and Network Assignment Framework for Heterogeneous Mobile Networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Network utility theory has been extensively employed for resource management purposes. However, traditional utility functions cannot support attributes that affect the resource allocation, such as mobility or more advanced traffic models. Especially in the context of a heterogeneous wireless network, transient parameters can have varying effects on each access network type. This work proposes a new utility function that can support multiple design requirements for mobile networks including advanced traffic models, user classes, handover and session priorities. We integrate the new utility function with the Super Base Station framework and devise a novel trigger-based network and resource assignment framework that efficiently copes with the complexity of a heterogeneous wireless network. Our simulation results show that the proposed sub-optimal trigger-based framework performs equally well as the complex optimal scheme."
      }
    ]
  },
  {
    "tid":"457",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Verification of Generalized Inconsistency-Aware Knowledge and Action Bases (Extended Version).' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"  Knowledge and Action Bases (KABs) have been put forward as a semantically rich representation of a domain, using a DL KB to account for its static aspects, and actions to evolve its extensional part over time, possibly introducing new objects. Recently, KABs have been extended to manage inconsistency, with ad-hoc verification techniques geared towards specific semantics. This work provides a twofold contribution along this line of research. On the one hand, we enrich KABs with a high-level, compact action language inspired by Golog, obtaining so called Golog-KABs (GKABs). On the other hand, we introduce a parametric execution semantics for GKABs, so as to elegantly accomodate a plethora of inconsistency-aware semantics based on the notion of repair. We then provide several reductions for the verification of sophisticated first-order temporal properties over inconsistency-aware GKABs, and show that it can be addressed using known techniques, developed for standard KABs. "
      }
    ]
  },
  {
    "tid":"458",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Theoretical Framework for Satellite-Based Web Multicasting Services' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Abstract: In this paper we study Web multicasting, a service offered by satellite operators to Internet Service Providers around the world. This service employs satellite connections for disseminating periodically Web content to regional and \"institutional\" WWW caches. We propose a theoretical framework for Web multicasting and formalize the notions of Utility and QoS perceived by customers of Web multicasting services. We explore two alternative charging schemes, Usage- and Subscription-based pricing, and propose a framework for negotiating the provision of the Web multicasting service between a satellite operator and its potential customers. We use this negotiation framework to compare theoretically the two pricing schemes at hand. We show that a given level of QoS can be guaranteed under Subscription-based pricing at a cost at least as low as under Usage-based pricing. Our theoretical framework can be used further by satellite operators and prospective customers to define the content provided through Web multi-casting, estimate its quality, negotiate its price and assess its overall effectiveness."
      }
    ]
  },
  {
    "tid":"459",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Geometric Matrix Completion With Deep Conditional Random Fields' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The problem of completing high-dimensional matrices from a limited set of observations arises in many big data applications, especially recommender systems. The existing matrix completion models generally follow either a memory- or a model-based approach, whereas geometric matrix completion (GMC) models combine the best from both approaches. Existing deep-learning-based geometric models yield good performance, but, in order to operate, they require a fixed structure graph capturing the relationships among the users and items. This graph is typically constructed by evaluating a pre-defined similarity metric on the available observations or by using side information, e.g., user profiles. In contrast, Markov-random-fields-based models do not require a fixed structure graph but rely on handcrafted features to make predictions. When no side information is available and the number of available observations becomes very low, existing solutions are pushed to their limits. In this article, we propose a GMC approach that addresses these challenges. We consider matrix completion as a structured prediction problem in a conditional random field (CRF), which is characterized by a maximum a posteriori (MAP) inference, and we propose a deep model that predicts the missing entries by solving the MAP inference problem. The proposed model simultaneously learns the similarities among matrix entries, computes the CRF potentials, and solves the inference problem. Its training is performed in an end-to-end manner, with a method to supervise the learning of entry similarities. Comprehensive experiments demonstrate the superior performance of the proposed model compared to various state-of-the-art models on popular benchmark data sets and underline its superior capacity to deal with highly incomplete matrices."
      }
    ]
  },
  {
    "tid":"460",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Automatic chinese text classification using n-gram model' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Automatic Chinese text classification is an important and well-known research topic in the field of information retrieval and natural language processing. However, past researches often ignore the problem of word segmentation and the relationship between words. This paper proposes an N-gram-based language model for Chinese text classification which considers the relationship between words. To prevent from the out-of-vocabulary problem, a novel smoothing method based on logistic regression is also proposed to improve the performance. The experimental result shows that our approach outperforms the previous N-gram-based classification model above 11% on micro-average F-measure."
      }
    ]
  },
  {
    "tid":"461",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Scheduling resource allocation with timeslot penalty for changeover' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Given a time slotted list of resource capacities, we address the problem of scheduling resource allocation considering that a change in allocation results in the changeover penalty of one timeslot. The goal is to maximize the overall allocation of resources. We prove that no 1-lookahead algorithm can be better than 8/5-competitive. We provide improved analysis of Wait Dominate Hold (WDH) algorithm that was previously known to be 4-competitive. We prove that WDH is 8/3-competitive. We also consider k-lookahead algorithms, and prove lower bound of (k + 2)/(k + 1) on their competitiveness and give an online algorithm that is 2-competitive."
      }
    ]
  },
  {
    "tid":"462",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Security through redundant data diversity' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Unlike other diversity-based approaches, N-variant systems thwart attacks without requiring secrets. Instead, they use redundancy (to require an attacker to simultaneously compromise multiple variants with the same input) and tailored diversity (to make it impossible to compromise all the variants with the same input for given attack classes). In this work, we develop a method for using data diversity in N-variant systems to provide high-assurance arguments against a class of data corruption attacks. Data is transformed in the variants so identical concrete data values have different interpretations. In order to corrupt the data without detection, an attacker would need to alter the corresponding data in each variant in a different way while sending the same inputs to all variants. We demonstrate our approach with a case study using that thwarts attacks that corrupt UID values."
      }
    ]
  },
  {
    "tid":"463",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Representation of vehicle dynamics in haptic teleoperation of aerial robots' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper considers the question of providing effective feedback of vehicle dynamic forces to a pilot in haptic teleoperation of aerial robots. We claim that the usual state-of-the-art haptic interface, based on research motivated by robotic manipulator slaves and virtual haptic environments, does a poor job of reflecting dynamic forces of a mobile robotic vehicle to the user. This leads us to propose a novel force feedback user interface for mobile robotic vehicles with dynamics. An analysis of the closed-loop force-displacement transfer functions experienced by the master joystick for the classical and the new approach clearly indicate the advantages of the proposed formulation. Both the classical and the proposed approach have been implemented in the teleoperation of a quadrotor vehicle and we present quantitative and cognitive performance data from a user study that corroborates the expected performance advantages."
      }
    ]
  },
  {
    "tid":"464",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Closer destination selection scheme for mobile sink and charger enabled WRSNs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In WRSNs (Wireless Rechargeable Sensor Networks), the wireless energy transfer technology enables charging sensor batteries and maintaining perpetual operation of the network. The MC (mobile charger) is able to provide stable and reliable energy supply for sensor nodes. Although many researchers study the efficient scheduling of the MC, none of them considers the situation where both the static sink and the MC can collect packets. In this case, each sensor node should choose the appropriate destination, i.e., the static sink or the MC. In this paper, we propose closer destination selection scheme for mobile sink and charger enabled WRSNs where a static sink and an MC coexist. In order for each sensor node to send packets to the closer destination, i.e., the static sink or the MC, when an MC tours to charge sensor nodes, it constructs another temporal tree-based topology network by sending a route construction packet to sensor nodes within the limited area. Whenever the MC stays at a sensor node, it calculates the maximum number of hops that sensor nodes should switch to a temporal network. This temporal network that the MC creates does not disturb the original network. By constructing a temporal network, an MC relaxes the load of sensor nodes around the static sink without unnecessary path extension. We evaluate the performance of our scheme through the computer simulation. Results indicate that our scheme improves the packet arrival rate and energy efficiency compared to the conventional scheme."
      }
    ]
  },
  {
    "tid":"465",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Enhanced Coordination in Sensor Networks through Flexible Service Provisioning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many applications operate in heterogeneous wireless sensor networks, which represent a challenging programming environment due to the wide range of device capabilities. Servilla addresses this difficulty in developing applications by offering a new middleware framework based on service provisioning. Using Servilla, developers can construct platform-independent applications over a dynamic and diverse set of devices. A salient feature of Servilla is its support for the discovery and binding to local and remote services, which enables flexible and energy-efficient in-network collaboration among heterogeneous devices. Furthermore, Servilla provides a modular middleware architecture that can be easily tailored to devices with a wide range of resources, allowing resource-constrained devices to provide services while leveraging the capabilities of more powerful devices. Servilla has been implemented on TinyOS for two representative hardware platforms (Imote2 and TelosB) with drastically different resources. Microbenchmarks demonstrate the efficiency of Servilla's implementation, while an application case study on structural health monitoring demonstrates the efficacy of its coordination model for integrating heterogeneous devices."
      }
    ]
  },
  {
    "tid":"466",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title '3D Scanning Method for Fast Motion using Single Grid Pattern with Coarse-to-fine Technique' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"An active 3D scanning method that can capture a fast motion is strongly required in wide areas. Since most commercial products using a laser pro- jector basically reconstruct the shape with a point or a line for each scan, a fast motion cannot be captured in principle. In addition, an extended method using a structured light can drastically reduce the number of projecting pat- terns, however, they still require several patterns and a fast motion cannot be captured. One solution for the purpose is to use a single pattern (one shot scan). Although, one shot scanning methods have been intensively studied, they often have stability problems and their result tend to have low resolution. In this paper, we develop a new system which achieves dense and robust 3D measurement from a single grid pattern. A 3D reconstruction can be achieved by identifying grid patterns using coplanarity constraints. We also propose a coarse-to-fine method to increase the density of the shape with a single pattern."
      }
    ]
  },
  {
    "tid":"467",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'DOTS: Decoupling Operation and Topology in Differentiable Architecture Search' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Differentiable Architecture Search (DARTS) has attracted extensive attention due to its efficiency in searching for cell structures. DARTS mainly focuses on the operation search and derives the cell topology from the operation weights. However, the operation weights can not indicate the importance of cell topology and result in poor topology rating correctness. To tackle this, we propose to Decouple the Operation and Topology Search (DOTS), which decouples the topology representation from operation weights and makes an explicit topology search. DOTS is achieved by introducing a topology search space that contains combinations of candidate edges. The proposed search space directly reflects the search objective and can be easily extended to support a flexible number of edges in the searched cell. Existing gradient-based NAS methods can be incorporated into DOTS for further improvement by the topology search. Considering that some operations (e.g., Skip-Connection) can affect the topology, we propose a group operation search scheme to preserve topology-related operations for a better topology search. The experiments on CIFAR10/100 and ImageNet demonstrate that DOTS is an effective solution for differentiable NAS."
      }
    ]
  },
  {
    "tid":"468",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Rate-diversity and resource-aware broadcast and multicast in multi-rate wireless mesh networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper focuses on the problem of increasing the traffic capacity (volume of admissible traffic) of broadcast and multicast flows in a wireless mesh network (WMN). We study and suggest routing strategies where the process of constructing the forwarding tree considers three distinct features: (a) the ability of individual mesh nodes to perform link-layer broadcasts at multiple rates, (b) the wireless broadcast advantage, whereby a single broadcast transmission covers multiple neighboring receivers and (c) the residual transmission capacity at a WMN node, subject to intereference-based constraints from existing traffic flows in its neighborhood. Our metric of interest is the the total number of broadcast and multicast flows that can be admitted into the network, without resulting in unacceptable degradation in metrics such as packet loss and dissemination latency. Our discrete event simulations show that the broadcast tree construction heuristic which takes both transmission rate and residual bandwidth into account out-performs those that do not. Building on our work on resource-aware broadcast tree construction, we propose a resource-aware multicast tree construction algorithm which exploits the multiple link-layer rates, the wireless broadcast advantage and the amount of resources available. Simulation results show that this algorithm performs better than heuristics based on pruning a broadcast tree or shortest path trees."
      }
    ]
  },
  {
    "tid":"469",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Toward patient safety in closed-loop medical device systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A model-driven design and validation of closed-loop medical device systems is presented. Currently, few if any medical systems on the market support closed-loop control of interconnected medical devices, and mechanisms for regulatory approval of such systems are lacking. We present a system implementing a clinical scenario where closed-loop control may reduce the possibility of human error and improve safety of the patient. The safety of the system is studied with a simple controller proposed in the literature. We demonstrate that, under certain failure conditions, safety of the patient is not guaranteed. Finally, a more complex controller is described and ensures safety even when failures are possible. This investigation is an early attempt to introduce automatic control in clinical scenarios and to delineate a methodology to validate such patient-in-the-loop systems for safe and correct operation."
      }
    ]
  },
  {
    "tid":"470",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A comparative study on selection of cluster number and local subspace dimension in the mixture PCA models' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"How to determine the number of clusters and the dimensions of local principal subspaces is an important and challenging problem in various applications. Based on a probabilistic model of local PCA, this problem can be solved by one of existing statistical model selection criteria in a two-phase procedure. However, such a two-phase procedure is too time-consuming especially when there is no prior knowledge. The BYY harmony learning has provided a promising mechanism to make automatic model selection in parallel with parameter learning. This paper investigates the BYY harmony learning with automatic model selection on a mixture PCA model in comparison with three typical model selection criteria: AIC, CAIC, and MDL. This comparative study is made by experiments for different model selection tasks on simulated data sets under different conditions. Experiments have shown that automatic model selection by the BYY harmony learning are not only as good as or even better than conventional methods in terms of performances, but also considerably supervisory in terms of much less computational cost."
      }
    ]
  },
  {
    "tid":"471",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The design and implementation of architectural components for the integration of the IP multimedia subsystem and wireless sensor networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The IP multimedia subsystem is becoming the de facto standard for IP-based multimedia services, while wireless sensor networks are gaining in popularity due to their ability to capture a rich set of contextual information. Integrating the sensing capabilities of WSNs in the IMS can open the door to a wide range of context-aware applications in areas such as wireless healthcare and pervasive gaming. We have previously proposed a presence-based architecture for WSN/IMS integration. This architecture relies on two key components: a WSN/IMS gateway acting as an interworking unit between WSNs and the IMS; and an extended presence server serving as a context information management node in the core network. In this article we focus on the design and implementation of these two components. Furthermore, two applications (a pervasive game and a personalized call control application) are used to concretely show how new applications can be developed using our architecture. Performance has also been evaluated. Several important findings were made in the course of this work; one is that the IMS integration with a large and evolving variety of WSNs may be a never-ending endeavor -- the gateway requiring constant upgrading due to the lack of standard APIs for the interaction with sensors produced by different vendors. Another finding is that while the introduction of context as an application building block in the IMS ensures the availability of additional contextual information in the network and enables fast and easy development of context-aware applications, the lack of mature IMS application development toolkits remains a roadblock."
      }
    ]
  },
  {
    "tid":"472",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mizan: a system for dynamic load balancing in large-scale graph processing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Pregel [23] was recently introduced as a scalable graph mining system that can provide significant performance improvements over traditional MapReduce implementations. Existing implementations focus primarily on graph partitioning as a preprocessing step to balance computation across compute nodes. In this paper, we examine the runtime characteristics of a Pregel system. We show that graph partitioning alone is insufficient for minimizing end-to-end computation. Especially where data is very large or the runtime behavior of the algorithm is unknown, an adaptive approach is needed. To this end, we introduce Mizan, a Pregel system that achieves efficient load balancing to better adapt to changes in computing needs. Unlike known implementations of Pregel, Mizan does not assume any a priori knowledge of the structure of the graph or behavior of the algorithm. Instead, it monitors the runtime characteristics of the system. Mizan then performs efficient fine-grained vertex migration to balance computation and communication. We have fully implemented Mizan; using extensive evaluation we show that---especially for highly-dynamic workloads---Mizan provides up to 84% improvement over techniques leveraging static graph pre-partitioning."
      }
    ]
  },
  {
    "tid":"473",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimal Beamforming for Dual-Hop MIMO AF Relay Networks With Cochannel Interferences.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we investigate a beamforming (BF) scheme for a dual-hop multiple-input multiple-output (MIMO) amplify-and-forward (AF) relay network in the presence of multiple cochannel interferences (CCIs) at both the relay and the destination nodes. In order to maximize the signal-to-interference-plus-noise ratio at the destination node, the optimal BF weights for the relay network in the presen..."
      }
    ]
  },
  {
    "tid":"474",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Content-adaptive deblocking for high efficiency video coding' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a content-adaptive deblocking method is proposed to improve the visual quality of block-based DCT compressed videos. We find that the edge information obtained through the global orientation energy edge detection (OEED) on an initially deblocked image provides a robust partition of local directional features (LDFs). Based on this partition, for the directional featured region, we design corresponding filter orientation and thresholds to best preserve image details while reducing blocking artifacts; for the consecutive non-featured region, we impose extra smoothing to suppress the visually severe blocking artifacts. Experimental results demonstrate that our method effectively improves the visual quality while well maintaining the objective fidelity of block-based DCT compressed videos, compared with the H.264/AVC deblocking method and other existing directional deblocking methods."
      }
    ]
  },
  {
    "tid":"475",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Using a Corpus for Teaching Turkish Morphology' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper reports on the preliminary phase of our ongoing research towards\ndeveloping an intelligent tutoring environment for Turkish grammar. One of the\ncomponents of this environment is a corpus search tool which, among other\naspects of the language, will be used to present the learner sample sentences\nalong with their morphological analyses. Following a brief introduction to the\nTurkish language and its morphology, the paper describes the morphological\nanalysis and ambiguity resolution used to construct the corpus used in the\nsearch tool. Finally, implementation issues and details involving the user\ninterface of the tool are discussed."
      }
    ]
  },
  {
    "tid":"476",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Beyond the Euler characteristic: Approximating the genus of general graphs.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Computing the Euler genus of a graph is a fundamental problem in graph theory and topology. It has been shown to be NP-hard by Thomassen [27] and a linear-time fixed-parameter algorithm has been obtained by Mohar [20]. Despite extensive study, the approximability of the Euler genus remains wide open. While the existence of a constant factor approximation is not ruled out, the currently best-known upper bound is a trivial O(n/g)-approximation that follows from bounds on the Euler characteristic. In this paper, we give the first non-trivial approximation algorithm for this problem. Specifically, we present a polynomial-time algorithm which given a graph G of Euler genus g outputs an embedding of G into a surface of Euler genus gO(1). Combined with the above O(n/g)-approximation, our result also implies a O(n1-\u03b1)-approximation, for some universal constant \u03b1> 0. Our approximation algorithm also has implications for the design of algorithms on graphs of small genus. Several of these algorithms require that an embedding of the graph into a surface of small genus is given as part of the input. Our result implies that many of these algorithms can be implemented even when the embedding of the input graph is unknown."
      }
    ]
  },
  {
    "tid":"477",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Unified Video Recommendation by Cross-Network User Modeling.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Online video sharing sites are increasingly encouraging their users to connect to the social network venues such as Facebook and Twitter, with goals to boost user interaction and better disseminate the high-quality video content. This in turn provides huge possibilities to conduct cross-network collaboration for personalized video recommendation. However, very few efforts have been devoted to leveraging users\u2019 social media profiles in the auxiliary network to capture and personalize their video preferences, so as to recommend videos of interest. In this article, we propose a unified YouTube video recommendation solution by transferring and integrating users\u2019 rich social and content information in Twitter network. While general recommender systems often suffer from typical problems like cold-start and data sparsity, our proposed recommendation solution is able to effectively learn from users\u2019 abundant auxiliary information on Twitter for enhanced user modeling and well address the typical problems in a unified framework. In this framework, two stages are mainly involved: (1) auxiliary-network data transfer, where user preferences are transferred from an auxiliary network by learning cross-network knowledge associations; and (2) cross-network data integration, where transferred user preferences are integrated with the observed behaviors on a target network in an adaptive fashion. Experimental results show that the proposed cross-network collaborative solution achieves superior performance not only in terms of accuracy, but also in improving the diversity and novelty of the recommended videos."
      }
    ]
  },
  {
    "tid":"478",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Convincing Top Management of the Strategic Potential of Information Systems.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recent research has shown that the difficulty of convincing top management of the potential strategic impact of information systems impedes information systems planning. Interviews with 20 top information systems executives revealed reasons for this difficulty and techniques that they use in attempting to overcome it. The results suggest the importance of information systems executives' skills and activities necessary for the selling of information systems products and services. The research also raises three controversial questions for future investigators."
      }
    ]
  },
  {
    "tid":"479",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'EMoD: Efficient Motion Detection of Device-Free Objects Using Passive RFID Tags' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Efficient and accurate tracking of device-free objects is critical for anti-intrusion systems. Prior solutions for device-free object tracking are mainly based on costly sensing infrastructures, resulting in barriers to practical applications. In this paper, we propose an accurate and efficient motion detection system, named EMoD, to track device-free objects based on cheap passive RFID tags. EMoD is the first RFID system that can estimate the moving direction as well as the current location of a device-free object by measuring critical power variation sequences of passive tags. Compared with previous solutions, the unique advantage of EMoD, i.e., the capability to estimate moving directions, enables object tracking using a much sparser tag deployment. We contribute to both theory and practice of this phenomenon by presenting the interference model that precisely explains it and using extensive experiments to validate it. We design a practical EMoD based intrusion detection system and implement a prototype by commercial off-the-shelf (COTS) RFID reader and tags. The real-world experiments results show that EMoD is effective in tracking the trajectory of moving object in various environments."
      }
    ]
  },
  {
    "tid":"480",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'High-Throughput Microchannels for Single Cell Immobilization' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Nowadays single cell analysis becomes a more and more important method to gather the information of individual cells and study the heterogeneity of cells caused by random expression of gene, protein and the level of metabolism. Many device and technologies of single cell analysis have been developed to meet these needs. In this paper, we presented a high-throughput microchannel for single cell immobilization with small sheer pressure. It features high density arrays, which can accommodate up to 130~300 traps within 1~2 mm\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup>\n. According to our experiment, about 91% of capture unit can be occupied by the single cell in 40 s, using the optimized structure of microchannels. Therefore, we expect that the high throughput microchannels can be of great importance for the biological research."
      }
    ]
  },
  {
    "tid":"481",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'NCBI BLASTP on the convey HC1-EX' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The BLAST sequence alignment program is a central application in bioinformatics. The de facto standard version, NCBI BLAST, uses complex heuristics which make it challenging to simultaneously achieve both high performance and exact agreement. In previous work, a system that used novel FPGA-based filters reduced the input database by over 99.97% without loss of sensitivity. In the present work we report experiences in getting from a prototype to a potential product for the Convey HC1-EX. There are several issues. The first is the efforts made to maintain timing for a highly complex configuration as it is optimized by including additional filter stages. This requires implementation and optimization of new interface logic as well as floor-planning. The second is the system-level tradeoffs necessary to maintain correctness. The issue here is preventing low frequency events, which necessarily cannot be mapped to the FPGA, from diluting the performance benefits without sacrificing sensitivity. We present results for various usage scenarios and find a factor of nearly 5x speed-up over a fully parallel implementation of the reference code on a contemporaneous CPU. We believe that the resulting system is the leading accelerated NCBI BLAST. The significance of this work is that, while such in-depth work is necessary to achieve highperformance for complex systems, these issues are rarely described nin the academic literature."
      }
    ]
  },
  {
    "tid":"482",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An evaluation of binary xml encoding optimizations for fast stream based xml processing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper provides an objective evaluation of the performance impacts of binary XML encodings, using a fast stream-based XQuery processor as our representative application. Instead of proposing one binary format and comparing it against standard XML parsers, we investigate the individual effects of several binary encoding techniques that are shared by many proposals. Our goal is to provide a deeper understanding of the performance impacts of binary XML encodings in order to clarify the ongoing and often contentious debate over their merits, particularly in the domain of high performance XML stream processing."
      }
    ]
  },
  {
    "tid":"483",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Parallel branch-and-bound approach with MPI technology in inferring chemical compounds with path frequency' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Drug design is the approach of finding drugs by design using computational tools. When designing a new drug, the structure of the drug molecule can be modeled by classification of potential chemical compounds. Kernel Methods have been successfully used in classifying potential chemical compounds. Frequency of labeled paths has been proposed to map compounds into feature in order to classify the characteristics of target compounds. In this study, we proposed an algorithm based on Kernel method via parallel computing technology to reduce computation time. This less constrain of timing allows us to aim at back tracking a full scheme of all of the possible pre-images, regardless of their difference in molecular structure, only if they shared with the same feature vector. Our method is modified on BB-CIPF and used MPI to reduce the computation time. The experimental results show that our algorithms can reduce the computation time effectively for chemical compound inference problem."
      }
    ]
  },
  {
    "tid":"484",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Triangular prism generation algorithm for polyhedron decomposition' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"RP(Rapid Prototyping) is often called as Layered Manufacturing because of layer by layer building strategy. Layer building strategy is classified into two methodologies. One is based on the 2D layer and the other is based on the 3D layer. 2D layer is simply created by the intersection between the polyhedron and a slicing plane whereas 3D layer is created with some constraints such as cuttability and manufacturability. Currently, 3D Layer is generated by using the boundary surface information in the native solid modeling format. However, most input data in Rapid Prototyping is the polyhedral surface data. We propose a geometric algorithm that uses the triangular prism to create 3D layers. Examples are shown to show the validity."
      }
    ]
  },
  {
    "tid":"485",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Implementing home energy management system with UPnP and mobile applications' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The power grid is undergoing a revolution and modernization through integrating information and communication technologies with the power system and also installing new electricity system technologies. Distributed controls in a certain self-reliant power-service area are critical means to modernize the power system to be the Smart Grid. Since the self-reliant area is assumed to perform physical and/or expert-engineered operations that should be integrated, monitored and controlled by a computational cyber system, the area is one area where the cyber and physical systems interact together, and home in the Smart Grid is one example for the area. In this paper, we propose a home energy management system (HEMS) to realize full automation combining computational and physical systems in a home area, which excludes operator-based control and management. The proposed system is designed to be built on the Universal Plug-and-Play architecture, and it can be accessed remotely over the Internet via mobile applications that work in Apple iPhone. Based on the design, we actually implemented the HEMS and constructed an empirical testbed that consists of networked personal computers, home appliances, and portable wireless devices. With the empirical testbed, we verified the monitoring and controlling functions of the HEMS. Additionally we carried out an extensive simulation study with the measured data of power consumption and the collected data of power price in order to see the effectiveness of the proposed HEMS. We also discuss about some limitations and challenges that we have encountered in designing and implementing the HEMS."
      }
    ]
  },
  {
    "tid":"486",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Unsupervised data-driven feature vector normalization with acoustic model adaptation for robust speech recognition' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, an unsupervised data-driven robust speech recognition approach is proposed based on a joint feature vector normalization and acoustic model adaptation. Feature vector normalization reduces the acoustic mismatch between training and testing conditions by mapping the feature vectors towards the training space. Model adaptation modifies the parameters of the acoustic models to match the test space. However, since neither is optimal, both approaches use an intermediate space between training and testing spaces to map either the feature vectors or acoustic models. The joint optimization of both approaches provides a common intermediate space with a better match between normalized feature vectors and adapted acoustic models. In this paper, feature vector normalization is based on a minimum mean square error (MMSE) criterion. A Class Dependent Multi-Environment Model LInear Normalization (CD-MEMLIN) based on two classes (silence/speech) with a Cross Probability Model (CD-MEMLIN-CPM) is used. CD-MEMLIN-CPM assumes that each class of clean and noisy spaces can be modeled with a Gaussian mixture model (GMM), training a linear transformation for each pair of Gaussians in an unsupervised data-driven training process. This feature vector normalization maps the recognition space feature vector to a normalized space. The acoustic model adaptation maps the training space to the normalized space by defining a set of linear transformations over an expanded HMM-state space, compensating for those degradations that the feature vector normalization is not able to model, like rotations. Experiments have been carried out with the Spanish SpeechDat Car database and Aurora 2 databases using both the standard Mel-frequency cepstral coefficient (MFCC) and advanced ETSI front-ends. Consistent improvements were reached for both corpora and front-ends. Using the standard MFCC front-end, a 92.08% average improvement on WER for Spanish SpeechDat Car and a 69.75% average improvement for clean condition evaluation of Aurora 2 was obtained, improving those results reached with ETSI advanced front-end (83.28% and 67.41%, respectively). Using the ETSI advanced front-end with the proposed solution, a 75.47% average improvement was obtained for the clean condition evaluation of Aurora 2 database."
      }
    ]
  },
  {
    "tid":"487",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Processing electronic medical records to improve predictive analytics outcomes for hospital readmissions.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Hospital readmissions are costly but largely preventable. In recent years, many researchers have used predictive analytics to build models that can minimize the adverse economic and social consequences of readmissions in chronic diseases. Most of these studies, however, have focused on improving the results either through the development of better models or through employing richer data sets. A very small number of them have focused on a comprehensive data preprocessing to improve the efficacy of analytics methods for better predictions. In this study, we propose a new data processing approach that extracts individual- and database-level historical information from the medical records to improve the performance of readmission analytics. We test and validate this method using two rather large data sets that belong to chronic diseases with the highest rates of hospital readmissions. We conclude that proper processing of large clinical data sets with analytics and big data technologies can provide competitive advantages to health care organizations."
      }
    ]
  },
  {
    "tid":"488",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Determining modular organization of protein interaction networks by maximizing modularity density.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With ever increasing amount of available data on biological networks, modeling and understanding the structure of these large networks is an important problem with profound biological implications. Cellular functions and biochemical events are coordinately carried out by groups of proteins interacting each other in biological modules. Identifying of such modules in protein interaction networks is very important for understanding the structure and function of these fundamental cellular networks. Therefore, developing an effective computational method to uncover biological modules should be highly challenging and indispensable.The purpose of this study is to introduce a new quantitative measure modularity density into the field of biomolecular networks and develop new algorithms for detecting functional modules in protein-protein interaction (PPI) networks. Specifically, we adopt the simulated annealing (SA) to maximize the modularity density and evaluate its efficiency on simulated networks. In order to address the computational complexity of SA procedure, we devise a spectral method for optimizing the index and apply it to a yeast PPI network.Our analysis of detected modules by the present method suggests that most of these modules have well biological significance in context of protein complexes. Comparison with the MCL and the modularity based methods shows the efficiency of our method."
      }
    ]
  },
  {
    "tid":"489",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mmmp: A Mac Protocol To Ensure Qos For Multimedia Traffic Over Multi-Hop Ad Hoc Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we discuss a novel reservation-based, asynchronous MAC protocol called 'Multi-rate Multi-hop MAC Protocol' (MMMP) for multi-hop ad hoc networks that provides QoS guarantees for multimedia traffic. MMMP achieves this by providing service differentiation for multirate real-time traffic (both constant and variable bit rate traffic) and guaranteeing a bounded end-to-end delay for the same while still catering to the throughput requirements of non real time traffic. In addition, it administers bandwidth preservation via a feature called 'Smart Drop' and implements efficient bandwidth usage through a mechanism called 'Release Bandwidth'. Simulation results on the QualNet simulator indicate that MMMP outperforms IEEE 802.11 on all performance metrics and can efficiently handle a large range of traffic intensity. It also outperforms other similar state-of-the-art MAC protocols."
      }
    ]
  },
  {
    "tid":"490",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Throwable tetrahedral robot with transformation capability' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a tetrahedral mobile robot with central axis for transformation to the flat vehicle is presented. The throwable robot with the function of going into narrow spaces when its in the flat-vehicle mode is explained in detail. A prototype has been developed to illustrate the concept. Motion experiments confirm the novel properties of this mechanism: mode changing function and omnidirectional motion. Basic Motion experiments, with a test vehicle are also presented. Keywords: Search and Rescue, Throwable, Tetrahedral Shaped, Omni-Ball, Transformation, Mechanical Design."
      }
    ]
  },
  {
    "tid":"491",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'InPrePPI: an integrated evaluation method based on genomic context for predicting protein-protein interactions in prokaryotic genomes.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Although many genomic features have been used in the prediction of protein-protein interactions (PPIs), frequently only one is used in a computational method. After realizing the limited power in the prediction using only one genomic feature, investigators are now moving toward integration. So far, there have been few integration studies for PPI prediction; one failed to yield appreciable improvement of prediction and the others did not conduct performance comparison. It remains unclear whether an integration of multiple genomic features can improve the PPI prediction and, if it can, how to integrate these features.In this study, we first performed a systematic evaluation on the PPI prediction in Escherichia coli (E. coli) by four genomic context based methods: the phylogenetic profile method, the gene cluster method, the gene fusion method, and the gene neighbor method. The number of predicted PPIs and the average degree in the predicted PPI networks varied greatly among the four methods. Further, no method outperformed the others when we tested using three well-defined positive datasets from the KEGG, EcoCyc, and DIP databases. Based on these comparisons, we developed a novel integrated method, named InPrePPI. InPrePPI first normalizes the AC value (an integrated value of the accuracy and coverage) of each method using three positive datasets, then calculates a weight for each method, and finally uses the weight to calculate an integrated score for each protein pair predicted by the four genomic context based methods. We demonstrate that InPrePPI outperforms each of the four individual methods and, in general, the other two existing integrated methods: the joint observation method and the integrated prediction method in STRING. These four methods and InPrePPI are implemented in a user-friendly web interface.This study evaluated the PPI prediction by four genomic context based methods, and presents an integrated evaluation method that shows better performance in E. coli."
      }
    ]
  },
  {
    "tid":"492",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Queuing Analysis of Packet Dropping over a Wireless Link with Retransmissions' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"One of the most important performance measure for real time applications is the packet loss probability. In a wireless environment losses are first due to noisy channels (fading, shadowing etc). In order to improve the radio link, one often retransmits packets that have not been well received (using the Automatic Retransmission reQuest-ARQ). This however may lead to queuing phenomena due to retransmissions and to losses of packets due to buffer overflow. We present a queuing analysis in order to compute the combined effect of the noisy channel, the retransmissions and the buffer overflow. We use advanced spectral methods of quasi-birth and death process in order to analyze the system for general multistate Markov channels and provide an approximation approach based on the theory of singular perturbation."
      }
    ]
  },
  {
    "tid":"493",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A dynamic power-aware partitioner with task migration for multicore embedded systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Nowadays, a key design issue in embedded systems is how to reduce the power consumption, since batteries have a limited energy budget. For this purpose, several techniques such as Dynamic Voltage Scaling (DVS) or task migration can be used. DVS allows reducing power by selecting the optimal voltage supply, while task migration achieves this effect by balancing the workload among cores. This paper first analyzes the impact on energy due to task migration in multicore embedded systems with DVS capability and using the well-known Worst Fit (WF) partitioning heuristic. To reduce overhead, migrations are only performed at the time that a task arrives to and/or leaves the system and, in such a case, only one migration is allowed. The huge potential on energy saving due to task migration, leads us to propose a new dynamic partitioner, namely DP, that migrates tasks in a more efficient way than typical partitioners. Unlike WF, the proposed algorithm examines which is the optimal target core before allowing a migration. Experimental results show that DP can improve energy consumption in a factor up to 2.74 over the typical WF algorithm."
      }
    ]
  },
  {
    "tid":"494",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Diffie-Hellman key distribution in wireless multi-way relay networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Diffie-Hellman protocol is a classical secret key exchange protocol for secure communications. This paper considers the extension of the original two-party Diffie-Hellman protocol to multi-party key distribution in wireless multi-way relay networks where multiple users can only communicate with one another through a single relay. Two efficient key exchange protocols are proposed. A performance comparison with existing methods adapted to the relay networks shows the enhanced efficiency and the originality of the proposed protocols designed specifically for the multi-way relay networks."
      }
    ]
  },
  {
    "tid":"495",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Assessing Student's Dynamic Knowledge State by Exploring the Question Difficulty Effect' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Knowledge Tracing (KT), which aims to assess students' dynamic knowledge states when practicing on various questions, is a fundamental research task for offering intelligent services in online learning systems. Researchers have devoted significant efforts to developing KT models with impressive performance. However, in existing KT methods, the related question difficulty level, which directly affects students' knowledge state in learning, has not been effectively explored and employed. In this paper, we focus on exploring the question difficulty effect on learning to improve student's knowledge state assessment and propose the DIfficulty Matching Knowledge Tracing (DIMKT) model. Specifically, we first explicitly incorporate the difficulty level into the question representation. Then, to establish the relation between students' knowledge state and the question difficulty level during the practice process, we accordingly design an adaptive sequential neural network in three stages: (1) measuring students' subjective feelings of the question difficulty before practice; (2) estimating students' personalized knowledge acquisition while answering questions of different difficulty levels; (3) updating students' knowledge state in varying degrees to match the question difficulty level after practice. Finally, we conduct extensive experiments on real-world datasets, and the results demonstrate that DIMKT outperforms state-of-the-art KT models. Moreover, DIMKT shows superior interpretability by exploring the question difficulty effect when making predictions. Our codes are available at https://github.com/shshen-closer/DIMKT."
      }
    ]
  },
  {
    "tid":"496",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Video event detection using generalized subclass discriminant analysis and linear support vector machines' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a two-phase approach to event detection in video is proposed. This combines a novel nonlinear Discriminant Analysis (DA) method called Generalized Subclass DA (GSDA), to identify a discriminant subspace, and a Linear Support Vector Machine (LSVM), to efficiently learn the event in the derived subspace. The proposed GSDA-LSVM framework is used as an alternative to the Kernel Support Vector Machine (KSVM) approach, which despite its excellent classification accuracy requires significant computational resources for learning the events (i.e., for identifying the kernel parameters and KSVM penalty term) in large-scale video collections. In contrary, using the GSDA-LSVM approach the SVM penalty term can be rapidly identified in the lower dimensional subspace. Moreover, an additional speed up in deriving this lower-dimensional space is achieved by using the proposed GSDA method instead of conventional nonlinear subclass DA methods such as KSDA or KMSDA. This is made possible by GSDA exploiting the special structure of the inter-between-subclass scatter matrix to reformulate the original KSDA eigenvalue problem to one involving matrices of much smaller dimension. The proposed GSDA-LSVM approach leads to more accurate event detection and to computational efficiency gains, as shown by experimental results on the extensive TRECVID MED 2010 and 2012 datasets."
      }
    ]
  },
  {
    "tid":"497",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'CrossZig: Combating Cross-Technology Interference in Low-Power Wireless Networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Low-power wireless devices suffer notoriously from Cross-Technology Interference (CTI). To enable co-existence, researchers have proposed a variety of interference mitigation strategies. Existing solutions, however, are designed to work with the limitations of currently available radio chips. In this paper, we investigate how to exploit physical layer properties of 802.15.4 signals to better address CTI. We present CrossZig, a cross-layer solution that takes advantage of physical layer information and processing to improve low-power communication under CTI. To this end, CrossZig utilizes physical layer information to detect presence of CTI in a corrupted packet and to apply an adaptive packet recovery which incorporates a novel cross-layer based packet merging and an adaptive FEC coding. We implement a prototype of CrossZig for the low-power IEEE 802.15.4 in a software-defined radio platform. We show the adaptability and the performance gain of CrossZig through experimental evaluation considering both micro-benchmarking and system performance under various interference patterns. Our results demonstrate that CrossZig can achieve a high accuracy in error localization (94.3% accuracy) and interference type identification (less than 5% error rate for SINR ranges below 3 dB). Moreover, our system shows consistent performance improvements under interference from various interfering technologies.\n\n"
      }
    ]
  },
  {
    "tid":"498",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Spatially Varying Spectral Filtering of Signals on the Unit Sphere' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a general framework for spatially-varying spectral filtering of signals defined on the unit sphere, as an analogy to joint time-frequency filtering. For this purpose, we first map spherical signals from spatial domain into joint spatio-spectral domain, where a spatio-spectral signal transformation or modification is introduced. For mapping spatial signals into joint spatio-spectral domain, we use the spatially localized spherical harmonic transform (SLSHT) from the literature. We then propose a suitable scheme to transform the modified signal from the spatio-spectral domain back to an admissible signal in the spatial domain using the least squares approach. We also show that the overall action of the SLSHT and spatio-spectral signal modification can be described through a single transformation matrix, which is useful in practice. Finally, we discuss two specific and useful instances of spatially-varying spectral filtering, defined through multiplicative and convolutive modification of the SLSHT distribution, and show through numerical examples their effectiveness in selective spectral filtering of different spatial regions of the signal."
      }
    ]
  },
  {
    "tid":"499",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Exponential bounds for DPLL below the satisfiability threshold' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"For each k \u2264 4, we give \u03c4k 0 such that a random k-CNF formula F with n variables and \u230arkn\u230b clauses is satisfiable with high probability, but ORDERED-DLL takes exponential time on F with uniformly positive probability. Using results of [2], this can be strengthened to a high probability result for certain natural backtracking schemes and extended to many other DPLL algorithms."
      }
    ]
  },
  {
    "tid":"500",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A study on DPL model of heat transfer in bi-layer tissues during MFH treatment.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, dual-phase-lag bioheat transfer model subjected to Fourier and non-Fourier boundary conditions for bi-layer tissues has been solved using finite element Legendre wavelet Galerkin method (FELWGM) during magnetic fluid hyperthermia. FELWGM localizes small scale variation of solution and fast switching of functional bases. It has been observed that moderate hyperthermia temperature range (41\u201346\u00b0C) can be better achieved in spherical symmetric coordinate system and treatment method will be independent of the Fourier and non-Fourier boundary conditions used. The effect of phase-lag times has been observed only in tumor region. FCC FePt magnetic nano-particle produces more effective treatment with respect to other magnetic nano-particles. The effect of variability of magnetic heat source parameters (magnetic induction, frequency, diameter of magnetic nano-particles, volume fractional of magnetic nano-particles and ligand layer thickness) has been investigated. The physical property of these parameters has been described in detail during magnetic fluid hyperthermia (MFH) treatment and also discussed the clinical application of MFH in Oncology."
      }
    ]
  },
  {
    "tid":"501",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Continuous spatial assignment of moving users' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Consider a set of servers and a set of users, where each server has a coverage region (i.e., an area of service) and a capacity (i.e., a maximum number of users it can serve). Our task is to assign every user to one server subject to the coverage and capacity constraints. To offer the highest quality of service, we wish to minimize the average distance between users and their assigned server. This is an instance of a well-studied problem in operations research, termed optimal assignment. Even though there exist several solutions for the static case (where user locations are fixed), there is currently no method for dynamic settings. In this paper, we consider the continuous assignment problem (CAP), where an optimal assignment must be constantly maintained between mobile users and a set of servers. The fact that the users are mobile necessitates real-time reassignment so that the quality of service remains high (i.e., their distance from their assigned servers is minimized). The large scale and the time-critical nature of targeted applications require fast CAP solutions. We propose an algorithm that utilizes the geometric characteristics of the problem and significantly accelerates the initial assignment computation and its subsequent maintenance. Our method applies to different cost functions (e.g., average squared distance) and to any Minkowski distance metric (e.g., Euclidean, L 1 norm, etc.)."
      }
    ]
  },
  {
    "tid":"502",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'DObjects: Enabling Distributed Data Services for Metacomputing Platforms' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many applications rely heavily on large amounts of data in the distributed storages collected over time or produced by large scale scientific experiments or simulations. The key constraints for building a distributed data query infrastructure for such applications are: scalability, consistency, heterogeneity and network and resource dynamics. We designed and developed DObjects, a general-purpose query and data operations infrastructure that can be integrated with metacomputing middleware. This paper describes the architecture of our data services and shows how those services were integrated with the metacomputing framework offering users an open platform for building distributed applications that require access to data integrated from multiple distributed data sources."
      }
    ]
  },
  {
    "tid":"503",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'ClusterTag: Interactive Visualization, Clustering and Tagging Tool for Big Image Collections.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Exploring and annotating collections of images without meta-data is a complex task which requires convenient ways of presenting datasets to a user. Visual analytics and information visualization can help users by providing interfaces, and in this paper, we present an open source application that allows users from any domain to use feature-based clustering of large image collections to perform explorative browsing and annotation. For this, we use various image feature extraction mechanisms, different unsupervised clustering algorithms and hierarchical image collection visualization. The performance of the presented open source software allows users to process and display thousands of images at the same time by utilizing heterogeneous resources such as GPUs and different optimization techniques."
      }
    ]
  },
  {
    "tid":"504",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Interpretable Convolutional Neural Networks with Dual Local and Global Attention for Review Rating Prediction' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recently, many e-commerce websites have encouraged their users to rate shopping items and write review texts. This review information has been very useful for understanding user preferences and item properties, as well as enhancing the capability to make personalized recommendations of these websites. In this paper, we propose to model user preferences and item properties using convolutional neural networks (CNNs) with dual local and global attention, motivated by the superiority of CNNs to extract complex features. By using aggregated review texts from a user and aggregated review text for an item, our model can learn the unique features (embedding) of each user and each item. These features are then used to predict ratings. We train these user and item networks jointly which enable the interaction between users and items in a similar way as matrix factorization. The local attention provides us insight on a user's preferences or an item's properties. The global attention helps CNNs focus on the semantic meaning of the whole review text. Thus, the combined local and global attentions enable an interpretable and better-learned representation of users and items. We validate the proposed models by testing on popular review datasets in Yelp and Amazon and compare the results with matrix factorization (MF), the hidden factor and topical (HFT) model, and the recently proposed convolutional matrix factorization (ConvMF+). Our proposed CNNs with dual attention model outperforms HFT and ConvMF+ in terms of mean square errors (MSE). In addition, we compare the user/item embeddings learned from these models for classification and recommendation. These results also confirm the superior quality of user/item embeddings learned from our model."
      }
    ]
  },
  {
    "tid":"505",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Scalable Multi-Group Key Management For Advanced Metering Infrastructure' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Advanced Metering Infrastructure (AMI) is composed of systems and networks to incorporate changes for modernizing the electricity grid, reduce peak loads, and meet energy efficiency targets. AMI is a privileged target for security attacks with potentially great damage against infrastructures and privacy. For this reason, Key Management has been identified as one of the most challenging topics in AMI development. In this paper, we propose a new Scalable multi-group key management for AMI (SAMI) to secure data communications in an Advanced Metering Infrastructure. It is a key management scheme that can support unicast, multicast and broadcast communications based on an efficient multi-group key graph technique. An analysis of security and performance, and a comparison of our scheme with recently proposed schemes show that our solution induces low storage overhead (reduction reaches 83%) and low communication overhead (reduction reaches 99%) compared to existing solutions."
      }
    ]
  },
  {
    "tid":"506",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Decentralized mapping of robot-aided sensor networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Abstract\u2014A key problem in the,deployment,of sensor networks,is that of determining,the location of each,sensor such that subsequent,data gathered,can be registered. We would also like the network to provide localization for mobile entities, allowing them,to navigate and explore the environment. In this paper, we present a robust decentralized algorithm for mapping the nodes in a sparsely connected,sensor network,using rangeonly measurements,and odometry,from,a mobile,robot. Our approach,utilizes an Extended Kalman,Filter (EKF) in polar space allowing,us to model,the nonlinearities within,the range-only measurements using Gaussian distributions. We also extend this unimodal,centralized EKF to a multi-modal,decentralized framework,enabling,us to accurately model,the ambiguities,in range-based position estimation. Each node within the network estimates,its position along,with its neighbor\u2019s position and uses a message-passing,algorithm,to propagate,its belief to its neighbors. Thus, the global network localization problem is solved in pieces, by each node independently estimating its local network, greatly reducing the computation done by each node. We demonstrate,the effectiveness of our approach,using simulated,and,real-world experiments,with little to no prior information,about the node locations."
      }
    ]
  },
  {
    "tid":"507",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Low Complexity Multiple Description Coding Method for Wireless Video' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we present a new multiple description video coding method. It starts from GOB alternation to create two descriptions for each frame encoded, carriage of motion information in both descriptions helps to improve the quality of single channel reconstruction, and a process called low quality macroblock update helps to not only further improve the quality of single channel reconstruction but also reduce the mismatch between the encoder and decoder. The advantages of the proposed approach include: effectiveness, which has better performance than the well known MDTC video coder both in terms of Redundancy Rate Distortion, and in the packet loss scenario; low complexity, and standard compliant to the extent that each description can be decoded by a standard video decoder."
      }
    ]
  },
  {
    "tid":"508",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Selective sampling based on dynamic certainty propagation for image retrieval' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In relevance feedback of image retrieval, selective sampling is often used to alleviate the burden of labeling by selecting only the most informative data to label. Traditional data selection scheme often selects a batch of data at a time and label them all together, which neglects the data's correlation and thus jeopardizes the effectiveness. In this paper, we propose a novel Dynamic Certainty Propagation (DCP) scheme for informative data selection. For each unlabeled data, we define the notion of certainty to quantify our confidence in its predicted label. Every time, we only label one single data point with the lowest degree of certainty. Then we update the rest unlabeled data's certainty dynamically according to their correlation. This one-by-one labeling offers us extra guidance from the last labeled data for the next labeling. Experiments show that the DCP scheme outperforms the traditional method evidently."
      }
    ]
  },
  {
    "tid":"509",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Post-CTS delay insertion' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A post-clock-tree-synthesis (post-CTS) optimization method is proposed that suggests delay insertion at the leaves of the clock tree in order to implement a limited version of clock skew scheduling. Delay insertion is limited on each clock tree branch simultaneous with a global monitoring of the total amount of delay insertion. The delay insertion for nonzero clock skew operation is performed only at the clock sinks in order to preserve the structure and the optimizations implemented in the clock tree synthesis stage. The methodology is implemented as a linear programming model amenable to two design objectives: fixing timing violations or optimizing the clock period. Experimental results show that the clock networks of the largest ISCAS'89 circuits can be corrected post-CTS to resolve the timing conflicts in approximately 90% of the circuits with minimal delay insertion (0.159\u00d7clock period per clock path on average). It is also shown that the majority of the clock period improvement achievable through unrestricted clock skew scheduling are obtained through very limited insertion (\u224843% average improvement through 10% of max insertion)."
      }
    ]
  },
  {
    "tid":"510",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An overview of ETSI TC SmartBAN MAC protocol' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we introduce medium access control (MAC) protocol defined by ETSI TC (technical committee) SmartBAN (smart body area network). The key features of the protocol are a two-channel concept with different channels for data and control traffic, and a unique Multi-use Channel Access as one alternative channel access mode. The two-channel concept provides fast channel acquisition and easy Hub to Hub communications. Multi-use Channel Access guarantees for very-low latency emergency messaging for time-critical applications. In addition, Re-use Access mode of the Multi-use Channel Access enables the utilization of the scheduled but unused time slots thus increasing the channel utilization. The protocol is mainly a joint work between Centre for Wireless Communications (CWC) at the University of Oulu and Toshiba Research Europe Ltd."
      }
    ]
  },
  {
    "tid":"511",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Dealing with user heterogeneity in P2P multiparty video conferencing: Layered coding versus receiver partitioning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Layered coding elegantly handles user bandwidth heterogeneity in video conferencing, however, it incurs rate and complexity overheads. An alternative is partitioning the receiver sets and using non-layered coding for each group. In this paper, we investigate how to maximize the received video quality for both systems under uplink and downlink capacity constraints, while limiting the number of hops that the videos travel by two. Towards this end, we first show that any multicast tree is equivalent to a collection of depth-1 and depth-2 trees, under outbound and inbound flow constraints. For the layered system, we propose an algorithm that simultaneously solves for the number of video layers, the rate and distribution tree of each layer. For the receiver partitioning system, we develop an algorithm for determining the receiver partitions and tree construction for each group. Through numerical comparison study, we show that the receiver partitioning system achieves significantly higher video quality than the layered system, due to its higher coding efficiency."
      }
    ]
  },
  {
    "tid":"512",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Facial performance enhancement using dynamic shape space analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The facial performance of an individual is inherently rich in subtle deformation and timing details. Although these subtleties make the performance realistic and compelling, they often elude both motion capture and hand animation. We present a technique for adding fine-scale details and expressiveness to low-resolution art-directed facial performances, such as those created manually using a rig, via marker-based capture, by fitting a morphable model to a video, or through Kinect reconstruction using recent faceshift technology. We employ a high-resolution facial performance capture system to acquire a representative performance of an individual in which he or she explores the full range of facial expressiveness. From the captured data, our system extracts an expressiveness model that encodes subtle spatial and temporal deformation details specific to that particular individual. Once this model has been built, these details can be transferred to low-resolution art-directed performances. We demonstrate results on various forms of input; after our enhancement, the resulting animations exhibit the same nuances and fine spatial details as the captured performance, with optional temporal enhancement to match the dynamics of the actor. Finally, we show that our technique outperforms the current state-of-the-art in example-based facial animation."
      }
    ]
  },
  {
    "tid":"513",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Summarizing Unconstrained Videos Using Salient Montages.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present a novel method to summarize unconstrained videos using salient montages (i.e., a \u201cmelange\u201d of frames in the video as shown in Fig. 1), by finding \u201cmontageable moments\u201d and identifying the salient people and actions to depict in each montage. Our method aims at addressing the increasing need for generating concise visualizations from the large number of videos being captured from portabl..."
      }
    ]
  },
  {
    "tid":"514",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On harmonic fixed-priority scheduling of periodic real-time tasks with constrained deadlines.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"It is well known that a harmonic task set, i.e., task periods are integer multiples of each other, can better utilize a processor to achieve high system utilization. However, the current definition of harmonic task set is limited only to tasks with deadlines equal to their periods. In this paper, we extend the concept of \\\"harmonic task set\\\" to tasks with constrained deadlines, i.e., deadlines less than or equal to their periods. We show that a harmonic task set with constrained deadlines has a better schedulability than the non-harmonic one with the same task utilization. We employ this characteristic for task partitioning on multi-core platform, and our extensive experimental results show that, by taking the task harmonic relationship into consideration, our partitioning approach can greatly improve the schedulability of real-time tasks on multi-core platforms."
      }
    ]
  },
  {
    "tid":"515",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Hierarchical video object segmentation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a general video object segmentation framework which views object segmentation from a unified Bayesian perspective and optimizes the MAP formulated problem in a progressive manner. Based on object detection and tracking results, a three-level hierarchical video object segmentation approach is presented. At the first level, an offline learned segmentor is applied to each object tracking result of current frame to get a coarse segmentation. At the second level, the coarse segmentation is updated into an intermediate segmentation by a temporal model which propagates the fine segmentation of previous frame to current frame based on a discriminative feature points voting process. At the third level, the intermediate segmentation is refined by an iterative procedure which uses online collected color-and-shape information to get the final result. We apply the approach to pedestrian segmentation on many challenging datasets that demonstrates its effectiveness."
      }
    ]
  },
  {
    "tid":"516",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Hashing-Based Approximate Probabilistic Inference in Hybrid Domains' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In recent years, there has been considerable progress on fast randomized algorithms that approximate probabilistic inference with tight tolerance and confidence guarantees. The idea here is to formulate inference as a counting task over an annotated propositional theory, called weighted model counting (WMC), which can be partitioned into smaller tasks using universal hashing. An inherent limitation of this approach, however, is that it only admits the inference of discrete probability distributions. In this work, we consider the problem of approximating inference tasks for a probability distribution defined over discrete and continuous random variables. Building on a notion called weighted model integration, which is a strict generalization of WMC and is based on annotating Boolean and arithmetic constraints, we show how probabilistic inference in hybrid domains can be put within reach of hashing-based WMC solvers. Empirical evaluations demonstrate the applicability and promise of the proposal."
      }
    ]
  },
  {
    "tid":"517",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Security Risk Management via Dynamic Games with Learning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a game-theoretic and learning approach to security risk management based on a model that captures the diffusion of risk in an organization with multiple technical and business processes. Of particular interest is the way the interdependencies between processes affect the evolution of the organization's risk profile as time progresses, which is first developed as a probabilistic risk framework and then studied within a discrete Markov model. Using zero-sum dynamic Markov games, we analyze the interaction between a malicious adversary whose actions increases the risk level of the organization and a defender agent, e.g. security and risk management division of the organization, which aims to mitigate risks. We derive min-max (saddle point) solutions of this game to obtain the optimal risk management strategies for the organization to achieve a certain level of performance. This methodology also applies to worst-case scenario analysis where the adversary can be interpreted as a nature player in the game. In practice, the parameters of the Markov game may not be known due to the costly nature of collecting and processing information about the adversary as well an organization with many components itself. We apply ideas from Q-learning to analyze the behavior of the agents when little information is known about the environment in which the attacker and defender interact. The framework developed and results obtained are illustrated with a small example scenario and numerical analysis."
      }
    ]
  },
  {
    "tid":"518",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Dynamic Helper Threaded Prefetching on the Sun UltraSPARC CMP Processor' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Data prefetching via helper threading has been extensively investigated on Simultaneous Multi- Threading (SMT) or Virtual Multi-Threading (VMT) architectures. Although reportedly large cache latency can be hidden by helper threads at runtime, most techniques rely on hardware support to reduce context switch overhead between the main thread and helper thread as well as rely on static profile feedback to construct the help thread code. This paper develops a new solution by exploiting helper threaded prefetching through dynamic optimization on the latest UltraSPARC Chip-Multiprocessing (CMP) processor. Our experiments show that by utilizing the otherwise idle processor core, a single user-level helper thread is sufficient to improve the runtime performance of the main thread without triggering multiple thread slices. Moreover, since the multiple cores are physically decoupled in the CMP, contention introduced by helper threading is minimal. This paper also discusses several key technical challenges of building a lightweight dynamic optimization/software scouting system on the UltraSPARC/Solaris platform."
      }
    ]
  },
  {
    "tid":"519",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A probabilistic result for the max-cut problem on random graphs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We consider the max-cut problem on a random graph G with n vertices and weights w\"i\"j being independent bounded random variables with the same fixed positive expectation @m and variance @s^2. It is well known that the max-cut number mc(G) always exceeds 12@?\"i\"~."
      }
    ]
  },
  {
    "tid":"520",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Friends or Foes: Distributed and Randomized Algorithms to Determine Dishonest Recommenders in Online Social Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Viral marketing is becoming important due to the popularity of online social networks (OSNs). Companies may provide incentives (e.g., via free samples of a product) to a small group of users in an OSN, and these users provide recommendations to their friends, which eventually increases the overall sales of a given product. Nevertheless, this also opens a door for malicious behaviors: dishonest users may intentionally give misleading recommendations to their friends so as to distort the normal sales distribution. In this paper, we propose a detection framework to identify dishonest users in the OSNs. In particular, we present a set of fully distributed and randomized algorithms, and also quantify the performance of the algorithms by deriving probability of false positive, probability of false negative, and the distribution of number of detection rounds. Extensive simulations are also carried out to illustrate the impact of misleading recommendations and the effectiveness of our detection algorithms. The methodology we present here will enhance the security level of viral marketing in the OSNs."
      }
    ]
  },
  {
    "tid":"521",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Pool-based anonymous communication framework for\u00a0high-performance computing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose and analyze in details the revised model of XPROB, an infinite family of pool-based anonymous communication systems that can be used in various applications including high performance computing environments. XPROB overcomes the limitations of APROB Channel that only resists a global delaying adversary (GDA). Each instance of XPROB uses a pool mix as its core component to provide resistance against a global active adversary (GAA), a stronger yet more practical opponent than a GDA. For XPROB, a GAA can drop messages from users but cannot break the anonymity of the senders of messages. Analysis and experimental evaluations show that each instance of XPROB provides greater anonymity than APROB Channel for the same traffic load and user behaviors (rate and number of messages sent). In XPROB, any message can be delivered with high probability within a few rounds after its arrival into the system; thus, an opponent cannot be certain when a message will be delivered. Furthermore, users can choose their own preference balance between anonymity and delay. Through the evaluation, we prove that XPROB can provide anonymity for users in high-performance computing environments."
      }
    ]
  },
  {
    "tid":"522",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Pattern recognition of chest X-ray images' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"AISCR-V2 is a software system for automatic interpretation of chest radiograms. In this paper, an outline of the system is briefly given, and then the method for recognition of rib images is described in detail. The procedure employed here is very unique as a processing method for complex pictures and constitutes the most important part of the system for deriving final decisions correctly. Next, the method for detection of abnormal shadows in the lungs is presented. Finally, experimental results for chest pho-tofluorograms are shown. These results are promising as a first step in automatic diagnosis of chest X-ray films and are very interesting as an example of pattern recognition of extremely complex images."
      }
    ]
  },
  {
    "tid":"523",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Effect of Vehicle Mobility on Connectivity of Vehicular Ad Hoc Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Connectivity is a fundamental requirement in the planning, design and evaluation of vehicular ad hoc networks (VANET). In this paper, we propose a new equivalent speed parameter and develop an analytical model to explain the effect of vehicle mobility on the connectivity of highway segments in a VANET. We prove that the equivalent speed is different from the average vehicle speed and it decreases as the standard deviation of the vehicle speed increases. Using the equivalent speed we derive a novel analytical expression for the average number of vehicles on a highway segment, which allows us to accurately predict the network 1 connectivity. We verify the correctness of our analytical approach by comparing the numerical results with simulations. The results show that increasing the average vehicle speed increases the equivalent speed, which leads to a decrease in the average number of vehicles on a highway segment and consequently degrades connectivity. On the other hand increasing the standard deviation of the vehicle speed decreases the equivalent speed, which leads to an increase in the average number of vehicles on a highway segment and consequently improves connectivity. The results also show that vehicles in a VANET can adaptively choose their transmission range to ensure network connectivity in highway segments while minimising power consumption."
      }
    ]
  },
  {
    "tid":"524",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'SE CLEVER: A secure message oriented Middleware for Cloud federation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we address several critical security issues that bottle up the wide adoption of the Cloud Computing technology. In the last years the Cloud Security Alliance (CSA) has identified many security issues that should be considered during the design of Cloud systems. We experienced them developing the Security-Enhanced (SE) CLEVER, the secure release of CLEVER, a Message Oriented Middleware for Cloud computing based on the well-known XMPP protocol. SE CLEVER provides a secure inter-module and inter-Cloud communication system useful for managing Federated environments. The secure communication system of SE CLEVER was designed leveraging the XMPP flexibility. The experimental results show how the secure capabilities introduced in SE CLEVER do not affect the overall performances of the middleware."
      }
    ]
  },
  {
    "tid":"525",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Web page publication time detection and its application for page rank' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Publication Time (P-time for short) of Web pages is often required in many application areas. In this paper, we address the issue of P-time detection and its application for page rank. We first propose an approach to extract P-time for a page with explicit P-time displayed on its body. We then present a method to infer P-time for a page without P-time. We further introduce a temporal sensitive page rank model using P-time. Experiments demonstrate that our methods outperform the baseline methods significantly."
      }
    ]
  },
  {
    "tid":"526",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Iterative decision-feedback channel estimation for turbo SIC in DFT-precoded OFDMA using QO-STBC' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes using iterative decision-feedback channel estimation (DFCE) in a turbo soft interference canceller (SIC) for a quasi-orthogonal (QO) - space-time block code (STBC) with four antennas in discrete Fourier transform (DFT)-precoded orthogonal frequency division multiple access (OFDMA). In the proposed method, channel estimation (CE) is updated at each turbo SIC iteration loop by using soft-symbol estimation based on the log-likelihood ratio (LLR) of an extrinsic probability at the Max-Log-MAP (maximum a posteriori probability) decoder output. Hence, the accuracy of the generated residual inter-code interference (ICI) due to non-orthogonality in the QO-STBC improves in the turbo SIC, which leads to improvement in the achievable performance. Computer simulation results show that by applying the proposed iterative DFCE in the turbo SIC, the required average received signal-to-noise power ratio (SNR) at the average block error rate (BLER) of 10-2 is decreased by approximately 1.5 and 1.0 dB compared to those for STBC using reference signal (RS) based CE and DFCE, respectively, for the coding rate of R = 1/3 in turbo code. Hence, the iterative DFCE works cooperatively in the turbo SIC to decrease the ICI in the full rate QO-STBC."
      }
    ]
  },
  {
    "tid":"527",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Liver and tumor segmentation and analysis from CT of diseased patients via a generic affine invariant shape parameterization and graph cuts' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The paper presents the automated segmentation of livers from abdominal CT images of diseased populations from images with inconsistent enhancement. A novel three-dimensional (3D) affine invariant shape parameterization is employed to compare local shape across organs. By generating a regular sampling of the organ's surface, this parameterization can be effectively used to compare features of a set of closed 3D surfaces point-to-point while avoiding common problems with the parameterization of concave surfaces. From an initial segmentation, the areas of atypical local shape are determined using training sets. A geodesic active contour corrects locally the segmentations of organs in abnormal images and optimized graph cuts segment the vasculature and hepatic tumors using shape and enhancement constraints. Liver segmentation errors are reduced significantly, all tumors are detected and the tumor burden is estimated with 0.9% error. Results from test data demonstrate the method's robustness to analyze livers from difficult clinical cases to allow temporal monitoring of patients with hepatic cancer."
      }
    ]
  },
  {
    "tid":"528",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Trajectory analysis using switched motion fields: a parametric approach' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a new model for trajectories in video sequences using mixtures of motion fields. Each field is described by a simple parametric model with only a few parameters. We show that, despite the simplicity of the motion fields, the overall model is able to generate complex trajectories occuring in video analysis."
      }
    ]
  },
  {
    "tid":"529",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Sensed Data Sharing in Cloud Federation for Advances in Health Information Exchange' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper the authors focus on sensing systems supporting data exchange among several healthcare administrative domains. The challenge in this area is twofold: efficient management of a huge amount of data produced by medical devices, bio-sensors and information systems, sharing sensed data for scientific and clinical purposes. The authors present a new information system that exploits Cloud computing capabilities to overcome such issues, also guaranteeing patients' privacy. Their proposal integrates different healthcare institutions into a federated environment, thus establishing a trust context among the institutions themselves. The storage service is designed according to a fully distributed approach and it is based on the wide-used Open Source framework Hadoop, which is enriched to establish a compelling federated system. They adopt the XRI technology to formalize an XML-based data model which allows to simplify the classification, searching and retrieval of medical data."
      }
    ]
  },
  {
    "tid":"530",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'From 3-D Speaker Cloning To Text-To-Audiovisual-Speech' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Visible speech movements were motion captured and parameterized. Coarticulated targets were extracted from VCVs and modeled to generate arbitrary German utterances by target interpolation. The system was extended to synthesize English utterances by a mapping to German phonemes. An evaluation by means of a modified rhyme test reveals that the synthetic videos of isolated words increase the recognition scores from 27 % to 47.5 % when added to audio only presentation."
      }
    ]
  },
  {
    "tid":"531",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Are smartphones really useful for scientific computing?' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Smartphones are a new kind of mobile devices that allow users to take their office anywhere and anytime with them. The number of smartphones is rapidly growing. Most of the time their capabilities are underused, therefore several authors have studied how to exploit smartphones for assisting scientific computing. Yet, as far as we know, there is no study aimed at determining whether smartphones can do a significant contribution to this area as resource providers. This paper shows that smartphones are not that slow when compared to standard mobile devices, such as notebooks. Furthermore, a notebook running on battery only performed 8 times more work than a low-end smartphone before their batteries run out. However, the low-end smartphone is 145 times slower than the notebook, and the smartphone battery has less capacity than the notebook battery. Since smartphones can execute large amount of work running on battery, we think that smartphones can have a major role in building the next-generation HPC infrastructures."
      }
    ]
  },
  {
    "tid":"532",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'PKI-based trust management in inter-domain scenarios' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Hierarchical cross-certification fits well within large organizations that want their root CA to have direct control over all subordinate CAs. However, both Peer-to-Peer and Bridge CA cross-certification models suits better than the hierarchical one with organizations where a certain level of flexibility is needed to form and revoke trust relationships with other organizations as changing policy or business needs dictate. It seems that this second approach better fits the current and next-generation inter-domain networking models existing in both the wired and wireless Internet. In this context, this paper analyses some relevant inter-domain scenarios and derives the main requirements in terms of cross-certification from them. It then describes the design and lab implementation of a pan-European scenario which is based on a research network composed by a set of organizations that may have their own PKIs running, and that are interested to link with others in terms of certification services. It provides a complete design, implementation and performance analysis for this complex scenario, including a procedure and practical recommendations for building and validating certification paths."
      }
    ]
  },
  {
    "tid":"533",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Analyzing Pedestrians' Walking Patterns Using Single-Row Laser Range Scanners' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose a novel system for analyzing pedestrians' walking patterns by exploiting single-row laser range scanners that measure distances of surrounding objects by reflecting eye-safe laser beams. A walking model is built in the spatial-temporal domain to describe the periodicity of the movement of the feet, and a mean-shift technique is applied to recover model parameters. Compared with camera-based methods, our system provides a novel technique to analyze the behavior of pedestrians. The experiments show the validity of the algorithm."
      }
    ]
  },
  {
    "tid":"534",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Design and compilation of an object-oriented macroprogramming language for wireless sensor networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Wireless sensor network (WSN) programming is still largely performed by experts in a node-centric way using low-level languages such as C. Although numerous higher-level abstractions exist, each simplifying a specific aspect of distributed programming, real applications often require to combine multiple abstractions into a single program. Using current programming frameworks, this represents a difficult task. In previous work, we therefore defined a conceptual framework that facilitates abstraction composition by defining sound compositional rules among few fundamental abstraction categories. The framework is extensible: programmers can add new abstractions within the boundaries determined by the compositional rules. In this paper we describe the design of a language-called MPL-that instantiates this conceptual framework. To support the extensible nature of the framework, the language is object-oriented, which allows programmers to add new abstractions by inheriting from existing classes that implement predefined interfaces. We modeled the syntax after Java, to make it more palatable to inexperienced embedded programmers. Compared to Java, we modified the language to enable efficient execution on WSN devices. We designed and implemented a compiler that translates MPL language into executable C code, which spares the overhead of a virtual machine. By comparing MPL implementations against functionally-equivalent Contiki/C implementations of several benchmark applications, we determined that the performance overhead of MPL is limited, and yet the programming task is simplified."
      }
    ]
  },
  {
    "tid":"535",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Synthesis of Partial Behavior Models from Properties and Scenarios' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Synthesis of behavior models from software development artifacts such as scenario-based descriptions or requirements specifications helps reduce the effort of model construction. However, the models favored by existing synthesis approaches are not sufficiently expressive to describe both universal constraints provided by requirements and existential statements provided by scenarios. In this paper, we propose a novel synthesis technique that constructs behavior models in the form of ModalTransition Systems (MTS) from a combination of safety properties and scenarios. MTSs distinguish required, possible, and proscribed behavior, and their elaboration not only guarantees the preservation of the properties and scenarios used for synthesis but also supports further elicitation of new requirements."
      }
    ]
  },
  {
    "tid":"536",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimization of cooperative spectrum sensing under noise uncertainty' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Noise uncertainty, which is unavoidable in practice, can severely limit the detection performance of cooperative spectrum sensing. Derived from the conventional hard combination scheme, the improved one-out-of-N rule is presently the most effective algorithm to reduce the influence of noise uncertainty. However, in this algorithm the abandonment of local test statistic when it falls in the uncertainty region may lose some useful information. In this paper, first we analyze the optimality of the improved one-out-of-N rule and indicate that there should be an optimal number of cooperative users that minimizes the total error rate. Then, a new weighted hard combination scheme which makes full use of the uncertainty region is further proposed. The optimizations of weight coefficient and user number in this new scheme are also investigated. Numerical results show that the minimal total error rate and its corresponding user number of this scheme are both lower than those of the improved one-out-of-N rule."
      }
    ]
  },
  {
    "tid":"537",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Connectivity restoration in disjoint wireless sensor networks using limited number of mobile relays' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Disjoint Wireless Sensor Networks (WSNs) can be reconnected by placing additional relay nodes in the damaged areas. However, in some cases there may not be enough relays to reconnect all the partitions with the sink node. In such a case, some of the relays can exploit their motion capabilities and temporarily act as a mobile data collector (MDC) between partitions providing intermittent connectivity for the nodes sitting in those partitions. Nonetheless, due to increased data latency intermittent connectivity creates, the number of such MDCs need to be minimized. On the other hand, given that the energy resources for an MDC is limited, an upper bound on the travel distance overhead for an MDC needs to be imposed. This paper proposes a relay placement algorithm which guarantees connectivity by maximizing the number of stable connections while meeting the maximum tour constraint on the MDCs. The approach first determines the number and location of relays to restore connectivity by establishing stable links using a Steiner Minimum Tree (SMT) heuristic. Assuming that the number of available relays is less than the needed count, the algorithm determines how many of the available relays need to be stationary and how many of them should act as MDCs. By initially assuming all relays as MDCs, an iterative procedure is followed to reduce the MDC count while meeting the maximum tour length constraint. Specifically, groups of partitions are created and assigned to MDCs for touring. The proposed approach is validated with extensive simulations under a variety of conditions."
      }
    ]
  },
  {
    "tid":"538",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The Three-Tier Security Scheme in Wireless Sensor Networks with Mobile Sinks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mobile sinks (MSs) are vital in many wireless sensor network (WSN) applications for efficient data accumulation, localized sensor reprogramming, and for distinguishing and revoking compromised sensors. However, in sensor networks that make use of the existing key predistribution schemes for pairwise key establishment and authentication between sensor nodes and mobile sinks, the employment of mobile sinks for data collection elevates a new security challenge: in the basic probabilistic and q-composite key predistribution schemes, an attacker can easily obtain a large number of keys by capturing a small fraction of nodes, and hence, can gain control of the network by deploying a replicated mobile sink preloaded with some compromised keys. This article describes a three-tier general framework that permits the use of any pairwise key predistribution scheme as its basic component. The new framework requires two separate key pools, one for the mobile sink to access the network, and one for pairwise key establishment between the sensors. To further reduce the damages caused by stationary access node replication attacks, we have strengthened the authentication mechanism between the sensor and the stationary access node in the proposed framework. Through detailed analysis, we show that our security framework has a higher network resilience to a mobile sink replication attack as compared to the polynomial pool-based scheme."
      }
    ]
  },
  {
    "tid":"539",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Poster: Do walking motions enhance visually induced self-motion illusions in virtual reality?' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Locomotion interfaces that support physical self-motion in virtual reality facilitate spatial updating, but have relatively high cost and typically require large physical spaces. A better understanding of the illusion of self-motion, or vection, presents a potential solution to this problem. Though circular self-motion illusions induced using only visuals or only walking have been investigated previously, the interaction between these two types has not. We conducted an experiment to examine the additive effects of walking stimuli and visual motion cues on intensity and convincingness of circular veetion. Our results indicate a trend towards decreased vection onset time when illusory rotation stimuli were combined. Measures of intensity and convincingness were also rated higher for the combined stimulus condition when compared with walking or visual stimuli separately. Consequently, lean and elegant virtual reality interface designs should include both walking and visual stimuli for a compelling experience of self-motion."
      }
    ]
  },
  {
    "tid":"540",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Efficient Hybrid Music Recommender System Using an Incrementally Trainable Probabilistic Generative Model' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a hybrid music recommender system that ranks musical pieces while efficiently maintaining collaborative and content-based data, i.e., rating scores given by users and acoustic features of audio signals. This hybrid approach overcomes the conventional tradeoff between recommendation accuracy and variety of recommended artists. Collaborative filtering, which is used on e-commerce sites, cannot recommend nonbrated pieces and provides a narrow variety of artists. Content-based filtering does not have satisfactory accuracy because it is based on the heuristics that the user's favorite pieces will have similar musical content despite there being exceptions. To attain a higher recommendation accuracy along with a wider variety of artists, we use a probabilistic generative model that unifies the collaborative and content-based data in a principled way. This model can explain the generative mechanism of the observed data in the probability theory. The probability distribution over users, pieces, and features is decomposed into three conditionally independent ones by introducing latent variables. This decomposition enables us to efficiently and incrementally adapt the model for increasing numbers of users and rating scores. We evaluated our system by using audio signals of commercial CDs and their corresponding rating scores obtained from an e-commerce site. The results revealed that our system accurately recommended pieces including nonrated ones from a wide variety of artists and maintained a high degree of accuracy even when new users and rating scores were added."
      }
    ]
  },
  {
    "tid":"541",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On robust image spam filtering via comprehensive visual modeling' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The Internet has brought about fundamental changes in the way peoples generate and exchange media information. Over the last decade, unsolicited message images (image spams) have become one of the most serious problems for Internet service providers (ISPs), business firms and general end users. In this paper, we report a novel system called RoBoTs (Robust BoosTrap based spam detector) to support accurate and robust image spam filtering. The system is developed based on multiple visual properties extracted from different levels of granularity, aiming to capture more discriminative contents for effective spam image identification. In addition, a resampling based learning framework is developed to effectively integrate random forest and linear discriminative analysis (LDA) to generate comprehensive signature of spam images. It can facilitate more accurate and robust spam classification process with very limited amount of initial training examples. Using three public available test collections, the proposed system is empirically compared with the state-of-the-art techniques. Our results demonstrate its significantly higher performance from different perspectives. Author-HighlightsWe develop a novel scheme to model contents of spam image and compute comprehensive signatures.A hybrid framework is developed to detect spam images effectively.Our approach achieves substantial performance improvement on spam detection in terms of effectiveness and robustness."
      }
    ]
  },
  {
    "tid":"542",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multislot estimation of fast-varying space-time channels in TD-CDMA systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this letter we propose a subspace method to esti- mate the time-varying space-time channel by taking into account the different rates of variation between delays/directions of arrival (slowly-varying) and faded amplitudes (fast-varying) of the mul- tipath propagation. The stationarity of angles and delays across several time-slots is exploited to perform an unstructured estima- tion that avoids the explicit computation of the path parameters. Numerical analysis for a TD-CDMA system shows that the asymp- totic performance is reached with a reasonable number of slots. Index Terms\u2014Antenna array, channel estimation, multi-slot, subspace method, TD-CDMA, TD-SCDMA, ULTRA-TDD."
      }
    ]
  },
  {
    "tid":"543",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fairness-Aware classifier with prejudice remover regularizer' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With the spread of data mining technologies and the accumulation of social data, such technologies and data are being used for determinations that seriously affect individuals' lives. For example, credit scoring is frequently determined based on the records of past credit data together with statistical prediction techniques. Needless to say, such determinations must be nondiscriminatory and fair in sensitive features, such as race, gender, religion, and so on. Several researchers have recently begun to attempt the development of analysis techniques that are aware of social fairness or discrimination. They have shown that simply avoiding the use of sensitive features is insufficient for eliminating biases in determinations, due to the indirect influence of sensitive information. In this paper, we first discuss three causes of unfairness in machine learning. We then propose a regularization approach that is applicable to any prediction algorithm with probabilistic discriminative models. We further apply this approach to logistic regression and empirically show its effectiveness and efficiency."
      }
    ]
  },
  {
    "tid":"544",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Energy beamforming for full-duplex wireless-powered communication networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we consider a full-duplex (FD) wireless-powered communication network (WPCN), where one FD hybrid access point (HAP) equipped with multiple antennas simultaneously transmits energy to and receives information from multiple users. Firstly, we propose a space division wireless energy allocation scheme and calculate the harvested energy in downlink wireless energy transfer (WET) for each user. Secondly, we derive an approximate closed-form expression of user\u2019s achievable ergodic rate in uplink wireless information transfer (WIT). Thirdly, the energy allocation for different users is optimized under the max\u2013min user fairness constraint, and a closed-form solution is obtained. Numerical results show that the simulation and the approximation of achievable rates are well matched, and energy beamforming can effectively suppress self-interference (SI), and improve rates as well as fairness among users. Moreover, FD-WPCNs are shown to outperform half-duplex (HD) WPCNs in rates with same number of antennas."
      }
    ]
  },
  {
    "tid":"545",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Interference mitigation between ultra-wideband sensor network and other legal systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Ultra-wideband impulse radio (UWB-IR) sensor network has intensive military and commercial applications. However, the interference between UWB and other existed networks should be casually investigated. In this paper, we consider interference mitigation in UWB sensors in the context of cognitive radio (CR). Firstly, we suggest a general state transition model to characterize the working states evolution of legal networks, also referred to as primary users (PU). Spectrum sensing, used to identify the state of PU, is formulated as detection of a corresponding state sequence. Maximum posterior probability (MAP) criterion is adopted to perform spectrum sensing. By exploring potential gain of state transitions, detection probability for nearby networks is improved significantly. Subsequently, based on the radius basis function neural network (RBF), we present a novel spectrum sculptor to design UWB waveforms. Attributed to the excellent reconfiguration of RBF, our scheme can produce UWB waveforms tracing available spectrums. The designed waveforms can entirely utilize multiple unoccupied bands to maintain uninterrupted communications. Also, sufficient spectral attenuation can be generated in specific bands to mitigate mutual interference between UWB sensors and other networks. Besides, orthogonal waveforms can be easily derived, which either improves transmission performance or provides a flexible accessing strategy for UWB sensors."
      }
    ]
  },
  {
    "tid":"546",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'MaxOutProbe: An Algorithm for Increasing the Size of Partially Observed Networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Networked representations of real-world phenomena are often partially observed, which lead to incomplete networks. Analysis of such incomplete networks can lead to skewed results. We examine the following problem: given an incomplete network, which $b$ nodes should be probed to bring the largest number of new nodes into the observed network? Many graph-mining tasks require having observed a considerable amount of the network. Examples include community discovery, belief propagation, influence maximization, etc. For instance, consider someone who has observed a portion (say 1%) of the Twitter retweet network via random tweet sampling. She wants to estimate the size of the largest connected component of the fully observed retweet network. To improve her estimate, how should she use her limited budget to reduce the incompleteness of the network? In this work, we propose a novel algorithm, called MaxOutProbe, which uses a budget $b$ (on nodes probed) to increase the size of the observed network in terms of the number of nodes. Our experiments, across a range of datasets and conditions, demonstrate the advantages of MaxOutProbe over existing methods."
      }
    ]
  },
  {
    "tid":"547",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Microblog Entity Linking with Social Temporal Context' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Nowadays microblogging sites, such as Twitter and Chinese Sina Weibo, have established themselves as an invaluable information source, which provides a huge collection of manually-generated tweets with broad range of topics from daily life to breaking news. Entity linking is indispensable for understanding and maintaining such information, which in turn facilitates many real-world applications such as tweet clustering and classification, personalized microblog search, and so forth. However, tweets are short, informal and error-prone, rendering traditional approaches for entity linking in documents largely inapplicable. Recent work addresses this problem by utilising information from other tweets and linking entities in a batch manner. Nevertheless, the high computational complexity makes this approach infeasible for real-time applications given the high arrival rate of tweets. In this paper, we propose an efficient solution to link entities in tweets by analyzing their social and temporal context. Our proposed framework takes into consideration three features, namely entity popularity, entity recency, and user interest information embedded in social interactions to assist the entity linking task. Effective indexing structures along with incremental algorithms have also been developed to reduce the computation and maintenance costs of our approach. Experimental results based on real tweet datasets verify the effectiveness and efficiency of our proposals."
      }
    ]
  },
  {
    "tid":"548",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'ScalaGiST: Scalable Generalized Search Trees for MapReduce Systems [Innovative Systems Paper].' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"MapReduce has become the state-of-the-art for data parallel processing. Nevertheless, Hadoop, an open-source equivalent of MapReduce, has been noted to have sub-optimal performance in the database context since it is initially designed to operate on raw data without utilizing any type of indexes. To alleviate the problem, we present ScalaGiST - scalable generalized search tree that can be seamlessly integrated with Hadoop, together with a cost-based data access optimizer for efficient query processing at run-time. ScalaGiST provides extensibility in terms of data and query types, hence is able to support unconventional queries (e.g., multi-dimensional range and k-NN queries) in MapReduce systems, and can be dynamically deployed in large cluster environments for handling big users and data. We have built ScalaGiST and demonstrated that it can be easily instantiated to common B+-tree and R-tree indexes yet for dynamic distributed environments. Our extensive performance study shows that ScalaGiST can provide efficient write and read performance, elastic scaling property, as well as effective support for MapReduce execution of ad-hoc analytic queries. Performance comparisions with recent proposals of specialized distributed index structures, such as SpatialHadoop, Data Mapping, and RT-CAN further confirm its efficiency."
      }
    ]
  },
  {
    "tid":"549",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Towards Reverse-Engineering Black-Box Neural Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many deployed learned models are black boxes: given input, returns output. Internal information about the model, such as the architecture, optimisation procedure, or training data, is not disclosed explicitly as it might contain proprietary information or make the system more vulnerable. This work shows that such attributes of neural networks can be exposed from a sequence of queries. This has multiple implications. On the one hand, our work exposes the vulnerability of black-box neural networks to different types of attacks -- we show that the revealed internal information helps generate more effective adversarial examples against the black box model. On the other hand, this technique can be used for better protection of private content from automatic recognition models using adversarial examples. Our paper suggests that it is actually hard to draw a line between white box and black box models."
      }
    ]
  },
  {
    "tid":"550",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Comparison of ns2.34's ZigBee/802.15.4 implementation to Memsic's IRIS Motes' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, Wireless Personal Area Network (IEEE 802.15.4) wireless channel utilization using Memsic (formerly Crossbow) IRIS Motes is measured and compared to ns2.34 simulation and theoretical models. Results show divergence between calculated, simulated and experimental results at higher throughput. Experimental results also highlight the limitations of the MoteWorks framework with regard to channel throughput measurements. A set of mitigation strategies is outlined to improve future research efforts aiming to characterize the validity of simulation models using the Memsic wireless sensor nodes."
      }
    ]
  },
  {
    "tid":"551",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Change Point Detection For Information Diffusion Tree' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose a method of detecting the points at which the speed of information diffusion changed from an observed diffusion sequence data over a social network, explicitly taking the network structure into account. Thus, change in diffusion is both spatial and temporal. This is different from most of the existing change detection approaches in which all the diffusion information is projected on a single time line and the search is made in this time axis. We formulate this as a search problem of change points and their respective change rates under the framework of maximum log-likelihood embedded in MDL. Time complexity of the search is almost proportional to the number of observed data points and the method is very efficient. We tested this using both a real Twitter date (ground truth not known) and the synthetic data (ground truth known), and demonstrated that the proposed method can detect the change points efficiently and the results are very different from the existing sequence-based (time axis) approach (Kleinberg's method)."
      }
    ]
  },
  {
    "tid":"552",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Towards scheduling for Internet-of-Things applications on clouds: a simulated annealing approach' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The Internet of Things IoT and Cloud Computing are two novel paradigms both rapidly evolving in their particular areas of application. The former is enabled through several technologies ranging from communication systems to distributed intelligence, whereas the latter provides the means for massively parallel computation on demand. Therefore, we can include Cloud Computing as an enabling factor in the greater picture of the IoT. Given the complexity of IoT's computing concepts, it would be prudent to assume that this complexity will eventually provide computing workloads that are different in kind to the current ones. Thus, it becomes important to study how current scheduling optimization techniques can be adapted to such computing tasks. In this study, we have evaluated the application of simulated annealing in a multi-cloud system serving a workload of processes with low parallelism but with high arrival rates and highly variant run-times. A discrete event simulator was used in order to assess both the performance and the cost of the system. Simulation results indicate that significant gains both in performance and in cost can be achieved using the proposed scheduling technique in this context. Copyright \u00a9 2013 John Wiley & Sons, Ltd."
      }
    ]
  },
  {
    "tid":"553",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multilevel image recognition using discriminative patches and kernel covariance descriptor' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Computer-aided diagnosis of medical images has emerged as an important tool to objectively improve the performance, accuracy and consistency for clinical workflow. To computerize the medical image diagnostic recognition problem, there are three fundamental problems: where to look (i.e., where is the region of interest from the whole image/volume), image feature description/encoding, and similarity metrics for classification or matching. In this paper, we exploit the motivation, implementation and performance evaluation of task-driven iterative, discriminative image patch mining; covariance matrix based descriptor via intensity, gradient and spatial layout; and log-Euclidean distance kernel for support vector machine, to address these three aspects respectively. To cope with often visually ambiguous image patterns for the region of interest in medical diagnosis, discovery of multilabel selective discriminative patches is desired. Covariance of several image statistics summarizes their second order interactions within an image patch and is proved as an effective image descriptor, with low dimensionality compared with joint statistics and fast computation regardless of the patch size. We extensively evaluate two extended Gaussian kernels using affine-invariant Riemannian metric or log-Euclidean metric with support vector machines (SVM), on two medical image classification problems of degenerative disc disease (DDD) detection on cortical shell unwrapped CT maps and colitis detection on CT key images. The proposed approach is validated with promising quantitative results on these challenging tasks. Our experimental findings and discussion also unveil some interesting insights on the covariance feature composition with or without spatial layout for classification and retrieval, and different kernel constructions for SVM. This will also shed some light on future work using covariance feature and kernel classification for medical image analysis."
      }
    ]
  },
  {
    "tid":"554",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Epidemic-Style management of semantic overlays for content-based searching' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A lot of recent research on content-based P2P searching for file- sharing applications has focused on exploiting semantic relations between peers to facilitate searching. To the best of our knowledge, all methods proposed to date suggest reactive ways to seize peers' semantic relations. That is, they rely on the usage of the underlying search mechanism, and infer semantic relations based on the queries placed and the corresponding replies received. In this paper we follow a different approach, proposing a proactive method to build a semantic overlay. Our method is based on an epidemic protocol that clusters peers with similar content. It is worth noting that this peer clustering is done in a completely implicit way, that is, without requiring the user to specify his preferences or to characterize the content of files he shares."
      }
    ]
  },
  {
    "tid":"555",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Adaptive CCA for IEEE 802.15.4 Wireless Sensor Networks to Mitigate Interference' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"IEEE 802.15.4 Wireless Sensor Networks (WSNs) share the 2.4 GHz Industrial, Scientific, and Medical (ISM) license-free band with many other wireless technologies such as IEEE 802.11b/g Wireless Local Area Networks (WLANs). Because of the low-power, however, IEEE 802.15.4 WSNs are potentially vulnerable to the interference introduced by the other wireless technologies such as IEEE 802.11b/g WLANs, which have much higher power. Particularly, in the presence of heavy interference, IEEE 802.15.4 nodes can hardly get a chance to access the channel, which could result in discarding a large amount of packets. In this paper, we propose a decentralized approach to help IEEE 802.15.4 nodes mitigate interference. By adaptively and distributively adjusting Clear Channel Assessment (CCA) thresholds of IEEE 802.15.4 nodes in the presence of heavy interference, the approach can substantially reduce the amount of discarded packets due to channel access failures, and therefore significantly enhance the performance of IEEE 802.15.4 WSNs. The approach is robust, responsive and easy to be implemented at a low cost. The effectiveness of the approach is validated by OPNET simulation."
      }
    ]
  },
  {
    "tid":"556",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A hybrid EF/DF protocol with rateless coded network code for two-way relay channel' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we investigate a rateless code design for a two-way relay channel (TWRC) where a network coding operation is employed at the relay. Rateless codes are used at both two users and the relay nodes as it can cope with various channel conditions. We propose a hybrid estimate and forward (EF)/decode and forward (DF) protocol with rateless codes for the TWRC. In the proposed scheme, the relay uses a joint belief propagation (BP) decoding process to decode the messages of two users. Depending on whether the relay recovers both users' messages, only one user's message or no users' messages, the relay will generate three different types of signals, namely, network code (NC), soft network code (SNC) or soft physical-layer network code (SPNC), respectively. We design the degree distribution of the rateless codes for the three nodes of the TWRC, to make it suitable not only for decoding at two users at high SNR, but also for decoding at the relay at low SNR. We show that the proposed hybrid EF/DF protocol with the designed rateless codes outperforms other schemes significantly in terms of bit error rate (BER) performance."
      }
    ]
  },
  {
    "tid":"557",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Stateless One-Way Multi-Head Finite Automata With Pebbles' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Stateless variants of deterministic one-way multi-head finite automata with pebbles, that is, automata where the heads can drop, sense, and pick up pebbles, are studied. The relation between heads and pebbles is investigated, and a proper double hierarcliy concerning these two resources is obtained. Moreover, it is shown that a conversion of an arbitrary automaton to a stateless automaton can always be achieved at the cost of additional heads and/or pebbles. On the other hand, there are languages where one head cannot be traded for any number of additional pebbles and vice versa. Finally, the emptiness problem and related problems are shown to be undecidable even for the 'simplest' model, namely, for stateless one-way finite automata with two heads and one pebble."
      }
    ]
  },
  {
    "tid":"558",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Unsupervised learning of hierarchical spatial structures in images' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The visual world demonstrates organized spatial pat- terns, among objects or regions in a scene, object-parts in an object, and low-level features in object-parts. These classes of spatial structures are inherently hierarchical in nature. Although seemingly quite different these spatial pat- terns are simply manifestations of different levels in a hier- archy. In this work, we present a unified approach to un- supervised learning of hierarchical spatial structures from a collection of images. Ours is a hierarchical rule-based model capturing spatial patterns, where each rule is repre- sented by a star-graph. We propose an unsupervised EM- style algorithm to learn our model from a collection of im- ages. We show that the inference problem of determining the set of learnt rules instantiated in an image is equiva- lent to finding the minimum-cost Steiner tree in a directed acyclic graph. We evaluate our approach on a diverse set of data sets of object categories, natural outdoor scenes and images from complex street scenes with multiple objects."
      }
    ]
  },
  {
    "tid":"559",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Distributed PMU for Electrical Substations With Wireless Redundant Process Bus' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Protection and measurement systems in electrical substations are required to have high availability. In an all-digital substation protection system, all the components (instrument transformers, processing units, merging units, intelligent electronic devices, communication network, and synchronization source) may affect the overall availability level. In this paper, a solution to enhance distributed PMU availability, during wired network failures, is presented. In the proposed scheme, the process bus has two parallel networks: 1) the classic wired Ethernet link and 2) a wireless link (implemented with industrial grade IEEE 802.11 devices), for sampled values packets, which carry measurement information. The time synchronization is carried out only through the wired Ethernet link, but the proposed solution is still able to compensate temporary failures of one of the communication links. Experimental tests have been performed to verify the performance of additional IEEE 802.11 link using different protocols and configurations. Communication parameters that can affect the PMU performance, like propagation latency, are characterized. It is shown that, if the measurement algorithm is opportunely designed, depending on the wireless link quality, it is possible to comply, with a single output, with M and P classes of the synchrophasor standard also during network restoration or, at least, to safeguard protection applications if higher latency occurs."
      }
    ]
  },
  {
    "tid":"560",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Game-theory-based batch identification of invalid signatures in wireless mobile networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Digital signature has been widely employed in wireless mobile networks to ensure the authenticity of messages and identity of nodes. A paramount concern in signature verification is reducing the verification delay to ensure the network QoS. To address this issue, researchers have proposed the batch cryptography technology. However, most of the existing works focus on designing batch verification algorithms without sufficiently considering the impact of invalid signatures. The performance of batch verification could dramatically drop, if there are verification failures caused by invalid signatures. In this paper, we propose a Game-theory-based Batch Identification Model (GBIM) for wireless mobile networks, enabling nodes to find invalid signatures with the optimal delay under heterogeneous and dynamic attack scenarios. Specifically, we design an incomplete information game model between a verifier and its attackers, and prove the existence of Nash Equilibrium, to select the dominant algorithm for identifying invalid signatures. Moreover, we propose an auto-match protocol to optimize the identification algorithm selection, when the attack strategies can be estimated based on history information. Comprehensive simulation results demonstrate that GBIM can identify invalid signatures more efficiently than existing algorithms."
      }
    ]
  },
  {
    "tid":"561",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The MAP3S Static-and-Regular Mesh Simulation and Wavefront Parallel-Programming Patterns' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents the simulation and wavefront parallel-programming patterns of the MAP3S pattern-based parallel programming system for distributed-memory environments. Both patterns target iterative computations on static-and-regular meshes. In addition to providing performance-oriented features, such as asynchronous communication and distribution of the computational workload that is tailored to fit the computation, the patterns also provide usability-oriented features, such as direct mesh-access, mesh memory-footprint distribution, and a versatile data-dependency specification scripting-language. Parallel programs developed using MAP3S achieve significant performance gains and capability enhancements on both low-end and high-end interconnect-equipped distributed-memory systems."
      }
    ]
  },
  {
    "tid":"562",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Automatic Seal Information Reader' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Seals contain a lot of vital information about the document. Seal detection is a necessary step to gain access to that information. In the present paper, we present a correlation based technique which exploits the existence of some constant character strings and their topology for detection of seals. We also present a technique which separates the false positives from the actual seals. We achieved an accuracy of about 98% for extraction of such kind of seals. Once the seal is extracted from a page image, an OCR has been employed to read the contextual information present in the seal. A knowledge based post-processing step is employed to enhance the accuracy of the recognized text strings."
      }
    ]
  },
  {
    "tid":"563",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'COMPARISON AND SELECTION OF OBJECTIVE FUNCTIONS IN MULTIOBJECTIVE COMMUNITY DETECTION' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Detecting communities of complex networks has been an effective way to identify substructures that could correspond to important functions. Conventional approaches usually consider community detection as a single-objective optimization problem, which may confine the solution to a particular community structure property. Recently, a new community detection paradigm is emerging: multiobjective optimization for community detection, which means simultaneously optimizing multiple criteria and obtaining a set of community partitions. The new paradigm has shown its advantages. However, an important issue is still open: what type of objectives should be optimized to improve the performance of multiobjective community detection? To exploit this issue, we first proposed a general multiobjective community detection solution called NSGA-Net and then analyzed the structural characteristics of communities identified by a variety of objective functions that have been used or can potentially be used for community detection. After that, we exploited correlation relations i.e., positively correlated, independent, or negatively correlated between any two objective functions. Extensive experiments on both artificial and real networks demonstrate that NSGA-Net optimizing over a pair of negatively correlated objectives usually leads to better performances compared with the single-objective algorithm optimizing over either of the original objectives, or even to other well-established community detection approaches."
      }
    ]
  },
  {
    "tid":"564",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Joint data alignment up to (lossy) transformations' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Joint data alignment is often regarded as a data simplifi- cation process. This idea is powerful and general, but raises two delicate issues. First, one must make sure that the use- ful information about the data is preserved by the alignment process. This is especially important when data are affected by non-invertible transformations, such as those originating from continuous domain deformations in a discrete image lattice. We propose a formulation that explicitly avoids this pitfall. Second, one must choose an appropriate measure of data complexity. We show that standard concepts such as entropy might not be optimal for the task, and we pro- pose alternative measures that reflect the regularity of the codebook space. We also propose a novel and efficient al- gorithm that allows joint alignment of a large number of samples (tens of thousands of image patches), and does not rely on the assumption that pixels are independent. This is done for the case where the data is postulated to live in an affine subspaces of the embedding space of the raw data. We apply our scheme to learn sparse bases for natural images that discount domain deformations and hence significantly decrease the complexity of codebooks while maintaining the same generative power."
      }
    ]
  },
  {
    "tid":"565",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'XPC: architectural support for secure and efficient cross process call' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Microkernel has many intriguing features like security, fault-tolerance, modularity and customizability, which recently stimulate a resurgent interest in both academia and industry (including seL4, QNX and Google's Fuchsia OS). However, IPC (inter-process communication), which is known as the Achilles' Heel of microkernels, is still the major factor for the overall (poor) OS performance. Besides, IPC also plays a vital role in monolithic kernels like Android Linux, as mobile applications frequently communicate with plenty of user-level services through IPC. Previous software optimizations of IPC usually cannot bypass the kernel which is responsible for domain switching and message copying/remapping; hardware solutions like tagged memory or capability replace page tables for isolation, but usually require non-trivial modification to existing software stack to adapt the new hardware primitives. In this paper, we propose a hardware-assisted OS primitive, XPC (Cross Process Call), for fast and secure synchronous IPC. XPC enables direct switch between IPC caller and callee without trapping into the kernel, and supports message passing across multiple processes through the invocation chain without copying. The primitive is compatible with the traditional address space based isolation mechanism and can be easily integrated into existing microkernels and monolithic kernels. We have implemented a prototype of XPC based on a Rocket RISC-V core with FPGA boards and ported two microkernel implementations, seL4 and Zircon, and one monolithic kernel implementation, Android Binder, for evaluation. We also implement XPC on GEM5 simulator to validate the generality. The result shows that XPC can reduce IPC call latency from 664 to 21 cycles, up to 54.2x improvement on Android Binder, and improve the performance of real-world applications on microkernels by 1.6x on Sqlite3 and 10x on an HTTP server with minimal hardware resource cost.\n\n"
      }
    ]
  },
  {
    "tid":"566",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'MP-FMIPv6: MLD proxy based fast multicast protocol for mobile IPv6' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mobile multicast is a research hotspot and can provide many applications. Some schemes have been proposed to support the mobile subscriber, but most of them study the construction algorithm of dynamic multicast delivery structure and use the analysis and simulation to evaluate the performance, little concern on the multicast disruption time and protocol cost. In this paper, we propose a Multicast Listener Discovery (MLD) Proxy based Fast Multicast protocol for Mobile IPv6 (MP-FMIPv6), and implement simplified MLD proxy function on Home Agent (HA) and extend MLD host part function on MN. The analytical results show that MLD proxy based fast multicast can reduce multicast disruption time and cost. To evaluate the performance of proposed scheme in real applications, we setup a test-bed and compare it with Bi-directional Tunneling (BT) and Remote Subscription (RS) methods. The experiment results show that the proposed scheme nearly improves the multicast disruption time by 2.4 times and 3.66 times, and saves the cost about 19.1% and 57.4%, respectively."
      }
    ]
  },
  {
    "tid":"567",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'UBL: unsupervised behavior learning for predicting performance anomalies in virtualized cloud systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Infrastructure-as-a-Service (IaaS) clouds are prone to performance anomalies due to their complex nature. Although previous work has shown the effectiveness of using statistical learning to detect performance anomalies, existing schemes often assume labelled training data, which requires significant human effort and can only handle previously known anomalies. We present an Unsupervised Behavior Learning (UBL) system for IaaS cloud computing infrastructures. UBL leverages Self-Organizing Maps to capture emergent system behaviors and predict unknown anomalies. For scalability, UBL uses residual resources in the cloud infrastructure for behavior learning and anomaly prediction with little add-on cost. We have implemented a prototype of the UBL system on top of the Xen platform and conducted extensive experiments using a range of distributed systems. Our results show that UBL can predict performance anomalies with high accuracy and achieve sufficient lead time for automatic anomaly prevention. UBL supports large-scale infrastructure-wide behavior learning with negligible overhead."
      }
    ]
  },
  {
    "tid":"568",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Time Stamp Based Scheduling for Energy Harvesting Systems with Hybrid Nonvolatile Hardware Support' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Nonvolatile processors have manifested strong vitality in energy harvesting systems due to their endurable features to intermittent power supply. However, repeating configurations of peripherals still occupy too much task execution time, which substantially reduces effectiveness of previous scheduling algorithms. In this paper, we adopt the hybrid nonvolatile hardware platform and then propose a time stamp based scheduling algorithm. The experimental results present that the proposed algorithm matches the platform seamlessly and outperforms state-of-the-art algorithms both in effectiveness and efficiency."
      }
    ]
  },
  {
    "tid":"569",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Ambiguity group based location recognition for multiple power line outages in smart grids' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The efficient location recognition of multiple outage lines is critical to not only cascading failure elimination but also repair cost reduction. Most of existing methods, however, fail to handle location recognition well. This failure typically occurs because the methods cannot overcome three challenges: the very limited number of Phasor Measurement Units (PMUs), the high computational complexity, and the possibility of concurrent multiple outage lines. This paper presents an efficient algorithm inspired by the ambiguity group theory to recognize the most likely outage line locations with limited PMUs. Using IEEE 14-, 57-, and 118-bus system, the experimental study demonstrates that the proposed technique can successfully recognize the most likely multiple outage line locations with a reasonable computation complexity."
      }
    ]
  },
  {
    "tid":"570",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Energy Efficient Computational Offloading Framework for Mobile Cloud Computing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The latest developments in mobile computing technology have changed user preferences for computing. However, in spite of all the advancements in the recent years, Smart Mobile Devices (SMDs) are still low potential computing devices which are limited in memory capacity, CPU speed and battery power lifetime. Therefore, Mobile Cloud Computing (MCC) employs computational offloading for enabling computationally intensive mobile applications on SMDs. However, state-of-the-art computational offloading frameworks lack of considering the additional overhead of components migration at runtime. Therefore resources intensive and energy consuming distributed application execution platform is established. This paper proposes a novel distributed Energy Efficient Computational Offloading Framework (EECOF) for the processing of intensive mobile applications in MCC. The framework focuses on leveraging application processing services of cloud datacenters with minimal instances of computationally intensive component migration at runtime. As a result, the size of data transmission and energy consumption cost is reduced in computational offloading for MCC. We evaluate the proposed framework by benchmarking prototype application in the real MCC environment. Analysis of the results show that by employing EECOF the size of data transmission over the wireless network medium is reduced by 84 % and energy consumption cost is reduced by 69.9 % in offloading different components of the prototype application. Hence, EECOF provides an energy efficient application layer solution for computational offloading in MCC."
      }
    ]
  },
  {
    "tid":"571",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Flexible Broadcasting of Scalable Video Streams to Heterogeneous Mobile Devices' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We study the scalable video broadcasting problem in mobile TV broadcast networks, where each TV channel is encoded into a scalable video stream with multiple layers, and several TV channels are concurrently broadcast over a shared air medium to many mobile devices with heterogeneous resources. Our goal is to encapsulate and broadcast video streams encoded in scalable manner to enable heterogeneous mobile devices to render the most appropriate video substreams while achieving high energy saving and low channel switching delay. The appropriate streams depend on the device capability and the target energy consumption level. We propose two new broadcast schemes, which are flexible in the sense that they allow diverse bit rates among layers of the same stream. Such flexibility enables videos to be optimally encoded in terms of coding efficiency, and allows the coded video streams to be better matched with the capability of mobile devices. We analyze the performance of the proposed broadcast schemes. In addition, we have implemented the proposed schemes in a real mobile TV testbed to show their practicality and efficiency. Our extensive experiments confirm that the proposed schemes enable energy saving differentiation: between 75 and 95 percent were observed. Moreover, one of the schemes achieves low channel switching delays: 200 msec is possible with typical system parameters."
      }
    ]
  },
  {
    "tid":"572",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'ExpressQ: Identifying Keyword Context and Search Target in Relational Keyword Queries' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Keyword search in relational databases has gained popularity due to its ease of use. However, the challenge to return query answers that satisfy users' information need remains. Traditional keyword queries have limited expressive capability and are ambiguous. In this work, we extend keyword queries to enhance their expressive power and describe an semantic approach to process these queries. Our approach considers keywords that match meta-data such as the names of relations and attributes, and utilizes them to provide the context of subsequent keywords in the query. Based on the ORM schema graph which captures the semantics of objects and relationships in the database, we determine the objects and relationships referred to by the keywords in order to infer the search target of the query. Then, we construct a set of minimal connected graphs called query patterns, to represent user's possible search intentions. Finally, we translate the top-k ranked query patterns into SQL statements in order to retrieve information that the user is interested in. We develop a system prototype called ExpressQ to process the extended keyword queries. Experimental results show that our system is able to generate SQL statements that retrieve user intended information effectively."
      }
    ]
  },
  {
    "tid":"573",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Image guided complex dose delivery for small animal radiotherapy' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The purpose of the small animal radiation research platform (SARRP) is to demonstrate computed tomography, conformal irradiation, and treatment planning capabilities for small animals. The SARRP is a portable system for precision irradiation with beam sizes down to approximately 0.5 mm and optimally planned radiation with on-board cone-beam CT (CBCT) guidance. This paper describes the utilization of the precision radiation beam on the SARRP for ldquodose paintingrdquo complex structures in the mouse brain. Pencil beam dose painting aims to target the tumor as accurately as possible and give it the highest dose of radiation while sparing normal tissue as much as possible."
      }
    ]
  },
  {
    "tid":"574",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Tools and strategies for debugging distributed stream processing applications' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Distributed data stream processing applications are often characterized by data flow graphs consisting of a large number of built-in and user-defined operators connected via streams. These flow graphs are typically deployed on a large set of nodes. The data processing is carried out on-the-fly, as tuples arrive at possibly very high rates, with minimum latency. It is well known that developing and debugging distributed, multi-threaded, and asynchronous applications, such as stream processing applications, can be challenging. Thus, without domain-specific debugging support, developers struggle when debugging distributed applications. In this paper, we describe tools and language support to support debugging distributed stream processing applications. Our key insight is to view debugging of stream processing applications from four different, but related, perspectives. First, debugging the semantics of the application involves verifying the operator-level composition and inspecting the flows at the logical level. Second, debugging the user-defined operators involves traditional source-code debugging, but strongly tied to the stream-level interactions. Third, debugging the deployment details of the application require understanding the runtime physical layout and configuration of the application. Fourth, debugging the performance of the application requires inspecting various performance metrics (such as communication rates, CPU utilization, etc.) associated with streams, operators, and nodes in the system. In light of this characterization, we developed several tools such as a debugger-aware compiler and an associated stream debugger, composition and deployment visualizers, and performance visualizers, as well as language support, such as configuration knobs for logging and tracing, deployment configurations such as operator-to-process and process-to-node mappings, monitoring directives to inspect streams, and special sink adapters to intercept and dump streaming data to files and sockets, to name a few. We describe these tools in the context of Spade\u2014a language for creating distributed stream processing applications, and System S\u2014a distributed stream processing middleware under development at the IBM Watson Research Center. Published in 2009 by John Wiley & Sons, Ltd. This article is a U.S. Government work and is in the public domain in the U.S.A."
      }
    ]
  },
  {
    "tid":"575",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Nuclear segmentation in microscope cell images: a hand-segmented dataset and comparison of algorithms' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Image segmentation is an essential step in many image analysis pipelines and many algorithms have been proposed to solve this problem. However, they are often evaluated subjectively or based on a small number of examples. To fill this gap, we hand-segmented a set of 97 fluorescence microscopy images (a total of 4009 cells) and objectively evaluated some previously proposed segmentation algorithms. We focus on algorithms appropriate for high-throughput settings, where only minimal user intervention is feasible. The hand-labeled dataset (and all software used to compare methods) is publicly available to enable others to use it as a benchmark for newly proposed algorithms."
      }
    ]
  },
  {
    "tid":"576",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Leveraging multiviews of trust and similarity to enhance clustering-based recommender systems.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Although demonstrated to be efficient and scalable to large-scale data sets, clustering-based recommender systems suffer from relatively low accuracy and coverage. To address these issues, we develop a multiview clustering method through which users are iteratively clustered from the views of both rating patterns and social trust relationships. To accommodate users who appear in two different clusters simultaneously, we employ a support vector regression model to determine a prediction for a given item, based on user-, item- and prediction-related features. To accommodate (cold) users who cannot be clustered due to insufficient data, we propose a probabilistic method to derive a prediction from the views of both ratings and trust relationships. Experimental results on three real-world data sets demonstrate that our approach can effectively improve both the accuracy and coverage of recommendations as well as in the cold start situation, moving clustering-based recommender systems closer towards practical use."
      }
    ]
  },
  {
    "tid":"577",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Modeling TCG-Based secure systems with colored petri nets' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With the rapid progresses in trusted computing related research and application, many trusted computing based security mechanisms have been proposed to defend against threats in open, dynamic and distributed environments. These mechanisms are supposed to serve as the security foundations in the underlying systems. However, the correctness of these security mechanisms still require further examination and validation. We propose a Colored Petri Nets (CPN or CP-nets) based approach to model the trusted computing based secure system. In particular, with CPN, we model process management, data protection and late launch mechanisms in the systems. Further, as case studies we use these models to investigate the memory protection mechanism in TrustVisor and remote attestation based on dynamic root of trust, respectively; and the results demonstrate that our models are indeed capable of depicting real secure system based on trusted computing. With the advantages of CPN based modeling and analysis (e.g., graphical representation, well defined semantics and a large number of formal analysis methods), our models can well serve as the foundation for formal analysis on the security properties of trusted computing enhanced systems."
      }
    ]
  },
  {
    "tid":"578",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Investigating the role of phoneme-level modifications in emotional speech resynthesis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recent studies in our lab show that emotions in speech are man- ifested as, besides supra-segmental trends, distinct variations in phoneme-level prosodic and spectral parameters. In this pa- per, we further investigate the significance of this finding in the context of emotional speech synthesis. Specifically, we study phoneme-level signal property manipulation in transforming the emotional information conveyed in a speech utterance. We ana- lyze the effect of individual and combined modifications of F0, duration, energy and spectrum using data recorded by a pro- fessional actress with happy, angry, sad and neutral expressive- ness. We use content matched source-target pairs and apply TD- PSOLA for prosody and LPC for spectrum modifications by di- rectly extracting the required parameters from the target speech. Listening tests conducted with 10 naive raters show that modi- fication of prosody and spectral envelope parameters by them- selves is not sufficient. However, when applied together, modi- fying spectrum and prosody at the phone level gives successful results for most emotion pairs, except conversion to happy tar- gets. We also observe that at the phoneme level, spectral enve- lope modifications are more effective than local prosodic mod- ifications; and that, duration modifications are more effective than pitch modifications. The results confirm our hypothesis that phoneme level modifications can be used to fine tune the ensuing suprasegmental-parameter-based modifications to im- prove the overall quality of synthesized emotions."
      }
    ]
  },
  {
    "tid":"579",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Recognizing 3-D Objects Using Surface Descriptions' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The authors provide a complete method for describing and recognizing 3-D objects, using surface information. Their system takes as input dense range date and automatically produces a symbolic description of the objects in the scene in terms of their visible surface patches. This segmented representation may be viewed as a graph whose nodes capture information about the individual surface patches and whose links represent the relationships between them, such as occlusion and connectivity. On the basis of these relations, a graph for a given scene is decomposed into subgraphs corresponding to different objects. A model is represented by a set of such descriptions from multiple viewing angles, typically four to six. Models can therefore be acquired and represented automatically. Matching between the objects in a scene and the models is performed by three modules: the screener, in which the most likely candidate views for each object are found; the graph matcher, which compares the potential matching graphs and computes the 3-D transformation between them; and the analyzer, which takes a critical look at the results and proposes to split and merge object graphs."
      }
    ]
  },
  {
    "tid":"580",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Unified Approach To Planning Versatile Motions For An Autonomous Digital Actor' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Enabling a digital actor to move autonomously in a virtual environment is a challenging problem that has attracted much attention in recent years. The systems proposed in several researches have been able to plan the walking motions of a humanoid on an uneven terrain. In this paper, we aim to design a planning system that can generate various types of motions for a humanoid with a unified planning approach. Based on our previous work, we add two additional motion abilities: leaping and moving obstacles into the system. In previous work, the order of moving obstacles is determined first, and then the corresponding paths for the pushing/pulling motions are generated. In this work, we take a unified approach that accounts for all types of motions at the same time. We have implemented a planning system with this unified approach for a humanoid moving in a layered virtual environment. Several simulation examples are demonstrated in this paper to illustrate the effectiveness of the system."
      }
    ]
  },
  {
    "tid":"581",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An efficient joint dynamic detection technique for wireless transmission of JPEG2000 encoded images' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"JPEG 2000 stands out among the image compression standards as its facilities to provide tools for error-resilience and compression efficiency and image quality. However, these tools cannot correct transmission errors, they can only detect the error, conceal it, and resynchronize the decoder. Therefore, they are not efficient in wireless scenarios. Recently, JPEG 2000 has been extended to support efficient transmissions of JPEG 2000 compressed images over an error-prone wireless network. Though, error protection and correction in wireless image transmission is still a key problem for the multimedia community. In this paper, we propose a joint dynamic error detection for JPEG 2000 encoded image transmission in order to obtain more effective wireless throughput of compressed images. We define an extended code structure to perform joint encoding for transmitters and propose a novel joint error dynamic detection scheme for receivers. An extensive set of simulations was carried out in order to evaluate and demonstrate the performance of our solution, with the proposed method applied to practical CDMA communication systems."
      }
    ]
  },
  {
    "tid":"582",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Document clustering based on non-negative matrix factorization' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a novel document clustering method based on the non-negative factorization of the term-document matrix of the given document corpus. In the latent semantic space derived by the non-negative matrix factorization (NMF), each axis captures the base topic of a particular document cluster, and each document is represented as an additive combination of the base topics. The cluster membership of each document can be easily determined by finding the base topic (the axis) with which the document has the largest projection value. Our experimental evaluations show that the proposed document clustering method surpasses the latent semantic indexing and the spectral clustering methods not only in the easy and reliable derivation of document clustering results, but also in document clustering accuracies."
      }
    ]
  },
  {
    "tid":"583",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Extension of iVAT to asymmetric matrices' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The iVAT algorithm reorders (symmetric) dissimilarity data so that an image of the data may reveal cluster substructure. This paper extends the method so that it can handle asymmetric dissimilarity data. The extension is based on replacing the asymmetric input data with its unique least-squared error approximation by a symmetric matrix. Examples are given to illustrate the new method, called asymmetric iVAT (asiVAT)."
      }
    ]
  },
  {
    "tid":"584",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Boundary macroblock padding in MPEG-4 video decoding using a graphics coprocessor' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"MPEG-4 is the latest multimedia coding standard that supports object-based coding and manipulation of natural video and synthetic graphics objects. Due to its various features and high coding efficiency, MPEG-4 is becoming popular in video streaming applications. Many graphics coprocessors provide the acceleration of inverse discrete cosine transform (IDCT) and motion compensation for real-time video decoding. Therefore, it is desired to use the graphics coprocessors to accelerate MPEG-4 video decoding as well. Since MPEG-4 video decoding for rectangular video objects is similar to other video coding standards, e.g., MPEG-2, the IDCT and motion compensation can still be executed on the graphics coprocessors. However, we have found that boundary macroblock padding, which is an essential processing step in decoding arbitrarily shaped video objects, could not be efficiently accelerated on the graphics coprocessors due to its complexity. Although we can implement the boundary macroblock padding on the host processor, the frame data processed on the graphics coprocessor need to be transferred to the host processor for padding. In addition, the padded data on the host processor need to be sent back to the graphics coprocessor to be used as a reference for subsequent frames. To avoid this overhead, we present two approaches of boundary macroblock padding. In the first approach, the boundary macroblock padding is partitioned into two tasks, one of which the host processor can perform without the overhead of data transfers. In the second approach, we propose two new instructions and an algorithm that can be easily adopted in the next-generation graphics coprocessors or mediaprocessors, which gives a performance improvement of up to a factor of nine compared to that with the Pentium III."
      }
    ]
  },
  {
    "tid":"585",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Surveying the Development of Biometric User Authentication on Mobile Phones' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Designing reliable user authentication on mobile phones is becoming an increasingly important task to protect users\u2019 private information and data. Since biometric approaches can provide many advantages over the traditional authentication methods, they have become a significant topic for both academia and industry. The major goal of biometric user authentication is to authenticate legitimate users and identify imposters based on physiological and behavioral characteristics. In this article, we survey the development of existing biometric authentication techniques on mobile phones, especially on touchenabled devices, with reference to eleven biometric approaches (five physiological and six behavioral). We present a taxonomy of existing efforts regarding biometric authentication on mobile phones and analyze their feasibility of deployment on touchenabled mobile phones. In addition, we systematically characterize a generic biometric authentication system with eight potential attack points and survey practical attacks and potential countermeasures on mobile phones. Moreover, we propose a framework for establishing a reliable authentication mechanism through implementing a multimodal biometric user authentication in an appropriate way. Experimental results are presented to validate this framework using touch dynamics, and the results show that multimodal biometrics can be deployed on touch-enabled phones to significantly reduce the false rates of a single biometric system. Finally, we identify challenges and open problems in this area, and suggest that touch dynamics will become a mainstream aspect in designing future user authentication on mobile phones."
      }
    ]
  },
  {
    "tid":"586",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On the Multicast Capacity of the Wireless Broadcast Channel' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"For a multicast network, in which a common source is transmitted to $N$ users, the problem of maximizing the average rate subject to a coverage constraint (minimum quality of service) is studied. Considering such a network with single-antenna nodes and assuming that the channel state information is available only at the receiver side, the highest expected rate achievable by a random user in the network, called the expected typical rate, is derived in two scenarios: hard coverage constraint and soft coverage constraint. In the first case, the coverage is expressed in terms of the outage probability, while in the second case, the expected rate should satisfy a certain minimum requirement. It is shown that the optimum solution (achieving the highest expected typical rate for given coverage requirements) in both cases is achieved by an infinite layer superposition code for which the optimal power allocation among different layers is derived. The analysis is extended to a scenario with multiple transmit antennas. For the MISO case, a suboptimal coding scheme is proposed, which is shown to be asymptotically optimal, when the number of transmit antennas grows at least logarithmically with the number of users in the network."
      }
    ]
  },
  {
    "tid":"587",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A system-aware optimized data organization for efficient scientific analytics' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Large-scale scientific applications on High End Computing systems produce a large volume of highly complex datasets. Such data imposes a grand challenge to conventional storage systems for the need of efficient I/O solutions during both the simulation runtime and data post-processing phases. With the mounting needs of scientific discovery, the read performance of large-scale simulations has becomes a critical issue for the HPC community. In this study, we propose a system-aware optimized data organization strategy that can organize data blocks of multidimensional scientific data efficiently based on simulation output and the underlying storage systems, thereby enabling efficient scientific analytics. Our experimental results demonstrate a performance speedup up to 72 times for the combustion simulation S3D, compared to the logically contiguous data layout."
      }
    ]
  },
  {
    "tid":"588",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Curb Detection Based on a Multi-Frame Persistence Map for Urban Driving Scenarios' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"An approach for the detection of straight and curved curbs (border of relevant traffic isles, sidewalks, etc) is presented, in the context of urban driving assistance systems. A rectangular elevation map is built from 3D dense stereo data. Edge detection is applied to the elevation map in order to highlight height variations. We propose a method to reduce significantly the 3D noise from dense stereo, using a multiframe persistence me persistence map: temporal filtering is performed for edge points, based on the ego car motion, and only persistent points are validated. The Hough accumulator for lines is built with the persistent edge points. A scheme for extracting straight curbs (as curb segments) and curved curbs (as chains of curb segments) is proposed. Each curb segment is refined using a RANSAC approach to fit optimally the 3D data of the curb. The algorithm was evaluated in an urban scenario. It works in real-time and provides robust detection of curbs."
      }
    ]
  },
  {
    "tid":"589",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Service-Based Semantic Search in P2P Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recently, Semantic Web Services have been increasingly provided to search, access and manipulate information made available from autonomous and heterogeneous systems interacting in a P2P environment. Platform independency is obtained and the possibility of integrating different information sources over the Web is augmented. Service providers can act as peers on the P2P network being able to supply services and to reply to incoming service requests. Highly dynamicity and absence of a common resource conceptualization characterize the environment. As a consequence, a semantic infrastructure to organize on-the-fly distributed services and to support their automatic location is recommended. In this paper we present basic concepts and experimental evaluation of a reference architecture, SERVANT, apt to support service-based semantic search, by means of a distributed service registry."
      }
    ]
  },
  {
    "tid":"590",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Generic interface architecture supporting cognitive resource management in future wireless networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The proliferation of different wireless technologies has led to fragmentation of interfaces and APIs that are used to control and manage air interfaces and wireless links. The emergence of new networking and application concepts that support increasingly heterogeneous access methods, especially when combined with future cognitive radio technologies, significantly increase the already high complexi..."
      }
    ]
  },
  {
    "tid":"591",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Joint Scheduling and Fast Cell Selection in OFDMA Wireless Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In modern broadband cellular networks, the omnidirectional antenna at each cell is replaced by three or six directional antennas, one in every sector. While every sector can run its own scheduling algorithm, bandwidth utilization can be significantly increased if a joint scheduler makes these decisions for all the sectors. This gives rise to a new problem, referred to as \u201cjoint scheduling,\u201d addressed in this paper for the first time. The problem is proven to be NP-hard, but we propose efficient algorithms with a worst-case performance guarantee for solving it. We then show that the proposed algorithms indeed substantially increase the network throughput."
      }
    ]
  },
  {
    "tid":"592",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Bistatic Pol-InSAR Scenario and Evaluation by Forest Scattering Simulations' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper comes within the context of exploring the potential of bistatic Pol-InSAR. Numerical simulations are carried out by means of MIPERS, multistatic interferometric polarimetric electromagnetic model for remote sensing which has already proved many abilities. The latter involves a electro-magnetic coherent approach of a discrete natural media description and enables us to model Pol-InSAR observables over forests in the bistatic configuration. Taking into account bistatic SAR constrains in terms of resolution, critical base-line, SNR, range and height ambiguities, new configurations for Pol-InSAR applications are envisioned."
      }
    ]
  },
  {
    "tid":"593",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mutual Community Detection across Multiple Partially Aligned Social Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"  Community detection in online social networks has been a hot research topic in recent years. Meanwhile, to enjoy more social network services, users nowadays are usually involved in multiple online social networks simultaneously, some of which can share common information and structures. Networks that involve some common users are named as multiple \"partially aligned networks\". In this paper, we want to detect communities of multiple partially aligned networks simultaneously, which is formally defined as the \"Mutual Clustering\" problem. The \"Mutual Clustering\" problem is very challenging as it has two important issues to address: (1) how to preserve the network characteristics in mutual community detection? and (2) how to utilize the information in other aligned networks to refine and disambiguate the community structures of the shared users? To solve these two challenges, a novel community detection method, MCD (Mutual Community Detector), is proposed in this paper. MCD can detect social community structures of users in multiple partially aligned networks at the same time with full considerations of (1) characteristics of each network, and (2) information of the shared users across aligned networks. Extensive experiments conducted on two real-world partially aligned heterogeneous social networks demonstrate that MCD can solve the \"Mutual Clustering\" problem very well. "
      }
    ]
  },
  {
    "tid":"594",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Multivariate Classification Algorithm for Malicious Node Detection in Large-Scale WSNs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"WSN is a distributed network exposed to an open environment, which is vulnerable to malicious nodes. To find out malicious nodes among a WSN with mass sensor nodes, this paper presents a malicious detection method based on multi-variate classification. Given the types of a few sensor nodes, it extracts sensor nodes' preferences related with the known types of malicious node, establishes the sample space of all sensor nodes that participate in network activities. Then, according to the study on the type-known sensor nodes' samples based on the multivariate classification algorithm, a classifier is generated, and all of the unknown-type sensor nodes are classified. The experiment results show that as long as the value of sensor nodes preferences and the number of active sensor nodes is stable, the false detection rate is stabilized under 0.5%."
      }
    ]
  },
  {
    "tid":"595",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Bottleneck-first scheduling for real-time traffic in IEEE 802.11 infrastructure-based mesh networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper studies the real-time traffic scheduling in IEEE 802.11 infrastructure-based wireless mesh networks. Providing strict latency guarantee for real-time traffic in a wireless mesh network is difficult, and one of the main challenges is the difficulty in coordinating temporal operations of the mesh access points (APs). In this paper we propose a bottleneck-first scheduling scheme (BFS) for voice traffic. In the proposed scheme, a central station is responsible for making scheduling decisions for all the real-time packet transmissions at the APs. Scheduling decisions at the APs with a higher traffic load are done before those with a lower traffic load. At each AP, voice packets with more end-to-end hops are scheduled first. Numerical results show that the proposed scheduling scheme achieves low transmission delay and high capacity in the mesh networks, compared to the simple first-come-first-serve scheduling scheme."
      }
    ]
  },
  {
    "tid":"596",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Design and Implementation of Secure Prefetch Mechanism for Multi-step Name Resolution in DNS' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"According to the Domain Name Industry Brief 2015 by VeriSign, the amount of domain names has reached to 296 million across all top-level domains (TLDs) with increase of 5.9 percent year over year. Considering each domain name may have at least one resource record requiring multi-step name resolution such as MX (Mail eXchange), CNAME (Canonical Name) and SRV (Service), etc. the DNS traffic and name resolution delay is in increase trend. In this paper, we propose a secure prefetch mechanism for multi-step name resolution in DNS to speedup the Internet services that use those records and reduce DNS traffic. We implemented a prototype system by customizing conventional BIND (Berkeley Internet Name Domain) on Linux system and evaluated the proposed mechanism using domain name measurement tool. The results confirmed that the prototype system worked properly as we designed. Moreover, we compared the latency of multi-step name resolution in the prototype system with that in the conventional BIND and confirmed that not only the number of query times but also approximately 30% of latency can be reduced in our proposed mechanism."
      }
    ]
  },
  {
    "tid":"597",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Atlas-Based Segmentation Propagation Framework Using Locally Affine Registration --- Application to Automatic Whole Heart Segmentation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we present a novel registration algorithm for locally affine registrations. This method preserves the anatomical and intensity class relationships between the local regions. A regularisation procedure is used to maintain a global diffeomorphic transformation. Combined with a novel generic method for accurately inverting the final deformation field, we include our techniques within an atlas-based segmentation propagation framework. We applied our method to automatically segment the whole heart from cardiac magnetic resonance images from a cohort of 18 volunteers (acquisition resolution 2 \u00d7 2 \u00d7 2 mm). The results show that the proposed method provides a robust initialisation for the atlas-based segmentation propagation framework refined with a fluid registration. We validated our approach against other registration strategies, and demonstrated that we improved the accuracy of the whole heart segmentations (1.8 \u00b1 0.42 mm)."
      }
    ]
  },
  {
    "tid":"598",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimal transition probability of reversible data hiding for general distortion metrics and its applications.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recently, a recursive code construction (RCC) approaching the rate-distortion bound of reversible data hiding (RDH) was proposed. However, to estimate the rate-distortion bound or execute RCC, one should first estimate the optimal transition probability matrix (OTPM). By previous methods, OTPM can be effectively estimated only for some specific distortion metrics, such as square error distortion or L1-Norm. In this paper, we proposed a unified framework of estimating the OTPM for general distortion metrics, with which we can calculate the rate-distortion bound of RDH for general cases and extend RCC to improve state-of-the-art RDH schemes based on any distortion metrics."
      }
    ]
  },
  {
    "tid":"599",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Power-performance trade-offs in nanometer-scale multi-level caches considering total leakage' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we investigate the impact of T_{ox} and Vth on power performance trade-offs for on-chip caches.We start by examining the optimization of the various components of a single level cache and then extend this to two level cache systems.In addition to leakage, our studies also account for the dynamic power expanded as a result of cache misses.Our results show that one can often reduce overall power by increasing the size of the L2 cache if we only allow one pair of Vth/T_{ox} in L2.However, if we allow the memory cells and the peripherals to have their own Vth's and T_{ox}'s, we show that a two-level cache system with smaller L2's will yield less total leakage.We further show that two Vth's and two T_{ox}'s are sufficient to get close to an optimal solution, and that Vth is generally a better design knob than T_{ox} for leakage optimization, thus it is better to restrict the number of T_{ox}'s rather than Vth's if cost is a concern."
      }
    ]
  },
  {
    "tid":"600",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An embedded stereo speech and audio coding method based on principal component analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper a compressive sampling method of MLT coefficients which is used for extracting stereo information is adopted based on principal component analysis (PCA) and Modulated Lapped Transform (MLT). With this method, an embedded variable bit-rates stereo speech and audio coding algorithm is proposed in this paper. In this codec, the stereo signal sampled at 32 kHz and 16 kHz can be coded in terms of scalable bit rates, the structure of bit-stream is embedded and the bit-stream can be divided into several layers. The core codec is ITU-T G.729.1 which can process mono signal with 7 kHz bandwidth. Besides there are four extra bit-rates added include 40, 48, 56, and 64kb/s. The maximum bit-rates of wideband stereo signal and super-wideband stereo signal are 48kb/s and 64kb/s, respectively. The objective and subjective test results show that the quality of the proposed codec is no worse than the reference codec which is requested by ITU-T."
      }
    ]
  },
  {
    "tid":"601",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An uplink CDMA system architecture with diverse QoS guarantees for heterogeneous traffic' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The uplink access control problems for cellular code-division multiple-access (CDMA) systems that service heterogeneous traffic with various types of quality-of-service (QoS) and use multicode CDMA to support variable bit rates are addressed. Considering its distinct QoS requirements, class-I real-time traffic (e.g., voice and video) is differentiated from class-II non-real-time traffic (e.g., data). Connection-oriented transmission is achieved by assigning mobile-oriented code channels for class-I traffic, where each corresponding mobile needs to pass an admission test. Class-II traffic is transmitted in a best-effort manner through a transmission-rate request access scheme which utilizes the bandwidth left unused by class-I traffic. Whenever a mobile has class-II messages to transmit, the mobile requests code channels via a base station-oriented transmission-request code channel, then, according to the base station scheduling, the transmission is scheduled and permitted. Addressed are the admission test for class-I connections, transmission power allocation, and how to maximize the aggregate throughput for class-II traffic. The admission region of voice and video connections and the optimum target signal-to-interference ratio of class-II traffic are derived numerically. The performance of class-II traffic transmissions in terms of average delay is also evaluated and discussed"
      }
    ]
  },
  {
    "tid":"602",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'MicroRazerS: rapid alignment of small RNA reads.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Motivation: Deep sequencing has become the method of choice for determining the small RNA content of a cell. Mapping the sequenced reads onto their reference genome serves as the basis for all further analyses, namely for identification and quantification. A method frequently used is Mega BLAST followed by several filtering steps, even though it is slow and inefficient for this task. Also, none of the currently available short read aligners has established itself for the particular task of small RNA mapping. Results: We present MicroRazerS, a tool optimized for mapping small RNAs onto a reference genome. It is an order of magnitude faster than Mega BLAST and comparable in speed with other short read mapping tools. In addition, it is more sensitive and easy to handle and adjust."
      }
    ]
  },
  {
    "tid":"603",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Moments-Correlating DPA.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We generalize correlation-enhanced power analysis collision attacks into moments-correlating DPA. The resulting distinguisher is applicable to the profiled and non-profiled (collision) settings and is able to exploit information lying in any statistical moment. It also benefits from a simple rule-of-thumb to estimate its data complexity. Experimental results show that such a tool allows answering with confidence to some important questions regarding the design of side-channel countermeasures (e.g. what is the most informative statistical moment in the leakages of a threshold implementation). We further argue that moments-correlating DPA is a natural candidate for leakage detection tests, enjoying the simplicity of correlation power analysis and advanced features for the evaluation of higher-order attacks with an easy-to-compute confidence level."
      }
    ]
  },
  {
    "tid":"604",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Feedback-Based Control for Providing Real-Time Services With the 802.11e MAC' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The 802.11e working group has recently proposed the hybrid coordination function (HCF) to provide service differentiation for supporting real-time transmissions over 802.11 WLANs. The HCF is made of a contention-based channel access, known as enhanced distributed coordination access, and of a HCF controlled channel access (HCCA), which requires a Hybrid Coordinator for bandwidth allocation to nodes hosting applications with QoS requirements. The 802.11e proposal includes a simple scheduler providing a Constant Bit Rate service, which is not well suited for bursty media flows. This paper proposes two feedback-based bandwidth allocation algorithms to be used within the HCCA, which have been referred to as feedback based dynamic scheduler (FBDS) and proportional-integral (PI)-FBDS. These algorithms have been designed with the objective of providing services with bounded delays. Given that the 802.11e standard allows queue lengths to be fed back, a control theoretic approach has been employed to design the FBDS, which exploits a simple proportional controller, and the PI-FBDS, which implements a proportional-integral controller. Proposed algorithms can be easily implemented since their computational complexities scale linearly with the number of traffic streams. Moreover, a call admission control scheme has been proposed as an extension of the one described in the 802.11e draft. Performance of the proposed algorithms have been theoretically analyzed and computer simulations, using the ns-2 simulator, have been carried out to compare their behaviors in realistic scenarios where video, voice, and FTP flows, coexist at various network loads. Simulation results have shown that, unlike the simple scheduler of the 802.11e draft, both FBDS and PI-FBDS are able to provide services with real-time constraints. However, while the FBDS admits a smaller quota of traffic streams than the simple scheduler, PI-FBDS allows the same quota of traffic that would be admitted using the simpl- - e scheduler, but still providing delay bound guarantees"
      }
    ]
  },
  {
    "tid":"605",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Perfect sorting by reversals' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In computational biology, gene order data is often modelled as signed permutations. A classical problem in genome comparison is to detect conserved segments in a permutation, that is, genes that are co-localised in several species, indicating that they remained grouped during evolution. A second largely studied problem related to gene order data is to compute a minimum scenario of reversals that transforms a signed permutation into another. Several studies began to mix the two problems, and it was observed that their results are not always compatible: often parsimonious scenarios of reversals break conserved segments. In a recent study, B\u00e9rard, Bergeron and Chauve stated as an open question whether it was possible to design a polynomial time algorithm to decide if there exists a minimum scenario of reversals that transforms a genome into another while keeping the clusters of co-localised genes together. In this paper, we give this polynomial algorithm, and thus generalise the theoretical result of the aforementioned paper."
      }
    ]
  },
  {
    "tid":"606",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Stochastic local search for multiprocessor scheduling for minimum total tardiness' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The multi-processor total tardiness problem (MPTTP)is an NP-hard scheduling problem, in which the the goal is to minimise the tardness of a set of jobs that are processed on a number of processors. Exact algorithms like branch and bound have proven to be impractical for the MPTTP, leaving stochastic local search (SLS) algorithms as the main alternative to find high-quality schedules. Among the available SLS techniques, iterated local search (ILS) has been shown to be an effective algorithm for the single processor case. Therefore, here we extend this technique to the multi-processor case, but our computational results indicate that ILS performance is not fully satisfying. To enhance ILS performance, we consider the use of population-based ILS extensions. Our final experimental results show that the usage of a population of search trajectories yields a more robust algorithm capable of finding best known solutions to difficult instances more reliably and in less computation time than a single ILS trajectory."
      }
    ]
  },
  {
    "tid":"607",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Energy-efficient resource allocation in cooperative wireless networks using Nash bargaining solution' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We consider the problem of energy-efficient time slot allocation between two rational nodes under the quality of services (QoS) constraints of each node in cooperative communication networks. First, we consider a cooperative communication system model and propose an energy consumption objective function. In our system, each node can act as a source as well as a potential relay, and both nodes try to complete the transmission of a frame in an energy-efficient way. Then we prove that the optimization problem is a two-person bargaining problem and we use Nash bargaining solution game (NBSG) to achieve an efficient and fair solution. Simulation results indicate that the Nash bargaining solution (NBS) is efficient, because both nodes can save energy in a cooperative way until the channel state becomes extremely worse. Furthermore, the duration of win-win in NBS scheme is the longest among all the proposed schemes and the energy saved by each node is approximately equal, which reflects NBS fairness."
      }
    ]
  },
  {
    "tid":"608",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Integration of Coordination Mechanisms in the BITE Multi-Robot Architecture' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"task-allocation points during task execution. At such points, teamwork behaviors (known as interaction behaviors) are triggered to automate the coordination processes. BITE's micro-kernel approach differs from existing archi- tectures, that are monolithic, in the sense that their coor- dination mechanisms are essentially hard-wired into them, and form part of the architectures. Earlier work has pro- vided initial evidence that e xibility in selecting coordination mechanisms (depending on the context) can lead to improved performance, even when the task-related skills themselves do not change (3), (10). Thus we argue that BITE's approach (which allows such e xibility) is preferable. This paper explores this argument in depth. BITE has been implemented for use with the Sony AIBO robots, and for robots supporting the player-stage API (6). We provide an in-depth discussion of BITE's structure and control mechanism, and detailed results from a number of experiments using BITE with Sony AIBO robots. These experiments demonstrate key capabilities in BITE. We addi- tionally explicitly note BITE's weaknesses, in the hope that these clarify important open issues. This paper is organized as follows. Section II discusses related work. Section III discusses the structure of BITE, and how it facilitates integration coordination mechanisms. Section IV presents qualitative and quantitative evaluation of BITE's approach. Section V concludes."
      }
    ]
  },
  {
    "tid":"609",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Approach to Data Reduction for Learning from Big Datasets: Integrating Stacking, Rotation, and Agent Population Learning Techniques.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In the paper, several data reduction techniques for machine learning from big datasets are discussed and evaluated. The discussed approach focuses on combining several techniques including stacking, rotation, and data reduction aimed at improving the performance of the machine classification. Stacking is seen as the technique allowing to take advantage of the multiple classification models. The rotation-based techniques are used to increase the heterogeneity of the stacking ensembles. Data reduction makes it possible to classify instances belonging to big datasets. We propose to use an agent-based population learning algorithm for data reduction in the feature and instance dimensions. For diversification of the classifier ensembles within the rotation also, alternatively, principal component analysis and independent component analysis are used. The research question addressed in the paper is formulated as follows: does the performance of a classifier using the reduced dataset be improved by integrating the data reduction mechanism with the rotation-based technique and the stacking?"
      }
    ]
  },
  {
    "tid":"610",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Long-Term Person Re-identification Using True Motion from Videos' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Most person re-identification approaches and benchmarks assume that pedestrians go across the surveillance network without significant appearance changes in a brief period, which explicitly restricts person re-identification to a short-term event and incurs inter-sample similarity measurement by appearance matching. However, pedestrians are likely to reappear in the surveillance network after a long-time interval (long-term) and change their wearing in many real-world scenarios. These scenarios inevitably cause appearances between subjects more ambiguous and indistinguishable. In this paper we consider these scenarios and propose a unified feature representation based on true motion cues from videos named FIne moTion encoDing (FITD). Our hypothesis is that people keep constant motion patterns under non-distraction walking condition. Therefore, the motion characteristics are more reliable than static appearance feature to describe a walking person. Particularly, we extract motion patterns hierarchically by encoding trajectory-aligned descriptors with Fisher vectors in a spatial-aligned pyramid. To verify benefits of the proposed FITD, we collect a new dataset typically for the long-term situations. Extensive experiments demonstrate the merits of our FITD especially for the long-term scenarios."
      }
    ]
  },
  {
    "tid":"611",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Exception tracking in an open world.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A refinement is a predicate on the elements of a type that describes their execution behavior. Much work has gone into developing refinements in a closed world, in which the classes of values of a type are fixed statically, as in the case of the natural numbers with zero and succ. Relatively little work has gone into developing refinements in an open world in which new classes may be added dynamically. Here we examine the problem of exception tracking, a perennially problematic typing concept for programming languages, from the point of view of refinements in an open world. Exceptions are decomposed into separate control and data mechanisms, the latter motivating the need for open-world refinements. Exception tracking is thereby repositioned as a matter of program verification, rather than structural typing, integrating behavioral typing with theorem proving even in an open world. Some further applications of dynamic classification and open-world refinements are suggested."
      }
    ]
  },
  {
    "tid":"612",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fuzzy Set Representation of Kansei Texture and its Visualization for Online Shopping.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A fuzzy set representation method of Kansei Texture is proposed to express individual difference of Kansei Texture feelings for the purpose of online shopping. The method provides buyers with criteria whether a request to send samples is necessary according to the variance degree of individual differences, and it also offers sellers with information regarding the possibility of returned goods in case of significant individual differences with regard to expensive prices. The correlation coefficient of the degree of individual difference and sample demand is 0.78 (P<0.05, t-test), i.e., a directly proportional relationship is observed between the two degrees. There is a tendency for expensive goods, e.g., those with price greater than $50, to be returned in the case of a large individual difference degree, i.e., the individual difference degree of Kansei Texture with price information provides a useful strategy for estimating the possibility of returned goods. Moreover, the relationship between stress and individual difference is also shown. Further validity verification is planned in order to realize practical applications in the real market."
      }
    ]
  },
  {
    "tid":"613",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Distributed Localization Strategies for Sensor Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We consider the problem of localizing random sensor networks by means of time of arrival capabilities. Interactions among sensors are modeled by a mass-spring system. Masses (sensors) initially have a random estimation of their position. The knowledge of their distance from other masses is translated into connect masses with springs whose length should reach the estimated distances. This determines a set of forces to which masses are subject to. Starting from this configuration we propose several strategies reducing the overall time needed to reach the desired level of equilibrium."
      }
    ]
  },
  {
    "tid":"614",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Decayed DivRank: capturing relevance, diversity and prestige in information networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many network-based ranking approaches have been proposed to rank objects according to different criteria, including relevance, prestige and diversity. However, existing approaches either only aim at one or two of the criteria, or handle them with additional heuristics in multiple steps. Inspired by DivRank, we propose a unified ranking model, Decayed DivRank (DDRank), to meet the three criteria simultaneously. Empirical experiments on paper citation network show that DDRank can outperform existing algorithms in capturing relevance, diversity and prestige simultaneously in ranking."
      }
    ]
  },
  {
    "tid":"615",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A distributed data system for interdisciplinary ecosystem services modeling' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Ecosystem services are diverse, and ecosystem modeling requires data from various sources and outputs from other models. To facilitate the data management sharing and accessing in the interdisciplinary ecosystem modeling, we brought forward a distributed architecture, and address the interoperability problems with OGC standards. The paper introduces the database and metadata schema we designed to support data management and processing inside dataset. We developed two Web systems to facilitate both modeling systems and modelers on data discovering and accessing."
      }
    ]
  },
  {
    "tid":"616",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Effects of discrete objective functions with different granularities on the search behavior of EMO algorithms' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Objective functions in combinatorial optimization are discrete. The number of possible values of a discrete objective function is totally different from problem to problem. Optimization of a discrete objective function is often very difficult. In the case of multiobjective optimization, a different objective function has a different number of possible values. This means that each axis of the objective space has a different granularity. Some axes may have fine granularities while others are coarse. In this paper, we examine the effect of discrete objective functions with different granularities on the search behavior of EMO (evolutionary multiobjective optimization) algorithms through computational experiments. Experimental results show that a discrete objective function with a coarse granularity slows down the search of EMO algorithms along that objective. An interesting observation is that such a slow-down along one objective often leads to the speed-up of the search along other objectives. We also examine the effect of adding a small random noise to each discrete objective function in order to increase the number of possible objective values."
      }
    ]
  },
  {
    "tid":"617",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Saft: Reliable Transport In Mobile Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"End-to-end transport protocols perform poorly in mobile environments, primarily due to the frequent route breaks induced by node mobility. We present a framework for hop-by-hop transport protocols and propose a new reliable transport protocol: SaFT (Store-and-Forward Transport). We evaluate its performance in mobile ad hoc networks vis-a-vis TCP. Overall, SaFT achieves up to 3 times faster delivery of messages in such networks. We conclude that store-and-forward protocols are suited well to provide reliable communication in mobile ad hoc networks."
      }
    ]
  },
  {
    "tid":"618",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'From variable weighting to cluster characterization in topographic unsupervised learning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We introduce a new learning approach, which provides simultaneously Self-Organizing Map (SOM) and local weight vector for each cluster. The proposed approach is computationally simple, and learns a different features vector weights for each cell (relevance vector). Based on the Self-Organizing Map approach, we present two new simultaneously clustering and weighting algorithms: local weighting observation lwo-SOM and local weighting distance lwd-SOM. Both algorithms achieve the same goal by minimizing different cost functions. After learning phase, a selection method with weight vectors is used to prune the irrelevant variables and thus we can characterize the clusters. We illustrate the performance of the proposed approach using different data sets. A number of synthetic and real data are experimented on to show the benefits of the proposed local weighting using self-organizing models."
      }
    ]
  },
  {
    "tid":"619",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Innovative device source/drain and channel implantation for MOS transistors in ultra low power subthreshold circuit applications' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper introduces a new comprehensive doping scheme for the transistors in subthreshold circuits. The proposed doping is characterized by the absence of halos at the source and drain ends. We propose a Gaussian doping distribution inside the source and drain regions, and a low-high-low distribution across the depth of the transistor from the channel surface towards the body region. It also has a low-high-low doping distribution along the length of the transistor below the channel region. The analysis is performed by changing the doping densities, body bias and gate-source voltage (V\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">gs</sub>\n) to observe the variations of off-current (I\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">off</sub>\n), threshold voltage and I\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">on</sub>\n/I\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">off</sub>\n ratio. Results show that the optimized device with the proposed doping profile would provide higher drain current (I\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">on</sub>\n) and I\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">on</sub>\n/I\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">off</sub>\n at a certain body bias."
      }
    ]
  },
  {
    "tid":"620",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Prediction Interval Estimation For Wind Farm Power Generation Forecasts Using Support Vector Machines' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Accurate forecasting of wind power generation is quite an important as well as challenging task for the system operators and market participants due to its high uncertainty. It is essential to quantify uncertainties associated with wind power generation forecasts for their efficient application in optimal management of wind farms and integration into power systems. Prediction intervals (PIs) are well known statistical tools which are used to quantify the uncertainty related to forecasts by estimating the ranges of the future target variables. This paper investigates the application of a novel support vector machine based methodology to directly estimate the lower and upper bounds of the PIs without expensive computational burden and inaccurate assumptions about the distribution of the data. The efficiency of the method for uncertainty quantification is examined using monthly data from a wind farm in Australia. PIs for short term application are generated with a confidence level of 90%. Experimental results confirm the ability of the method in constructing reliable PIs without resorting to complex computational methods."
      }
    ]
  },
  {
    "tid":"621",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A hierarchical dense deformable model for 3D face reconstruction from skull' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"3D face reconstruction from skull has been investigated deeply by computer scientists in the past two decades because it is important for identification. The dominant methods construct 3D face from the soft tissue thickness measured at a set of landmarks on skull. The quantity and position of the landmarks are very vital for 3D face reconstruction, but there is no uniform standard for the selection of the landmarks. Additionally, the acquirement of the landmarks on skull is difficult without manual assistance. In this paper, an automatic 3D face reconstruction method based on a hierarchical dense deformable model is proposed. To construct the model, the skull and face samples are acquired by CT scanner and represented as dense triangle mesh. Then a non-rigid dense mesh registration algorithm is presented to align all the samples in point-to-point correspondence. Based on the aligned samples, a global deformable model is constructed, and three local models are constructed from the segmented patches of the eye, nose and mouth. For a given skull, the globe and local deformable models are iteratively matched with it, and the reconstructed facial surface is obtained by fusing the globe and local reconstruction results. To validate the presented method, a measurement in the coefficient domain of a face deformable model is defined. The experimental results indicate that the proposed method has good performance for 3D face reconstruction from skull."
      }
    ]
  },
  {
    "tid":"622",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Distributed point coordination function for IEEE 802.11 wireless ad hoc networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The Distributed Point Coordination Function (DPCF) is presented in this paper as a novel Medium Access Control Protocol (MAC) for wireless ad hoc networks. DPCF extends the operation of the Point Coordination Function (PCF) defined in the IEEE 802.11 Standard to operate over wireless networks without infrastructure. In PCF, a central point coordinator polls the users to get access to the channel and data collisions are completely avoided, thus yielding high performance. In order to extend its high performance to networks without infrastructure, the DPCF is proposed in this paper as a combination of the Distributed Coordination Function (DCF) and the PCF. The general idea is to combine a dynamic, temporary, and spontaneous clustering mechanism based on DCF with the execution of PCF within each cluster. The backwards compatibility of DPCF with legacy 802.11 networks is also assessed in this paper. Comprehensive computer-based simulations demonstrate the high performance of this new protocol in both single-hop and multi-hop networks."
      }
    ]
  },
  {
    "tid":"623",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On The Design Of Low Power 1-Bit Full Adder Cell' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A 1-bit full adder cell based on majority function is designed and simulated. In this design the time consuming XOR gates are eliminated. Low-power consumption is targeted in implementation of our design. The circuit being studied is optimized for energy efficiency at 0.18-mu m CMOS process technology. The new circuit has been compared to the previous work based on power consumption, speed and power delay product (PDP). HSPICE and Cadence simulations show that the proposed adder can work more reliably at different range of supply voltage. The proposed design has the best PDP in comparison with the others."
      }
    ]
  },
  {
    "tid":"624",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Precise packet loss pattern generation by intentional interference' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Intermediate-quality links often cause vulnerable connectivity in wireless sensor networks, but packet losses caused by such volatile links are not easy to trace. In order to equip link layer protocol designers with a reliable test and debugging tool, we develop a reactive interferer to generate packet loss patterns precisely. By using intentional interference to emulate parameterized lossy links with very low intrusiveness, our tool facilitates both robustness evaluation of protocols and flaw detection in protocol implementation."
      }
    ]
  },
  {
    "tid":"625",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'sgnesR: An R package for simulating gene expression data from an underlying real gene network structure considering delay parameters.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"sgnesR (Stochastic Gene Network Expression Simulator in R) is an R package that provides an interface to simulate gene expression data from a given gene network using the stochastic simulation algorithm (SSA). The package allows various options for delay parameters and can easily included in reactions for promoter delay, RNA delay and Protein delay. A user can tune these parameters to model various types of reactions within a cell. As examples, we present two network models to generate expression profiles. We also demonstrated the inference of networks and the evaluation of association measure of edge and non-edge components from the generated expression profiles.The purpose of sgnesR is to enable an easy to use and a quick implementation for generating realistic gene expression data from biologically relevant networks that can be user selected.sgnesR is freely available for academic use. The R package has been tested for R 3.2.0 under Linux, Windows and Mac OS X."
      }
    ]
  },
  {
    "tid":"626",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Exploiting users\u2019 social relations to forward data in opportunistic networks: The HiBOp solution' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Opportunistic networks, in which nodes opportunistically exploit any pair-wise contact to identify next hops towards the destination, are one of the most interesting technologies to support the pervasive networking vision. Opportunistic networks allow content sharing between mobile users without requiring any pre-existing Internet infrastructure, and tolerate partitions, long disconnections, and topology instability in general. In this paper we propose a context-aware framework for routing and forwarding in opportunistic networks. The framework is general, and able to host various flavors of context-aware routing. In this work we also present a particular protocol, HiBOp, which, by exploiting the framework, learns and represents through context information, the users\u2019 behavior and their social relations, and uses this knowledge to drive the forwarding process. The comparison of HiBOp with reference to alternative solutions shows that a context-aware approach based on users\u2019 social relations turns out to be a very efficient solution for forwarding in opportunistic networks. We show performance improvements over the reference solutions both in terms of resource utilization and in terms of user perceived QoS."
      }
    ]
  },
  {
    "tid":"627",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Early Diagnosis of Alzheimer's Disease by Joint Feature Selection and Classification on Temporally Structured Support Vector Machine.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The diagnosis of Alzheimer's disease (AD) from neuroimaging data at the pre-clinical stage has been intensively investigated because of the immense social and economic cost. In the past decade, computational approaches on longitudinal image sequences have been actively investigated with special attention to Mild Cognitive Impairment (MCI), which is an intermediate stage between normal control (NC) and AD. However, current state-of-the-art diagnosis methods have limited power in clinical practice, due to the excessive requirements such as equal and immoderate number of scans in longitudinal imaging data. More critically, very few methods are specifically designed for the early alarm of AD uptake. To address these limitations, we propose a flexible spatial-temporal solution for early detection of AD by recognizing abnormal structure changes from longitudinal MR image sequence. Specifically, our method is leveraged by the non-reversible nature of AD progression. We employ temporally structured SVM to accurately alarm AD at early stage by enforcing the monotony on classification result to avoid unrealistic and inconsistent diagnosis result along time. Furthermore, in order to select best features which can well collaborate with the classifier, we present as joint feature selection and classification framework. The evaluation on more than 150 longitudinal subjects from ADNI dataset shows that our method is able to alarm the conversion of AD 12 months prior to the clinical diagnosis with at least 82.5 % accuracy. It is worth noting that our proposed method works on widely used MR images and does not have restriction on the number of scans in the longitudinal sequence, which is very attractive to real clinical practice."
      }
    ]
  },
  {
    "tid":"628",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Artificial Immune System for Heterogeneous Multiprocessor Scheduling with Task Duplication' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this study, we investigate the task scheduling problem in heterogeneous computing environments and propose a novel scheduling algorithm, called the Artificial Immune System with Duplication (AISD) algorithm that efficiently tackles the problem. The AISD algorithm incorporates the clonal selection principle in the immune system and task duplication into the scheduling process. Based on the performance results obtained from extensive experiments conducted with a comprehensive set of both randomly generated and well-known application task graphs and various system configurations, AISD consistently outperformed the two existing algorithms by a noticeable margin, especially when scheduling communication intensive task graphs."
      }
    ]
  },
  {
    "tid":"629",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A novel dynamic fusion method using localized generalization error model' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A new dynamic classifier fusion method named L-GEM Fusion Method (LFM) for Multiple Classifier Systems (MCSs) is proposed. The localized generalization error upper bound for the neighborhood of a testing sample is calculated and used to estimate the local competence of base classifiers in MCSs. Different from the recent dynamic classifier selection methods, the proposed method consider not only the training error but also the sensitivity of the base classifier. Experimental results show that the MCSs using LFM has more accurate than other popular dynamic fusion methods."
      }
    ]
  },
  {
    "tid":"630",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Event-based control as an enabler for high power density processors' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper is part of a long-term research on the application of event-based control to the thermal management of high-power, high-density microprocessors. Specifically, in this work we introduce and discuss a purpose-specific event-based realisation of a digital controller for thermal management, which integrates into a scheme that also takes care of the power/performance tradeoff, and carry out a stability and a preliminary performance analysis. We also present, extending previous research, a comprehensive Modelica library suitable to carry out control studies in the addressed domain. We finally show experimental results on a modern processor architecture, that also compare our solution to the state of the art, to demonstrate the effectiveness of the proposal."
      }
    ]
  },
  {
    "tid":"631",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Power Allocation and Beamforming in Overlay Cognitive Radio Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We consider the overlay cognitive radio channel where the cognitive user is admitted to transmit simultaneously with the primary user provided that the instantaneous rate of primary link is not degraded. Assuming causal knowledge of the primary user's message at the cognitive transmitter (CT), we analyze the transmission rates of the cognitive user in the single-input multiple-output (SIMO) and the multiple-input single-output (MISO) configurations. In particular, the CT uses a part of its transmit power to assist the primary user in delivering the primary user's message and the other part of its power is used to deliver its own message. The transmit power and the beamformers are designed to maximize the rate of the cognitive user while fixing the the rate of the primary user. Our simulation results show that the proposed scheme yields improved system performance compared to the interweave scheme where any interference to the primary user is prohibited."
      }
    ]
  },
  {
    "tid":"632",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An edge-based approach for robust foreground detection' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Foreground segmentation is an essential task in many image processing applications and a commonly used approach to obtain foreground objects from the background. Many techniques exist, but due to shadows and changes in illumination the segmentation of foreground objects from the background remains challenging. In this paper, we present a powerful framework for detections of moving objects in realtime video processing applications under various lighting changes. The novel approach is based on a combination of edge detection and recursive smoothing techniques. We use edge dependencies as statistical features of foreground and background regions and define the foreground as regions containing moving edges. The background is described by short- and long-term estimates. Experiments prove the robustness of our method in the presence of lighting changes in sequences compared to other widely used background subtraction techniques."
      }
    ]
  },
  {
    "tid":"633",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A hybrid reality environment and its application to the study of earthquake engineering' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Visualization can provide the much needed computer-assisted design and analysis environment to foster problem-based learning, while Virtual Reality (VR) can provide the environment for hands-on manipulation, stimulating interactive learning in engineering and the sciences. In this paper, an interactive 2D and 3D (hybrid) environment is described, which facilitates collaborative learning and research and utilizes techniques in visualization and VR, therefore enhancing the interpretation of physical problems within these fields. The environment described, termed VizClass, incorporates a specially designed lecture room and laboratory integrating both 2-D and 3-D spatial activities by coupling a series of interactive projection display boards (touch-sensitive whiteboards) and a semi-immersive 3D wall display. The environment is particularly appealing for studying critical, complex engineering problems, for example, where time-varying feature modifications and coupling between multiple modes of movement are occurring. This paper describes the hardware architecture designed for this new hybrid environment as well as an initial application within the environment to the study of a real case history building subjected to a variety of earthquakes. The example simulation uses field measured seismic data sources, and illustrations of simple visual paradigms to provide an enhanced understanding of the physical model, the damage accumulated by the model, and the association between the measured and observed data. A detailed evaluation survey was also conducted to determine the merits of the presented environment and the techniques implemented. Results substantiate the plausibility of using these techniques for more general, everyday users. Over 70% of the survey participants believed that the techniques implemented were valuable for engineers."
      }
    ]
  },
  {
    "tid":"634",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Defragging Subgraph Features for Graph Classification' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Graph classification is an important tool for analysing structured and semi-structured data, where subgraphs are commonly used as the feature representation. However, the number and size of subgraph features crucially depend on the threshold parameters of frequent subgraph mining algorithms. Any improper setting of the parameters will generate many trivial short-pattern subgraph fragments which dominate the feature space, distort graph classifiers and bury interesting long-pattern subgraphs. In this paper, we propose a new Subgraph Join Feature Selection (SJFS) algorithm. The SJFS algorithm, by forcing graph classifiers to join short-pattern subgraph fragments, can defrag trivial subgraph features and deliver long-pattern interesting subgraphs. Experimental results on both synthetic and real-world social network graph data demonstrate the performance of the proposed method."
      }
    ]
  },
  {
    "tid":"635",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Assessing System Availability Using an Enterprise Architecture Analysis Approach' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"During the last decade, a model based technique known as enterprise architecture has grown into an established approach for management of information systems in organizations. The use of enterprise architecture primarily promotes good decision-making and communication between business and IT stakeholders. This paper visualizes a scenario where enterprise architecture models are utilized to evaluate the availability of an information system. The problem is approached by creating models based on metamodels tailored to the domain of enterprise architecture analysis. As the instantiated models are fed with data particular to the information system, one can deduce how the organization needs to act in order to improve the system's availability."
      }
    ]
  },
  {
    "tid":"636",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimizing mix-zone coverage in pervasive wireless networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Location privacy is a major concern in pervasive networks where static device identifiers enable malicious eavesdroppers to continuously track users and their movements. In order to prevent such identifier-based tracking, devices could coordinate regular identifier change operations in special areas called mix-zones. Although mix-zones provide spatio-temporal de-correlation between old and new identifiers, depending on the position of the mix-zone, identifier changes can generate a substantial inconvenience or \u201ccost\u201d to the users in terms of lost communications and increased energy consumption. In this paper, we address this trade-off between privacy and cost by studying the problem of determining an optimal set of mix-zones such that the degree of mixing in the network is maximized and the overall network-wide mixing cost is minimized. We follow a graph-theoretic approach and model the optimal mixing problem as a generalization of the vertex cover problem, called the Mix Cover MC problem. We propose three approximation algorithms for the MC problem and derive a lower bound on the solution quality guaranteed by them. Additionally, we outline two other heuristics for solving the MC problem. These heuristics are simple, but do not provide any guarantees on the solution quality. By means of extensive empirical evaluation using real data, we compare the performance and solution quality of these algorithms. The combinatorics-based approach used in this work enables us to study the feasibility of determining optimal mix-zones regularly and under dynamic network conditions."
      }
    ]
  },
  {
    "tid":"637",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'World-stabilized annotations and virtual scene navigation for remote collaboration' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present a system that supports an augmented shared visual space for live mobile remote collaboration on physical tasks. The remote user can explore the scene independently of the local user's current camera position and can communicate via spatial annotations that are immediately visible to the local user in augmented reality. Our system operates on off-the-shelf hardware and uses real-time visual tracking and modeling, thus not requiring any preparation or instrumentation of the environment. It creates a synergy between video conferencing and remote scene exploration under a unique coherent interface. To evaluate the collaboration with our system, we conducted an extensive outdoor user study with 60 participants comparing our system with two baseline interfaces. Our results indicate an overwhelming user preference (80%) for our system, a high level of usability, as well as performance benefits compared with one of the two baselines."
      }
    ]
  },
  {
    "tid":"638",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Improving Adversarial Discriminative Domain Adaptation.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Adversarial discriminative domain adaptation (ADDA) is an efficient framework for unsupervised domain adaptation, where the source and target domains are assumed to have the same classes, but no labels are available for the target domain. While ADDA has already achieved better training efficiency and competitive accuracy in comparison to other adversarial based methods, we investigate whether we can improve performance by incorporating task knowledge into the adversarial loss functions. We achieve this by extending the discriminator output over the source classes and leverage on the distribution over the source encoder posteriors, which is fixed during adversarial training, in order to align a shared encoder distribution to the source domain. The shared encoder can receive a proportion of examples from both the source and target datasets, in order to smooth the learned distribution and improve its convergence properties during adversarial training. We additionally consider how the extended discriminator can be regularized in order to further improve performance, by treating the discriminator as a denoising autoencoder and corrupting its input. Our final design employs maximum mean discrepancy and reconstruction-based loss functions for adversarial training. We validate our framework on standard datasets like MNIST, USPS, SVHN, MNIST-M and Office-31. Our results on all datasets show that our proposal is both simple and efficient, as it competes or outperforms the state-of-the-art in unsupervised domain adaptation, whilst offering lower complexity than other recent adversarial methods such as DIFA and CoGAN."
      }
    ]
  },
  {
    "tid":"639",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Variable Selection By Rank-One Updates For Least Squares Support Vector Machines' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Least squares support vector machines (LS-SVM) classifiers are a class of simple, yet powerful, kernel methods whose solution follows from a set of linear equations. Here, forward and backward algorithms, based on this technique, are proposed for fast and efficient variable selection. By exploiting the structure of the LS-SVM solution a closed form expression for the leave-one-out (LOO) estimator, useful for selecting variables, is obtained. For inclusion or removal of a new variable, rank-one adjustments in the kernel matrix (linear kernel) allow for updating, rather than recomputing, the LS-SVM solution. The proposed approach is applied to microarray data for gene selection. Simulations clearly show lower computational complexity along with good stability on the generalization performance when compared to other related algorithms."
      }
    ]
  },
  {
    "tid":"640",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A GA-Based Simulation System for WMNs: Performance Analysis of WMN-GA System for Different WMN Architectures and Uniform Distribution Considering DCF and EDCA' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Wireless Mesh Networks (WMNs) are attracting a lot of attention from wireless network researchers. Node placement problems have been investigated for a long time in the optimization field due to numerous applications in location science. In this paper, we evaluate the performance of two WMN architectures considering throughput, delay, jitter and fairness index metrics. For simulations, we used ns-3 and Optimized Link State Routing (OLSR). We compare the performance for Distributed Coordination Function (DCF) and Enhanced Distributed Channel Access (EDCA) for uniform distribution of mesh clients by sending multiple Constant Bit Rate (CBR) flows in the network. The simulation results show that for I/B WMN, the throughput of EDCA is a little bit higher than Hybrid WMN. However, for Hybrid WMN, the throughput of DCF is higher than EDCA. The delay and jitter of both architectures are almost the same. However, in the case of DCF for 20 flows, the delay and jitter of I/B WMN is a lower compared with Hybrid WMN. The fairness index of 10 flows is higher than other flows for both WMN architectures."
      }
    ]
  },
  {
    "tid":"641",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Formal Specification of Gateways in Integrated Architectures' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Complex embedded computer systems can encompass multiple application subsystems, such as a multimedia, a powertrain, a comfort and a safety subsystem in the in-vehicle electronic system of a typical premium car. Information exchanges between these application subsystems are essential to realize composite services that involve more than one application subsystem and to reduce redundant computations and sensors. A major challenge is to resolve the property mismatches at the interfaces between application subsystems, such as incoherent naming, divergent syntax, or different communication protocols. Also, fault isolation capabilities are required to prevent common mode failures induced by the propagation of faults between application subsystems. The contribution of this paper is a formal specification of gateways that contain structured collections of time-sensitive variables associated with timing information (called real-time databases) in order to separate the application subsystems. The formal specification can serve as a basis for automatic code generation or formal verification."
      }
    ]
  },
  {
    "tid":"642",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Semi-automatic audio semantic concept discovery for multimedia retrieval' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Huge amount of videos on the Internet have rare textual information, which makes video retrieval challenging given a text query. Previous work explored semantic concepts for content analysis to assist retrieval. However, the human-defined concepts might fail to cover the data and there is a potential gap between these concepts and the semantics expected from user's query. Also, building a corpus is expensive and time-consuming. To address these issues, we propose a semi-automatic framework to discover the semantic concepts. We limit ourselves in audio modality here. In the paper, we also discuss how to select meaningful vocabulary from the discovered hierarchical sub-categories and provide an approach to detect all the concepts without further annotation. We evaluate the method on NIST 2011 multimedia event detection (MED) dataset."
      }
    ]
  },
  {
    "tid":"643",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Precise memory leak detection for java software using container profiling' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A memory leak in a Java program occurs when object references that are no longer needed are unnecessarily maintained. Such leaks are difficult to detect because static analysis typically cannot precisely identify these redundant references, and existing dynamic leak detection tools track and report fine-grained information about individual objects, producing results that are usually hard to interpret and lack precision. In this article we introduce a novel container-based heap-tracking technique, based on the fact that many memory leaks in Java programs occur due to incorrect uses of containers, leading to containers that keep references to unused data entries. The novelty of the described work is twofold: (1) instead of tracking arbitrary objects and finding leaks by analyzing references to unused objects, the technique tracks only containers and directly identifies the source of the leak, and (2) the technique computes a confidence value for each container based on a combination of its memory consumption and its elements' staleness (time since last retrieval), while previous approaches do not consider such combined metrics. Our experimental results show that the reports generated by the proposed technique can be very precise: for two bugs reported by Sun, a known bug in SPECjbb 2000, and an example bug from IBM developerWorks, the top containers in the reports include the containers that leak memory."
      }
    ]
  },
  {
    "tid":"644",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient and accurate haplotype inference by combining parsimony and pedigree information' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Existing genotyping technologies have enabled researchers to genotype hundreds of thousands of SNPs efficiently and inexpensively. Methods for the imputation of non-genotyped SNPs and the inference of haplotype information from genotypes, however, remain important, since they have the potential to increase the power of statistical association tests. In many cases, studies are conducted in sets of individuals where the pedigree information is relevant, and can be used to increase the power of tests and to decrease the impact of population structure on the obtained results. This paper proposes a new Boolean optimization model for haplotype inference combining two combinatorial approaches: the Minimum Recombinant Haplotyping Configuration (MRHC), which minimizes the number of recombinant events within a pedigree, and the Haplotype Inference by Pure Parsimony (HIPP), that aims at finding a solution with a minimum number of distinct haplotypes within a population. The paper also describes the use of well-known techniques, which yield significant performance gains. Concrete examples include symmetry breaking, identification of lower bounds, and the use of an appropriate constraint solver. Experimental results show that the new PedRPoly model is competitive both in terms of accuracy and efficiency."
      }
    ]
  },
  {
    "tid":"645",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Validating the Correctness of Outsourced Computational Tasks Using Pseudorandom Number Generators' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"As the complexity of software continues to grow, so does the criticality of executing the software correctly. In addition to these issues, the trend towards outsourcing computations to third-party clouds brings in an additional challenge of validating the results returned by the cloud. In this paper, we propose a computationally efficient novel mechanism for validating the execution of a cloud provider. In particular, we propose the use of pseudo-random generators with fast-leap property (FLAP) to validate the results of an execution. Our strategy is to embed random number generation subroutines using pseudorandom number generators with the FLAP property into the source code of a program that is to be outsourced for execution. During the execution of the program on the cloud, a unique pseudorandom number sequence is produced, which will serve as the computational digest. To the cloud provider, unless the subtask is correctly executed, the computational digest is difficult to obtain when a sophisticated embedding scheme is used. However, to the application owner, the computational digest can be economically validated. The pseudorandom number generator's FLAP property allows one to economically regenerate any number in the computational digest, which enables the efficient checking of the correctness of the computational digest. Validating the computational digest of each subtask provides an efficient way to detect accidental and/or deliberate faults in the program execution carried out on a potentially unreliable or untrustworthy cloud provider. This enables high-confidence computing on large-scale cloud computing infrastructure."
      }
    ]
  },
  {
    "tid":"646",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'MDiag: Mobility-assisted diagnosis for wireless sensor networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Though widely employed in various applications, wireless sensor networks (WSNs) are liable to failures, especially after deployment. Since the on-site failures are difficult to reproduce, it is of critical importance to perform in-situ diagnosis. Current in-situ diagnosis methods are either intrusive or inefficient, because they either inject diagnosis agents into each sensor node or build up another network for diagnosis purpose. To tackle these issues, we propose MDiag, a mobility-assisted diagnosis approach that employs smartphones to patrol the WSNs and diagnose failures. Diagnosing with a smartphone which is not a component of WSNs does not intrude the execution of the WSNs. Moreover, patrolling the smartphone in the WSNs to investigate failures is more efficient than deploying another diagnosis network. Statistical rules are designed to guide the detection of abnormal cases. Aiming at improving the patrol efficiency, a patrol approach MSEP (maximum snooping efficiency patrol) is proposed. MSEP is designed to achieve better performance than the naive method, the greedy method, and the baseline method in increasing the detection rate and reducing the patrol time of MDiag. Experiments with real sensor nodes and emulations validate the effectiveness of MDiag in detecting anomalous cases caused by faults."
      }
    ]
  },
  {
    "tid":"647",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Guest editors' introduction: the new world of large embedded memories' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Presents the guest editorial for this issue of the publication."
      }
    ]
  },
  {
    "tid":"648",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient computation of clipped Voronoi diagram for mesh generation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The Voronoi diagram is a fundamental geometric structure widely used in various fields, especially in computer graphics and geometry computing. For a set of points in a compact domain (i.e. a bounded and closed 2D region or a 3D volume), some Voronoi cells of their Voronoi diagram are infinite or partially outside of the domain, but in practice only the parts of the cells inside the domain are needed, as when computing the centroidal Voronoi tessellation. Such a Voronoi diagram confined to a compact domain is called a clipped Voronoi diagram. We present an efficient algorithm to compute the clipped Voronoi diagram for a set of sites with respect to a compact 2D region or a 3D volume. We also apply the proposed method to optimal mesh generation based on the centroidal Voronoi tessellation."
      }
    ]
  },
  {
    "tid":"649",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Post-silicon power mapping techniques for integrated circuits' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose a new methodology for post-silicon power validation using the captured thermal infrared emissions from the back-side of operational integrated circuits. We first identify the challenges associated with thermal to power inversion, and then we address these challenges by devising a quadratic optimization formulation that incorporates Tikhonov filtering techniques to find the most accurate power maps. To validate our methodology, a programmable circuit of micro-heaters is implemented to create a number of reference power maps. The thermal emissions from the circuit are captured using an infrared camera and then inverted to yield highly accurate post-silicon power maps."
      }
    ]
  },
  {
    "tid":"650",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Bounds on skyline probability for databases with uncertain preferences.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"For determining skyline objects for an uncertain database with uncertain preferences, it is necessary to compute the skyline probability of a given object with respect to other objects. The problem boils down to computing the probability of the union of events from the probabilities of all possible joint probabilities. Linear Bonferroni bound is concerned with computing the bounds on the probability of the union of events with partial information. We use this technique to estimate the skyline probability of an object and propose a polynomial-time algorithm for computing sharp upper bound. We show that the use of partial information does not affect the quality of solution but helps in improving the efficiency. We formulate the problem as a Linear Programming Problem (LPP) and characterize a set of feasible points that is believed to contain all extreme points of the LPP. The maximization of the objective function over this set of points is equivalent to a bi-polar quadratic optimization problem. We use a spectral relaxation technique to solve the bi-polar quadratic optimization problem. The proposed algorithm is of O ( n 3 ) time complexity and is the first ever polynomial-time algorithm to determine skyline probability. We show that the bounds computed by our proposed algorithm determine almost the same set of skyline objects as that with the deterministic algorithm. Experimental results are presented to corroborate this claim. In this paper we propose three different bounds on skyline probability.Computation of skyline points using these bounds is more accurate than the earlier sampling based technique.Our algorithm determines almost the same set of skyline objects as that with the exact method.We validate our hypothesis by experiments on real and synthetic datasets."
      }
    ]
  },
  {
    "tid":"651",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Automatic partitioning of parallel loops and data arrays for distributed shared-memory multiprocessors' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Presents a theoretical framework for automatically partitioning parallel loops to minimize cache coherency traffic on shared-memory multiprocessors. While several previous papers have looked at hyperplane partitioning of iteration spaces to reduce communication traffic, the problem of deriving the optimal tiling parameters for minimal communication in loops with general affine index expressions has remained open. Our paper solves this open problem by presenting a method for deriving an optimal hyperparallelepiped tiling of iteration spaces for minimal communication in multiprocessors with caches. We show that the same theoretical framework can also be used to determine optimal tiling parameters for both data and loop partitioning in distributed memory multicomputers. Our framework uses matrices to represent iteration and data space mappings and the notion of uniformly intersecting references to capture temporal locality in array references. We introduce the notion of data footprints to estimate the communication traffic between processors and use linear algebraic methods and lattice theory to compute precisely the size of data footprints. We have implemented this framework in a compiler for Alewife, a distributed shared-memory multiprocessor"
      }
    ]
  },
  {
    "tid":"652",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Hybrid fuzzy genetics-based machine learning with entropy-based inhomogeneous interval discretization' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Discretization of continuous attributes is a key issue in classifier design from numerical data. In the machine learning community, continuous attributes are discretized into intervals. An entropy measure is often used to determine the cutting points for interval discretization. In the fuzzy system community, continuous attributes are usually discretized into overlapping fuzzy sets. Learning and optimization techniques are used to adjust the membership function of each fuzzy set. One interesting research issue is a comparison between interval partitions and fuzzy partitions. We address this issue by using an entropy-based interval discretization method in hybrid fuzzy genetics-based machine learning (GBML). Our hybrid fuzzy GBML algorithm is applied to a number of data sets where interval discretization is fuzzified with different fuzzification grades from zero (i.e., interval partitions) to one (i.e., completely fuzzified partitions). Experimental results from various fuzzification grades are compared with each other."
      }
    ]
  },
  {
    "tid":"653",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Near-DRAM Acceleration with Single-ISA Heterogeneous Processing in Standard Memory Modules' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Energy consumed for transferring data across the processor memory hierarchy constitutes a large fraction of total system energy consumption, and this fraction has steadily increased with technology scaling. This article presents a near-DRAM acceleration (NDA) architecture wherein lightweight processors (LWPs) with the same ISA as their host processor are 3D-stacked atop commodity DRAM devices in a standard memory module to efficiently process data. In contrast to previous architectures, the authors' NDA architecture requires negligible changes to commodity DRAM device and standard memory module architectures. This allows the NDA to be more easily adopted in both existing and emerging systems. Experiments demonstrate that, on average, the authors' NDA-based system consumes almost 65 percent less energy at nearly two times higher performance than the baseline system."
      }
    ]
  },
  {
    "tid":"654",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient Agile Sink Selection in Wireless Sensor Networks Based on Compressed Sensing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Collection of the sensed data in a wireless sensor network at one or more sink (s) is a well studied problem and there are a lot of efficient solutions for a variety of wireless sensor network configurations and application requirements. These methods are often optimized towards collection of the sensed data at a predetermined base station or sink. This inherently reduces the agility of the wireless sensor network as the flow of information is not easily changeable after the establishment of the routing and data collection algorithms. This paper presents an efficient data dissemination method based on the compressed sensing theory that allows each sensor node to take the role of a sink. Agile sink selection is especially advantageous in scenarios where the sink or the end user of the wireless sensor network is mobile. The proposed method allows availing the global state of the environment by fetching a small set of data from any arbitrary node. Our evaluations prove the better performance of our technique over existing methods. Also a comparison with an oracle-based approach gives sufficient experimental evidences of a nearly optimal performance of our method."
      }
    ]
  },
  {
    "tid":"655",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficiency evaluation model with constraint resource: an application to banking operations.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Competition is often presented in a free market. Efficiency evaluation of decision-making units (DMUs) needs accommodation of such competition among various units due to constrained resources. This paper develops an innovative quantitative approach to address the above-mentioned performance evaluation problem with constrained resource using output-oriented DEA. The proposed model allows DMUs to identify the maximum input reduction and resource savings to achieve performance improvement. Relations between the proposed model and classical output-oriented DEA models are explored and some economic insights are derived from these models. The proposed approach is validated by use of computational examples."
      }
    ]
  },
  {
    "tid":"656",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Empirical Comparison of Pen Pressure and Pen Tilt Input Techniques' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Three experiments were conducted in this study to investigate the human ability to control pen pressure and pen tilt input, coupling cursor position, angle and scale respectively. Comparisons between pressure input and tilt input have been made in the three experiments. Experimental results show that decreasing pressure input resulted in very bad performance and was not a good input technique for any of the three experiments. In \"experiment1-coupling to cursor position\", the tilt input technique performed relatively better than the increasing pressure input technique, even though the tilt technique had a slightly higher error rate. In \"experiment2-coupling to cursor angle\", the tilt input performed relatively better than the increasing pressure input in terms of time. In \"experiment3-coupling to cursor scale\", tilt input performed a little better than increasing pressure input in terms of adjustment time, but a little worse in terms of the degree of control that users exerted. These results will be useful for further user interface design."
      }
    ]
  },
  {
    "tid":"657",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Ontological query answering under expressive Entity-Relationship schemata' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The Entity-Relationship (ER) model is a fundamental tool for database design, recently extended and employed in knowledge representation and reasoning due to its expressiveness and comprehensibility. We address the problem of answering conjunctive queries under constraints representing schemata expressed in an extended version of the Entity-Relationship model. This extended model, called ER^+, comprises is-a constraints among entities and relationships, plus functional and mandatory participation constraints. In particular, it allows for arbitrary permutations of the roles in is-a among relationships. A key notion that ensures high tractability in ER^+ schemata is separability, i.e., the absence of interaction between the functional participation constraints and the other constructs of ER^+. We provide a precise syntactic characterization of separable ER^+ schemata by means of a necessary and sufficient condition. We present a complete complexity analysis of the conjunctive query answering problem under separable ER^+ schemata, and also under several sublanguages of ER^+. We show that the addition of so-called negative constraints does not increase the complexity of query answering. With such constraints, our model properly generalizes the most widely adopted tractable ontology languages, including those in the DL-Lite family."
      }
    ]
  },
  {
    "tid":"658",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Low-Power Multi-Pin Maze Routing Methodology' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"As VLSI technologies scale into the deep submicron (DSM) realm, the minimum feature size continues to shrink. In contrast, the average die size is expected to remain constant or to slightly increase with each technology generation. This results in an average increase in the global interconnect lengths. In order to mitigate the impact of these global wires, buffer insertion is the most widely used technique. However, unconstrained buffering is bound to adversely affect the overall chip performance. In fact, the number of global interconnect buffers is expected to reach several hundreds of thousands to achieve an appropriate timing closure. This increase in buffers is destined to have a huge impact on the chip power consumption. In order to mitigate the impact of the power consumed by the interconnect buffers, a low power multi-pin routing methodology is proposed. The problem is tackled based on a graph theoretic representation of the interconnects that explores the various buffer sizing options in order to identify the lowest power path that can be assigned to the interconnect being routed. The formulation was found to be of a pseudo-polynomial complexity which fits well within the context of the increased number of buffers. The methodology is tested using the MCNC floorplan benchmarks to verify the correctness and the complexity. These tests showed that an average power saving as high as 45% with a 10% sacrifice in delay is observed."
      }
    ]
  },
  {
    "tid":"659",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The Belief Roadmap: Efficient Planning in Linear POMDPs by Factoring the Covariance' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we address the problem of trajectory planning with imperfect state information. In many real-world domains, the position of a mobile agent cannot he known perfectly; instead, the agent maintains a probabilistic belief about its position. Planning in these domains requires computing the best trajectory through the space of possible beliefs. We show that planning in belief space can he done efficiently for linear Gaussian systems by using a factored form of the covariance matrix. This factored form allows several prediction and measurement steps to be combined into a single linear transfer function, leading to very efficient posterior belief prediction during planning. We give a belief-space variant of the Probabilistic Roadmap algorithm called the Belief Roadmap (BRM) and show that the BRM can compute plans substantially faster than conventional belief space planning. We also show performance results for planning a path across MIT campus without perfect localization."
      }
    ]
  },
  {
    "tid":"660",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Siting park-and-ride facilities using a multi-objective spatial optimization model' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Park-and-ride services are an important component of many public transportation systems in the United States. Locating park-and-ride facilities is an essential step when planning for these services. In this research we focus on three major siting/modeling concerns that need to be addressed when siting park-and-ride facilities: covering as much potential demand as possible, locating park-and-ride facilities as close as possible to major roadways, and siting such facilities in the context of an existing system. Unfortunately, existing models do not enable each of these concerns to be simultaneously addressed. This paper presents a multi-objective spatial optimization model for integrating these considerations. This model is applied for siting park-and-ride facilities in Columbus, Ohio. Application results show the usefulness of the developed model in supporting transit planning in an urban region."
      }
    ]
  },
  {
    "tid":"661",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Boosting with side information' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In many problems of machine learning and computer vision, there exists side information, i.e., information contained in the training data and not available in the testing phase. This motivates the recent development of a new learning approach known as learning with side information that aims to incorporate side information for improved learning algorithms. In this work, we describe a new training method of boosting classifiers that uses side information, which we term as AdaBoost+. In particular, AdaBoost+ employs a novel classification label imputation method to construct extra weak classifiers from the available information that simulate the performance of better weak classifiers obtained from the features in side information. We apply our method to two problems, namely handwritten digit recognition and facial expression recognition from low resolution images, where it demonstrates its effectiveness in classification performance."
      }
    ]
  },
  {
    "tid":"662",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Channel codes for mitigating intersymbol interference in diffusion-based molecular communications' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Molecular communications emerges as a promising scheme for communication between nanoscale devices. In diffusion-based molecular communications, molecules as information symbols are released by transmitters and diffuse in the fluid or air environments to transmit messages. Under the diffusion channel modeled by Brownian motion, information sequences suffer from molecule crossovers, i.e., molecules released at an earlier time may arrive later, causing intersymbol interference (ISI). In this paper, we investigate practical channel codes for combating ISI. An ISI-free coding scheme is proposed to increase the communication reliability while keeping the encoding/decoding complexity reasonably low. We exemplify an ISI-free code and theoretically approximate its bit error rate (BER) performance. In addition, repetition codes are revisited out of the complexity concern and proved to be desirable. The BER approximations of the repetition code family are given as well. Compared with convolutional codes, the proposed ISI-free code and repetition codes offer comparable performance with much lower complexity for diffusion-based molecular communication systems."
      }
    ]
  },
  {
    "tid":"663",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A New Sensor Fault Isolation Method for T-S Fuzzy Systems.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper is concerned with the fault isolation problem for T-S fuzzy systems with sensor faults. With the help of a set theoretic description of T-S fuzzy models, a new fault isolation scheme is proposed. It consists of a set of fuzzy observers and each of them corresponds to a specified sensor, where the antecedent and consequent parts of the observer are independent on the sensor output. Diffe..."
      }
    ]
  },
  {
    "tid":"664",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Unlimited editions: three approaches to the dissemination and display of digital art' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The paper reflects on three approaches to the dissemination and display of digital art. \\'1c\"s[edition]\\'1d\" is a novel, web-based service that offers limited editions of \"\\'1cdigital prints\"\\'1d. Analysis of user comments suggests that the metaphor of a \\'1c\"limited digital edition\"\\'1d raises issues and to some extent is resisted. The second approach is the Flickr Brushes Gallery, where digital painters post images and comment on one another's work. Analysis of comment boards indicates that the shared art and comments are a form of gift exchange. Finally, the paper discusses a field study in which artists exhibited their work as it develops over time in digital frames and also in an immersive digital projection room. Analysis of field notes and interviews indicate that the digital frame approach was unsuccessful because of aesthetic and environmental concerns. The immersive projection suggested that more experiential approaches may be more interesting. It is argued that there is an inherent resistance in digital media to previous models of art commoditization. None of the approaches discussed here resolve the dilemma but rather indicate the scope and complexity of the issues."
      }
    ]
  },
  {
    "tid":"665",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Polynomial games and determinacy' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Two-player, zero-sum, non-cooperative, blindfold games in extensive form with incomplete information are considered in this paper. Any information about past moves which players played is stored in a database, and each player can access the database. A polynomial game is a game in which, at each step, all players withdraw at most a polynomial amount of previous information from the database. We show resource-bounded determinacy of some kinds of finite, zero-sum, polynomial games whose pay-off sets are computable by non-deterministic polynomial-time function-oracle Turing machines. We call a pay-off set F-determined if, for any polynomial game G associated with the given pay-off set, either player has a winning strategy which is in F for any subgames of G. We show that there exists an FP-strongly-determined pay-off set which is computed by an exponential-time oracle Turing machine, where FP is the set of polynomial-time computable functions. We also discuss several relationships between the determinacy of polynomial games and recursion-theoretic properties for the classes co-NP and co-NEXP. We show that the polynomial version of the axiom of choice holds under some assumption of polynomial determinacy for a pay-off set which is polynomial-time computable with parallel queries. This principle of choice implies that co-NP has the separation property."
      }
    ]
  },
  {
    "tid":"666",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Vertical Methodology for the Design Space Exploration of Graphene-enabled Wireless Communications' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Graphene-based antennas (or shortly named, graphennas) are envisaged to be the cornerstone of novel wireless communication systems by virtue of their reduced size, in the micrometer range, and an expected radiation frequency of a few terahertz. Progress in the characterization of graphennas and terahertz propagation indicate that nanoscale phenomena will significantly impact on the performance of such graphene-enabled wireless communication systems. Motivated by this fact and the novelty of the investigations, a methodology for the early-stage exploration of the performance of graphene-enabled wireless links is proposed in this paper. The novelty of this methodology resides in its highly vertical approach, which aims to bridge the wide conceptual gap between nanoscale physics and time-domain physical layer modeling and design. With it, the performance of a graphene-enabled wireless link is evaluated as a function of the chemical potential of graphene, the composition of the propagation medium, and the transmission distance in a simple case scenario."
      }
    ]
  },
  {
    "tid":"667",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Performance comparison of single-precision SPICE Model-Evaluation on FPGA, GPU, Cell, and multi-core processors' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Automated code generation and performance tuning techniques for concurrent architectures such as GPUs, Cell and FPGAs can provide integer factor speedups over multi-core processor organizations for data-parallel, floating-point computation in SPICE model-evaluation. Our Verilog AMS compiler produces code for parallel evaluation of non-linear circuit models suitable for use in SPICE simulations where the same model is evaluated several times for all the devices in the circuit. Our compiler uses architecture specific parallelization strategies (OpenMP for multi-core, PThreads for Cell, CUDA for GPU, statically scheduled VLIW for FPGA) when producing code for these different architectures. We automatically explore different implementation configurations (e.g. unroll factor, vector length) using our performance-tuner to identify the best possible configuration for each architecture. We demonstrate speedups of 3- 182times for a Xilinx Virtex5 LX 330T, 1.3-33times for an IBM Cell, and 3-131times for an NVIDIA 9600 GT GPU over a 3 GHz Intel Xeon 5160 implementation for a variety of single-precision device models."
      }
    ]
  },
  {
    "tid":"668",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'RiTE: Providing On-Demand Data for Right-Time Data Warehousing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Data warehouses (DWs) have traditionally been loaded with data at regular time intervals, e.g., monthly, weekly, or daily, using fast bulk loading techniques. Recently, the trend is to insert all (or only some) new source data very quickly into DWs, called near-realtime DWs (right-time DWs). This is done using regular INSERT statements, resulting in too low insert speeds. There is thus a great need for a solution that makes inserted data available quickly, while still providing bulk-load insert speeds. This paper presents RiTE (\"Right-Time ETL\"), a middleware system that provides exactly that. A data producer (ETL) can insert data that becomes available to data consumers on demand. RiTE includes an innovative main-memory based catalyst that provides fast storage and offers concurrency control. A number of policies controlling the bulk movement of data based on user requirements for persistency, availability, freshness, etc. are supported. The system works transparently to both producer and consumers. The system is integrated with an open source DBMS, and experiments show that it provides \"the best of both worlds\", i.e., INSERT-like data availability, but with bulk-load speeds (up to 10 times faster)."
      }
    ]
  },
  {
    "tid":"669",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An i-vector Extractor Suitable for Speaker Recognition with both Microphone and Telephone Speech.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"It is widely believed that speaker verification systems perform better when there is sufficient background training data to deal with nuisance effects of transmission channels. It is also known that these systems perform at their best when the sound environment of the training data is similar to that of the context of use (test context). For some applications however, training data from the same type of sound environment is scarce, whereas a considerable amount of data from a different type of environment is available. In this paper, we propose a new architecture for text-independent speaker verification systems that are satisfactorily trained by virtue of a limited amount of application-specific data, supplemented with a sufficient amount of training data from some other context. This architecture is based on the extraction of parameters (i-vectors) from a low-dimensional space (total variability space) proposed by Dehak [1]. Our aim is to extend Dehak's work to speaker recognition on sparse data, namely microphone speech. The main challenge is to overcome the fact that insufficient application-specific data is available to accurately estimate the total variability covariance matrix. We propose a method based on Joint Factor Analysis (JFA) to estimate microphone eigenchannels (sparse data) with telephone eigenchannels (sufficient data). For classification, we experimented with the following two approaches: Support Vector Machines (SVM) and Cosine Distance Scoring (CDS) classifier, based on cosine distances. We present recognition results for the part of female voices in the interview data of the NIST 2008 SRE. The best performance is obtained when our system is fused with the state-of-the-art JFA. We achieve 13% relative improvement on equal error rate and the minimum value of detection cost function decreases from 0.0219 to 0.0164."
      }
    ]
  },
  {
    "tid":"670",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Recognition of 2-d shapes inspired by the human visual system' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a new method suitable for shape recognition, inspired by the Primary Visual Cortex of the Human Visual System. It extracts a description for 2-dimensional shapes with a closed contour, regardless of their size, rotation and position, with low computational cost. The paper introduces a new computational approach to the modeling of the hypercolumns of the Primary Visual Cortex, which requires significantly less computational burden and which is highly parallel. A new shape descriptor based on the relative angles of an object is also proposed. It produces close results for different shapes of the same object, it is proportion-flexible and it can identify distorted shapes correctly. Experimental results prove that the method is adequate for shape-based image retrieval and classification, as well as for efficiently storage of edges and line drawings."
      }
    ]
  },
  {
    "tid":"671",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'I-Filter: Identical Structured Control Flow String filter for accelerated malware variant classification' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"As the number of malware variants has grown rapidly, classification speed has become crucial in security issues. While several techniques for malware variant classification have been proposed, they involve a speed-accuracy trade-off. In an attempt to achieve a speedy and accurate malware variant classification, we thoroughly analyze previously proposed methods and identify a critical performance bottleneck in string-to-string matching. This paper presents and evaluates a technique called I-Filter that enhances the performance of the previous approach, approximate matching. I-Filter has the following novel mechanism, the hash-based equivalent procedure matching technique. Our performance evaluation confirms that a performance improvement of on average 1,043 times through I-Filtering."
      }
    ]
  },
  {
    "tid":"672",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Treatment of some mathematical models describing long-range transport of air pollutants on vector processors' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The long-range transport of air pollutants is an important environmental problem that concerns not only separate countries, but also large regions containing many countries. A mathematical model, described by a system of partial differential equations (PDE's), can successfully be used in many studies of this physical phenomenon. The system of PDE's has necessarily to be considered on a large space domain (a space domain containing the whole of Europe is often used) and over a long time-interval (as a rule longer than one year, because of the seasonal variations of the concentrations studied). This indicates that after some kind of space discretization a very large system of ordinary differential equations (containing up to O(10 5 ) equations) has to be handled at each time-step. Therefore it is clear that it is absolutely necessary to introduce some simplifications in order to make the discretized problem tractable numerically. It is natural to require that these simplifications are made by the use of physical reasons only; i.e. to eliminate some connections of the physical problem studied with other physical processes that are not important (with regard to the physical process studied) and, thus, it should be expected that the results are not affected by the elimination. However, the truth is that very often simplifications are introduced in order to make the problem tractable on the computer under consideration. This is very unfortunate and should be avoided if possible. One can avoid many unnecessary simplifications in the study of air pollution phenomena by the use of efficient numerical algorithms on a vector processor (the processor actually used being a CRAY X-MP48 computer. The code, by which the air pollution model is treated numerically, is vectorized. Special care is taken for the most time-consuming parts: the FFT subroutines used and some linear algebra operations. It is demonstrated that the vectorized code is tractable numerically on a CRAY X-MP48 computer, while this is not true for the sequential computer used (an IBM 3081D computer). Different possibilities for further improvements and for other applications of the algorithms are discussed."
      }
    ]
  },
  {
    "tid":"673",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Modal Logics with a Linear Hierarchy of Local Propositional Quantifiers' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Local propositions arise in the context of the semantics for logics of knowledge in multi-agent systems. A proposition is local to an agent when it depends only on that agent's local state. We con- sider a logic, LLP, that extends S5, the modal logic of necessity (in which the modality refers to truth at all worlds) by adding a quantifier ranging over the set of all propositions and, for each agent, a propo- sitional quantifier ranging over the agent's local propositions. LLP is able to express a large variety of epistemic modalities, including knowledge, common knowledge and distributed knowledge. However, this expressiveness comes at a cost: the logic is equivalent to second order predicate logic when two independent agents are present (5), hence undecidable and not axiomatizable. This paper identifies a class of multi-agent S5 structures, hierarchical structures, in which the agents' information has the structure of a linear hierarchy. All systems with just a single agent are hierarchical. It is shown that LLP becomes decidable with respect to hierarchical systems. The main result of the paper is the completeness of an axiomatization for the hierarchical case."
      }
    ]
  },
  {
    "tid":"674",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Improved mispronunciation detection with deep neural network trained acoustic models and transfer learning based logistic regression classifiers.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"\u2022The discrimination of acoustic model for GOP calculation is improved by DNN training.\u2022F0 is added to DNN model for the detection of misusing lexical stress or tone.\u2022The GOP measure is improved to robustly evaluate pronunciation of non-native speech.\u2022A transfer learning based classifier is proposed to mispronunciation detection."
      }
    ]
  },
  {
    "tid":"675",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Dynamic token generation for constrained tasks in a multi-robot system' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The problem of assigning tasks to a group of robots acting in a dynamic environment is a fundamental issue for a MRS and is relevant to several real world applications. Several techniques have been studied to address this problem, however when the system needs to scale up with size, communication quickly becomes an important issue to address; moreover, tasks to be executed can be tied by constraints that have to be considered in the task assignment process. In this paper we present a distributed task assignment approach that ensures very low communication overhead and can manage constraints among roles. The basic idea of our approach is to use tokens to represent tasks to be executed, each team member creates, executes and propagates tokens based on a broad knowledge of their team mates. We test and evaluate our approach by means of experiments both on a simulates and on a real robot scenario"
      }
    ]
  },
  {
    "tid":"676",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimal dedicated protection approach to shared risk link group failures using network coding' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Survivable routing serves as a key role in connection-oriented communication networks for achieving desired service availability for each connection. This is particularly critical for the success of all-optical mesh networks where each lightpath carries a huge amount of data. Currently, 1+1 dedicated path protection appears to be the most widely deployed network resilience mechanism because it offers instantaneous recovery from network failures. However, 1+1 protection consumes almost twice as much capacity as required, which imposes a stringent constraint on network resource utilization. In addition, finding an SRLG-disjoint path is essential for 1+1 protection, which is nonetheless subject to non-trivial computation complexity and may fail in some SRLG scenarios. To address these problems, we introduce a novel framework of 1+1 protection, called Generalized Dedicated Protection (GDP), for achieving instantaneous recovery from any SRLG failure event. It is demonstrated, that finding a non-bifurcated optimal solution for GDP is NP-complete. Thus, the paper presents a novel scheme applying Generalized Dedicated Protection and Network Coding (GDP-NC) to ensure both optimal resource utilization among dedicated protection approaches and instantaneous recovery for single unicast flows, which can be split into multiple parts in all-optical networks. We demonstrate that the proposed GDP-NC survivable routing problem is polynomial-time solvable, owing to the ability to bifurcate flows. This flexibility comes at the expense of additional hardware for linear combination operations for the optical flows."
      }
    ]
  },
  {
    "tid":"677",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Menu-based Content Search Support System Considering Mobile User's Situations.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Due to the popularization of mobile phones, many people search Web contents using their own mobile phones. Since people bring their mobile phones everywhere, they can search Web contents in various situations (e.g., walking and sitting). In particular, when users are walking, they cannot pay enough attentions to operate their devices and it is difficult to conduct complicated operations such as character inputting. In addition, users\u2019 search objectives are also affected by the time and their location. To assist content search in such situations, in this paper, we propose a content search support system that presents a few options that enable mobile users to get desired contents easily considering the users\u2019 situations. The system infers users\u2019 situation based on sensory data and presents initial options considering users\u2019 search objective corresponding to the inferred situation. Through a user experiment, we confirmed that participants could get desired information easily by using our system and it was effective to adaptively present options based on the inferred situations."
      }
    ]
  },
  {
    "tid":"678",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Supporting QoS with location aware prereservation in mobile ad hoc networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Due to MAC collisions and mobility, quality of service (QoS) provisioning in mobile ad hoc networks is very challenging. In this paper, we propose an innovative scheme to grant high priority to communications between some important nodes in TDMA based ad hoc networks. In addition to bandwidth reservation in previous QoS routing approaches, our scheme adopts a new approach: bandwidth pre-reservation. Some bandwidth is pre-reserved at intermediate nodes in a quadrangle- shaped area formed between important nodes. The key idea is to utilize each node's geographic location information and minimize potential scheduling conflicts for transmissions. In this way, time slot collisions in adjacent wireless links along a path can be reduced so that more high priority connections can be accepted into the network. Extensive simulations show that our scheme can successfully provide better communication quality at a relatively low price."
      }
    ]
  },
  {
    "tid":"679",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Automating Product Derivation in Software Product Line Engineering' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper deals with deriving software products from as oftware product line (SPL) in an efficient and automated way. We present an approach that (1) repres- ents the SPL with as et of integrated models, (2) specifies variability and configuration options for possible product variants and (3) automatically derive se xecutable products with model transformations and aspect-oriented techniques. The approach is discussed with as ample SPL of scientific calculators."
      }
    ]
  },
  {
    "tid":"680",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Education of wireless and ATM networking concepts using hands-on laboratory experience' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Traditional computer networking courses primarily tend to provide students with hands-on software development and network performance experience with TCP/IP and Ethernet networks. Our objectives are to enhance the curriculum of networking courses and the expertise of students by introducing next-generation networks such as wireless and ATM networks. To facilitate this, we have established a wireless ATM network instructional facility (NIF) at Washington State University (WSU). Projects based on ATM networks and wireless networks have been introduced in a course on undergraduate computer networks taught at WSU during Spring 1998. This paper describes the laboratory setup, the projects and assignments, and the feedback obtained from the students."
      }
    ]
  },
  {
    "tid":"681",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Test and Reliability of Emerging Non-volatile Memories' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The search for alternative memory technologies has attracted significant attention toward emerging non-volatile memories. Among them, STT-MRAM, PCM, RRAM have shown promising characteristic to gain a position inside the memory hierarchy of computing platforms, and even enable new computing paradigms. However like any other emerging technology these devices are affected by concerns to be resolved before they could become a mainstream. This paper reviews the main reliability and testability challenges of aforementioned emerging non-volatile memories and highlights the main future considerations toward them."
      }
    ]
  },
  {
    "tid":"682",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Snow phenomena modeling through online public media' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose a method for the environmental monitoring through the publicly available media User Generated Content (UGC). In particular we address the problem of the snow cover and level estimation by analyzing the social media data such as geotagged photographs and public webcams installed in mountain regions. The entire pipeline of the process is presented to the audience: from the data crawling and automatic relevance classification (does or does not the photograph contain a significant mountain profile) to the image content analysis and environmental models (identification of the snow covered area on the photograph). Each presented component is self-contained and can be inspected individually, the connections between the components however are strongly highlighted allowing the viewer to understand intuitively the entire pipeline structure."
      }
    ]
  },
  {
    "tid":"683",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Automated data-driven discovery of motif-based protein function classifiers' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper describes an approach to data-driven discovery of decision trees or rules for assigning protein sequences to functional families using sequence motifs. This method is able to capture regularities that can be described in terms of presence or absence of arbitrary combinations of motifs, A training set of peptidase sequences labeled with the corresponding MEROPS functional families or clans is used to automatically construct decision trees that capture regularities sufficient to assign the sequences to their respective functional families. The performance of the resulting decision tree classifiers is then evaluated on an independent test set. We compared the rules constructed using motifs generated by a multiple sequence alignment based motif discovery tool (MEME) with rules constructed using expert annotated PROSITE motifs (patterns and profiles). Our results indicate that the former provide a potentially powerful high throughput technique for constructing protein function classifiers when adequate training data are available. Examination of the generated rules in relation to known three-dimensional structures of members in the case of two families (MEROPS families C14 and M12) suggests that the proposed technique may be able to identify combinations of sequence motifs that characterize functionally significant three-dimensional structural features of proteins."
      }
    ]
  },
  {
    "tid":"684",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient data dissemination in cooperative multi-RSU Vehicular Ad Hoc Networks (VANETs).' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Enhanced Cooperative Load Balancing (ECLB) approach is proposed in VANETs.ECLB takes the advantages of fixed road layout and tolerable delay of requests.ECLB considers the mobility and the dynamic direction changes of vehicles.ECLB balances the workload among the junction-RSUs and edge-RSUs.ECLB outperforms the existing approaches. Many safety and non-safety related applications have been envisioned in VANETs. However, efficient data dissemination considering the mobility of vehicle is must for the success of these applications. Although the Road Side Unit (RSU) is a stationary unit, both RSU and vehicle have limited transmission range that restricts to shorter connection time. This endures a higher request drop rate especially at the overloaded RSUs. A cooperative load balancing (CLB) among the RSUs to use their residual bandwidth can be an effective solution to reduce the request drop rate. In this paper, we investigate that considering the remaining delay tolerance of submitted requests and the knowledge of fixed road layout, the performance of the cooperative load balancing system can be further improved significantly. We show that this performance gain comes from serving the requests based on the urgency and the efficient load balancing among the junction-RSUs and edge-RSUs. Based on the observations, we propose an Enhanced CLB (ECLB) approach in this paper. To demonstrate the efficiency of the ECLB approach a number of well-known scheduling algorithms are integrated and an extensive simulation experiments are conducted in the vehicular communication environment that supports the superiority of ECLB over the existing approaches."
      }
    ]
  },
  {
    "tid":"685",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The crosswatch traffic intersection analyzer: a roadmap for the future' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The \"Crosswatch\" project is a smartphone-based system developed by the authors for providing guidance to blind and visually impaired pe-destrians at traffic intersections. Building on past work on Crosswatch functionality to help the user achieve proper alignment with the crosswalk and read the status of Walk lights to know when it is time to cross, we outline the direction Crosswatch should take to help realize its potential for becoming a practical system: namely, augmenting computer vision with other information sources, including geographic information systems (GIS) and sensor data, to provide a much larger range of information about traffic intersections to the pedestrian."
      }
    ]
  },
  {
    "tid":"686",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Parallelising the mean value analysis algorithm' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":" This paper describes the design and implementation of a MeanValue Analysis (MVA) algorithm on a distributed memory system,using the MPI library for communication. "
      }
    ]
  },
  {
    "tid":"687",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Network optimization in supply chain: A KBGA approach' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we present a Knowledge Based Genetic Algorithm (KBGA) for the network optimization of Supply Chain (SC). The proposed algorithm integrates the knowledge base for generating the initial population, selecting the individuals for reproduction and reproducing new individuals. From the literature, it has been seen that simple genetic-algorithm-based heuristics for this problem lead to and large number of generations. This paper extends the simple genetic algorithm (SGA) and proposes a new methodology to handle a complex variety of variables in a typical SC problem. To achieve this aim, three new genetic operators-knowledge based: initialization, selection, crossover, and mutation are introduced. The methodology developed here helps to improve the performance of classical GA by obtaining the results in fewer generations. To show the efficacy of the algorithm, KBGA also tested on the numerical example which is taken from the literature. It has also been tested on more complex problems."
      }
    ]
  },
  {
    "tid":"688",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On the feasibility of using microfluidic technologies for communications in Labs-on-a-Chip' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Microfluidics refers to manipulation and control of little amount of fluids flowing into channels of micro-scale size and is the basic technology utilized by Labs-on-a-Chip (LoC) systems. In the recent past researchers have analyzed the possibility to support information exchange between different elements of a LoC. However, support of communication in microfluidic systems relies on the use of appropriate information coding strategies and deep knowledge of channel propagation characteristics. Accordingly, in this paper we compare two different schemes for information coding and evaluate the corresponding capacities depending on the characteristics of the fluids utilized. Furthermore, we discuss how such encoding schemes can be integrated in networking solutions for LoCs."
      }
    ]
  },
  {
    "tid":"689",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Vision-Based Caging Grasps of Polyhedron-Like Workpieces With a Binary Industrial Gripper' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Development of a flexible low-cost robotic system to grasp various 3D workpieces is of practical important. Traditional approaches on 3D grasping usually need to compute the effective contact positions based on sufficient conditions, such as \u201cforce-closure\u201d or \u201cform-closure,\u201d to prevent all the motions of the grasped objects. Compared with the previous work motivated by high-precision applications, caging provides a way to manipulate an object without needing to immobilize it. However, most caging conditions consider only 2D motions of the object in grasping. In some cases, other motions in 3D space, e.g., the pitch and roll rotations of the objects, would also possibly change a caging configuration to an uncaging configuration. This paper aims to discuss caging with frictionless contact, by taking into consideration of all the motions of the object. We first discuss the relationship between the gripper configuration and the state of the grasped object in grasping, where all the motions of the object are taken into account. We then establish the sufficient conditions to find a set of 3D caging configurations in terms of the width of the object's projection and the gap of the gripper, such that we can utilize the projection to find the feasible placements of the pins to determine the caging configuration. Furthermore, we discuss how to find the caging configuration that is capable of leading to a form-closure based on attractive region formed in the configuration space."
      }
    ]
  },
  {
    "tid":"690",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Characterizing Relations between Architectural Views' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"It is commonly agreed that an architectural description (AD) consists of multiple views. Each view describes the architecture from the perspective of particular stakeholder concerns. Although views are constructed separately, they are related as they describe the same system.A thorough study of the literature reveals that research on relations between views is fragmented and that a comprehensive study is hampered by an absence of common terminology. This has become apparent in the discussion on inter-view relational concepts in the revision of IEEE 1471 as ISO/IEC 42010 (Systems and Software Engineering -- Architectural Description).This paper puts forward a framework that employs a consistent terminology to characterize relations between views. The framework sheds light on the usage, scope and mechanisms for relations, and is illustrated using several representative approaches from the literature. We conclude with a reflection on whether the revision of ISO 42010 aligns with our findings."
      }
    ]
  },
  {
    "tid":"691",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Nonparametric hierarchical Bayesian model for functional brain parcellation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We develop a method for unsupervised analysis of functional brain images that learns group-level patterns of functional response. Our algorithm is based on a generative model that comprises two main layers. At the lower level, we express the functional brain response to each stimulus as a binary activation variable. At the next level, we define a prior over the sets of activation variables in all subjects. We use a Hierarchical Dirichlet Process as the prior in order to simultaneously learn the patterns of response that are shared across the group, and to estimate the number of these patterns supported by data. Inference based on this model enables automatic discovery and characterization of salient and consistent patterns in functional signals. We apply our method to data from a study that explores the response of the visual cortex to a collection of images. The discovered profiles of activation correspond to selectivity to a number of image categories such as faces, bodies, and scenes. More generally, our results appear superior to the results of alternative data-driven methods in capturing the category structure in the space of stimuli."
      }
    ]
  },
  {
    "tid":"692",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Stochastic Approach to Separate Diffuse and Specular Reflections' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents separation of specular and diffuse reflection components from an image pair. The proposed approach is based on the dichromatic reflectance model and Markov random field models. The proposed method estimates specular and diffuse components by minimizing observation color noise and prior potential of specular reflectance. The specular reflection component is modelled as an MRF and estimated in maximum a posteriori framework. This paper proposes likelihood term of specular component and the prior model of specular reflectance based on Phong's shading. Some experiments show that the proposed approach separates the specular and diffuse reflection components effectively. The separated specular components can be utilized in image-based lighting which renders a scene with virtual lighting sources."
      }
    ]
  },
  {
    "tid":"693",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Spectral Methods for View-Based 3-D Object Recognition Using Silhouettes' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The shockg raph is an emerging shape representation for object recognition, in which a 2-D silhouette is decomposed into a set of qualitative parts, captured in a directed acyclic graph. Although a number of approaches have been proposed for shock graph matching, these approaches do not address the equally important indexing problem. We extend our previous workin both shock graph matching and hierarchical structure indexing to propose the first unified framework for view-based 3-D object recognition using shock graphs. The heart of the framework is an improved spectral characterization of shock graph structure that not only drives a powerful indexing mechanism (to retrieve similar candidates from a large database), but also drives a matching algorithm that can accommodate noise and occlusion. We describe the components of our system and evaluate its performance using both unoccluded and occluded queries. The large set of recognition trials (over 25,000) from a large database (over 1400 views) represents one of the most ambitious shockg raph-based recognition experiments conducted to date. This paper represents an expanded version of [12]."
      }
    ]
  },
  {
    "tid":"694",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Towards Practical Whitebox Cryptography: Optimizing Efficiency and Space Hardness.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Whitebox cryptography aims to provide security for cryptographic algorithms in an untrusted environment where the adversary has full access to their implementation. Typical security goals for whitebox cryptography include key extraction security and decomposition security: Indeed, it should be infeasible to recover the secret key from the implementation and it should be hard to decompose the implementation by finding a more compact representation without recovering the secret key, which mitigates code lifting. Whereas all published whitebox implementations for standard cryptographic algorithms such as DES or AES are prone to practical key extraction attacks, there have been two dedicated design approaches for whitebox block ciphers: ASASA by Birykov et al. at ASIACRYPT'14 and SPACE by Bogdanov and Isobe at CCS'15. While ASASA suffers from decomposition attacks, SPACE reduces the security against key extraction and decomposition attacks in the white box to the security of a standard block cipher such as AES in the standard blackbox setting. However, due to the security-prioritized design strategy, SPACE imposes a sometimes prohibitive performance overhead in the real world as it needs many AES calls to encrypt a single block. In this paper, we address the issue by designing a family of dedicated whitebox block ciphers SPNbox and a family of underlying small block ciphers with software efficiency and constant-time execution in mind. While still relying on the standard blackbox block cipher security for the resistance against key extraction and decomposition, SPNbox attains speed-ups of up to 6.5 times in the black box and up to 18 times in the white box on Intel Skylake and ARMv8 CPUs, compared to SPACE. The designs allow for constant-time implementations in the blackbox setting and meet the practical requirements to whitebox cryptography in real-world applications such as DRM or mobile payments. Moreover, we formalize resistance towards decomposition in form of weak and strong space hardness at various security levels. We obtain bounds on space hardness in all those adversarial models. Thus, for the first time, SPNbox provides a practical whitebox block cipher that features well-understood key extraction security, rigorous analysis towards decomposition security, demonstrated real-world efficiency on various platforms and constant-time implementations. This paves the way to enhancing susceptible real-world applications with whitebox cryptography."
      }
    ]
  },
  {
    "tid":"695",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Traffic-Aware Energy Efficient Scheme for WSN Employing an Adaptable Wakeup Period' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recently, Wireless Sensor Networks (WSN) have come to require an energy efficiency and a guaranteed delivery delay time, on the contrary to traditional Medium Access Control (MAC) protocols that have mostly aimed for a high throughput. In order to satisfy these new significant requirements, many of the newer WSN MAC protocols employ duty cycled mechanism, in order to improve energy efficient techniques in the MAC layer. This duty cycled mechanism is oriented to the toggling of the transceiver's conditions composed of the wakeup and sleep states. However, regrettably, the channel conditions of a wireless network can be changed anytime according to the traffic congestion. If a high congestion is present, an insufficient wakeup period can cause the successful communication ratio to worsen. This can be a weakness in previous duty cycled mechanisms due to their lack of consideration for the network conditions. Therefore, we have devised and designed an efficient new scheme, the adaptable wakeup period, which can resolve this issue. We present analyses and the simulated results of our work, together with a comparison between the synchronous and asynchronous MAC protocols, using two types of analysis models that we suggest."
      }
    ]
  },
  {
    "tid":"696",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Online Social Network Profile Linkage Based on Cost-Sensitive Feature Acquisition' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Billions of people spend their virtual life time on hundreds of social networking sites for different social needs. Each social footprint of a person in a particular social networking site reflects some special aspects of himself. To adequately investigate a user's preference for applications such as recommendation and executive search, we need to connect up all these aspects to generate a comprehensive profile of the identity. Profile linkage provides an effective solution to identify the same identity's profiles from different social networks. With various types of resources, comparing profiles may require plenty of expensive and time-consuming features such as avatars. To boost the online social network profile linkage solution, we propose a cost-sensitive approach that only acquires these expensive and time-consuming features when needed. By evaluating on the real-world datasets from Twitter and LinkedIn, our approach performs at over 85% F-1-measure and has the ability to prune over 80% of the unnecessary feature acquisitions, at a marginal cost of 10% performance loss."
      }
    ]
  },
  {
    "tid":"697",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Towards a Context Aware Multipath-TCP' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Multi-homing has become ubiquitous in the mobile domain. Despite this, mobile devices are not yet capable of fully realizing the potential of the increasing variety and availability of network resources. Multipath-TCP (MPTCP) is one of the most prominent technologies for multi-homed mobile devices, that is able to effectively tackle both resource pooling and mobility. Despite the success of MPTCP we believe that the implementation can be enhanced for the mobile domain, by increasing the granularity of control a device has over how its network interfaces are used. In this paper we present and evaluate MPTCP - Context Aware (MPTCP-CA), which introduces a policy oriented approach to the management and control of MPTCP connections. Our evaluation demonstrates three use cases that leverage context awareness to improve the utilization of multiple network interfaces with respect to the users requirements."
      }
    ]
  },
  {
    "tid":"698",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Adaptive Content-Based Tile Partitioning Algorithm For The Hevc Standard' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes a content-based Tile partitioning algorithm designed to exploit video properties in the tiling process, aiming at the reduction of the coding losses generated by the use of Tiles. In the proposed algorithm two steps are performed to define the vertical and horizontal Tile boundaries willing to group the high correlated samples into the same Tile partition. The definition of the Tiles' boundaries is performed by analyzing the variance map extracted from the raw picture information. Experimental results have shown that the proposed algorithm is able to reduce the inherent coding efficiency losses of using Tiles when compared to the conventional uniform spaced Tile partitions."
      }
    ]
  },
  {
    "tid":"699",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fundamental Limits On Communication Latency For Distributed Control Via Electromechanical Waves' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cyber enabling infrastructures such as the smart grid transforms many of the it's aspects. A key to many of the promised benefits is the performance of the communication network, mainly in terms of delay and quality. In one area of development, distributed control that rely on received system measurements have shown enhanced system performance against disturbances. We establish a propagation of events framework between the communication system and the physical power system based on electromechanical waves. The proposed framework is used to arrive at fundamental limits of communication latency beyond which distributed control would not be able to properly address disturbances."
      }
    ]
  },
  {
    "tid":"700",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Visualization And Analysis Of Rotating Stall For Transonic Jet Engine Simulation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Identification of early signs of rotating stall is essential for the study of turbine engine stability. With recent advancements of high performance computing. high-resolution unsteady flow fields allow in depth exploration of rotating stall and its possible causes. Performing stall analysis, however, involves significant effort to process large amounts of simulation data, especially when investigating abnormalities across many time steps. In order to assist scientists during the exploration process, we present a visual analytics framework to identify suspected spatiotemporal regions through a comparative visualization so that scientists are able to focus on relevant data in more detail. To achieve this, we propose efficient stall analysis algorithms derived from domain knowledge and convey the analysis results through juxtaposed interactive plots. Using our integrated visualization system, scientists can visually investigate the detected regions for potential stall initiation and further explore these regions to enhance the understanding of this phenomenon. Positive feedback from scientists demonstrate the efficacy of our system in analyzing rotating stall."
      }
    ]
  },
  {
    "tid":"701",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The Effects Of Psychological Factors On Online Consumer Behavior' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The topic of online consumer behavior has been examined under various contexts over these years. Although researchers from a variety of business disciplines have made significant progress over the past few years, there are very few studies that directly investigate the relationship between psychology and online consumer behavior. In view of this, this study provides a review of the relevant empirical literature and proposes a research framework. This proposed framework not only provides us with a cohesive view of the relationship between psychology and online consumer behavior, but also serves as a salient guideline for researchers in this area. This study concludes with a research agenda for the study of the relationship between psychology and online consumer behavior."
      }
    ]
  },
  {
    "tid":"702",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The Effect of Problem Perturbations on Nonlinear Dynamical Systems and their Reduced-Order Models' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Reduced-order models are used extensively in many areas of science and engineering for simulation, design, and control. Reduction techniques for nonlinear dynamical systems produce models that depend strongly on the nominal set of parameters for which the reduction is carried out. In this paper we address the following two questions: \u201cWhat is the effect of perturbations in the problem parameters on the output functional of a nonlinear dynamical system?\u201d and \u201cto what extent does the reduced-order model capture this effect?\u201d"
      }
    ]
  },
  {
    "tid":"703",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'May i talk about other shops here?: modeling territory and invasion in front of shops' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper models the concept of the \\\"territory\\\" of shops. First, we interviewed three shopkeepers and found that they perceived the space near their shop as their territory and that they interpreted some types of behaviors as invasive. Second, we confirmed that potential visitors share this notion of territory. We also confirmed that the size of the territory depends on the characteristics of a shop's facade. While there is little territory in front of walls, there is more territory in front of shelves and entrances. Our robot traversed around two real shopping malls that included 50 shops and took 3-D scans of their environment shapes. Each shop's facade was analyzed and the shop territory was computed. The computation results match people's perception. The recognition rate accuracy reached 93.5% for the territory areas. User evaluations in a virtual shop environment confirmed that a robot with a territory model behaves better than one without it."
      }
    ]
  },
  {
    "tid":"704",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimal Energy-Efficient Routing for Wireless Sensor Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The network lifetime for wireless sensor network plays an important role to survivability, Thus, we indicate the importance of routing protocol to network lifetime, and model the expected retransmission time as a convex function with respect to aggregate jlow on each sensor node. Thus we formulate the optimal energy-eficient routing as a non-linear min-max programming problem with convex productform, which can be optimally solved by optimal routing framework. Based on the optimal routing framework, we propose Lagrangean-based algorithm and primal optimal algorithm. By the combination of these two algorithms, we can optimally and efficiently get the routing assignment to maximize the network life in the sensor network. From experiments,we observe that when the optimal network lifetime increases as the number of sensor nodes increase. While the shortest path-based heuristic algorithm can only achieve about 48% network lifetime compared to our solution approach."
      }
    ]
  },
  {
    "tid":"705",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Assessing the safety of knowledge patterns in OWL ontologies' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The availability of a concrete language for embedding knowledge patterns inside OWL ontologies makes it possible to analyze their impact on the semantics when applied to the ontologies themselves. Starting from recent results available in the literature, this work proposes a sufficient condition for identifying safe patterns encoded in OPPL. The resulting framework can be used to implement OWL ontology engineering tools that help knowledge engineers to understand the level of extensibility of their models as well as pattern users to determine what are the safe ways of utilizing a pattern in their ontologies."
      }
    ]
  },
  {
    "tid":"706",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'NashWifi: Coordinated management of heterogenous home 802.11 networks using bargaining' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we present an economically principled framework for coordinated configurations of heterogeneous wireless networks. Our approach, called NashWiFi, is based on the concept of Nash bargaining. We argue that NashWiFi can improve both the global performance of a large diverse 802.11 deployment, while also improving the local performance of individual applications in an equitable fashion. NashWiFi can accommodate applications with diverse needs ranging from file transfer to HD streaming. Extensive experiments using an implementation of NashWiFi support in the MadWifi driver and large scale simulations using Qualnet establish the salient features of our approach."
      }
    ]
  },
  {
    "tid":"707",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Actionable high-coherent-utility fuzzy itemset mining' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many fuzzy data mining approaches have been proposed for finding fuzzy association rules with the predefined minimum support from quantitative transaction databases. Since each item has its own utility, utility itemset mining has become increasingly important. However, common problems with existing approaches are that an appropriate minimum support is difficult to determine and that the derived rules usually expose common-sense knowledge, which may not be interesting from a business point of view. This study thus proposes an algorithm for mining high-coherent-utility fuzzy itemsets to overcome problems with the properties of propositional logic. Quantitative transactions are first transformed into fuzzy sets. Then, the utility of each fuzzy itemset is calculated according to the given external utility table. If the value is larger than or equal to the minimum utility ratio, the itemset is considered as a high-utility fuzzy itemset. Finally, contingency tables are calculated and used for checking whether a high-utility fuzzy itemset satisfies four criteria. If so, it is a high-coherent-utility fuzzy itemset. Experiments on the foodmart and simulated datasets are made to show that the derived itemsets by the proposed algorithm not only can reach better profit than selling them separately, but also can provide fewer but more useful utility itemsets for decision-makers."
      }
    ]
  },
  {
    "tid":"708",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Report Of The Results Of An Ims Learning Design Expert Workshop (Complete Version)' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Europe has seen a number of learning design initiatives in the past years. Among them are the Open University UK Learning Design Initiative, JISC's Design for Learning Programme, as well as the Open University of the Netherlands' development of the Educational Modelling Language and its conversion into the specification IMS Learning Design. Particularly the last initiative inspired an IMS Learning Design Expert Workshop, which was held at the University of Vienna on November 20 & 21, 2008. This report contains a description of the purpose of the workshop, its methodologies and the results. Participating experts first brainstormed visions and problems of IMS Learning Design, and then developed potential solutions to some of the identified problems. Three groups formed to work on two of the identified problems in more depth: the usability and utility problem, and the life cycle of a unit of learning. The proposed solutions regarding the usability and utility problem were to investigate how teachers' and learners' representations of a learning design can be brought together, and to set up a research program to identify how teachers cognitively proceed when designing courses, and to map this knowledge to IMS LD. In regard to the life cycle of a unit of learning problem, the group suggested a system that continually exchanges information between runtime and editing systems so that units of learning can be updated accordingly."
      }
    ]
  },
  {
    "tid":"709",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Learning Application Models for Utility Resource Planning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Shared computing utilities allocate compute, network and storage resources to competing applications on demand. An awareness of the demands and behaviors of the hosted applications can help the system to manage its resources more effectively. This paper proposes an active learning approach that analyzes performance histories to build predictive models of frequently used applications; the histories consist of measures gathered from noninvasive instrumentation on previous runs with varying assignments of compute, network and storage resources. An initial prototype uses linear regression to predict application interactions with candidate resources and combines them to forecast completion time for a candidate resource assignment. Experimental results from the prototype show that the mean forecasting errors range from 1% to 11% for a set of batch tasks captured from a production cluster. Examples illustrate how a system can use the learned models to guide task placement and data staging."
      }
    ]
  },
  {
    "tid":"710",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Ideal mode selection of a cardiac pacing system' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mode transition in any inappropriate mode can be a common cause of any mishap in a complex health-care system. This paper presents an approach for formalizing and reasoning about optimal mode transition in a health-care system that uses several operating modes in various operating states. Modes are formalized and their relation to a state-based formalism is established through a refinement approach. The efficiency of this approach is presented by formalizing an ideal operating mode transition of a cardiac pacemaker case study. An incremental approach is used to develop the system and its detailed design is verified through a series of refinements. The consequence of this approach is to improve system structuring, elicitation of system assumptions and expected functionality, as well as requirement traceability using modes in state-based modeling. Models are expressed in Event B modeling language and validated by a model checker tool: ProB."
      }
    ]
  },
  {
    "tid":"711",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Super-Resolution Compressed Sensing: An Iterative Reweighted Algorithm for Joint Parameter Learning and Sparse Signal Recovery.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In many practical applications such as direction-of- arrival (DOA) estimation and line spectral estimation, the sparsifying dictionary is usually characterized by a set of unknown parameters in a continuous domain. To apply the conventional compressed sensing to such applications, the continuous parameter space has to be discretized to a finite set of grid points. Discretization, however, incurs e..."
      }
    ]
  },
  {
    "tid":"712",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An effective image retrieval scheme using color, texture and shape features' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we present a new and effective color image retrieval scheme for combining all the three i.e. color, texture and shape information, which achieved higher retrieval efficiency. Firstly, the image is predetermined by using fast color quantization algorithm with clusters merging, and then a small number of dominant colors and their percentages can be obtained. Secondly, the spatial texture features are extracted using a steerable filter decomposition, which offers an efficient and flexible approximation of early processing in the human visual system. Thirdly, the pseudo-Zernike moments of an image are used for shape descriptor, which have better features representation capabilities and are more robust to noise than other moment representations. Finally, the combination of the color, texture and shape features provide a robust feature set for image retrieval. Experimental results show that the proposed color image retrieval is more accurate and efficient in retrieving the user-interested images."
      }
    ]
  },
  {
    "tid":"713",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'QoE-Based Multi-Exposure Fusion in Hierarchical Multivariate Gaussian CRF' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many state-of-the-art fusion methods, combining details in images taken under different exposures into one well-exposed image, can be found in the literature. However, insufficient study has been conducted to explore how perceptual factors can provide viewers better quality of experience on fused images. We propose two perceptual quality measures: perceived local contrast and color saturation, which are embedded in our novel hierarchical multivariate Gaussian conditional random field model, to illustrate improved performance for multi-exposure fusion. We show that our method generates images with better quality than existing methods for a variety of scenes."
      }
    ]
  },
  {
    "tid":"714",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Selecting resources for distributed dataflow systems according to runtime targets' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Distributed dataflow systems like Spark or Flink enable users to analyze large datasets. Users create programs by providing sequential user-defined functions for a set of well-defined operations, select a set of resources, and the systems automatically distribute the jobs across these resources. However, selecting resources for specific performance needs is inherently difficult and users consequently tend to overprovision, which results in poor cluster utilization. At the same time, many important jobs are executed recurringly in production clusters. This paper presents Bell, a practical system that monitors job execution, models the scale-out behavior of jobs based on previous runs, and selects resources according to user-provided runtime targets. Bell automatically chooses between different runtime prediction models to optimally support different distributed dataflow systems. Bell is implemented as a job submission tool for YARN and, thus, works with existing cluster setups. We evaluated Bell's runtime prediction with six exemplary data analytics jobs using both Spark and Flink. We present the learned scale-out models for these jobs and evaluate the relative prediction error using cross-validation, showing that our model selection approach provides better overall performance than the individual prediction models."
      }
    ]
  },
  {
    "tid":"715",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Semantic Regularities in Document Representations.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recent work exhibited that distributed word representations are good at capturing linguistic regularities in language. This allows vector-oriented reasoning based on simple linear algebra between words. Since many different methods have been proposed for learning document representations, it is natural to ask whether there is also linear structure in these learned representations to allow similar reasoning at document level. To answer this question, we design a new document analogy task for testing the semantic regularities in document representations, and conduct empirical evaluations over several state-of-the-art document representation models. The results reveal that neural embedding based document representations work better on this analogy task than conventional methods, and we provide some preliminary explanations over these observations."
      }
    ]
  },
  {
    "tid":"716",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A discourse model for interaction design based on theories of human communication' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Most current models of interaction design build on scenarios and task analysis. We think that interaction design should be more along the lines of communication between humans. With this motivation, our paper presents a new approach to modeling interaction design based on insights from theories of human communication. From such discourse models, we aim for automated generation of user interfaces."
      }
    ]
  },
  {
    "tid":"717",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fast Incremental Method for Nonconvex Optimization.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We analyze a fast incremental aggregated gradient method for optimizing nonconvex problems of the form $min_x sum_i f_i(x)$. Specifically, we analyze the SAGA algorithm within an Incremental First-order Oracle framework, and show that it converges to a stationary point provably faster than both gradient descent and stochastic gradient descent. We also discuss a Polyaku0027s special class of nonconvex problems for which SAGA converges at a linear rate to the global optimum. Finally, we analyze the practically valuable regularized and minibatch variants of SAGA. To our knowledge, this paper presents the first analysis of fast convergence for an incremental aggregated gradient method for nonconvex problems."
      }
    ]
  },
  {
    "tid":"718",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mathematical programs with equilibrium constraints (MPECs) in process engineering' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mathematical programs with equilibrium constraints (MPECs) form a relatively new and interesting subclass of nonlinear programming problems. In this paper we propose a novel method of solving MPECs by appropriate reformulation of the equilibrium conditions. The reformulation can be easily incorporated in a certain class of interior point algorithms for nonlinear optimization. The algorithm used in the study follows a primal-dual interior point approach and shows encouraging results on a test suite of MPECs. The algorithm is also able to perform optimization of distillation columns with phase changes and tray optimization using only continuous variables. We also consider a number of topics to improve performance of the algorithm and to identify classes of process engineering problems that can be handled as MPECs."
      }
    ]
  },
  {
    "tid":"719",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Combining multiple sets of rules for improving classification via measuring their closenesses' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a new method for measuring the closeness of multiple sets of rules that are combined using Dempster's rule of combination to improve classification performance. The closeness provides an insight into combining multiple sets of rules in classification - in what circumambience the performance of combinations of some sets of rules using Dempster's rule is better than that of others. Experiments have been carried out over the 20-newsgroups benchmark data collection, and the empirical results show that when the closeness between two sets of rules is higher than that of others, the performance of its combination using Dempster's rule is better than the others."
      }
    ]
  },
  {
    "tid":"720",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Resource Provision for Batch and Interactive Workloads in Data Centers' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we describe a scheduling framework that allocates resources to both batch jobs and interactive jobs simultaneously in a private cloud with a static amount of resources. In the system, every job has an individual service level agreement (SLA), and violating the SLA incurs penalty. We propose a model to formally quantify the SLA violation penalty of both batch and interactive jobs. The analysis on the interactive jobs focuses on queuing analysis and response time. The analysis on batch jobs focuses on the non-preemptive job scheduling for multiple processing units. Based on this model we also propose algorithms to estimate the penalty for both batch jobs and interactive jobs, and algorithms that reduce the total SLA violation penalty. Our experiment results suggest that our system effectively reduces the total penalty by allocating the right amount of resources to heterogeneous jobs in a private cloud system."
      }
    ]
  },
  {
    "tid":"721",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A two-tier Wireless Sensor Network infrastructure for large-scale real-time groundwater monitoring' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we describe the design, implementation and deployment details of a two-tier real-time environmental monitoring network in Nebraska. Our state-wide sensor network infrastructure uses typical Wireless Sensor Network (WSN) structure at tier-two to conduct dynamic sensing tasks with high resolution and flexibility. The satellite communication technology is used at tier-one to provide reliable and low-cost long-haul connectivity between each local WSN and the central base station. By the end of 2009, the entire tier-one infrastructure has been designed and deployed to provide state-wide wireless connectivity for 54 monitoring sites equipped with 1-4 water level transducers. A data and network management framework has also been developed to enable the remote network retasking and network performance analysis from a central base station. The network web portal offers integrated real-time and historical groundwater data to worldwide users. The deployed infrastructure will serve as the prototype network to expedite the commercial adoption of large-scale WSN design for long-term environmental monitoring applications."
      }
    ]
  },
  {
    "tid":"722",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Semantic video indexing and summarization using subtitles' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"How to build semantic index for multimedia data is an important and challenging problem for multimedia information systems. In this paper, we present a novel approach to build a semantic video index for digital videos by analyzing the subtitle files of DVD/DivX videos. The proposed approach for building semantic video index consists of 3 stages, viz., script extraction, script partition and script vector representation. First, the scripts are extracted from the subtitle files that are available in the DVD/DivX videos. Then, the extracted scripts are partitioned into segments. Finally, the partitioned script segments are converted into a tfidf vector based representation, which acts as the semantic index. The efficiency of the semantic index is demonstrated through video retrieval and summarization applications. Experimental results demonstrate that the proposed approach is very promising."
      }
    ]
  },
  {
    "tid":"723",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'SmartSLAM: localization and mapping across multi-environments' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In the absence of absolute localization tools such as GPS, a robot can still successfully navigate by conducting Simultaneous Localization and Mapping (SLAM). All SLAM algorithms to date can only be applied in one environment at a time. In this paper we propose to extend SLAM to multi-environments. In SmartSLAM the robot first classifies its entourage using environment recognition code and then performs SLAM using landmarks that are appropriate for its surrounding milieu. One thousand images of various indoor and outdoor environments were collected and used as training data for a three-layered feedforward backpropagation neural network. This neural network was then tested on two sets of query images of indoor environments and another two sets of outdoor environments, yielding 83% and 95% correct classification rates for the indoor images and 80% and 79% success rates for the outdoor images."
      }
    ]
  },
  {
    "tid":"724",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Computational data privacy in wireless networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Wireless networks appeal to privacy eavesdroppers that secretly capture packets from the open medium and read the metadata and data content in search of any type of information through data mining and statistical analysis. Addressing the problems of personal privacy for wireless users will require flexible privacy protection mechanisms adaptively to the frequently varying context. Privacy quantification is prerequisite for enabling a context-aware privacy protection in wireless networks. This paper proposes to quantify the data privacy during wireless communication processes. We give a computational quantification method of data privacy through introducing the concepts of privacy entropy and privacy joint entropy, which permits users and applications to on-demand customize their preferential sensitivity extent of privacy leakage and protection strength. Accordingly, we put forward a data-privacy protection scheme in order to explain how to utilize the proposed computational quantification method of data privacy against the excessive disclosure of data privacy. The results show that the computational quantitation method can effectively characterize the real-time fluctuation of data privacy during communication processes and provide the reliable judgment to context-aware privacy protection, which enables to real-timely control the present data privacy to an anticipated target and to balance a tradeoff between the data privacy and communication efficiency."
      }
    ]
  },
  {
    "tid":"725",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A modularized and distributed simulation environment for scalability analysis of smart grid ICT infrastructures' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Various ICT system architectures for Smart Metering, Demand Side Management and Distributed Energy Generation are currently being evaluated in numerous pilot projects, which analyze the applicability of wireless, wired and powerline technologies for managing decentralized demand and supply components on distribution network level. In order to analyze the scalability of the proposed ICT system architectures, a detailed simulation model and a comparison of different access and inhouse technologies in combination with a variety of traffic engineering approaches is necessary. Depending on the technology and use-cases, the upcoming Machine-to-Machine (M2M) traffic volume leads to an additional load, which can significantly lower the Quality of Service (QoS) of communication networks and affects the reaction rate provided by these network for both Smart Grid application and additional services. To analyze this influence, this paper introduces a distributed simulation environment for investigating the arising traffic volume caused by Smart Grid services in order to enable a detailed network analysis depending on the used technologies. Furthermore, the simulation environment provides the ability to generate large scaled scenarios based on a geographical database, administrate multiple channel models using a revision based repository and combine both features in an automatic, predefined simulation with auto-resolved dependencies using Web Services."
      }
    ]
  },
  {
    "tid":"726",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A privacy-preserving data collection model for digital community.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The widespread use of mobile devices in digital community has promoted the variety of data collecting methods. However, the privacy of individuals plays an important role in data processing or data transmission, and such information should be protected. In this paper, (\u03b1, k)-anonymity model, a widely used privacy-preserving model, is adopted as a security frame. Then, a privacy-preserving data collection model ((\u03b1, k))-CM based on (\u03b1, k)-anonymity is proposed and the threat model is analyzed. To resist the possible attack, we propose a generalization-encryption method to achieve a desired privacy level in (\u03b1, k)-CM. Generalization can decrease the data size and save the resource might induce information loss in data process; while encryption can decrease information loss, however, it can cause the waste of resource. Generalization-encryption method dynamically encrypts a portion of the data with maximum information loss and adjusts the portion to balance the trade-off metric in the process of generalization. Experimental results and theoretical analysis show that this method is effective in terms of privacy levels and data quality with low resource consumption."
      }
    ]
  },
  {
    "tid":"727",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Marker-controlled watershed segmentation of nuclei in H&E stained breast cancer biopsy images' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we present an unsupervised automatic method for segmentation of nuclei in H&E stained breast cancer biopsy images. Colour deconvolution and morphological operations are used to preprocess the images in order to remove irrelevant structures. Candidate nuclei locations, obtained with the fast radial symmetry transform, act as markers for a marker-controlled watershed segmentation. Watershed regions that are unlikely to represent nuclei are removed in the postprocessing stage. The proposed algorithm is evaluated on a number of images that are representative of the diversity in pathology in this type of tissue. The method shows good performance in terms of the number of segmented nuclei and segmentation accuracy."
      }
    ]
  },
  {
    "tid":"728",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Self-Organization of Tactile Receptive Fields: Exploring Their Textural Origin and Their Representational Properties' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In our earlier work, we found that feature space induced by tactile receptive fields (TRFs) are better than that by visual receptive fields (VRFs) in texture boundary detection tasks. This suggests that TRFs could be intimately associated with texture-like input. In this paper, we investigate how TRFs can develop in a cortical learning context. Our main hypothesis is that TRFs can be self-organized using the same cortical development mechanism found in the visual cortex, simply by exposing it to texture-like inputs (as opposed to natural-scene-like inputs). To test our hypothesis, we used the LISSOM model of visual cortical development. Our main results show that texture-like inputs lead to the self-organization of TRFs while natural-scene-like inputs lead to VRFs. These results suggest that TRFs can better represent texture than VRFs. We further analyzed the effectiveness of TRFs in representing texture, using kernel Fisher discriminant (KFD) and the results, along with texture classification performance, confirm that this is indeed the case. We expect these results to help us better understand the nature of texture, as a fundamentally tactile property."
      }
    ]
  },
  {
    "tid":"729",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title '3DFaceGAN: Adversarial Nets for 3D Face Representation, Generation, and Translation.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Over the past few years, Generative Adversarial Networks (GANs) have garnered increased interest among researchers in Computer Vision, with applications including, but not limited to, image generation, translation, imputation, and super-resolution. Nevertheless, no GAN-based method has been proposed in the literature that can successfully represent, generate or translate 3D facial shapes (meshes). This can be primarily attributed to two facts, namely that (a) publicly available 3D face databases are scarce as well as limited in terms of sample size and variability (e.g., few subjects, little diversity in race and gender), and (b) mesh convolutions for deep networks present several challenges that are not entirely tackled in the literature, leading to operator approximations and model instability, often failing to preserve high-frequency components of the distribution. As a result, linear methods such as Principal Component Analysis (PCA) have been mainly utilized towards 3D shape analysis, despite being unable to capture non-linearities and high frequency details of the 3D face\u2014such as eyelid and lip variations. In this work, we present 3DFaceGAN, the first GAN tailored towards modeling the distribution of 3D facial surfaces, while retaining the high frequency details of 3D face shapes. We conduct an extensive series of both qualitative and quantitative experiments, where the merits of 3DFaceGAN are clearly demonstrated against other, state-of-the-art methods in tasks such as 3D shape representation, generation, and translation."
      }
    ]
  },
  {
    "tid":"730",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Maximum likelihood decoding of QPSK signal in power line communications over Nakagami-m additive noise' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Power line communications is the use of power lines for the dual purpose of power transmission and data transmission. It is an emerging field of communication for the home area network of smart grid. Power line noise, namely background noise and impulsive noise, significantly affects the performance of power line communication systems. In this paper, we derive the condition for optimum detection of quadrature phase shift keying signals over Nakagami-m distributed additive background noise in power line communication system. The probability density function and the cumulative distribution function of the decision variables for the real part and imaginary part of the background noise are derived. We use the copula approach to model the dependence among the decision variables. The analytical average symbol error rate and average bit error rate of the PLC system is numerically computed. Simulations suggest that the proposed receiver performs significantly better than the existing suboptimal receiver."
      }
    ]
  },
  {
    "tid":"731",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Allia: alliance-based service discovery for ad-hoc environments' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Static directory based service discovery is unsuitable for m-commerce in ad-hoc environments. In this paper, we present Allia: a peer-to-peer caching based and policy-driven agent-service discovery framework to facilitate cross-platform service discovery in ad-hoc environments for mobile electronic commerce applications. Our approach removes the problems associated with structured compound formation of agent communities in mobile commerce environment and achieves high degree of flexibility in adapting itself to the changes of the ad-hoc environment. Our framework takes into consideration device capabilities and limitations, user preferences regarding usage of the devices, application specifics with respect to mobile commerce and adapts accordingly. We also present an initial implementation of Allia over ThinkPads and iPAQs by extending the LEAP Agent Platform and using Bluetooth as the underlying network protocol."
      }
    ]
  },
  {
    "tid":"732",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Understanding and protecting privacy: formal semantics and principled audit mechanisms' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Privacy has become a significant concern in modern society as personal information about individuals is increasingly collected, used, and shared, often using digital technologies, by a wide range of organizations. Certain information handling practices of organizations that monitor individuals' activities on the Web, data aggregation companies that compile massive databases of personal information, cell phone companies that collect and use location data about individuals, online social networks and search engines\u2014while enabling useful services\u2014have aroused much indignation and protest in the name of privacy. Similarly, as healthcare organizations are embracing electronic health record systems and patient portals to enable patients, employees, and business affiliates more efficient access to personal health information, there is trepidation that the privacy of patients may not be adequately protected if information handling practices are not carefully designed and enforced. Given this state of affairs, it is very important to arrive at a general understanding of (a) why certain information handling practices arouse moral indignation, what practices or policies are appropriate in a given setting, and (b) how to represent and enforce such policies using information processing systems. This article summarizes progress on a research program driven by goal (b). We describe a semantic model and logic of privacy that formalizes privacy as a right to appropriate flows of personal information\u2014a position taken by contextual integrity, a philosphical theory of privacy for answering questions of the form identified in (a). The logic is designed with the goal of enabling specification and enforcement of practical privacy policies. It has been used to develop the first complete formalization of two US privacy laws\u2014the HIPAA Privacy Rule that prescribes and proscribes flows of personal health information, and the Gramm-Leach-Bliley Act that similarly governs flows of personal financial information. Observing that preventive access control mechanisms are not sufficient to enforce such privacy policies, we develop two complementary audit mechanisms for policy enforcement. These mechanisms enable auditing of practical privacy policies, including the entire HIPAA Privacy Rule. The article concludes with a vision for further research in this area."
      }
    ]
  },
  {
    "tid":"733",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A review of routing protocols for mobile ad hoc networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The 1990s have seen a rapid growth of research interests in mobile ad hoc networking. The infrastructureless and the dynamic nature of these networks demands new set of networking strategies to be implemented in order to provide efficient end-to-end communication. This, along with the diverse application of these networks in many different scenarios such as battlefield and disaster recovery, have seen MANETs being researched by many different organisations and institutes. MANETs employ the traditional TCP/IP structure to provide end-to-end communication between nodes. However, due to their mobility and the limited resource in wireless networks, each layer in the TCP/IP model require redefinition or modifications to function efficiently in MANETs. One interesting research area in MANET is routing. Routing in the MANETs is a challenging task and has received a tremendous amount of attention from researches. This has led to development of many different routing protocols for MANETs, and each author of each proposed protocol argues that the strategy proposed provides an improvement over a number of different strategies considered in the literature for a given network scenario. Therefore, it is quite difficult to determine which protocols may perform best under a number of different network scenarios, such as increasing node density and traffic. In this paper, we provide an overview of a wide range of routing protocols proposed in the literature. We also provide a performance comparison of all routing protocols and suggest which protocols may perform best in large networks."
      }
    ]
  },
  {
    "tid":"734",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Random or Genetic Algorithm Search for Object-Oriented Test Suite Generation?' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Achieving high structural coverage is an important aim in software testing. Several search-based techniques have proved successful at automatically generating tests that achieve high coverage. However, despite the well- established arguments behind using evolutionary search algorithms (e.g., genetic algorithms) in preference to random search, it remains an open question whether the benefits can actually be observed in practice when generating unit test suites for object-oriented classes. In this paper, we report an empirical study on the effects of using a genetic algorithm (GA) to generate test suites over generating test suites incrementally with random search, by applying the EvoSuite unit test suite generator to 1,000 classes randomly selected from the SF110 corpus of open source projects. Surprisingly, the results show little difference between the coverage achieved by test suites generated with evolutionary search compared to those generated using random search. A detailed analysis reveals that the genetic algorithm covers more branches of the type where standard fitness functions provide guidance. In practice, however, we observed that the vast majority of branches in the analyzed projects provide no such guidance."
      }
    ]
  },
  {
    "tid":"735",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Provence: A Process Visualisation and Enactment Environment' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":" . A process-centered software development environment mustbe open and as non-intrusive as possible. Openness is important becausethe nature of software process work is experimental, making it difficultto determine the forms of assistance that are needed. An extensible architecturethat can incorporate new tools is more appropriate than onethat fixes its components and functionality. Providing process supportin a non-intrusive manner relieves organizations adopting process technologyfrom... "
      }
    ]
  },
  {
    "tid":"736",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Logical topology design for IP rerouting: ASONs versus static OTNs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"IP-based backbone networks are gradually moving to a network model consisting of high-speed routers that are flexibly interconnected by a mesh of light paths set up by an optical transport network that consists of wavelength division multiplexing (WDM) links and optical cross-connects. In such a model, the generalized MPLS protocol suite could provide the IP centric control plane component that will be used to deliver rapid and dynamic circuit provisioning of end-to-end optical light paths between the routers. This is called an automatic switched optical (transport) network (ASON). An ASON enables reconfiguration of the logical IP topology by setting up and tearing down light paths. This allows to up- or downgrade link capacities during a router failure to the capacities needed by the new routing of the affected traffic. Such survivability against (single) IP router failures is cost-effective, as capacity to the IP layer can be provided flexibly when necessary. We present and investigate a logical topology optimization problem that minimizes the total amount or cost of the needed resources (interfaces, wavelengths, WDM line-systems, amplifiers, etc.) in both the IP and the optical layer. A novel optimization aspect in this problem is the possibility, as a result of the ASON, to reuse the physical resources (like interface cards and WDM line-systems) over the different network states (the failure-free and all the router failure scenarios). We devised a simple optimization strategy to investigate the cost of the ASON approach and compare it with other schemes that survive single router failures."
      }
    ]
  },
  {
    "tid":"737",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Identifying Influential Nodes in Online Social Networks Using Principal Component Centrality' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Identifying the most influential nodes in social net- works is a key problem in social network analysis. However, without a strict definition of centrality the notion of what constitutes a central node in a network changes with application and the type of commodity flowing through a network. In this paper we identify social hubs, nodes at the center of influential neighborhoods, in massive online social networks using principal component centrality (PCC). We compare PCC with eigenvector centrality's (EVC), the de facto measure of node influence by virtue of their position in a network. We demonstrate PCC's performance by processing a friendship graph of 70,000 users of Google's Orkut social networking service and a gaming graph of 143, 020 users obtained from users of Facebook's 'Fighters' Club' application."
      }
    ]
  },
  {
    "tid":"738",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A New Strategy For Control Of Tandem Cold Metal Rolling' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The control of tandem cold rolling of metal strip constitutes a significant engineering challenge to control engineers. This is due mostly to the complexity of the interactions between the process variables, nonlinearities which change with the operating conditions, and long speed-dependent time delays. The present technology is based on control strategies that decompose the problem into several independent control sub-problems and as a result yield limited capability for improvement in performance. This paper describes a new control strategy wherein a state-dependent algebraic Riccati equation is solved pointwise to establish a control law for a multi-input-multi-output (MINO) controller with appropriate trimming functions. The new controller coupled to a nonlinear model of the process was tested by simulations. The resulting tolerance in mill exit thickness was held to about .2% for typical rolling operations, which compares favorably to the tolerances of .5% to .8% using existing controllers. In addition to improved performance, the simplicity of the new control method reduces the efforts required for design, commissioning, and maintenance."
      }
    ]
  },
  {
    "tid":"739",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Programming urban-area applications' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The evolution of smartphones has given rise to urban-area applications: applications that communicate in a city by means of the public (moving) infrastructure, e.g. buses and trams. In this setting, applications need to communicate and discover each other using intermediaries that move around the city and transfer data between them. This requires programmers to scatter code that deals with routing messages to the correct place and dealing with network failures all over their programs. Our approach allows the programmer to specify urban-area applications in a high-level manner without the burden of directly encoding communication using intermediaries. We present this as a translation from a high-level object-oriented programming paradigm to a low-level communication mechanism."
      }
    ]
  },
  {
    "tid":"740",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Generative adversarial classifier for handwriting characters super-resolution' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"\u2022cylinder primitive extraction is addressed as a clustering problem of possible cylinder candidates;\u2022candidates are generated by cutting the scene through several random planes;\u2022a novel cylinder similarity function based on dual quaternion algebra is defined;\u2022the clustering process, based on Game Theory, inherently discards incompatible candidates;\u2022the proposed method can deal with noisy, partially occluded and non-oriented point clouds."
      }
    ]
  },
  {
    "tid":"741",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Advanced mobility management for reduced interference and energy consumption in the two-tier LTE-Advanced network.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Femtocell deployment will play a key role for the wide adoption of LTE-Advanced, as it brings the access network closer to the end user in a cost-effective manner. This disruptive communication paradigm, however, necessitates the employment of advanced interference and mobility management to cope with the comparably denser yet unplanned network layout. This paper describes an advanced mobility management approach for the two-tier LTE-Advanced network, aiming to resourcefully utilize the femtocell superior characteristics in an energy-efficient and interference-aware manner. The key features of the proposed approach are (a) the exchange and utilization of standard signal quality measurements during the handover decision phase, to accurately estimate the mean user equipment (UE) transmit power on a per candidate cell basis, and (b) the use of a novel handover decision algorithm that jointly considers the impact of interference, power consumption, and user mobility. A comprehensive analysis of the required network signaling is provided, while extensive simulation results demonstrate that compared to existing approaches, the proposed approach attains improved performance at the cost of moderate increase of network signaling."
      }
    ]
  },
  {
    "tid":"742",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An improved teaching-learning-based optimization with neighborhood search for applications of ANN' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Teaching-learning-based optimization (TLBO) algorithm, which simulates the teaching-learning process of the class room, is one of the recently proposed swarm intelligent (SI) algorithms. The performance of TLBO is maintained by the teaching and learning process, but when the learners cannot found a better position than the old one at some successive iteration, the population might be trapped into local optima. In this paper, an improved teaching-learning-based optimization algorithm with neighborhood search (NSTLBO) is presented. In the proposed method, a ring neighborhood topology is introduced into the original TLBO algorithm to maintain the exploration ability of the population. Different than the traditional method to utilize the global information, the mutation of each learner is now restricted within a certain neighboring area so as to fully utilize the whole space and avoid over-congestion around local optima. Moreover, a mutation operation is presented to NSTLBO during the duplicate eliminations in order to maintain the diversity of population. To verify the performance of the proposed algorithm, thirty-two benchmark functions are utilized. Finally, three application problems of artificial neural network are examined. The results in thirty-two benchmark functions and three applications of ANN indicate that the proposed algorithm has shown interesting outcomes."
      }
    ]
  },
  {
    "tid":"743",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Exploring IBA Design Space for Improved Performance' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"InfiniBand architecture (IBA) is envisioned to be the default communication fabric for future system area networks (SANs) or clusters. However, IBA design is currently in its infancy since the released specification outlines only higher level functionalities, leaving it open for exploring various design alternatives. In this paper, we investigate four corelated techniques for providing high and predictable performance in IBA. These are: 1) using the shortest path first (SPF) algorithm for deterministic packet routing, 2) developing a multipath routing mechanism for minimizing congestion, 3) developing a selective packet dropping scheme to handle deadlock and congestion, and 4) providing multicasting support for customized applications. These designs are implemented in a pipelined, IBA-style switch architecture, and are evaluated using an integrated workload consisting of MPEG-2 video streams, best- effort traffic, and control traffic on a versatile IBA simulation testbed. Simulation results with 15-node and 30-node irregular networks indicate that the SPF routing, multipath routing, packet dropping, and multicasting schemes are quite effective in delivering high and assured performance in clusters"
      }
    ]
  },
  {
    "tid":"744",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Modeling the UMLS using an OODB.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The Unified Medical Language System combines many well established authoritative medical informatics terminologies in one system. Such a resource is very valuable to the healthcare industry. However, the UMLS is very large and complex and poses serious comprehension problems for users and maintenance personnel. Furthermore, the sets of concepts of semantic types are not semantically uniform and thus are difficult to study. We describe a method to represent two components of the UMLS, the Metathesaurus (META) and the Semantic Network, as an OODB. The resulting UMLS OODB schema is deeper and more refined than the Semantic Network. It offers semantically uniform classes, which improves support for comprehension and navigation of META. The UMLS OODB also exposes problems in the semantic type classifications."
      }
    ]
  },
  {
    "tid":"745",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Power consumption and processing models of servers in computation and storage based applications.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The electric power consumption of information systems has to be reduced in addition to increasing the performance and fault tolerance in order to realize green societies. Here, servers mainly consume electric power to perform processes requested by clients compared with clients. We consider computation (CP) and storage (ST) types of application processes on servers. In CP processes, CPU is mainly consumed as in scientific computation. In ST processes, files in storage drives are mainly manipulated. First, we present the storage and computation based power consumption (SCBPC) model of a server to perform ST and CP processes. In the SCBPC model, the power consumption rate of a server depends on the types of processes concurrently performed and is independent of the number of processes. Then, we discuss the storage and computation based processing (SCBP) model to perform CP and ST processes on a server. Here, the execution times of a CP process and an ST process depend on the number of CP processes and the number of CP and ST processes concurrently performed, respectively. By using the SCBPC and SCBP models, we discuss an energy-aware (EA) algorithm to select a server for a process so that not only the execution time but also the power consumption can be reduced. We evaluate the EA algorithm in terms of total power consumption of servers and total execution time of processes. We show not only the total power consumption but also the total execution time can be reduced in the EA algorithm."
      }
    ]
  },
  {
    "tid":"746",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Theory and algorithms for hop-count-based localization with random geometric graph models of dense sensor networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Wireless sensor networks can often be viewed in terms of a uniform deployment of a large number of nodes in a region of Euclidean space. Following deployment, the nodes self-organize into a mesh topology with a key aspect being self-localization. Having obtained a mesh topology in a dense, homogeneous deployment, a frequently used approximation is to take the hop distance between nodes to be proportional to the Euclidean distance between them. In this work, we analyze this approximation through two complementary analyses. We assume that the mesh topology is a random geometric graph on the nodes; and that some nodes are designated as anchors with known locations. First, we obtain high probability bounds on the Euclidean distances of all nodes that are h hops away from a fixed anchor node. In the second analysis, we provide a heuristic argument that leads to a direct approximation for the density function of the Euclidean distance between two nodes that are separated by a hop distance h. This approximation is shown, through simulation, to very closely match the true density function. Localization algorithms that draw upon the preceding analyses are then proposed and shown to perform better than some of the well-known algorithms present in the literature. Belief-propagation-based message-passing is then used to further enhance the performance of the proposed localization algorithms. To our knowledge, this is the first usage of message-passing for hop-count-based self-localization."
      }
    ]
  },
  {
    "tid":"747",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Knowledge diversity and firm performance: an ecological view.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Purpose - Knowledge has been considered a crucial organizational asset for gaining competitive advantages. It is critical for a firm to maintain a knowledge composition that is productive. This study aims to examine the applicability of the diversity-stability principle in ecology to knowledge management and further investigate the impact of knowledge diversity on firm performance. Design/methodology/approach - A theoretical framework for knowledge diversity and firm performance is proposed; a questionnaire survey was conducted to evaluate the research framework. Fifty-eight valid responses from experts were collected to measure knowledge strength and diversity of 20 enterprises in four industries, and financial indexes of the 20 enterprises from 2008 to 2012 were collected to analyze the research model. Findings - The results show that higher information technology (IT) capabilities in a firm lead to higher levels of knowledge strength and diversity. The strength and diversity of knowledge in a company can improve average company performance and reduce performance variations. Research limitations/implications - This paper presents a new perspective that applies the ecological concept of diversity to examine the value of knowledge in organizations. The findings expand our understanding of the role of IT and knowledge in organizational performance. A limitation is that the sample size is relatively small, which may limit the generalizability of the findings. Practical implications - CEOs and chief knowledge officers can apply the findings herein to assess their organizational knowledge profiles and maintain a healthy knowledge ecology in strategic planning. They should be aware that both knowledge strength and knowledge diversity are crucial to the stability of firm performance. Originality/value - The ecological view of knowledge management stresses the importance of maintaining a healthy intensity and diversity of knowledge at the macro level and indicates a new direction for knowledge management."
      }
    ]
  },
  {
    "tid":"748",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Investigation into Neural Net Optimization via Hessian Eigenvalue Density.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"To understand the dynamics of optimization in deep neural networks, we develop a tool to study the evolution of the entire Hessian spectrum throughout the optimization process. Using this, we study a number of hypotheses concerning smoothness, curvature, and sharpness in the deep learning literature. We then thoroughly analyze a crucial structural feature of the spectra: in non-batch normalized networks, we observe the rapid appearance of large isolated eigenvalues in the spectrum, along with a surprising concentration of the gradient in the corresponding eigenspaces. In batch normalized networks, these two effects are almost absent. We characterize these effects, and explain how they affect optimization speed through both theory and experiments. As part of this work, we adapt advanced tools from numerical linear algebra that allow scalable and accurate estimation of the entire Hessian spectrum of ImageNet-scale neural networks; this technique may be of independent interest in other applications."
      }
    ]
  },
  {
    "tid":"749",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'BER Analysis of Uplink OFDMA in the Presence of Carrier Frequency and Timing Offsets on Rician Fading Channels.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In uplink orthogonal frequency division multiple access (OFDMA), carrier frequency offsets (CFO) and/or timing offsets (TO) of other users with respect to a desired user can cause significant multiuser interference (MUI). In this paper, we derive an analytical bit error rate (BER) expression that quantify the degradation in BER due to the combined effect of both CFOs and TOs in uplink OFDMA on Rician fading channels. Such an analytical BER derivation for uplink OFDMA with CFOs and TOs on Rician fading channels has not been reported so far. For the case of non-zero CFOs/TOs, we obtain an approximate BER expression involving a single integral. Analytical and simulation BER results are shown to match very well."
      }
    ]
  },
  {
    "tid":"750",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fast and scalable approximate spectral graph matching for correspondence problems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Establishing consistent correspondences between two sets of features is a fundamental problem in computer vision. This problem can be well formulated as graph matching in which nodes and edges represent feature points and pairwise relationships between feature points, respectively. Spectral matching [19] is the state-of-the-art eigenvector-based method for graph matching. The spectral matching algorithm has been used successfully for small data, but its heavy memory requirement limited the maximum data sizes and contexts it can be used. In this paper, we propose FaSM, a fast and scalable approximate spectral matching algorithm. The main ideas are twofold. First, we exploit the redundancy in the data generation process to approximate the affinity matrix with the linear combination of Kronecker products between bases and index matrices. The bases and index matrices are highly compressed representation of the approximated affinity matrix, requiring much smaller memory than in previous works which store the whole affinity matrix. Second, we compute the eigenvector of the approximated affinity matrix using the small bases and index matrices without explicitly materializing the approximated matrix. Experimental results show that our proposed method is up to 33x faster, requiring up to 645x smaller memory than the exact algorithm, with little or no loss of accuracy."
      }
    ]
  },
  {
    "tid":"751",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Adaptive Cost-sensitive Online Classification.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cost-Sensitive Online Classification has drawn extensive attention in recent years, where the main approach is to directly online optimize two well-known cost-sensitive metrics: (i) weighted sum of sensitivity and specificity and (ii) weighted misclassification cost. However, previous existing methods only considered first-order information of data stream. It is insufficient in practice, since many recent studies have proved that incorporating second-order information enhances the prediction performance of classification models. Thus, we propose a family of cost-sensitive online classification algorithms with adaptive regularization in this paper. We theoretically analyze the proposed algorithms and empirically validate their effectiveness and properties in extensive experiments. Then, for better trade off between the performance and efficiency, we further introduce the sketching technique into our algorithms, which significantly accelerates the computational speed with quite slight performance loss. Finally, we apply our algorithms to tackle several online anomaly detection tasks from real world. Promising results prove that the proposed algorithms are effective and efficient in solving cost-sensitive online classification problems in various real-world domains."
      }
    ]
  },
  {
    "tid":"752",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Ghost cancellation of analog TV signals: with applications to IDTV, EDTV, and HDTV' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Techniques that can cancel ghosts in received analog TV (for improved-definition TV, extended-definition TV, and high-definition TV) signals are presented. The fact that there are short periods of time without the analog signal (the horizontal flyback interval between the lines) is utilized to periodically cleanse a finite impulse response (FIR) or an infinite impulse response (IIR) equalizer. This line-by-line processing (cleansing) overcomes the limitation of standard equalizers to allow for 40-50 dB of suppression of ghosts, even with nulls in the spectrum, as long as the ghost delay is less than the period of time without the analog signal. Furthermore, by using time inversion in combination with line-by-line processing, the stability problem of the conventional IIR equalizer can be eliminated. It is shown that it may be possible to implement this IIR equalizer on a single digital integrated circuit. Alternatively, an FIR equalizer can be used; although it requires multiple chips (i.e. more taps), it can acquire and adapt to the ghosted channel more rapidly than an IIR equalizer."
      }
    ]
  },
  {
    "tid":"753",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Automated verification of security policies in mobile code' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper describes an approach for the automated verification of mobile programs. Mobile systems are characterized by the explicit notion of locations (e.g., sites where they run) and the ability to execute at different locations, yielding a number of security issues.We give formal semantics to mobile systems as Labeled Kripke Structures, which encapsulate the notion of the location net. The location net summarizes the hierarchical nesting of threads constituting a mobile program and enables specifying security policies. We formalize a language for specifying security policies and show how mobile programs can be exhaustively analyzed against any given security policy by using model checking techniques. We developed and experimented with a prototype framework for analysis of mobile code, using the SATABS model checker. Our approach relies on SATABS's support for unbounded thread creation and enhances it with location net abstractions, which are essential for verifying large mobile programs. Our experimental results on various benchmarks are encouraging and demonstrate advantages of the model checking-based approach, which combines the validation of security properties with other checks, such as for buffer overflows."
      }
    ]
  },
  {
    "tid":"754",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Human action categories using motion descriptors' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we recognize human action based on an improved BOW model and latent topic model. We proposed an improved motion descriptor to build our bag of words, which is called the local spatial-temporal maximum value of optical flow. We force similar local features that appear in different positions on the image grid to be assigned to different visual words. This approach assigns the spatial information to each visual word. Then, we use the topic model of pLSA (probabilistic Latent Semantic Analysis) to classify. Our approach is tested on two datasets, the KTH datasets and WEIZMANN datasets. The result shows our method is effective."
      }
    ]
  },
  {
    "tid":"755",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Social Trust Path Recommendation System In Contextual Online Social Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Online Social Network (OSN) is becoming increasingly popular and being used as the means for a variety of activities, where trust is one of the most important factors for participants decision making. This demands the evaluation of the trustworthiness between two unknown participants along a certain social path between them in OSNs. This paper presents a social trust path recommendation system which allows a user to find the optimal social trust path between two participants in OSN with the state-of-the-art path selection method. In addition, users could specify their preferences of social contexts including social relationships and social trust between participants and social positions of participants in path selection. This recommendation system provides three types of social trust paths to evaluate the trustworthiness of the unknown participants effectively. i.e., the shortest path, the path with the maximal trust values (denoted as Max T) and the optimal social trust path, which can be used in many applications like to help an employee find potential trustworthy employees or to help a retailer to find trustworthy loyal customers."
      }
    ]
  },
  {
    "tid":"756",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Speech recognition under noisy environments using multiple microphones based on asynchronous and intermittent measurements.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose a robust speech recognition method under noisy environments using multiple microphones based on asynchronous and intermittent observation. In asynchronous and intermittent observation, the noise spectrum is estimated by the environmental noise observed in fragments from multiple microphones, and spectral subtraction is performed by this estimated noise spectrum. In this paper, we consider the case of estimating the noise spectrum from the noise observed by another microphone just before speech input. However, the noise spectrum needs to be compensated because of the difference in the location of the microphone in this case. Then, we examined compensating the noise spectrum by using the estimated LSFL on the log spectrum. By compensating the noise spectrum, the recognition rate improved compared with the case without compensation."
      }
    ]
  },
  {
    "tid":"757",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An expert system to generate associativity data for layout design' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we present an intelligent system which assists the layout designer in producing associativity data as input to an automated layout generation tool. The approach which we adopt combines techniques based on expert systems, object-based data structures and cluster analysis. The system eliminates manual input for the associativity data. The object data structure assures data consistency. The cluster analysis determines the strength of relationship between any two pieces of equipment. The expert system provides guidance for the subjective part of the layout design. This results in automation of associativity data generation, an improved user interface, and consistency and accuracy of data. The concept is illustrated by an example taken from the petrochemical industry."
      }
    ]
  },
  {
    "tid":"758",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Novel Collaborative Virtual Reality Game For Children With Asd To Foster Social Interaction' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Children with Autism spectrum disorders (ASD) often suffer from deficits in communication and social interaction, which lead to various social challenges in interacting with peers in collaborative tasks. The application of Collaborative Virtual Environment (CVE) technology in ASD intervention brings advantages in providing a safe, flexible, and collaborative environment. This paper proposes and describes the development of a novel distributed CVE system for playing a series of collaborative games using hand movement that is tracked in real-time via cameras. These games aim to positively impact the social interaction of users. A usability study indicated potential of this system in fostering collaboration and communication skills among children with and without ASD."
      }
    ]
  },
  {
    "tid":"759",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Robust and Rapid Adaption for Concept Drift in Software System Anomaly Detection' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Anomaly detection is critical for web-based software systems. Anecdotal evidence suggests that in these systems, the accuracy of a static anomaly detection method that was previously ensured is bound to degrade over time. It is due to the significant change of data distribution, namely concept drift, which is caused by software change or personal preferences evolving. Even though dozens of anomaly detectors have been proposed over the years in the context of software system, they have not tackled the problem of concept drift. In this paper, we present a framework, StepWise, which can detect concept drift without tuning detection threshold or per-KPI (Key Performance Indicator) model parameters in a large scale KPI streams, take external factors into account to distinguish the concept drift which under operators' expectations, and help any kind of anomaly detection algorithm to handle it rapidly. For the prototype deployed in Sogou, our empirical evaluation shows StepWise improve the average F-score by 206% for many widely-used anomaly detectors over a baseline without any concept drift detection."
      }
    ]
  },
  {
    "tid":"760",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Collection-based compound noun segmentation for Korean information retrieval' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Compound noun segmentation is a key first step in language processing for Korean. Thus far, most approaches require some form of human supervision, such as pre-existing dictionaries, segmented compound nouns, or heuristic rules. As a result, they suffer from the unknown word problem, which can be overcome by unsupervised approaches. However, previous unsupervised methods normally do not consider all possible segmentation candidates, and/or rely on character-based segmentation clues such as bi-grams or all-length n-grams. So, they are prone to falling into a local solution. To overcome the problem, this paper proposes an unsupervised segmentation algorithm that searches the most likely segmentation result from all possible segmentation candidates using a word-based segmentation context. As word-based segmentation clues, a dictionary is automatically generated from a corpus. Experiments using three test collections show that our segmentation algorithm is successfully applied to Korean information retrieval, improving a dictionary-based longest-matching algorithm."
      }
    ]
  },
  {
    "tid":"761",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Modeling and analysis of cross-session search tasks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The information needs of search engine users vary in complexity, depending on the task they are trying to accomplish. Some simple needs can be satisfied with a single query, whereas others require a series of queries issued over a longer period of time. While search engines effectively satisfy many simple needs, searchers receive little support when their information needs span session boundaries. In this work, we propose methods for modeling and analyzing user search behavior that extends over multiple search sessions. We focus on two problems: (i) given a user query, identify all of the related queries from previous sessions that the same user has issued, and (ii) given a multi-query task for a user, predict whether the user will return to this task in the future. We model both problems within a classification framework that uses features of individual queries and long-term user search behavior at different granularity. Experimental evaluation of the proposed models for both tasks indicates that it is possible to effectively model and analyze cross-session search behavior. Our findings have implications for improving search for complex information needs and designing search engine features to support cross-session search tasks."
      }
    ]
  },
  {
    "tid":"762",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Supervised representation learning for multi-label classification' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Representation learning is one of the most important aspects of multi-label learning because of the intricate nature of multi-label data. Current research on representation learning either fails to consider label knowledge or is affected by the lack of labeled data. Moreover, most of them learn the representations and incorporate the label information in a two-step manner. In this paper, due to the success of representation learning by deep learning we propose a novel framework based on neural networks named SERL to learn global feature representation by jointly considering all labels in an effective supervised manner. At its core, a two-encoding-layer autoencoder, which can utilize labeled and unlabeled data, is adopted to learn feature representation in the supervision of softmax regression. Specifically, the softmax regression incorporates label knowledge to improve the performance of both representation learning and multi-label learning by being jointly optimized with the autoencoder. Moreover, the autoencoder is expanded into two encoding layers to share knowledge with the softmax regression by sharing the second encoding weight matrix. We conduct extensive experiments on five real-world datasets to demonstrate the superiority of SERL over other state-of-the-art multi-label learning approaches."
      }
    ]
  },
  {
    "tid":"763",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A robust and flexible test environment for VoIP security tests' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Voice over IP (VoIP) is in wide use today, replacing phone lines in many scenarios. However, often, security isn't considered well enough, even though many security attacks are already known. More research on VoIP security is needed to enhance the level of security of VoIP systems and to show the implications of failing to take appropriate security measures. This paper presents an architecture and implementation of a robust and flexible VoIP test environment for security related tests. Experiences using the implemented environment for different VoIP security tests are shown to demonstrate the suitability of the proposed test environment for research purposes."
      }
    ]
  },
  {
    "tid":"764",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Socially-Optimal Online Spectrum Auctions For Secondary Wireless Communication' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Spectrum auctions are efficient mechanisms for licensed users to relinquish their under-utilized spectrum to secondary links for monetary remuneration. Truthfulness and social welfare maximization are two natural goals in such auctions, but cannot be achieved simultaneously with polynomial-time complexity by existing methods, even in a static network with fixed parameters. The challenge escalates in practical systems with QoS requirements and volatile traffic demands for secondary communication. Online, dynamic decisions are required for rate control, channel evaluation/bidding, and packet dropping at each secondary link, as well as for winner determination and pricing at the primary user. This work proposes an online spectrum auction framework with cross-layer decision making and randomized winner determination on the fly. The framework is truthful-in-expectation, and achieves close-to-offline-optimal time-averaged social welfare and individual utilities with polynomial time complexity. A new method is introduced for online channel evaluation in a stochastic setting. Simulation studies further verify the efficacy of the proposed auction in practical scenarios."
      }
    ]
  },
  {
    "tid":"765",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Determination of nonprototypical valence and arousal in popular music: features and performances' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mood of Music is among the most relevant and commercially promising, yet challenging attributes for retrieval in large music collections. In this respect this article first provides a short overview on methods and performances in the field. While most past research so far dealt with low-level audio descriptors to this aim, this article reports on results exploiting information on middle-level as the rhythmic and chordal structure or lyrics of a musical piece. Special attention is given to realism and nonprototypicality of the selected songs in the database: all feature information is obtained by fully automatic preclassification apart from the lyrics which are automatically retrieved from on-line sources. Further more, instead of exclusively picking songs with agreement of several annotators upon perceived mood, a full collection of 69 double CDs, or 2 648 titles, respectively, is processed. Due to the severity of this task; different modelling forms in the arousal and valence space are investigated, and relevance per feature group is reported."
      }
    ]
  },
  {
    "tid":"766",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An automated imaging algorithm for macula detection in fundus images' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The Macula is an important part of the retina of human eye which is responsible for sharp central vision. Accurate and automatic detection of macula from fundus images is an essential step to develop automated screening tool for ocular pathologies. The proposed work presents an imaging method for detection of macula in fundus images automatically. The proposed method includes a strategic windowing based approach for accurate detection of macula. Instead of searching macula from whole fundus image, a search region is considered with the help of optic disc and then macula is detected from that search region using double windowing based method. Normal and affected fundus images from a local eye hospital were used to test the performance of proposed method and achieved encouraging results. The proposed method of macula detection is accurate, computationally cheap and hence can be helpful in real time automated screening of various eye diseases."
      }
    ]
  },
  {
    "tid":"767",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Improving QoS and Utilisation in modern multi-core servers with Dynamic Cache Partitioning.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Co-execution of multiple workloads in modern multi-core servers may create severe performance degradation and unpredictable execution behavior, impacting significantly their Quality of Service (QoS) levels. To safeguard the QoS levels of high priority workloads, current resource allocation policies are quite conservative, disallowing their co-execution with low priority ones, creating a wasteful tradeoff between QoS and aggregated system throughput. In this paper we utilise the cache monitoring and allocation facilities provided by modern processors and implement a dynamic cache partitioning scheme, where high-priority workloads are monitored and allocated the amount of shared cache that they actually need. This way, we are able to simultaneously maintain their QoS very close to the levels of full cache allocation and boost the systemu0027s throughput by allocating the surplus cache space to co-executing, low-priority applications."
      }
    ]
  },
  {
    "tid":"768",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Centralized and Distributed Energy Efficiency Designs in Wireless Backhaul HetNets.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper studies the joint design of downlink transmit beamforming and power allocation in two-tier wireless backhaul small cell heterogeneous networks. We consider reverse time division duplexing combined with equal spectrum splitting between two tiers for interference mitigation. We formulate a constrained optimization problem with the objective of maximizing the proposed access energy efficie..."
      }
    ]
  },
  {
    "tid":"769",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Online Packet Dispatching for Delay Optimal Concurrent Transmissions in Heterogeneous Multi-RAT Networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we consider the problem of concurrent transmissions in a wireless network consisting of multiple radio access technologies (multi-RATs). That is, a single flow of packets is dispatched over multiple RATs so that the complementary advantages of different RATs can be exploited. One of the challenging issues arising in concurrent transmissions is the packet out-of-order problem due to ..."
      }
    ]
  },
  {
    "tid":"770",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Human Reappearance Detection Based On On-Line Learning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many video surveillance applications require detecting human reappearances in a scene monitored by a camera or over a network of cameras. This is the human reappearance detection (HRD) problem. Studying this problem is important for analyzing a surveillance scenario at semantic level. In this paper, we propose a novel online learning framework for solving HRD problem. Both generative model and discriminative model are employed in this framework and a voting scheme is presented to fuse the decisions of both models for determining whether a just entered person is one of those who have shown up, i.e. whether a reappearance happens. Both models will be updated based on mistake-driven online learning strategy. Our experimental results show that the adopted online learning framework not only improves the reappearance detection accuracy but also achieves high robustness in various surveillance scenes."
      }
    ]
  },
  {
    "tid":"771",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Parameterized Complexity of Superstring Problems.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In the  problem we are given a set of strings  and integer  and the question is to decide whether there is a superstring  of length at most  containing all strings of  as substrings. We obtain several parameterized algorithms and complexity results for this problem. In particular, we give an algorithm which in time  finds a superstring of length at most  containing at least  strings of . We complement this by a lower bound showing that such a parameterization does not admit a polynomial kernel up to some complexity assumption. We also obtain several results about \u201cbelow guaranteed values\u201d parameterization of the problem. We show that parameterization by compression admits a polynomial kernel while parameterization \u201cbelow matching\u201d is hard."
      }
    ]
  },
  {
    "tid":"772",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Learning Large Euclidean Margin for Sketch-based Image Retrieval.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper addresses the problem of Sketch-Based Image Retrieval (SBIR), for which bridge the gap between the data representations of sketch images and photo images is considered as the key. Previous works mostly focus on learning a feature space to minimize intra-class distances for both sketches and photos. In contrast, we propose a novel loss function, named Euclidean Margin Softmax (EMS), that not only minimizes intra-class distances but also maximizes inter-class distances simultaneously. It enables us to learn a feature space with high discriminability, leading to highly accurate retrieval. In addition, this loss function is applied to a conditional network architecture, which could incorporate the prior knowledge of whether a sample is a sketch or a photo. We show that the conditional information can be conveniently incorporated to the recently proposed Squeeze and Excitation (SE) module, lead to a conditional SE (CSE) module. Extensive experiments are conducted on two widely used SBIR benchmark datasets. Our approach, although being very simple, achieved new state-of-the-art on both datasets, surpassing existing methods by a large margin."
      }
    ]
  },
  {
    "tid":"773",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Action selection for active and cooperative global localization based on localizability estimation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we investigate the action selection problem for multiple mobile robots active and cooperative global localization in large environments. Firstly, an off-line localizability estimation approach using known probabilistic grid map (PGM) is briefly proposed and a localizability matrix is presented to describe properties of expected localization probability distribution (LPD). Based on the proposed localizability matrix, a novel on-line action selection mechanism is further presented which enables mobile robots to actively collaborate with each other and select complementary actions with redundant information exclusion. This action selection mechanism brings the lowest expected localization covariance in each localization cycle and can accelerate the converging speed of global localization. Efficiency in time complexity also ensures that the proposed method could be used in real applications. The appropriateness of our approach is demonstrated through simulations and real experiments."
      }
    ]
  },
  {
    "tid":"774",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An adaptive in-network aggregation operator for query processing in wireless sensor networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A wireless sensor network (WSN) is composed of tens or hundreds of spatially distributed autonomous nodes, called sensors. Sensors are devices used to collect data from the environment related to the detection or measurement of physical phenomena. In fact, a WSN consists of groups of sensors where each group is responsible for providing information about one or more physical phenomena (e.g., group for collecting temperature data). Sensors are limited in power, computational capacity, and memory. Therefore, a query engine and query operators for processing queries in WSNs should be able to handle resource limitations such as memory and battery life. Adaptability has been explored as an alternative approach when dealing with these conditions. Adaptive query operators (algorithms) can adjust their behavior in response to specific events that take place during data processing. In this paper, we propose an adaptive in-network aggregation operator for query processing in sensor nodes of a WSN, called ADAGA (ADaptive AGgregation Algorithm for sensor networks). The ADAGA adapts its behavior according to memory and energy usage by dynamically adjusting data-collection and data-sending time intervals. ADAGA can correctly aggregate data in WSNs with packet replication. Moreover, ADAGA is able to predict non-performed detection values by analyzing collected values. Thus, ADAGA is able to produce results as close as possible to real results (obtained when no resource constraint is faced). The results obtained through experiments prove the efficiency of ADAGA."
      }
    ]
  },
  {
    "tid":"775",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Joint Turbo Decoding for Overloaded MIMO-OFDM Systems.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"To achieve an excellent error-correcting capability, the Third-Generation Partnership Project (3GPP) Long-Term Evolution (LTE) employs turbo codes as its forward error correction (FEC) standard. In addition, to achieve higher throughput, the LTE also implements multiple-input-multiple-output (MIMO) systems. Although a conventional turbo decoding scheme gives satisfactory performance in ideal MIMO ..."
      }
    ]
  },
  {
    "tid":"776",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On the Design of Cognitive-Radio-Inspired Asymmetric Network Coding Transmissions in MIMO Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a cognitive-radio-inspired asymmetric network coding (CR-AsNC) scheme is proposed for multiple-input-multiple-output (MIMO) cellular transmissions, where information exchange among users and base-station (BS) broadcasting can be accomplished simultaneously. The key idea is to apply the concept of cognitive radio (CR) in network coding transmissions, where the BS tries sending new information while helping users' transmissions as a relay. In particular, we design an asymmetric network coding method for information exchange between the BS and the users, although many existing works consider the design of network coding in symmetric scenarios. To approach the optimal performance, an iterative precoding design for CR-AsNC is first developed. Then, a channel-diagonalization-based precoding design with low complexity is proposed, to which power allocation can be optimized with a closed-form solution. The simulation results show that the proposed CR-AsNC scheme with precoding optimization can significantly improve system transmission performance."
      }
    ]
  },
  {
    "tid":"777",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Joint power allocation and subcarrier selection for energy efficiency maximization in OFDM systems under a holistic power model' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Energy efficiency (EE) maximization for OFDM transceiver has received much attention in next generation wireless communication systems. Recently, there have been a lot of works on EE maximization by balancing the transmission rate and the holistic power consumption. However, how to maximize EE in OFDM systems by joint power allocation and subcarrier selection under holistic power models has not been extensively investigated yet. In contrast to spectrum efficiency (SE) oriented power allocation and subcarrier selection, which is by a simple waterfilling policy, the EE oriented power allocation and subcarrier selection is a mixed integer programming, which is not trivial to solve. This is simply because an extra circuit power has to be paid as the cost of adding an subcarrier. Fortunately, by delving into the mixed integer programming, we found in this paper that the EE oriented power allocation also has form of waterfilling. Based on these results and approximate methods, explicit decision criterions are proposed for subcarrier selection, which provides insight into how the transmit power and the number of subcarriers balance with the channel states and the circuit power in EE oriented systems. Numerical results show that the proposed strategies achieve near-optimal performance of EE with low complexity."
      }
    ]
  },
  {
    "tid":"778",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Cloud-Based Mobile System For Biometrics Authentication' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper introduces a new framework to perform handwritten password authentication as an Internet service based on the cloud computing technology. Using the proposed cloud-based authentication platform, we would be able to apply several advanced practical algorithms, such as k-nearest neighbor, and artificial neural network classifier used for large-scale character recognition. The classifier algorithm uses parallel classifier combination method in order to achieve satisfying accuracy for both recognition and error rate."
      }
    ]
  },
  {
    "tid":"779",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Feature Space Analysis for Human Activity Recognition in Smart Environments' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Activity classification from smart environment data is typically done employing ad hoc solutions customised to the particular dataset at hand. In this work we introduce a general purpose collection of features for recognising human activities across datasets of different type, size and nature. The first experimental test of our feature collection achieves state of the art results on well known datasets, and we provide a feature importance analysis in order to compare the potential relevance of features for activity classification in different datasets."
      }
    ]
  },
  {
    "tid":"780",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Learning to recommend diverse items over implicit feedback on PANDOR.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we present a novel and publicly available dataset for online recommendation provided by Purch1. The dataset records the clicks generated by users of one of Purch's high-tech website over the ads they have been shown for one month. In addition, the dataset contains contextual information about offers such as offer titles and keywords, as well as the anonymized content of the page on which offers were displayed. Then, besides a detailed description of the dataset, we evaluate the performance of six popular baselines and propose a simple yet effective strategy on how to overcome the existing challenges inherent to implicit feedback and popularity bias introduced while designing an efficient and scalable recommendation algorithm. More specifically, we propose to demonstrate the importance of introducing diversity based on an appropriate representation of items in Recommender Systems, when the available feedback is strongly biased.\n\n"
      }
    ]
  },
  {
    "tid":"781",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Learning to Rank Microblog Posts for Real-Time Ad-Hoc Search.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Microblogging websites have emerged to the center of information production and diffusion, on which people can get useful information from other users' microblog posts. In the era of Big Data, we are overwhelmed by the large amount of microblog posts. To make good use of these informative data, an effective search tool is required specialized for microblog posts. However, it is not trivial to do microblog search due to the following reasons: 1 microblog posts are noisy and time-sensitive rendering general information retrieval models ineffective. 2 Conventional IR models are not designed to consider microblog-specific features. In this paper, we propose to utilize learning to rank model for microblog search. We combine content-based, microblog-specific and temporal features into learning to rank models, which are found to model microblog posts effectively. To study the performance of learning to rank models, we evaluate our models using tweet data set provided by TERC 2011 and TREC 2012 microblogs track with the comparison of three state-of-the-art information retrieval baselines, vector space model, language model, BM25 model. Extensive experimental studies demonstrate the effectiveness of learning to rank models and the usefulness to integrate microblog-specific and temporal information for microblog search task."
      }
    ]
  },
  {
    "tid":"782",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mutual localization in multi-robot systems using anonymous relative measurements' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose a decentralized method to perform mutual localization in multi-robot systems using anonymous relative measurements, i.e. measurements that do not include the identity of the measured robot. This is a challenging and practically relevant operating scenario that has received little attention in the literature. Our mutual localization algorithm includes two main components: a probabilistic multiple registration stage, which provides all data associations that are consistent with the relative robot measurements and the current belief, and a dynamic filtering stage, which incorporates odometric data into the estimation process. The design of the proposed method proceeds from a detailed formal analysis of the implications of anonymity on the mutual localization problem. Experimental results on a team of differential-drive robots illustrate the effectiveness of the approach, and in particular its robustness against false positives and negatives that may affect the robot measurement process. We also provide an experimental comparison that shows how the proposed method outperforms more classical approaches that may be designed building on existing techniques. The source code of the proposed method is available within the MLAM ROS stack."
      }
    ]
  },
  {
    "tid":"783",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mining sequential rules common to several sequences with the window size constraint' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present an algorithm for mining sequential rules common to several sequences, such that rules have to appear within a maximum time span. Experimental results with real-life datasets show that the algorithm can reduce the execution time, memory usage and the number of rules generated by several orders of magnitude compared to previous algorithms."
      }
    ]
  },
  {
    "tid":"784",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Sensor Network Protocol for Automatic Meter Reading in an Apartment Building' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A wireless sensor network for automatic meter reading needs to satisfy two contradicting requirements, i.e. long lifetime and prompt detection and notification of emergency. We propose a sensor network protocol for this purpose, in which sensor nodes operate on a low duty cycle while the latency of transmission is guaranteed to be less than it certain bound. In this protocol, each node is assigned a time slot in which it receives messages from other nodes. To accomplish slot assignment where nodes further from a BS are assigned earlier time slots for a packet to be transmitted to the BS in one cycle, we propose a slot assignment function with which a node can determine its own slot in a distributed way. We explore several slot assignment functions to find one which gives low and homogeneous contention over a grid network. The simulation results show that our protocol performs well close to the optimal case."
      }
    ]
  },
  {
    "tid":"785",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Heterogeneous face image matching using multi-scale features' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Heterogeneous Face Recognition (HFR) refers recognition of face images captured in different modalities, e.g. Visual (VIS), near infrared (NIR) and thermal infrared (TIR). Although heterogeneous face images of a given person differ by pixel values, the identity of the face should be classified as the same. This paper focuses on NIR-VIS HFR. Light Source Invariant Features (LSIFs) are derived to extract the invariant parts between two types of face images. The derived LSIFs rely only on the variation patterns of the skin parameters so that the effects generated from light source can be largely reduced. A common feature extraction method is designed to capture LSIFs based on a group of differential-based band-pass image filters, and we show that the scale for filters is critical. Our results in CASIA HFB database validate the effectiveness of the model and our recognition approach."
      }
    ]
  },
  {
    "tid":"786",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Look-Ahead with Mini-Bucket Heuristics for MPE.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The paper investigates the potential of look-ahead in the context of AND/OR search in graphical models using the Mini-Bucket heuristic for combinatorial optimization tasks (e.g., MAP/MPE or weighted CSPs). We present and analyze the complexity of computing the residual (a.k.a. Bellman update) of the Mini-Bucket heuristic and show how this can be used to identify which parts of the search space are more likely to benefit from look-ahead and how to bound its overhead. We also rephrase the look-ahead computation as a graphical model, to facilitate structure exploiting inference schemes. We demonstrate empirically that augmenting Mini-Bucket heuristics by look-ahead is a cost-effective way of increasing the power of Branch-And-Bound search."
      }
    ]
  },
  {
    "tid":"787",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multiple Access Computational Offloading: Communication Resource Allocation in the Two-User Case (Extended Version).' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"By offering shared computational facilities to which mobile devices can offload their computational tasks, the mobile edge computing framework is expanding the scope of applications that can be provided on resource-constrained devices. When multiple devices seek to use such a facility simultaneously, both the available computational resources and the available communication resources need to be appropriately allocated. In this manuscript, we seek insight into the impact of the choice of the multiple access scheme by developing solutions to the mobile energy minimization problem in the two-user case with plentiful shared computational resources. In that setting, the allocation of communication resources is constrained by the latency constraints of the applications, the computational capabilities and the transmission power constraints of the devices, and the achievable rate region of the chosen multiple access scheme. For both indivisible tasks and the limiting case of tasks that can be infinitesimally partitioned, we provide a closed-form and quasi-closed-form solution, respectively, for systems that can exploit the full capabilities of the multiple access channel, and for systems based on time-division multiple access (TDMA). For indivisible tasks, we also provide quasi-closed-form solutions for systems that employ sequential decoding without time sharing or independent decoding. Analyses of our results show that when the channel gains are equal and the transmission power budgets are larger than a threshold, TDMA (and the suboptimal multiple access schemes that we have considered) can achieve an optimal solution. However, when the channel gains of each user are significantly different and the latency constraints are tight, systems that take advantage of the full capabilities of the multiple access channel can substantially reduce the energy required to offload."
      }
    ]
  },
  {
    "tid":"788",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Scan to Nonscan Conversion via Test Cube Analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Increasing complexity of integrated circuits has forced the industry to abandon partial scan, which necessitates a computationally demanding and unaffordable sequential automatic test pattern generation (ATPG), and to instead adopt full scan, despite its costs. In this paper, we propose a partial scan scheme driven by a computationally efficient test cube analysis. We tackle the challenges associated with the identification of the conditions to restore the controllability and observability compromised due to partial scan, and with the formulation of these conditions in terms of test cube operations. Upon the identification of a maximal-sized set of scan flip-flops that are converted to nonscan, a simple postprocessing of the test cubes helps compute the values to be loaded into the scan flip-flops, eliminating the need to rerun ATPG, while at the same time ensuring the quality of full scan. We further enhance this framework through techniques that process the test data before and after the application of the proposed test cube analysis-driven partial scan technique, in order to enlarge the size of the nonscan flip-flop set. The proposed scheme combines the simplicity of the conventional ATPG flow with the area, performance, test time, and test power reduction benefits of partial scan. The proposed test cube analysis-driven partial scan scheme is orthogonal and thus fully compatible with other test cost-reduction techniques, such as test data compression and test power reduction, which can be applied in conjunction."
      }
    ]
  },
  {
    "tid":"789",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Smartphone based indoor tracking using magnetic and indoor maps' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Tracking target user's indoor location with sub meter accuracy using low cost or no infrastructure is an active research topic. Indoor localization uses short range signals like WiFi, radio, ultrasound or Bluetooth signals that are affected by multipath errors and human presence. Recent studies have used stable magnetic field maps along with inertial sensors for indoor localization. In this paper, we propose a method that uses inertial sensors, magnetic field maps and indoor maps in a particle filter based implementation to improve accuracy of localization and tracking. Our method uses gradient descent algorithm to correct inaccurate user heading direction estimates due to magnetic perturbations. The tracking performance of our method is tested for four different implementations of Particle filter algorithm using magnetic maps (magnitude or vector) with and without indoor maps. We have developed an Android application to implement these four approaches and performed several experiments in a test area. The best approach out of these four achieves a mean localization accuracy of 0.75 m with a standard deviation of 0.52 m."
      }
    ]
  },
  {
    "tid":"790",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An energy-efficient and sink-location privacy enhanced scheme for WSNs through ring based routing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Sink location privacy is one of the major issues in Wireless Sensor Networks (WSNs) where adversaries may locate the sink by observing the destination of packets, directing and scaling of data flow, which leads to exposure of the sink-location privacy. In this paper, we propose a Ring Based Routing (RBR) scheme to address the issue of sink-location privacy in WSNs. The RBR scheme is composed of multiple routing rings and routing lines where the nodal data is not directly sent to the sink but to the nearest routing ring. In our scheme, data is routed through each node in the ring and then sent to other routing rings via routing lines, where the number of anonymity sink nodes is equal to the total number of nodes in the network. More specifically, under the RBR scheme the routing rings move in irregular patterns which can confuse adversaries even if the sink location is static and this greatly improves the sink-location privacy. In addition, the routing rings are constructed according to comprehensive analysis of network energy, which can fully use the remaining energy and improve energy efficiency and network lifetime. Both theoretical analysis and simulation results show that our scheme can protect location privacy of sinks effectively. A Ring Based Routing (RBR) scheme to address the sink-location privacy is proposed.The RBR scheme is composed of multiple routing rings and routing lines.All data is routed through each node in the ring.RBR scheme have high network lifetime, high network security and high energy efficiency."
      }
    ]
  },
  {
    "tid":"791",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Zero-value point attacks on kummer-based cryptosystem' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The Zero-Value Point (ZVP) attack, one of side channel attacks, is very powerful to recover the secret information of elliptic curve cyrptosystem (ECC) on memory constraint devices by monitoring their power consumptions. In the ZVP attack, the zero-value registers are used in point addition and doubling formula of ECC to resist randomizations. Hence, the countermeasures against the differential power analysis (DPA), like Coron's and Joye-Tymen's randomization, do not work for the ZVP attack. The Kummer surface is a variety associated to the Jacobian of a genus 2 curve with a map. The pseudo-group structure on the Kummer surface defines a scalar multiplication, which is more efficient than that in HECC and comparable to ECC, especially in constraint environments. We inspect the pseudo-addition and doubling formula of the Kummer surface and show how to find zero-value registers. Our analysis shows that the scalar multiplication on the Kummer surface suffers from the ZVP attack, hence all Kummer-based cryptosystems are inevitable to the ZVP attack."
      }
    ]
  },
  {
    "tid":"792",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Probabilistic bounds on the length of a longest edge in Delaunay graphs of random points in d-dimensions' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Motivated by low energy consumption in geographic routing in wireless networks, there has been recent interest in determining bounds on the length of edges in the Delaunay graph of randomly distributed points. Asymptotic results are known for random networks in planar domains. In this paper, we obtain upper and lower bounds that hold with parametric probability in any dimension, for points distributed uniformly at random in domains with and without boundary. The results obtained are asymptotically tight for all relevant values of such probability and constant number of dimensions, and show that the overhead produced by boundary nodes in the plane holds also for higher dimensions. To our knowledge, this is the first comprehensive study on the lengths of long edges in Delaunay graphs."
      }
    ]
  },
  {
    "tid":"793",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Outage probability of multi-carrier NOMA systems under joint I/Q imbalance' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Non-orthogonal multiple access (NOMA) has been recently proposed as a viable technology that can potentially improve the spectral efficiency of fifth generation (5G) wireless networks and beyond. However, in practical communication scenarios, transceiver architectures inevitably suffer from radio-frequency (RF) front-end related impairments that can lead to degradation of the overall system performance, with in-phase/quadrature-phase imbalance (IQI) constituting a major impairment in direct-conversion transceivers. In the present work, we quantify the effects of joint transmitter/receiver IQI on the performance of NOMA based multi-carrier (MC) systems under multipath fading conditions. Furthermore, we derive the asymptotic diversity order of the considered MC NOMA set up. Capitalizing on these results, we demonstrate that the effects of IQI differ considerably among NOMA users and depend on the underlying system parameters. For example, it is shown that the first sorted user appears more robust to IQI, which indicates that higher order users are more sensitive to the considered non-negligible impairment."
      }
    ]
  },
  {
    "tid":"794",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Extending iTaSC to support inequality constraints and non-instantaneous task specification' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In [1], we presented our constraint-based programming approach, iTaSC, that formulates instantaneous sensor-based robot tasks as constraint sets, and subsequently solves a corresponding least-squares problem to obtain control set points, such as desired joint velocities or joint torques. This paper further extends this approach, (i) by explicitly supporting the inclusion of inequality constraints in the task and (ii) by supporting a broader class of objective functions for translating the task constraints into robot motion. These extensions are made while retaining a tractable mathematical problem structure (a convex program). Furthermore, first results on extending the approach to non-instantaneous tasks are presented. As illustrated in the paper, the power of the approach lies (i) at its versatility to specify a wide range of robot behaviors and the ease of making task adjustments, and (ii) at its generic nature, that permits using systematic procedures to derive the underlying control equations."
      }
    ]
  },
  {
    "tid":"795",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Open and Service-Oriented Architecture to Support the Automation of Learning Scenarios' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The specifications of automated learning scenarios can lead to advantages for virtual learning environments and important benefits for organizations, although research in this e-learning area has not addressed this issue. To achieve this goal, one requirement is to have an infrastructure able to support the execution of specifications of learning scenarios. This paper presents an open service-oriented architecture based on the Open Services Interface Definition OSID specifications proposed by the Open Knowledge Initiative OKI and other normative specifications. The architecture is used as a technological infrastructure in a virtual learning environment with more than 40,000 students enrolled and has been tested as the infrastructure of a tool to automate specifications of learning scenarios. A case study has been used to test the suitability of the architecture and describe such a tool for the future."
      }
    ]
  },
  {
    "tid":"796",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Interpolation error as a quality metric for stereo: Robust, or not?' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"To properly benchmark and stimulate current stereo algorithms specifically in the application context of view interpolation, a robust quantitative evaluation approach is important. As a prevailing quality assessment method, interpolation error has been widely used. It measures the distortions between an interpolated image and a real camera image for a desired virtual viewpoint. However, is it a robust quality metric, especially when state-of-the-art stereo technology is developing so fast? This paper hence focuses on revealing several rarely attended weaknesses that make the interpolation error evaluation paradigm vulnerable. In addition, we propose an alternative evaluation method as an early attempt at addressing these challenges, from a perspective of communication system. Evaluation of representative stereo methods from the Middlebury Web site shows that the new approach yields consistent quality assessment outcomes."
      }
    ]
  },
  {
    "tid":"797",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Range Top/Bottom k Queries in OLAP Sparse Data Cubes' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A range top k query finds the top k maximum values over all selected cells of an OLAP data cube where the selection is specified by the range of contiguous values for each dimension. In this paper, we propose a partition-based storage structure, which is capable of answering both range top and bottom k queries in OLAP sparse data cubes. This is achieved by partitioning a multi-dimensional sparse data cube and storing it in partition-major order into two one-dimensional arrays: one is for the dense partitions and the other one is for the sparse partitions. This algorithm supports both single cell update and bulk batch update. Nevertheless, the update cost for a set of cells in a partition is similar to the update cost of a single cell update, i.e. extra 2 I/Os in the most cases and the worst is extra 5 I/Os in some very rare cases. Our approach also reduces the storage cost of sparse data cubes."
      }
    ]
  },
  {
    "tid":"798",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Exploring User-Specific Information in Music Retrieval' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With the advancement of mobile computing technology and cloud-based streaming music service, user-centered music retrieval has become increasingly important. User-specific information has a fundamental impact on personal music preferences and interests. However, existing research pays little attention to the modeling and integration of user-specific information in music retrieval algorithms/models to facilitate music search. In this paper, we propose a novel model, named User-Information-Aware Music Interest Topic (UIA-MIT) model. The model is able to effectively capture the influence of user-specific information on music preferences, and further associate users' music preferences and search terms under the same latent space. Based on this model, a user information aware retrieval system is developed, which can search and re-rank the results based on age- and/or gender-specific music preferences. A comprehensive experimental study demonstrates that our methods can significantly improve the search accuracy over existing text-based music retrieval methods."
      }
    ]
  },
  {
    "tid":"799",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Coherent Conditional Probability in a Fuzzy Logic Setting' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Very recently, a (fuzzy modal) logic to reason about coherent conditional probability, in the sense of de Finetti, has been introduced by the authors. Under this approach, a conditional probability \u03bc (\u00b7 \u2223 \u00b7) is taken as a primitive notion defined over conditional events of the form \u201c\u03c6 given \u03c8\u201d, \u03c6 | \u03c8 for short, where \u03c8 is not the impossible event. The logic, called FCP(\u0141\u03a0), exploits an idea alread..."
      }
    ]
  },
  {
    "tid":"800",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multi-section continuum robot for endoscopic surgical clipping of intracranial aneurysms.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose the development and assessment of a multi-section continuum robot for endoscopic surgical clipping of intracranial aneurysms. The robot has two sections for bending actuated by tendon wires. By actuating the two sections independently, the robot can generate a variety of posture combinations by these sections while maintaining the tip angle. This feature offers more flexibility in positioning of the tip than a conventional endoscope for large viewing angles of up to 180 degrees. To estimate the flexible positioning of the tip, we developed kinematic mapping with friction in tendon wires. In a kinematic-mapping simulation, the two-section robot at the target scale (i.e., an outer diameter of 1.7 mm and a length of 60 mm) had a variety of tip positions within 50-mm ranges at the 180 degrees-angled view. In the experimental validation, the 1:10 scale prototype performed the three salient postures with different tip positions at the 180 degrees-angled view."
      }
    ]
  },
  {
    "tid":"801",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Control of liquid handling robotic systems: A feed-forward approach to suppress sloshing.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a feed-forward approach to reduce sloshing dynamics in liquid handling robotic systems. According to our solution, the dynamics of a liquid into an open vessel manipulated by a robot can be described by means of a spherical pendulum mechanical model. By doing this, the sloshing problem can be addressed as a vibration suppression problem for a second order system. More in details, the pendulum model is utilized to tune an exponential filter which shapes the reference trajectory for the robot, thus achieving a sloshing-free motion of the liquid inside the vessel."
      }
    ]
  },
  {
    "tid":"802",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Performance Comparison Of Data Reduction Techniques For Wireless Multimedia Sensor Network Applications' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With the increased use of smart phones, Wireless Multimedia Sensor Networks (WMSNs) will have opportunities to deploy such devices in several contexts for data collection and processing. While smart phones come with richer resources and can do complex processing, their battery is still limited. Background subtraction (BS) and compression techniques are common data reduction schemes, which have been used for camera sensors to reduce energy consumption in WMSNs. In this paper, we investigate the performance of various BS algorithms and compression techniques in terms of computation and communication energy, time, and quality. We have picked five different BS algorithms and two compression techniques and implemented them in an Android platform. Considering the fact that these BS algorithms will be run within the context of WMSNs where the data is subject to packet losses and errors, we also investigated the performance in terms of packet loss ratio in the network under various packet sizes. The experiment results indicated that the most energy-efficient BS algorithm could also provide the best quality in terms of the foreground detected. The results also indicate that data reduction techniques including BS algorithms and compression techniques can provide significant energy savings in terms of transmission energy costs."
      }
    ]
  },
  {
    "tid":"803",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Exploiting interference locality in coordinated multi-point transmission systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Coordinated Multi-Point (CoMP) transmission is emerging as a concept that can substantially suppress interference, thus improving the capacity of multi-cell wireless networks. However, existing CoMP techniques either require sharing of data and channel state information (CSI) for all links in the network, or have limited capability of interference suppression. In this paper, we propose distributed interference alignment and cancellation (DIAC) to overcome these limitations. DIAC builds on a key intuition of interference locality - since each link is interfered with a limited number of neighboring links, it is sufficient to coordinate with those strong interferers and ignore others, in order to bound the overhead in CoMP. DIAC realizes the localized coordination by integrating interference cancellation and distributed interference alignment, and can be applied to both the uplink and downlink of multi-cell wireless networks. We validate DIAC using both model-driven and trace-based simulation where the traces are collected by implementing a MIMO-OFDM channel estimator on a software radio platform. Our experiments show that DIAC can substantially improve the degrees of freedom in multi-cell wireless networks."
      }
    ]
  },
  {
    "tid":"804",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Performance Analysis of Inter Cluster Communication Methods in VLIW Architecture' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With increasing demands for high performance byembedded systems, especially by digital signal processingapplications, embedded processors must increaseavailable instruction level parallelism (ILP) withinsignificant constraints on power consumption and chipcost. Clustered VLIW (Very Long Instruction Word)architectures can be used to address this problem. Theirclusters have to communicate by some mechanism toensure that the data in one cluster can be used by otherclusters as and when required. We selected four differentmechanisms of inter cluster communication under thecategory of fully connected deadlock free networks. Thenwe designed and implemented the VLIW processorsemploying these mechanisms and analyzed them againstvarious performance parameters - cycle time, cycle countoverhead, instruction size and gate count. Also the effectsof changing the data width and number of registers in aregister file are analyzed."
      }
    ]
  },
  {
    "tid":"805",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Web-based SBLR method of multimedia tools for computer-aided drawing.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"As one of the most successful multimedia tools for digital media and creative industry, computer-aided drawing system assists users to convert the input real photos into painterly style images. Nowadays, it is widely developed as cloud brush engine service in many creative software tools and applications of artistic rendering such as Prisma, Photoshop Cloud, and Meitu, because the machine learning server has more powerful than the stand-alone version. In this paper, we propose a web collaborative Stroke-based Learning and Rendering (WebSBLR) system. Different from the existing methods that are mainly focused on the artistic filters, we concentrate on the stroke realistic rendering engine for browser on client using WebGL and HTML5. Moreover, we implement the learning-based stroke drawing path generation module on the server. By this way, we enable to achieve the computer-supported cooperative work (CSCW), especially for multi-screen synchronous interaction. The experiments demonstrated our method are efficient to web-based multi-screen painting simulation.It can successfully learn artists\u2019 styles and render pictures with consistent and smooth brush strokes."
      }
    ]
  },
  {
    "tid":"806",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Separator Design of Gesture Signals Based on Adaptive Threshold Using Wearable Sensors' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Gesture, especially finger gestures, can reflect various intentions of users in daily behaviors. Therefore, more and more researches are performed on recognition of finger gestures based on wearable sensors. Segmentation, detecting the Start and End of a gesture, is crucial for accurate recognition of the gesture. However it is hard to extract signal of each finger gesture timely and correctly from continuous signals of the sensor, since they are transient, noise sensitive, and with individual difference. In this paper, we design an adaptive separator employing Bayes theory and distribution statistic technology to overcome the difficulties. The separator can detect each segmentation from a series of gestures timely, correctly, and adaptive to each user."
      }
    ]
  },
  {
    "tid":"807",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A converged hardware solution for FFT, DCT and Walsh transform' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we are interested in developing a programmable baseband processor for multiple radio standards, including the wireless LAN standards 802.11a and 802.11b. 802.11a is based on OFDM and uses a 64-point FFT. Demodulation of the complementary code keying (CCK) used in 802.11b includes the computation of a modified Walsh transform. Similarities have been found between the radix-4 FFT and the fast Walsh transform (FWT) and this has enabled the design of a converged FFT and FWT processor. With small modifications this processor can also be used for calculating the discrete cosine transform (DCT). A converged FFT/FWT/DCT processor was designed and synthesized in a 0.13\u03bcm process. Results indicate that the hardware can run at 385 MHz, which means a 64-point FFT/DCT is calculated in 140 ns and a FWT for 802.11b 11Mb/s CCK in 47 ns. The area including memory is 0.40 mm2."
      }
    ]
  },
  {
    "tid":"808",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Language-Guided Controller Synthesis for Linear Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper considers the problem of controlling discrete-time linear systems from specifications given as formulas of syntactically co-safe linear temporal logic over linear predicates in the state variables. A systematic procedure is developed for the automatic computation of sets of initial states and feedback controllers such that all the resulting trajectories of the closed-loop system satisfy the given specifications. The procedure is based on the iterative construction and refinement of an automaton that enforces the satisfaction of the formula. Linear programming based approaches are proposed to compute the polytope-to-polytope controllers that label the transitions of the automaton. Extensions to discrete-time piecewise affine systems and specifications given as formulas of full linear temporal logic are included. The algorithms developed in this paper were implemented as a software package that is available for download. Their application and effectiveness are demonstrated for several case studies."
      }
    ]
  },
  {
    "tid":"809",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Framework for querying distributed objects managed by a grid infrastructure' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Queries over scientific data often imply expensive analyses of data requiring a lot of computational resources available in Grids. We are developing a customizable query processor built on top of an established Grid infrastructure, the NorduGrid middleware, and have implemented a framework for managing long running queries in Grid environment. With the framework the user does not specify the detailed job and parallelization descriptions required by NorduGrid. Instead s/he specifies queries in terms of an application-oriented schema describing contents of files managed by the Grid and accessed through wrappers. When a query is received by the system it generates NorduGrid job descriptions submitted to NorduGrid for execution. The framework considers limitations of NorduGrid. It includes a submission mechanism, a job babysitter, and a generic data exchange mechanism. The submission mechanism generates a number of jobs for parallel execution of a user query over wrapped data files. The task of the babysitter is to submit generated jobs to NorduGrid for the execution, to monitor their execution status, and to download results from the execution. The generic exchange mechanism provides a way to exchange objects through files between Grid execution nodes and user applications."
      }
    ]
  },
  {
    "tid":"810",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On the performance of MPI-openMP on a 12 nodes multi-core cluster' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With the increasing number of Quad-Core-based clusters and the introduction of compute nodes designed with large memory capacity shared by multiple cores, new problems related to scalability arise. In this paper, we analyze the overall performance of a cluster built with nodes having a dual Quad-Core Processor on each node. Some benchmark results are presented and some observations are mentioned when handling such processors on a benchmark test. A Quad-Core-based cluster's complexity arises from the fact that both local communication and network communications between the running processes need to be addressed. The potentials of an MPI-OpenMP approach are pinpointed because of its reduced communication overhead. At the end, we come to a conclusion that an MPI-OpenMP solution should be considered in such clusters since optimizing network communications between nodes is as important as optimizing local communications between processors in a multi-core cluster."
      }
    ]
  },
  {
    "tid":"811",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Tensor Completion Algorithms in Big Data Analytics.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Tensor completion is a problem of filling the missing or unobserved entries of partially observed tensors. Due to the multidimensional character of tensors in describing complex datasets, tensor completion algorithms and their applications have received wide attention and achievement in areas like data mining, computer vision, signal processing, and neuroscience. In this survey, we provide a modern overview of recent advances in tensor completion algorithms from the perspective of big data analytics characterized by diverse variety, large volume, and high velocity. We characterize these advances from the following four perspectives: general tensor completion algorithms, tensor completion with auxiliary information (variety), scalable tensor completion algorithms (volume), and dynamic tensor completion algorithms (velocity). Further, we identify several tensor completion applications on real-world data-driven problems and present some common experimental frameworks popularized in the literature along with several available software repositories. Our goal is to summarize these popular methods and introduce them to researchers and practitioners for promoting future research and applications. We conclude with a discussion of key challenges and promising research directions in this community for future exploration.\n\n"
      }
    ]
  },
  {
    "tid":"812",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Label Noise Reduction in Entity Typing by Heterogeneous Partial-Label Embedding.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Current systems of fine-grained entity typing use distant supervision in conjunction with existing knowledge bases to assign categories (type labels) to entity mentions. However, the type labels so obtained from knowledge bases are often noisy (i.e., incorrect for the entity mention's local context). We define a new task, Label Noise Reduction in Entity Typing (LNR), to be the automatic identification of correct type labels (type-paths) for training examples, given the set of candidate type labels obtained by distant supervision with a given type hierarchy. The unknown type labels for individual entity mentions and the semantic similarity between entity types pose unique challenges for solving the LNR task. We propose a general framework, called PLE, to jointly embed entity mentions, text features and entity types into the same low-dimensional space where, in that space, objects whose types are semantically close have similar representations. Then we estimate the type-path for each training example in a top-down manner using the learned embeddings. We formulate a global objective for learning the embeddings from text corpora and knowledge bases, which adopts a novel margin-based loss that is robust to noisy labels and faithfully models type correlation derived from knowledge bases. Our experiments on three public typing datasets demonstrate the effectiveness and robustness of PLE, with an average of 25% improvement in accuracy compared to next best method."
      }
    ]
  },
  {
    "tid":"813",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Visual attention modeling for 3D video using neural networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Visual attention is one of the most important mechanisms in the human visual perception. Recently, its modeling becomes a principal requirement for the optimization of the image processing systems. Numerous algorithms have already been designed for 2D saliency prediction. However, only few works can be found for 3D content. In this study, we propose a saliency model for stereoscopic 3D video. This algorithm extracts information from three dimensions of content, i.e. spatial, temporal and depth. This model benefits from the properties of interest points to be close to human fixations in order to build spatial salient features. Besides, as the perception of depth relies strongly on monocular cues, our model extracts the depth salient features using the pictorial depth sources. Since weights for fusion strategy are often selected in ad-hoc manner, in this work, we suggest to use a machine learning approach. The used artificial Neural Network allows to define adaptive weights based on the eye-tracking data. The results of the proposed algorithm are tested versus ground-truth information using the state-of-the-art techniques."
      }
    ]
  },
  {
    "tid":"814",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Power Allocation Strategies For Ofdm Gaussian Wiretap Channels With A Friendly Jammer' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper investigates power allocation strategies over a bank of independent parallel Gaussian wiretap channels where a legitimate transmitter and a legitimate receiver communicate in the presence of an eavesdropper and a friendly jammer. We give algorithms to compute the optimal power allocation strategy of the jammer in the degraded scenario. We also give an algorithm to compute power allocations strategies of the jammer in a general scenario, leading to significant performance gains in relation to isotropic jamming. Additionally, we provide a set of results that cast further insight into the problem. In our scenario, which is applicable to current OFDM communications systems, we demonstrate that the proposed jammer power allocation strategy can lead to considerable secrecy gains."
      }
    ]
  },
  {
    "tid":"815",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Latency-Aware Co-deployment Mechanism for Cloud-Based Services' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cloud computing attracts considerable attention from both industry and academic these years. Nowadays, a number of research investigations have been conducted on cloud-based services (e.g., IaaS, PaaS, SaaS, etc.). Deployment of cloud-based services is one of the most important research problems. In cloud computing, multiple services tend to cooperate with each other to accomplish complicated tasks. Deploying these services independently may not lead to good overall performance, since there are a lot of interactions among different services. Making an optimal co-deployment of multiple services is critical for reducing latency of user requests. When deploying highly related services, taking only distribution of users into consideration is not enough, since the deployment of one service would affect others. To attack this challenge, we employ cross service information as well as user locations to build a new model in integer programming formulation. To reduce the computation time of the model, we purpose a sequential model running iteratively to obtain approximate solution. Extensive experiments have been conducted over a large real-world dataset, involving 307 distributed computers in about 40 countries, and 1881 real-world Internet-based services in about 60 countries. The experimental results show the effectiveness of our proposed model. Our real-world dataset is publicly released to promote future research, which also makes our experiments reproducible."
      }
    ]
  },
  {
    "tid":"816",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Stereo vision based self-localization of autonomous mobile robots' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents vision based self-localization of tiny autonomous mobile robots in a known but highly dynamic environment. The problem covers tracking the robot position with an initial estimate to global self-localization. The algorithm enables the robot to find its initial position and to verify its location during every movement. The global position of the robot is estimated using trilateration based techniques whenever distinct landmark features are extracted. Distance measurements are used as they require fewer landmarks compared to methods using angle measurements. However, the minimum required features for global position estimation are not available throughout the entire state space. Therefore, the robot position is tracked once a global position estimate is available. Extended Kalman filter is used to fuse information from multiple heterogeneous sensors. Simulation results show that the new method that combines the global position estimation with tracking results in significant performance gain."
      }
    ]
  },
  {
    "tid":"817",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Preserving Location Privacy in Geosocial Applications' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Using geosocial applications, such as FourSquare, millions of people interact with their surroundings through their friends and their recommendations. Without adequate privacy protection, however, these systems can be easily misused, for example, to track users or target them for home invasion. In this paper, we introduce LocX, a novel alternative that provides significantly improved location privacy without adding uncertainty into query results or relying on strong assumptions about server security. Our key insight is to apply secure user-specific, distance-preserving coordinate transformations to all location data shared with the server. The friends of a user share this user's secrets so they can apply the same transformation. This allows all location queries to be evaluated correctly by the server, but our privacy mechanisms guarantee that servers are unable to see or infer the actual location data from the transformed data or from the data access. We show that LocX provides privacy even against a powerful adversary model, and we use prototype measurements to show that it provides privacy with very little performance overhead, making it suitable for today's mobile devices."
      }
    ]
  },
  {
    "tid":"818",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Non-Rigid MR/US Registration for Tracking Brain Deformations' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Abstract: During a neuro-surgical intervention, the brain tissues shift and warp. In order to keep an accurate positioning of the surgical instruments, one has to estimate this deformation from intra-operative images. 3D ultrasound (US) imaging is an innovative and low-cost modality which appears to be suited for such computer-assisted surgery tools. In this paper, we present a new image-based technique to register intra-operative 3D US with pre-operative Magnetic Resonance (MR) data. A first automatic rigid registration is achieved by the maximisation of a similarity measure that generalises the correlation ratio. Then, brain deformations are tracked in the 3D US time-sequence using a \"demon's\" like algorithm. Experiments show that a registration accuracy of the MR voxel size is achieved for the rigid part, and a qualitative accuracy of a few millimetres could be obtained for the complete tracking system."
      }
    ]
  },
  {
    "tid":"819",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Layered Approach to Managing Risks in OSS Projects.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a layered approach to managing risks in OSS projects. We define three layers: the first one for defining risk drivers by collecting and summarising available data from different data sources, including human-provided contextual information; the second layer, for converting these risk drivers into risk indicators; the third layer for assessing how these indicators impact the business of the adopting organisation. The contributions are: 1) the complexity of gathering data is isolated in one layer using appropriate techniques, 2) the context needed to interpret this data is provided by expert involvement evaluating risk scenarios and answering questionnaires in a second layer, 3) a pattern-based approach and risk reasoning techniques to link risks to business goals is proposed in the third layer."
      }
    ]
  },
  {
    "tid":"820",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Materials Discovery: Understanding Polycrystals from Large-Scale Electron Patterns' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper explores the idea of modeling a large image data collection of polycrystal electron patterns, in order to detect insights in understanding materials discovery. There is an emerging interest in applying big data processing, management and modeling methods to scientific images, which often come in a form and with patterns only interpretable to domain experts. While large-scale machine learning approaches have demonstrated certain superiority in analyzing, summarizing, and providing an understandable route to data types like natural images, speeches and texts, scientific images is still a relatively unexplored area. Deep convolutional neural networks, despite their recent triumph in natural image understanding, are still rarely seen adapted to experimental microscopic images, especially in a large scale. To the best of our knowledge, we present the first deep learning solution towards a scientific image indexing problem using a collection of over 300K microscopic images. The result obtained is 54% better than a dictionary lookup method which is state-of-theart in the materials science society"
      }
    ]
  },
  {
    "tid":"821",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'MyWEST: my Web Extraction Software Tool for effective mining of annotations from web-based databanks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"High-throughput technologies create the necessity to mine large amounts of gene annotations from diverse databanks, and to integrate the resulting data. Most databanks can be interrogated only via Web, for a single gene at a time, and query results are generally available only in the HTML format. Although some databanks provide batch retrieval of data via FTP, this requires expertise and resources for locally reimplementing the databank.We developed MyWEST, a tool aimed at researchers without extensive informatics skills or resources, which exploits user-defined templates to easily mine selected annotations from different Web-interfaced databanks, and aggregates and structures results in an automatically updated database. Using microarray results from a model system of retinoic acid-induced differentiation, MyWEST effectively gathered relevant annotations from various biomolecular databanks, highlighted significant biological characteristics and supported a global approach to the understanding of complex cellular mechanisms.MyWEST is freely available for non-profit use at http://www.medinfopoli.polimi.it/MyWEST/"
      }
    ]
  },
  {
    "tid":"822",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Nonlinear Lagrange relaxation based QoS routing revisited' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Abstract\u2014The development of efficient quality of service (QoS) routing algorithms in a high-speed networking or the next gener- ation IP networking environment is a very important and at the same time very difficult task due to the need to provide diverg ent services with multiple QoS requirements. Recently, a heuristic al- gorithm H MCOP, which is based on a nonlinear Lagrange relax- ation (NLR) technique, has been proposed to resolve the contra- diction between the time complexity and the quality of solution. Even though H MCOP has demonstrated outstanding capability for finding feasible solutions to the multi-path constraine d (MCP) problem, it has not exploited the full capability that an NLR-based technique could offer. In this paper, we propose a new NLR-based heuristic, in which the search process is interpreted from a prob- ability perspective. Simulation results indicate that, in compari- son with H MCOP, the proposed heuristic can achieve a signifi- cant performance improvement when it is used as a basic step for solving more complicated QoS routing problems. Index Terms\u2014 QoS support and performance, QoS routing,"
      }
    ]
  },
  {
    "tid":"823",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Convex Optimization Framework for Service Rate Allocation in Finite Communications Buffers' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We study the convexity of loss probability in communications and networking optimization problems that involve finite buffers, where the arrival process has a general distribution. Examples of such problems include scheduling, energy management and revenue and cost optimization problems. To achieve a computationally tractable optimization framework, we propose to adjust an existing non-convex loss probability formula for G/D/1 queues to present a convex and even more accurate loss probability model. We then use empirical data and computer simulations to examine the performance of the proposed design."
      }
    ]
  },
  {
    "tid":"824",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Information Hiding Based on Block Match Coding for Vector Quantization-Compressed Images' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a novel highly efficient lossless data hiding scheme is proposed to achieve the goal of hiding secret data into vector quantization (VQ)-compressed images that can be losslessly reconstructed after the secret data is extracted in the decoder. For VQ-compressed images, the index-modifying and the side-match VQ (SMVQ) techniques can be applied to encode indices and hide secret data. In general, data hiding accompanied by the SMVQ technique can yield a higher embedding capacity and a lower bit rate, yet more time consuming. In contrast, data hiding accompanied by the index-modifying technique can yield a lower embedding capacity and a higher bit rate, yet less time consuming. To possess the advantages of the two techniques while removing the shortcomings, the neighboring processed indices are employed to speed up the processes of generating state codebooks required for encoding and hiding. To evaluate the effectiveness of this approach, various test images are employed in the experiments. As documented in the experimental results, it is shown that the performance of the proposed scheme is superior to former schemes in terms of compression ratio, embedding rate, processing efficiency, and embedding capacity."
      }
    ]
  },
  {
    "tid":"825",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A decision support prototype tool for predicting student performance in an ODL environment' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Machine Learning algorithms fed with data sets which include information such as attendance data, test scores and other student information can provide tutors with powerful tools for decision-making. Until now, much of the research has been limited to the relation between single variables and student performance. Combining multiple variables as possible predictors of dropout has generally been overlooked. The aim of this work is to present a high level architecture and a case study for a prototype machine learning tool which can automatically recognize dropout-prone students in university level distance learning classes. Tracking student progress is a time-consuming job which can be handled automatically by such a tool. While the tutors will still have an essential role in monitoring and evaluating student progress, the tool can compile the data required for reasonable and efficient monitoring. What is more, the application of the tool is not restricted to predicting drop-out prone students: it can be also used for the prediction of students' marks, for the prediction of how many students will submit a written assignment, etc. It can also help tutors explore data and build models for prediction, forecasting and classification. Finally, the underlying architecture is independent of the data set and as such it can be used to develop other similar tools"
      }
    ]
  },
  {
    "tid":"826",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Design and Analysis of Prerake Ultrawideband Impulse Radio (PR-UWB-IR) With Turbo Outer Codes' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The performance of a precoded ultrawideband impulse radio (UWB-IR) system using prerake as the inner code and superorthogonal turbo code (SOTC) or repetition code (RC) as outer codes is investigated in this paper. Prerake UWB-IR transmits information bits with time-reversed channel information. Due to the low transmission power of the UWB-IR, the sequence is repeated through outer coding. The prerake technique demands that the channel information be available to the transmitter. The tradeoff between the feedback channel information and several system performance metrics such as the length of the outer code and bit error rate (BER) is studied. It is shown that there exists a minimum feedback quantity required to achieve a target system performance in different communication scenarios. The optimal quantity of feedback channel knowledge and intersymbol interference (ISI) are investigated and verified by computer simulation."
      }
    ]
  },
  {
    "tid":"827",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'H-SWD: Incorporating Hot Data Identification into Shingled Write Disks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Shingled write disk (SWD) is a magnetic hard disk drive that adopts the shingled magnetic recording (SMR) technology to overcome the areal density limit faced in conventional hard disk drives (HDDs). The SMR design enables SWDs to achieve two to three times higher areal density than the HDDs can reach, but it also makes SWDs unable to support random writes/in-place updates with no performance penalty. In particular, a SWD needs to concern about the random write/update interference, which indicates writing to one track overwrites the data previously stored on the subsequent tracks. Some research has been proposed to serve random write/update out-of-place to alleviate the performance degradation at the cost of bringing in the concept of garbage collection. However, none of these studies investigate SWDs based on the garbage collection performance. In this paper, we propose a SWD design called Hot data identification-based Shingled Write Disk (H-SWD). The H-SWD adopts a window-based hot data identification to effectively manage data in the hot bands and the cold bands such that it can significantly reduce the garbage collection overhead while preventing the random write/update interference. The experimental results with various realistic workloads demonstrates that H-SWD outperforms the Indirection System. Specifically, incorporating a simple hot data identification empowers the H-SWD design to remarkably improve garbage collection performance."
      }
    ]
  },
  {
    "tid":"828",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Bayesian Parametric Test for Multichannel Adaptive Signal Detection in Nonhomogeneous Environments' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper considers the problem of knowledge-aided space-time adaptive processing (STAP) in nonhomogeneous environments, where the covariance matrices of the training and test signals are assumed random and different from each other. A Bayesian detector is proposed by incorporating some a priori knowledge of the disturbance covariance matrices, and exploring their inherent block-Toeplitz structure. Specifically, the block-Toeplitz structure of the covariance matrix allows us to model the training signals as a multichannel auto-regressive (AR) process. The resulting detector is referred to as the Bayesian parametric adaptive matched filter (B-PAMF) which, compared with nonparametric Bayesian detectors, entails a lower training requirement and alleviates the computational complexity. Numerical results show that the proposed B-PAMF detector outperforms the standard PAMF test in nonhomogeneous environments."
      }
    ]
  },
  {
    "tid":"829",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Streaming Of Continuous Media For Distance Education Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Distance education created new challenges regarding the delivery of large size isochronous continuous streaming media (SM) objects. In this paper, we consider the design of a framework for customized SM presentations, where each presentation consists of a number of SM objects that should be retrieved and displayed to the user in a coherent fashion. We describe a retrieval optimizer (Prime) that captures the flexibilities and requirements imposed by the user query, user profile, and session profile. Then, it determines how this query script should be imposed against the continuous media (CM) server to reduce contention. We also provide a cost model to evaluate each proposed plan. Finally, we explain the role of memory buffering in alleviating the server bandwidth fragmentation problem. Our preliminary experimental results show the feasibility and effectiveness of our proposed model and techniques in generating near optimal retrieval."
      }
    ]
  },
  {
    "tid":"830",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Document ranking for variable bit-block compression signatures' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Variable bit-block compression (VBC) signature is extended for document ranking. Two different extensions were experimented: the weighted VBC (WVBC) scheme and the aggregate VBC (AVBC) scheme. For both, analytical bounds of the additional storage for the term frequencies were derived. The upper and lower bounds of WVBC signatures were better than the corresponding bounds for AVBC signatures. In general, these bounds are functions of the word size (in bits) of the term frequencies. Therefore, term frequencies were scaled to reduce the word size. Experiments showed that the additional storage cost is closer to the lower than the upper bound for both WVBC and AVBC signatures. In addition, WVBC signatures were better than AVBC signatures in terms of storage and retrieval speed. Logarithmic scaling was found to be significantly better than linear scaling, in measuring the agreement of document ranking against the case without scaling, using the Kendall rank-order correlation. If a 75% ranking performance is acceptable, then the additional storage of the term frequencies is only 3.4% of all the indexed documents."
      }
    ]
  },
  {
    "tid":"831",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Achieving Proportional Representation in Conference Programs.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We study the optimization problem of designing the program of a conference with parallel sessions, so that the intended participants are as happy as possible from the talks they can attend. Interestingly, this can be thought of as a two-dimensional extension of a scheme proposed by Chamberlin and Courant [1983] for achieving proportional representation in multi-winner elections. We show that different variations of the problem are computationally hard by exploiting relations of the problem with well-known hard graph problems. On the positive side, we present polynomial-time algorithms that compute conference programs that have a social utility that is provably close to the optimal one (within constant factors). Our algorithms are either combinatorial or based on linear programming and randomized rounding."
      }
    ]
  },
  {
    "tid":"832",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Distributed rating prediction in user generated content streams' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recommender systems predict user preferences based on a range of available information. For systems in which users generate streams of content (e.g., blogs, periodically-updated newsfeeds), users may rate the produced content that they read, and be given accurate predictions about future content they are most likely to prefer. We design a distributed mechanism for predicting user ratings that avoids the disclosure of information to a centralized authority or an untrusted third party: users disclose the rating they give to certain content only to the user that produced this content. We demonstrate how rating prediction in this context can be formulated as a matrix factorization problem. Using this intuition, we propose a distributed gradient descent algorithm for its solution that abides with the above restriction on how information is exchanged between users. We formally analyse the convergence properties of this algorithm, showing that it reduces a weighted root mean square error of the accuracy of predictions. Although our algorithm may be used many different ways, we evaluate it on the Neflix data set and prediction problem as a benchmark. In addition to the improved privacy properties that stem from its distributed nature, our algorithm is competitive with current centralized solutions. Finally, we demonstrate the algorithm's fast convergence in practice by conducting an online experiment with a prototype user-generated content exchange system implemented as a Facebook application."
      }
    ]
  },
  {
    "tid":"833",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Chaos-Based Fuzzy Regression Approach to Modeling Customer Satisfaction for Product Design' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The success of a new product is very much related to the customer satisfaction level of the product. Therefore, it is important to estimate the customer satisfaction level of a new product in its design stage. Quality function deployment is commonly used to develop customer satisfaction models for product design. Relationships between customer satisfaction and design attributes are highly fuzzy and nonlinear, but these relationship characteristics cannot be captured by existing customer satisfaction models. In this paper, we propose a novel chaos-based fuzzy regression (FR) approach with which fuzzy customer satisfaction models with second- and/or higher order terms, and interaction terms can be developed. The proposed approach uses a chaos optimization algorithm to generate the polynomial structures of customer satisfaction models. Thereafter, it employs an FR method to determine the fuzzy coefficients of the individual terms of models. To illustrate and validate the proposed approach, it is applied in the development of a customer satisfaction model for a mobile phone design. Five validation tests are conducted to compare modeling results from the chaos-based FR with those from statistical regression, FR, and fuzzy least-squares regression. Results of the validation tests show that the proposed approach outperforms the other three approaches in terms of mean relative errors and variance of errors and customer satisfaction models with second- and/or higher order terms, and interaction terms can be developed effectively using the proposed chaos-based FR approach."
      }
    ]
  },
  {
    "tid":"834",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Distributed top-k query processing by exploiting skyline summaries' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recently, a trend has been observed towards supporting rank-aware query operators, such as top-k, that enable users to retrieve only a limited set of the most interesting data objects. As data nowadays is commonly stored distributed over multiple servers, a challenging problem is to support rank-aware queries in distributed environments. In this paper, we propose a novel approach, called DiTo, for efficient top-k processing over multiple servers, where each server stores autonomously a fraction of the data. Towards this goal, we exploit the inherent relationship of top-k and skyline objects, and we employ the skyline objects of servers as a data summarization mechanism for efficiently identifying the servers that store top-k results. Relying on a thresholding scheme, DiTo retrieves the top-k result set progressively, while the number of queried servers and transferred data is minimized. Furthermore, we extend DiTo to support data summarizations of bounded size, thus restricting the cost of summary distribution and maintenance. To this end, we study the challenging problem of finding an abstraction of the skyline set of fixed size that influences the performance of DiTo only slightly. Our experimental evaluation shows that DiTo performs efficiently and provides a viable solution when a high degree of distribution is required."
      }
    ]
  },
  {
    "tid":"835",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Bi-weighted ensemble via HMM-based approaches for temporal data clustering.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"\u2022An effective solution is designed for the initialization problem based on the clustering ensemble technique.\u2022The performance of clustering ensemble is significantly improved via the proposed bi-weighting scheme.\u2022The intrinsic number of clusters can be automatically determined by applying agglomerative clustering in association with DSPA during the consensus process.\u2022Extensive experiments show that our proposed algorithm outperforms all the existing representative benchmarks."
      }
    ]
  },
  {
    "tid":"836",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Hybrid one-class classifier ensemble based on fuzzy integral for open-lexicon handwritten Arabic word recognition' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"One-class classifier (OCC) is involved for solving different kinds of problems due to its ability to represent a class distribution regardless the remaining classes. Its main advantage for multi-class classification is offering an open system and therefore allows easily extending new classes without retraining OCCs. So far, hidden Markov models, support vector machines and neural networks are the most used classifiers for Arabic word recognition, which provides a system with closed lexicon. In this paper, the OCCs are explored in order to perform an Arabic word recognition system with an open lexicon. Generally, pattern recognition systems designed by a single system suffer from limitations such as the lack of uniqueness and non-universality. Thus, combining multiple systems becomes an attractive research topic for performance and robustness enhancement. Fixed rules are commonly used us combiners for the hybrid OCC ensembles. The present paper aims to propose a combination scheme of OCCs based on the use of fuzzy integral (FI) operators. Furthermore, an alternative framework is proposed to design a parameter-independent and open-lexicon handwritten Arabic word recognition system as well as a new density measure function. Experimental results conducted on Arabic handwritten dataset using different types of OCCs with large number of classes highlight the superiority of FI for hybrid OCC ensembles."
      }
    ]
  },
  {
    "tid":"837",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Confidence measure based incremental adaptation for online language identification' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes an novel two-pass adaptation method for online language identification by using confidence measure based incremental language model adaptation. In this system, we firstly used semi-supervised language model adaptation to solve the problem of channel mismatch, and then used unsupervised incremental adaptation to adjust new language model during online language identification. For robust adaptation, we compare three confidence measures and then present a new fusion method with Bayesian classifier. Tested on the RMTS(Real-world Multi-channel Telephone Speech) database, experiments show that using semi-supervised language model adaptation, the target language detection rate rises from 73.26% to 80.02% and after unsupervised incremental language model adaptation, an extra rise over 3.91% (from 80.02% to 83.93%) is obtained."
      }
    ]
  },
  {
    "tid":"838",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient Model Synchronization with View Triple Graph Grammars.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Model synchronization is a crucial task in the context of Model Driven Engineering. Especially when creating and maintaining multiple suitable abstractions or views of a complex system, a bidirectional transformation is required to keep all views and the corresponding system synchronized by automatically propagating changes in both directions. Triple Graph Grammars (TGGs) are a declarative, rule-based bidirectional transformation language, which can be used to support model synchronization. In practice, most TGG tools restrict the supported class of TGGs for efficiency reasons. These restrictions are, however, seldom intuitive and are often difficult to understand and adhere to, especially for non-experts. View Triple Graph Grammars (VTGGs) are a restricted form of TGGs, which can be highly optimized for efficient view update propagation. We argue that the restrictions posed by VTGGs are explicit and intuitive for users, as they can be adequately motivated based on the main application scenarios for VTGGs. In this paper, we present for the first time a formalization of VTGGs, stating precisely the advantages and limitations of VTGGs as compared to TGGs, and backing our claims with initial runtime measurements from a practical case study."
      }
    ]
  },
  {
    "tid":"839",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fast Binocular Depth Inference via Bidirectional Motion Based Interpolation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Depth information provides fundamental supports to multimedia applications for both images and videos. Depth acquisition for stereo images has drawn much attention while few approaches are proposed for stereo videos. Conducting stereo matching frame-by-frame is time consuming and the result is temporally inconsistent. As a matter of fact, the redundancy shared by frame sequences may cause extra computational cost. Inspired by rapidly acquiring stereo video depth for some specific applications, we propose a novel bidirectional motion-based interpolation framework, which avoids frame-by-frame matching through making use of the motion estimation and the redundancy between frames. Firstly, comparable accurate depth maps are generated for self-adaptive selected frames via stereo matching. Then rough depth sequences inbetween are calculated using bidirectional motion-based interpolation. To improve the depth accuracy for non-selected frames, we propose a refinement approach to handle cracks and holes. The evaluation on both computer rendered and real world captured datasets show that our approach is competent for fast and accurate binocular video depth acquisition."
      }
    ]
  },
  {
    "tid":"840",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Continuous nonsingular terminal sliding mode control for systems with mismatched disturbances.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A continuous nonsingular terminal sliding mode control approach is proposed for mismatched disturbance attenuation. A novel nonlinear dynamic sliding mode surface is designed based on a finite-time disturbance observer. The time taken to reach the desired setpoint from any initial states under mismatched disturbance is guaranteed to be finite time. In addition, the proposed method exhibits the fine properties of nominal performance recovery as well as chattering alleviation."
      }
    ]
  },
  {
    "tid":"841",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Energy Minimization for Wireless Video Transmissions With Deadline and Reliability Constraints' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In wireless video transmissions, encoded video frames are often large in data load and truncated into many transport packets (TPs) for reliable transmissions. These TPs are to be delivered before a deadline at certain reliability that depends on the importance of the video frame. High power and bitrate transmission schemes are often deployed to ensure low loss rate, but at the cost of substantial energy consumption. This paper addresses the energy-minimizing transmission policy for highly reliable transmissions of a group of TP with a common deadline. We jointly adapt the transmission rate, transmission power, and retransmission limit to minimize the transmission energy while ensuring that the video frame is reliably delivered before a deadline. In a slow fading channel, we formulate a deterministic transmission policy that allocates a retransmission limit to the TPs and jointly optimizes with the transmission bitrate and power. Contrary to the intuition that avoids packet loss and retransmission to preserve energy, we demonstrate that allowing some retransmissions in the joint optimization consumes less energy, without compromising the target reliability. In a Rayleigh fading channel, a conventional approach adopts a supportable transmission rate and declares off-channel at deep fading. We generalize the approach to include various transmission schemes at different fading states and propose the probabilistic combination of these schemes. Given the fading statistics and a channel state, our policy determines to pause or to deploy a proper transmission scheme such that the video frame transmission is energy minimized, timely, and highly reliable. Extensive simulations confirm that the proposed transmission policies consume less energy than existing methods. In particular, when the deadline or the target reliability are tightened, the proposed policies yield even higher energy efficiency."
      }
    ]
  },
  {
    "tid":"842",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Imaging of Fractal Profiles.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a model for radar images of fractal (topologically 1-D) profiles is introduced. A twofold approach is followed: on one hand, we analytically solve the problem whenever small-slope profiles are in order; on the other hand, we present a partly analytical and partly numerical setup to cope with the general-slope case. By means of the analytical approach, we evaluate in closed form both the structure function and the power density spectrum of the radar signal. An appropriately smoothed (physical) fractional Brownian model (fBm) process is employed; its introduction is justified by the finite sensor resolution. A fractal scattering model is employed. It is shown that for a fractal profile modeled as an fBm stochastic process, the backscattered signal turns out to be strictly related to the associated fractional Gaussian noise process if a small-slope regime for the observed profile can be assumed. In the analytical-numerical framework, a profile with prescribed fractal parameters is first synthesized; then, fractal scattering methods (applicable to wider slope regimes with respect to the previous case) are employed to compute the signal backscattered toward the sensor. Finally, the power density spectrum of the acquired radar image is estimated. The obtained spectra are favorably compared with the theoretical results, and a parametric study is performed to assess the overall method behavior."
      }
    ]
  },
  {
    "tid":"843",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A computational geometry method for optimal resource allocation in network localization' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Wireless network localization (WNL) is an emerging paradigm for providing high-accuracy positional information in GPS-challenged environments. The localization performance of a node in WNL is determined by the allocation of transmit resources among its neighboring nodes. To achieve the best localization performance, we develop a computational geometry framework for optimal resource allocation in WNL. We first determine an affine map that transforms each resource allocation strategy into a point in 3-D Euclidian space. By exploiting geometric properties of these image points, we prove the sparsity property of the optimal resource allocation vector, i.e., the optimal localization performance can be achieved by allocating resources to only a small subset of neighboring nodes. Moreover, these geometric properties enable the reduction of the search space for optimal solutions, based on which we design efficient resource allocation strategies. Numerical results show that the proposed strategies can achieve significant improvements in both localization performance and computation efficiency. Our approach provides a new methodology for resource allocation in network localization, yielding exact optimal solutions rather than \u220a-approximate solutions."
      }
    ]
  },
  {
    "tid":"844",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Characterization of workload and resource consumption for an online travel and booking site' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Online travel and ticket booking is one of the top E-Commerce industries. As they present a mix of products: flights, hotels, tickets, restaurants, activities and vacational packages, they rely on a wide range of technologies to support them: Javascript, AJAX, XML, B2B Web services, Caching, Search Algorithms and Affiliation; resulting in a very rich and heterogeneous workload. Moreover, visits to travel sites present a great variability depending on time of the day, season, promotions, events, and linking; creating bursty traffic, making capacity planning a challenge. It is therefore of great importance to understand how users and crawlers interact on travel sites and their effect on server resources, for devising cost effective infrastructures and improving the Quality of Service for users. In this paper we present a detailed workload and resource consumption characterization of the web site of a top national Online Travel Agency. Characterization is performed on server logs, including both HTTP data and resource consumption of the requests, as well as the server load status during the execution. From the dataset we characterize user sessions, their patterns and how response time is affected as load on Web servers increases. We provide a fine grain analysis by performing experiments differentiating: types of request, time of the day, products, and resource requirements for each. Results show that the workload is bursty, as expected, that exhibit different properties between day and night traffic in terms of request type mix, that user session length cover a wide range of durations, which response time grows proportionally to server load, and that response time of external data providers also increase on peak hours, amongst other results. Such results can be useful for optimizing infrastructure costs, improving QoS for users, and development of realistic workload generators for similar applications."
      }
    ]
  },
  {
    "tid":"845",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Conversing with children: cartoon and video people elicit similar conversational behaviors' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Interactive animated characters have the potential to engage and educate children, but there is little research on children's interactions with animated characters and real people. We conducted an experiment with 69 children between the ages of 4 and 10 years to investigate how they might engage in conversation differently if their interactive partner appeared as a cartoon character or as a person. A subset of the participants interacted with characters that displayed exaggerated and damped facial motion. The children completed two conversations with an adult confederate who appeared once as herself through video and once as a cartoon character. We measured how much the children spoke and compared their gaze and gesture patterns. We asked them to rate their conversations and indicate their preferred partner. There was no difference in children's conversation behavior with the cartoon character and the person on video, even among those who preferred the person and when the cartoon exhibited altered motion. These results suggest that children will interact with animated characters as they would another person."
      }
    ]
  },
  {
    "tid":"846",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Knowledge Modelling through RDF Classification' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"One of the most urgent problems presented on the web scenario is the difficulty to capture effective user- oriented information. Search engines return tons of data which often are so general as well as useless; some- times the user request is so specific that no matches occur or they are too poor to satisfy the user expecta- tion. Often the same information is recoverable from different web pages and documents, but in an impre- cise or contradictory manner, so returned results are vague or disappointing. In last years, on the light of the emergency of the semantics, the tendency is to model meta-data about the web resources through RDF-based approaches which assure appreciable machine-oriented understandability. Objective of this work is framed in a wider project in-progress for semantic-based informa- tion discovery. Herein, in particular, we focus on the classification of RDF documents in order to capture se- mantics inside web resources as a valuable alternative to deal with the traditional content-based view of the web information."
      }
    ]
  },
  {
    "tid":"847",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'What Do Deep CNNs Learn About Objects?' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This report provides data on the number of veterans and VA health care users, and discusses the allegations of long delays in treatments."
      }
    ]
  },
  {
    "tid":"848",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A hybrid metaheuristic for the prize-collecting single machine scheduling problem with sequence-dependent setup times' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper investigates a single machine scheduling problem with strong industrial background, named the prize-collecting single machine scheduling problem with sequence-dependent setup times. In this problem, there are n candidate jobs for processing in a single machine, each job has a weight (or profit) and a processing time, and during processing a symmetric sequence-dependent setup time exists between two consecutive jobs. Since there is a maximum available time limitation of the machine, it is generally impossible to complete the processing of all the candidate jobs within this time limitation. The objective is to find a job processing sequence of maximal job weights (or profits) over a subset of all candidate jobs whose makespan does not exceed the given time limitation. This problem can be considered as an application of the orienteering problem (OP) in the field of discrete manufacturing. We formulate this problem as a mixed integer linear programming (MILP) model and propose a hybrid metaheuristic combining the structures of scatter search and variable neighborhood search. Computational results on a large number of randomly generated instances with different structures show that the proposed hybrid metaheuristic outperforms CPLEX and two metaheuristics proposed for the OP."
      }
    ]
  },
  {
    "tid":"849",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Model-Driven Development of Information Flow-Secure Systems with IFlow' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In our increasingly interconnected world, privacy can seem like an unattainable goal. We are surrounded by countless devices and web services that acquire and collect our personal data as we interact with them. In many cases, the confidentiality of such data is not guaranteed and is frequently (if not always intentionally) violated. Smartphone apps and Internet web services in particular are known to often leak their users' confidential data to other users or (affiliated) third parties. We present a novel model-driven approach called IFlow that allows the development of distributed applications consisting of mobile apps and web services with secure information flow. In IFlow, a UML model of an information flow-sensitive application is used to automatically generate deployable app and web service code as well as a formal model. By employing automatic, language-based information flow control as well as interactive verification, IFlow enables the developer to give verifiable guarantees to the user about how his private data is being treated by the application."
      }
    ]
  },
  {
    "tid":"850",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Power-Efficient Accelerator for Convolutional Neural Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Convolutional neural networks(CNNs) have been widely applied in various applications. However, the computation-intensive convolutional layers and memory-intensive fully connected layers have brought many challenges to the implementation of CNN on embedded platforms. To overcome this problem, this work proposes a power-efficient accelerator for CNNs, and different methods are applied to optimize the convolutional layers and fully connected layers. For the convolutional layer, the accelerator first rearranges the input features into matrix on-the-fly when storing them to the on-chip buffers. Thus the computation of convolutional layer can be completed through matrix multiplication. For the fully connected layer, the batch-based method is used to reduce the required memory bandwidth, which also can be completed through matrix multiplication. Then a two-layer pipelined computation method for matrix multiplication is proposed to increase the throughput. As a case study, we implement a widely used CNN model, LeNet-5, on an embedded device. It can achieve a peak performance of 34.48 GOP/s and the power efficiency with the value of 19.45 GOP/s/W under 100MHz clock frequency which outperforms previous approaches."
      }
    ]
  },
  {
    "tid":"851",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Low-cost itineraries for multi-hop agents designed for scalable monitoring of multiple subnets' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mobile agent (MA) technology represents a recent trend for implementing distributed management architectures, as an answer to the flexibility and scalability problems of centralised models. Management scalability limitations though, are not adequately addressed when monitoring tasks requiring the employment of multi-hop agents are considered. This is because agent-based architectures lack mechanisms that guarantee near-optimal agents itineraries so as to minimise the total migration cost in terms of the round-trip latency and the incurred traffic. This is of particular importance when the management of networks spanning multiple subnets is involved. To address these issues, we have adapted an algorithm originally designed to solve network design problems to the specific problematic of MA itinerary planning. The algorithm suggests the optimal number of MAs that minimise the overall cost and also constructs optimal itineraries for each of them. A Java-based implementation of the algorithm has been tested on realistic applications over a laboratory testbed, demonstrating significant cost savings. Simulation tests verified the algorithm's validity and competence over large enterprise networks."
      }
    ]
  },
  {
    "tid":"852",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Co-evolving meta-models and their instance models: A formal approach based on graph transformation.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Model-driven engineering focuses on models as primary artifacts of the software development process, which means programs are mainly generated by model-to-code transformations. In particular, modeling languages tailored to specific domains promise to increase the productivity of software developers and the quality of generated software. Modeling languages, however, evolve over time and therefore, existing models have to be migrated accordingly. The manual migration of models tends to be tedious and error-prone, therefore tools have been developed to (partly) automate this process. Nevertheless, the migration results may not always be well-defined."
      }
    ]
  },
  {
    "tid":"853",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Is SVM plus FS Better to Satisfy Decision by Majority?' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Government 2.0 activities have become very attractive and popular. Using the platforms to support the activities, anyone can anytime report issues in a city on the Web and share the reports with other people. Since a variety of reports are posted, officials in the city management section have to give priorities to the reports. However, it is not easy task to judge the importance of the reports since importance judgments vary depending on the officials and consequently the agreement rate becomes low. To remedy the low agreement rate problem of human judgment, it is necessary to create an automatic method to find reports with high priorities. Hirokawa et al. employed the Support Vector Machine (SVM) with word feature selection method (SVM+FS) to detect signs of danger from posted reports because signs of danger is one of high priority issues to be dealt with. However they did not compare the SVM+FS method with other conventional machine learning methods and it is not clear whether or not the SVM+FS method has better performance than the other methods. This paper compared the results of the SVM+FS method with conventional machine learning methods: SVM, Random Forest, and Naive Bayse with conventional word vectors, an LDA-based document vector, and word embedding by Word2Vec. Experimental results illustrate the validity and effectiveness of the SVM+FS method."
      }
    ]
  },
  {
    "tid":"854",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'PolyCheck: dynamic verification of iteration space transformations on affine programs.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":" High-level compiler transformations, especially loop transformations, are widely recognized as critical optimizations to restructure programs to improve data locality and expose parallelism. Guaranteeing the correctness of program transformations is essential, and to date three main approaches have been developed: proof of equivalence of affine programs, matching the execution traces of programs, and checking bit-by-bit equivalence of program outputs. Each technique suffers from limitations in the kind of transformations supported, space complexity, or the sensitivity to the testing dataset. In this paper, we take a novel approach that addresses all three limitations to provide an automatic bug checker to verify any iteration reordering transformations on affine programs, including non-affine transformations, with space consumption proportional to the original program data and robust to arbitrary datasets of a given size. We achieve this by exploiting the structure of affine program control- and data-flow to generate at compile-time lightweight checker code to be executed within the transformed program. Experimental results assess the correctness and effectiveness of our method and its increased coverage over previous approaches. \n\n"
      }
    ]
  },
  {
    "tid":"855",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Generalized Discrete Preference Games.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recently, much attention has been devoted to discrete preference games to model the formation of opinions in social networks. More specifically, these games model the agents' strategic decision of expressing publicly an opinion, which is a result of an interplay between the agent's private belief and the social pressure. However, these games have very limited expressive power; they can model only very simple social relations and they assume that all the agents respond to social pressure in the same way. In this paper, we define and study the novel class of generalized discrete preference games. These games have additional characteristics that can model social relations to allies or competitors and complex relations among more than two agents. Moreover, they introduce different levels of strength for each relation, and they personalize the dependence of each agent to her neighborhood. We show that these novel games admit generalized ordinal potential functions and, more importantly, that every two-strategy game that admits a generalized ordinal potential function is structurally equivalent to a generalized discrete preference game. This implies that the games in this novel class capture the full generality of two-strategy games in which the existence of (pure) equilibria is guaranteed by topological arguments."
      }
    ]
  },
  {
    "tid":"856",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Novel Spectral Methods for Built-In Self-Test in a System-on-a-Chip Environment' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This new method of built-in self-test (BIST) for sequential cores on a system-on-a-chip (SOC)generates test patterns using a real-time program that runs on an embedded processor. Alternatively, the same program can be run on an external low-cost tester. This program generates patterns using circuit-specific spectral information in the form of one or more Hadamard coefficients. The coefficients are extracted from high fault-coverage compacted pattern sets. When an embedded processor is available on SOC, the overhead is negligible. Also, sequential cores are tested in the functional mode, avoiding activation of non- functional timing paths. We present experimental results to show that for hard to test circuits, with any given test time, spectral patterns provide significantly higher fault coverage than weighted-random patterns."
      }
    ]
  },
  {
    "tid":"857",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Segmentation of myocardial volumes from real-time 3D echocardiography using an incompressibility constraint.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Real-time three-dimensional (RT3D) echocardiography is a new imaging modality that presents the unique opportunity to visualize the complex three-dimensional (3-D) shape and the motion of left ventricle (LV) in vivo. To take advantage of this opportunity, automatic segmentation of LV myocardium is essential. While there are a variety of efforts on the segmentation of LV endocardial (ENDO) boundaries, the segmentation of epicardial (EPI) boundaries is still problematic. In this paper, we present a new approach of coupled-surfaces propagation to address this problem. Our method is motivated by the idea that the volume of the myocardium is close to being constant during a cardiac cycle and takes this tight coupling as an important constraint. We employ two surfaces, each driven by the image-derived information that takes into account the ultrasound physics by modeling speckle using shifted Rayleigh distribution while maintaining the coupling. By evolving two surfaces simultaneously, the final representation of myocardium is thus achieved. Results from 328 sets of RT3D echocardiographic data are evaluated against the outlines of three observers. We show that the results from automatic segmentation are comparable to those from manual segmentation."
      }
    ]
  },
  {
    "tid":"858",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Leveraging Renewable Energy in Edge Clouds for Data Stream Analysis in IoT.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The emergence of Internet of Things (IoT) is participating to the increase of data- and energy-hungry applications. As connected devices do not yet offer enough capabilities for sustaining these applications, users perform computation offloading to the cloud. To avoid network bottlenecks and reduce the costs associated to data movement, edge cloud solutions have started being deployed, thus improving the Quality of Service. In this paper, we advocate for leveraging on-site renewable energy production in the different edge cloud nodes to green IoT systems while offering improved QoS compared to core cloud solutions. We propose an analytic model to decide whether to offload computation from the objects to the edge or to the core Cloud, depending on the renewable energy availability and the desired application QoS. This model is validated on our application use-case that deals with video stream analysis from vehicle cameras."
      }
    ]
  },
  {
    "tid":"859",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Sample efficiency improvement on neuroevolution via estimation-based elimination strategy' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose \\\\textit{estimation-based} elimination strategy, which improves sample efficiency of NeuroEvolution~(NE) algorithms. The fitness of new individuals was estimated using fitness of individuals evaluated in the past generations. The estimation was achieved by taking average fitness of individuals with high correlation with the new individual. Estimation-based elimination strategy avoids evaluating individuals with low estimated fitness. We adapt estimation-based elimination strategy for state-of-the-art NE algorithms: CMA-NeuroES and CMA-TWEANN@. From the experimental results of pole-balancing benchmark tasks, we show that the proposed strategy improves sample efficiency of the NE algorithms."
      }
    ]
  },
  {
    "tid":"860",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'From Non-Reactive to Reactive Walking in Humanoid Robots' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we report the implementation and the experimental validation of a controller to provide reactive walking gait capabilities of bipedal robots during the execution of predefined walking patterns. The proposed method is a cascade controller design to cope with external disturbances and to increase the robot stability. IMU states are used as inputs to generate modifications of the feet and the Center of Mass trajectories of the predefined walking gait. The method increases the walking stability minimizing the errors due to small terrain variations and external disturbances. The effectiveness of the proposed controller is validated in simulation and in real implementation on the full-body humanoid robot COMAN+."
      }
    ]
  },
  {
    "tid":"861",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Dstipe Algorithm For Fuzzy Spatio-Temporal Risk Calculation In Wireless Environments' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Time and location data play a very significant role in a variety of factory automation scenarios, such as automated vehicles and robots, their navigation, tracking, and monitoring, to services of optimization and security. Pervasive wireless capabilities combined with time and location information are enabling new applications in areas such as transportation systems, health care, elder care, military, emergency response, critical infrastructure, and law enforcement.A wireless object in proximity to some area for a duration of time may pose a risk hazard to the environment. This paper presents a novel fuzzy based spatio-temporal risk calculation DSTiPE method that a wireless object may present to the environment. The presented Matlab based application for cluster extraction is verified on a diagonal vehicle movement example."
      }
    ]
  },
  {
    "tid":"862",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Real-time conversational crowd assistants' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"When people work together, they converse about their current actions and intentions, building a shared context to inform their collaboration. Despite decades of research attempting to replicate this natural form of interaction in computers, the capabilities of conversational assistants are still extremely limited. In this paper, we investigate how human and machine intelligence can be combined to create assistants that work even in real-world situations. We introduce a crowd-powered conversational interface, called Chorus, that allows users to interact with a group of crowd workers as if they are a single conversional partner. We use Chorus as a personal assistant, and show that our incentive mechanism enables workers to hold consistent conversations and answer 84% of questions accurately. We then discuss a number of potential improvements that can be made by integrating artificial intelligence, and future systems that our work enables."
      }
    ]
  },
  {
    "tid":"863",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Truncated Least Squares Approach To The Detection Of Specular Highlights In Color Images' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"One of the most difficult aspects of dealing with illumination effects in computer vision is accounting for specularity in the images of real objects. The specular regions in an image are often saturated-which creates problem for all image processing algorithms that use decision thresholds. Such algorithms include those for edge detection, region segmentation, etc. Detecting specularity and whenever possible compensating for it are obviously advantageous. Along these lines, this paper represents a new specularity detection and compensation method which is based on the notion of truncated least-squares approximation to the function that maps the color distribution between two images of an object under different illumination conditions. We also present a protocol for the evaluation of the current method for specularity detection. Our protocol as currently formulated uses human subjects to grade the specularity detection method."
      }
    ]
  },
  {
    "tid":"864",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Slip and turnover avoidance control for a track-type mobile robot' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper describes a control method of mobile robots for avoiding slip and turnover in sloped terrain. An inexpensive vision/gyro sensor module that consists of a laser line generator, a USB camera and a gyro sensor, is suggested for obtaining terrain information nearby the robot system. Using information of the terrain and the robot state, the maximum limit of the forward velocity of the robot is defined for avoiding slip and turnover. Simultaneously the maximum value of the robot velocity is reflected to an operator in the form of a reflective force. Consequently the operator can recognize the maximum velocity of the robot determined by terrain information and the robot state. In this point of view, the inconsistency between the robot state and the user's command can be compensated by the reflective force. The experimental results show the effectiveness of the suggested method."
      }
    ]
  },
  {
    "tid":"865",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Security and privacy issues in P2P streaming systems: A survey' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Abstract Streaming applications over Peer-To-Peer (P2P) sys- tems have gained an enormous,popularity. Success al- ways implies increased concerns about security, pro- tection, privacy and all the other \u2018side\u2019 properties that transform an experimental application into a service. Research on security for P2P streaming started to flour- ish, but no comprehensive security analysis over the current P2P solutions has yet been attempted. There are no best practicesin system design, no (widely)accepted attack models, no measurement-based studies on secu- rity threats to P2P streaming, nor even general surveys investigatingspecific security aspects for these systems. This paper addresses this last aspect. Starting from existing analyses and security models in the literature, we give an overview on security and privacy considerations for P2P streaming systems. Our analysis emphasizes,two major facts: i) the Byzantine \u2013 Altruistic \u2013 Rational (BAR) model offers stronger se- curity guarantees compared to other approaches, at the"
      }
    ]
  },
  {
    "tid":"866",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Non-Conforming Behavior Detection For Voip-Based Network Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This work proposes a detection scheme that identifies non-conforming behavior in a VoIP network, based on statistical analysis and hypothesis testing. VoIP networks are a popular, low-cost alternative for telephony that offer lower rates especially for long-distance calls. Other services such as FollowMe, enhance the traditional voice-oriented nature of these networks. Consequently several security concerns such as fraud calls, are related to the high availability required by a VoIP system. Fraud calls account for an average loss of 3% to 5% of the operators' revenue. Thus the detection and prevention of the users from behaving in a non-conforming way, becomes crucial. A trustworthy and secure management and billing scheme is necessary, to guarantee the proper operation. This work proposes a behavioral control scheme for the VoIP clients. An initial training period defines the normal behavior. Then statistical analysis and t-testing is employed to extract results regarding the users' profiles, with pre-defined confidence levels. A Buffer zone creates a more flexible decision-making process. The scheme also offers the ability to configure its parameters, in order to react appropriately under different network conditions and detect possible misuses. It is implemented and its rational operation is verified via several simulation scenarios."
      }
    ]
  },
  {
    "tid":"867",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On the Usefulness of Liveness for Garbage Collection and Leak Detection' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The effectiveness of garbage collectors and leak detectors in identifying dead objects depends on the \"accuracy\" of their reachability traversal. Accuracy has two orthogonal dimensions: (i) whether the reachability traversal can distinguish between pointers and non-pointers (type accuracy), and (ii) whether the reachability traversal can identify memory locations that will be dereferenced in the future (liveness accuracy). While prior work has investigated the importance of type accuracy, there has been little work investigating the importance of liveness accuracy for garbage collection or leak detection. This paper presents an experimental study of the importance of liveness on the accuracy of the reachability traversal. We show that while liveness can significantly improve the effectiveness of a garbage collector or leak detector, the simpler liveness schemes are largely ineffective. One must analyze globals using an interprocedural analysis to get significant benefit."
      }
    ]
  },
  {
    "tid":"868",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Simultaneous Feature and Dictionary Learning for Image Set Based Face Recognition.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a simultaneous feature and dictionary learning (SFDL) method for image set-based face recognition, where each training and testing example contains a set of face images, which were captured from different variations of pose, illumination, expression, resolution, and motion. While a variety of feature learning and dictionary learning methods have been proposed in recent ye..."
      }
    ]
  },
  {
    "tid":"869",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Enhancing precision of Markov-based recommenders using location information' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recommender systems are a real example of human computer interaction systems that both consumer/user and seller/service-provider benefit from them. Different techniques have been published in order to improve the quality of these systems. One of the approaches is using context information such as location of users or items. Most of the location-aware recommender systems utilize users' location to improve memory-based collaborative filtering techniques. However, our proposed method is based on items' location and utilizes a Markov-based approach which can be easily applied to implicit datasets. The main application of this technique is for datasets containing location information of items. Experimental results on real dataset show that performance of our proposed method is much better than the classic CF methods."
      }
    ]
  },
  {
    "tid":"870",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'KEITAI-Space: Photo-Based Shared Virtual Space on Cellular Phones' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Virtual space, which models the real world, brings lotsof benefits when it covers a considerably wide area andcan be used in a mobile computing environment. However,there are some difficulties in implementing virtual space onthe mobile computing emvironment, mainly due to hardwarelimitations such as computation power, communication capability,and memory size. In this paper, we ease the difficultiesof using large-scale virtual space based on the realworld (LVBR) in the mobile environment. We have establisheda scalable and efficient method of constructing andmanaging LVBR on cellular phones, and have implementeda system named KEITAI-Space. KEITAI-Space is designedto provide the scalable photo-based virtual space to cellularphones that takes into account the hardware limitations.The virtual space is made of a collection of photos and canbe edited and expanded by people with cellular phones onhand."
      }
    ]
  },
  {
    "tid":"871",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient multi-modal retrieval in conceptual space' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a new, efficient retrieval system for large-scale multi-modal data including video tracks. With large-scale multi-modal data, the huge data size and various contents cause degradation of efficiency and precision of retrieval results. Recent research on image annotation and retrieval shows that image features based on the Bag-of-Visual Words approach with local descriptors such as SIFT perform surprisingly well with large-scale image datasets. Those powerful descriptors tend to be high-dimensional, imposing a high computational cost for approximate nearest neighbor searching in raw feature space. Our video retrieval method is focused on the correlation between image, sound, and location information recorded simultaneously, and to learn conceptual space describing the contents of the data to realize efficient searching. Experiments show good performance of our retrieval system with low memory usage and temporal complexity."
      }
    ]
  },
  {
    "tid":"872",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Extracting the windows clipboard from physical memory' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"When attempting to reconstruct the events leading up to a cyber security incident, one potentially important piece of information is the clipboard (Prosise et al., 2003). The clipboard has been present in Windows since Windows 3.1 and is the mechanism for transferring information from one application to another through copy and pasting actions. Being able to retrieve the last file copied or the last password used may provide investigators with invaluable information during a forensic investigation. This paper describes the Windows clipboard structure and the process of retrieving copy/paste information from Windows XP, Vista, and Windows 7 (both 32 bit and 64 bit) memory captures with data from applications including Notepad, Microsoft Word, and Microsoft Excel."
      }
    ]
  },
  {
    "tid":"873",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Low-Complexity Scheme for Frequency Estimation in Uplink OFDMA Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Frequency estimation in the uplink of an orthogonal frequency-division multiple-access system is a challenging task due to the presence of multiple carrier frequency offsets. Many existing solutions are too complex for practical implementation, while others are restricted to a specific subcarrier assignment strategy. Motivated by the above consideration, in this letter we propose a novel frequency estimator that allows flexible subcarrier assignment while requiring a low computational burden. Our scheme exploits the repetitive structure of the users' training sequences, which are properly designed so as to minimize the multiple-access interference arising in the presence of frequency errors. Computer simulations are used to assess the effectiveness of the proposed method and to make comparisons with competing alternatives."
      }
    ]
  },
  {
    "tid":"874",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Prediction of colon cancer using an evolutionary neural network' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Colon cancer is second only to lung cancer as a cause of cancer-related mortality in Western countries. Colon cancer is a genetic disease, propagated by the acquisition of somatic alterations that influence gene expression. DNA microarray technology provides a format for the simultaneous measurement of the expression level of thousands of genes in a single hybridization assay. The most exciting result of microarray technology has been the demonstration that patterns of gene expression can distinguish between tumors of different anatomical origin. Standard statistical methodologies in classification and prediction do not work well or even at all when N (a number of samples) <p (genes). Modification of conventional statistical methodologies or devise of new methodologies is needed for the analysis of colon cancer. Recently, designing artificial neural networks by evolutionary algorithms has emerged as a preferred alternative to the common practice of selecting the apparent best network. In this paper, we propose an evolutionary neural network that classifies gene expression profiles into normal or colon cancer cell. Experimental results on colon microarray data show that the proposed method is superior to other classifiers."
      }
    ]
  },
  {
    "tid":"875",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Prolonging WSN Lifetime with Data-Location Similarity and Weakest Node Protection' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Each sensor node in a wireless sensor network (WSN) mainly consumes energy to sense the environment and convey or relay the sensed data to a sink node. Once the resident energy in a sensor node is drained, this may cause incomplete sensing coverage, resulting in WSN failure. The lifetime of a network begins when the network starts working and ends when the first node becomes ineffective from energy exhaustion caused by any of the sensor nodes in the WSN. This paper proposes a scheme that clusters sensor nodes by node reading and by node location. By using the resident energy and coverage of each node, the scheme proposes a weakest node protection mechanism to balance the energy consumed by each node, to extend the network's lifetime. The evaluation results validate the proposed concept and show the proposed scheme can even obtain additional 12.5% network lifetime compared to other existing ones."
      }
    ]
  },
  {
    "tid":"876",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mechanism design in large games: incentives and privacy' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We study the problem of implementing equilibria of complete information games in settings of incomplete information, and address this problem using \\\"recommender mechanisms.\\\" A recommender mechanism is one that does not have the power to enforce outcomes or to force participation, rather it only has the power to suggestion outcomes on the basis of voluntary participation. We show that despite these restrictions, recommender mechanisms can implement equilibria of complete information games in settings of incomplete information under the condition that the game is large---i.e. that there are a large number of players, and any player's action affects any other's payoff by at most a small amount. Our result follows from a novel application of differential privacy. We show that any algorithm that computes a correlated equilibrium of a complete information game while satisfying a variant of differential privacy---which we call joint differential privacy---can be used as a recommender mechanism while satisfying our desired incentive properties. Our main technical result is an algorithm for computing a correlated equilibrium of a large game while satisfying joint differential privacy. Although our recommender mechanisms are designed to satisfy game-theoretic properties, our solution ends up satisfying a strong privacy property as well. No group of players can learn \\\"much\\\" about the type of any player outside the group from the recommendations of the mechanism, even if these players collude in an arbitrary way. As such, our algorithm is able to implement equilibria of complete information games, without revealing information about the realized types."
      }
    ]
  },
  {
    "tid":"877",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Semi-Supervised Text Classification Using Positive and Unlabeled Data' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Text classification using positive and unlabeled data refers to the problem of building text classifier using positive documents (P) of one class and unlabeled documents (U) of many other classes. U consists of positive and negative documents. Some existing methods for solving the PU-Learning problem are building a classifier in a two-step process. Generally speaking, these existing methods do not perform well when the size of P is too small. In this paper, we propose an improved method aiming at solving the PU-Learning problem with small P. This method combines the graph-based semi-supervised learning with the two-step method. Experiment indicates that our improved method performs well when the size of P is small."
      }
    ]
  },
  {
    "tid":"878",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Understanding the sustainability of a virtual community: model development and empirical test' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With the proliferation of Web 2.0 technologies, there is an expanded opportunity for individuals to get involved in information exchange. In this study, the sustainability of a virtual community for teachers and educators were investigated. The research model borrows the key concepts from the IS continuance model, social influence theory, the uses and gratifications paradigm, and relationship marketing to explain user intention to continue using a virtual community, as well as intention to recommend the community to others. Satisfaction, commitment, group norms are found to have significant impacts on intention to continue using and intention to recommend. Among the three factors, satisfaction has the highest impact on behavioral intentions. Individual-related factors (purposive value and self-discovery) are found to have significant impacts on user satisfaction, while social-related factors are more important in determining commitment and group norms. The results of this study provide important implications for both research and practice."
      }
    ]
  },
  {
    "tid":"879",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Concept-Based Visual Information Management with Large Lexical Corpus' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Most users want to find visual information based on the semantics of visual contents such as a name of person, semantic relations, an action happening in a scene, ...etc. However, techniques for content-based image or video retrieval are not mature enough to recognize visual semantic completely, whereas retrieval based on color, size, texture and shape are within the state of the art. Therefore, smart ways to manage textual annotation is visual information retrieval are necessary. In this paper, a framework for integration of textual and visual content searching mechanism is presented. The proposed framewrok includes ontology-based semantic query processing through efficient semantic similarity measurement. A new conceptual similarity distance measure between two conceptual entities in a large taxonomy structure is proposed and its efficiency is demonstrated. With the proposed method, an information retrieval system can benefit such as (1) reduction of the number of trial-and-errors to find correct keywords, (2) Improvement of precision rates by eliminating the semantic heterogeneity in description, and (3) Improvement of recall rates through precise modeling of concepts and their relations."
      }
    ]
  },
  {
    "tid":"880",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Annotating Historical Archives of Images' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recent programs like the Million Book Project and Google Print Library Project have archived several million books in digital format, and within a few years a significant fraction of world's books will be online. While the majority of the data will naturally be text, there will also be tens of millions of pages of images. Many of these images will defy automation annotation for the foreseeable future, but a considerable fraction of the images may be amiable to automatic annotation by algorithms that can link the historical image with a modern contemporary, with its attendant metatags. To perform this linking, there must be a suitable distance measure that appropriately combines the relevant features of shape, color, texture and text. However, the best combination of these features will vary from application to application and even from one manuscript to another. In this work, the authors propose a simple technique to learn the distance measure by perturbing the training set in a principled way."
      }
    ]
  },
  {
    "tid":"881",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Reflets: combining and revealing spaces for musical performances.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present Reflets, a mixed-reality environment for musical performances that allows for freely displaying virtual content on stage, such as 3D virtual musical interfaces or visual augmentations of instruments and performers. It relies on spectators and performers revealing virtual objects by slicing through them with body parts or objects, and on planar slightly reflective transparent panels that combine the stage and audience spaces. In this paper, we describe the approach and implementation challenges of Reflets. We then demonstrate that it matches the requirements of musical performances. It allows for placing virtual content anywhere on large stages, even overlapping with physical elements and provides a consistent rendering of this content for large numbers of spectators. It also preserves non-verbal communication between the audience and the performers, and is inherently engaging for the spectators. We finally show that Reflets opens musical performance opportunities such as augmented interaction between musicians and novel techniques for 3D sound shapes manipulation."
      }
    ]
  },
  {
    "tid":"882",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'New operators on triangular intuitionistic fuzzy numbers and their applications in system fault analysis.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Triangular intuitionistic fuzzy numbers (TIFNs) are one of the extensions of intuitionistic fuzzy sets and triangular fuzzy numbers, and they have been applied to fault-tree analysis in many studies. The arithmetic operations and logic operations of TIFNs have been addressed in relevant studies. However, the arithmetic operations of TIFNs in these studies all have defects. Based on information from the existing studies, this paper proposes new arithmetic operations and logic operators for TIFNs and applies them to fault analysis of a printed circuit board assembly system. The proposed operations are then compared to the existing ones to confirm their feasibility and rationality."
      }
    ]
  },
  {
    "tid":"883",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Power Control Game With Sinr-Pricing In Variable-Demand Wireless Data Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Game theory has been applied to model power control in wireless systems for years. Conventional power control games tend to consider unlimited backlogged user traffic. Different from the conventional methodology, this paper aims to investigate limited backlogged data traffic and construct a game-theoretic model tackling both one-shot and repeated power control problem in which variable user traffic demand needs to be taken into consideration. To improve the network performance in such situation, we devise a new SINR pricing scheme and propose an algorithm to calculate an optimal price. With this optimal price, we prove that the Nash equilibrium is Pareto efficient and max-min fair."
      }
    ]
  },
  {
    "tid":"884",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Measures for evaluating the decision performance of a decision table in rough set theory' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"As two classical measures, approximation accuracy and consistency degree can be employed to evaluate the decision performance of a decision table. However, these two measures cannot give elaborate depictions of the certainty and consistency of a decision table when their values are equal to zero. To overcome this shortcoming, we first classify decision tables in rough set theory into three types according to their consistency and introduce three new measures for evaluating the decision performance of a decision-rule set extracted from a decision table. We then analyze how each of these three measures depends on the condition granulation and decision granulation of each of the three types of decision tables. Experimental analyses on three practical data sets show that the three new measures appear to be well suited for evaluating the decision performance of a decision-rule set and are much better than the two classical measures."
      }
    ]
  },
  {
    "tid":"885",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Term associations in query expansion: a structural linguistic perspective' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many successful query expansion techniques ignore information about the term dependencies that exist within natural language. However, researchers have recently demonstrated that consistent and significant improvements in retrieval effectiveness can be achieved by explicitly modelling term dependencies within the query expansion process. This has created an increased interest in dependency-based models. State-of-the-art dependency-based approaches primarily model term associations known within structural linguistics as syntagmatic associations, which are formed when terms co-occur together more often than by chance. However, structural linguistics proposes that the meaning of a word is also dependent on its paradigmatic associations, which are formed between words that can substitute for each other without effecting the acceptability of a sentence. Given the reliance on word meanings when a user formulates their query, our approach takes the novel step of modelling both syntagmatic and paradigmatic associations within the query expansion process based on the (pseudo) relevant documents returned in web search. The results demonstrate that this approach can provide significant improvements in web retrieval effectiveness when compared to a strong benchmark retrieval system."
      }
    ]
  },
  {
    "tid":"886",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Fine-Grained Cross-Domain Access Control Mechanism for Social Internet of Things' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The integration of Internet of Things (IoT) and social networks is the important trend of new network technologies, which is Social Internet of Tings (SIoT). In many application scenarios, SIoT are controlled by different authorities. And the network nodes belong to different domain can share the sensor data by standard protocols. Moreover, in a mission-critical application scenario various types of data generate by all kinks of the nodes of SIoT may belong to different security levels, and thus are meant to be accessed only by selected types of users. Therefore, a fine-grained cross-domain access is the core problem for it. To address this, this paper focus on the fine-grained cross-domain access control mechanism of SIoT, which including trust model for certificate authority, user login protocol, cross-domain secure authentication protocol, and cross-domain fine-grained access control. The simulation results show the feasibility and effectiveness of the proposed scheme, which provide strong support for integrating IoT and social networks securely."
      }
    ]
  },
  {
    "tid":"887",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A novel key management and distribution solution for secure video multicast' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In secure video multicast systems, access control is a very important and challenging issue. A common practice is to encrypt the video content using a group key, shared only by legitimate clients. Since the clients can join and leave the multicast session frequently, the group key needs to be updated and delivered to the legitimate clients accordingly in order to provide forward/backward secrecy. Therefore, key management and distribution plays a crucial role in building a secure and scalable video multicast system. Conventional approaches usually use a dedicated secure channel independent of the video data channel, which is expensive and inefficient. In this paper, we propose a novel and scalable Media-dependent Key Management and Distribution (MKMD) solution that embeds key management messages in video data so as to save network bandwidth and enhance the security level, and most importantly, without sacrificing the quality of video content too much. We have built a prototype system of MKMD, and our extensive experimental results show that MKMD is a secure, scalable, reliable and efficient key management and distribution solution for video multicast system."
      }
    ]
  },
  {
    "tid":"888",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'ZMP support areas for multi-contact mobility under frictional constraints.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose a method for checking and enforcing multicontact stability based on the zero-tilting moment point (ZMP). The key to our development is the generalization of ZMP support areas to take into account: 1) frictional constraints and 2) multiple noncoplanar contacts. We introduce and investigate two kinds of ZMP support areas. First, we characterize and provide a fast geometric construction for the support area generated by valid contact forces, with no other constraint on the robot motion. We call this set the full support area. Next, we consider the control of humanoid robots by using the linear pendulum mode (LPM). We observe that the constraints stemming from the LPM induce a shrinking of the support area, even for walking on horizontal floors. We propose an algorithm to compute the new area, which we call the pendular support area. We show that, in the LPM, having the ZMP in the pendular support area is a necessary and sufficient condition for contact stability. Based on these developments, we implement a whole-body controller and generate feasible multicontact motions where an HRP-4 humanoid locomotes in challenging multicontact scenarios."
      }
    ]
  },
  {
    "tid":"889",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Analysing qos trade-offs in wireless sensor networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Quality of Service (QoS) support for wireless sensor networks (WSN) is a fairly new topic that is gaining more and more interest. This paper introduces a method for configuring the nodes of a WSN such that application-level QoS constraints are met. This is a complex task, since the search space is typically extremely large. The method is based on a recent algebraic approach to Pareto analysis, that we use to reason about QoS trade-offs. It features an algorithm that keeps the working set of possible configurations small, by analysing parts of the network in a hierarchical fashion, and meanwhile discarding configurations that are inferior to other configurations. Furthermore, we give WSN models for two different applications, in which QoS trade-offs are made explicit. Test results show that the models are accurate and that the method is scalable and thus practically usable for WSN, even with large numbers of nodes."
      }
    ]
  },
  {
    "tid":"890",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Traffic aware video dissemination over vehicular ad hoc networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Video dissemination to a group of vehicles is one of the many fundamental services envisioned for Vehicular Ad hoc Networks. For this purpose, in this paper we describe VoV, a video dissemination protocol that operates under extreme traffic conditions. Contrary to most existing approaches that focus exclusively on always-connected networks and tackle the broadcast storm problem inherent to them, VoV is designed to operate under any kind of traffic condition. We propose a new geographic-based broadcast suppression mechanism that gives higher priority to broadcast to vehicles inside especial forwarding zones. Furthermore, vehicles store and carry received messages in a local buffer in order to forward them to vehicles that were not covered by the first dissemination process, probably as a result of collisions or intermittent disconnections. Finally, VoV employs a rate control mechanism that sets the pace at which messages must be transmitted in an attempt to avoid channel overloading and to overcome the synchronization effects introduced by the channel hopping mechanism employed by IEEE 802.11p. When compared to two well-known solutions -- UV-CAST and AID -- we show that our proposal is more efficient in terms of message delivery, delay and overhead."
      }
    ]
  },
  {
    "tid":"891",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Simple Random Walks on Radio Networks (Simple Random Walks on Hyper-Graphs)' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In recent years, protocols that are based on the properties of random walks\non graphs have found many applications in communication and information\nnetworks, such as wireless networks, peer-to-peer networks and the Web. For\nwireless networks (and other networks), graphs are actually not the correct\nmodel of the communication; instead hyper-graphs better capture the\ncommunication over a wireless shared channel. Motivated by this example, we\nstudy in this paper random walks on hyper-graphs. First, we formalize the\nrandom walk process on hyper-graphs and generalize key notions from random\nwalks on graphs. We then give the novel definition of radio cover time, namely,\nthe expected time of a random walk to be heard (as opposed to visit) by all\nnodes. We then provide some basic bounds on the radio cover, in particular, we\nshow that while on graphs the radio cover time is O(mn), in hyper-graphs it is\nO(mnr) where n, m and r are the number of nodes, the number of edges and the\nrank of the hyper-graph, respectively. In addition, we define radio hitting\ntimes and give a polynomial algorithm to compute them. We conclude the paper\nwith results on specific hyper-graphs that model wireless networks in one and\ntwo dimensions."
      }
    ]
  },
  {
    "tid":"892",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A comparison of consecutive and concurrent input text entry techniques for mobile phones' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The numeric keypads on mobile phones generally consist of 12 keys (0-9, *, #). Ambiguity arises when the 36-character alpha-numeric English alphabet is mapped onto this smaller number of keys. In this paper, we first present a taxonomy of the various techniques for resolving this ambiguity, dividing them into techniques that use consecutive actions to first select a character grouping and then a character from within that grouping, and those that use concurrent actions to achieve the same end. We then present the design and implementation of a chording approach to text entry that uses concurrent key presses. We conducted a controlled experiment that compared this chording technique to one-handed and two-handed versions of the commonly used MultiTap technique. The results show that the concurrent chording technique significantly outperforms both versions of the consecutive action MultiTap technique."
      }
    ]
  },
  {
    "tid":"893",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Routing performance analysis of human-driven delay tolerant networks using the truncated levy walk model' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The routing performance of delay tolerant networks (DTN) is highly correlated with the distribution of inter-contact times (ICT), the time period between two successive contacts of the same two mobile nodes. As humans are often carriers of mobile communication devices, studying the patterns of human mobility is an essential tool to understand the performance of DTN protocols. From measurement studies of human contact behaviors, we find that their distributions closely resemble a form of power-law distributions called truncated Pareto. Human walk traces has a dichotomy distribution pattern of ICT; it has a power-law tendency up to some time, and decays exponentially after that time. Truncated Pareto distributions offer a simple yet cohesive mathematical model to express this dichotomy in the measured data. Using the residual and relaxation time theory [17] [4], we apply truncated Pareto distributions to quantify the performance of opportunistic routing in DTN. We further show that Truncated Levy walk (TLW) mobility model [22] commonly used in biology to describe the foraging patterns of animals [25], provide the same truncated power-law ICT distributions as observed from the empirical data, especially when mobility is confined within a finite area. This result confirms our recent finding that human walks contain similar statistical characteristics as Levy walks [22]."
      }
    ]
  },
  {
    "tid":"894",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Architecture of a network-aware P2P-TV application: the NAPA-WINE approach.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Peer to Peer streaming (P2P-TV) applications have recently emerged as cheap and efficient solutions to provide real time streaming services over the Internet. For the sake of simplicity, typical P2P-TV systems are designed and optimized following a pure layered approach, thus ignoring the effect of design choices on the underlying transport network. This simple approach, however, may constitute a ..."
      }
    ]
  },
  {
    "tid":"895",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An efficient data aggregation algorithm in delay tolerant vehicular networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We are interested in delay-constrained data aggregation in delay tolerant vehicular networks, in which contact opportunities are usually scarce and thus become critical resources. Data aggregation has been recognized as an effective technique for reducing communication cost while obtaining useful aggregated information. The unique characteristics of the network present great challenges to the delay-constrained aggregation issue. First, there is no always-connected path from a source node to a destination node. Second, there is an intrinsic tradeoff between communication cost and aggregation quality. In this paper we discuss the delay-constrained data aggregation problem in delay tolerant vehicular networks with the objective of maximizing the amount of information collected, which has not been well studied in the literature. We observe that there are heterogeneity and regularity with node contact patterns. Motivated by this observation, we propose an Efficient tree-based Data Aggregation algorithm called EDA. EDA first constructs a data aggregation tree based on the shortest path tree, and then assigns a waiting time budget to each node on the tree using a dynamic programming algorithm. We have performed extensive simulations on real taxi traces and results show that our EDA scheme incurs much lower transmission overhead while achieving the same performance compared with other schemes."
      }
    ]
  },
  {
    "tid":"896",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Introducing Virtual Execution Environments for Application Lifecycle Management and SLA-Driven Resource Distribution within Service Providers' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Resource management is a key challenge that service providers must adequately face in order to ensure their profitability. This paper describes a proof-of-concept framework for facilitating resource management in service providers, which allows reducing costs and at the same time fulfilling the quality of service agreed with the customers. This is accomplished by means of virtualization. Our approach provides application-specific virtual environments and consolidates them in order to achieve a better utilization of the providers resources. In addition, it implements self-adaptive capabilities for dynamically distributing the providers resources among these virtual environments based on Service Level Agreements. The proposed solution has been implemented as a part of the Semantically-Enhanced Resource Allocator prototype developed within the BREIN European project. The evaluation shows that our prototype is able to react in very short time under changing conditions and avoid SLA violations by rescheduling efficiently the resources."
      }
    ]
  },
  {
    "tid":"897",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Evaluating the On-Demand Mobile Charging in Wireless Sensor Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recently, adopting mobile energy chargers to replenish the energy supply of sensor nodes in wireless sensor networks has gained increasing attention from the research community. Different from energy harvesting systems, the utilization of mobile energy chargers is able to provide more reliable energy supply than the dynamic energy harvested from the surrounding environment. While pioneering works on the mobile recharging problem mainly focus on the optimal offline path planning for the mobile chargers, in this work, we aim to lay the theoretical foundation for the on-demand mobile charging problem, where individual sensor nodes request charging from the mobile charger when their energy runs low. Specifically, in this work we analyze the On-Demand Mobile Charging (DMC) problem using a simple but efficient Nearest-Job-Next with Preemption (NJNP) discipline for the mobile charger, and provide analytical results on the system throughput and charging latency from the perspectives of the mobile charger and individual sensor nodes, respectively. To demonstrate how the actual system design can benefit from our analytical results, we present two examples on determining the essential system parameters such as the optimal remaining energy level for individual sensor nodes to send out their recharging requests and the minimal energy capacity required for the mobile charger. Through extensive simulation with real-world system settings, we verify that our analytical results match the simulation results well and the system designs based on our analysis are effective."
      }
    ]
  },
  {
    "tid":"898",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On evolutionary subspace clustering with symbiosis.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Subspace clustering identifies the attribute support for each cluster as well as identifying the location and number of clusters. In the most general case, attributes associated with each cluster could be unique. A multi-objective evolutionary method is proposed to identify the unique attribute support of each cluster while detecting its data instances. The proposed algorithm, symbiotic evolutionary subspace clustering (S-ESC) borrows from \u2018symbiosis\u2019 in the sense that each clustering solution is defined in terms of a host (single member of the host population) and a number of coevolved cluster centroids (or symbionts in an independent symbiont population). Symbionts define clusters and therefore attribute subspaces, whereas hosts define sets of clusters to constitute a non-degenerate solution. The symbiotic representation of S-ESC is the key to making it scalable to high-dimensional datasets, while an integrated subsampling process makes it scalable to tasks with a large number of data items. Benchmarking is performed against a test suite of 59 subspace clustering tasks with four well known comparator algorithms from both the full-dimensional and subspace clustering literature: EM, MINECLUS, PROCLUS, STATPC. Performance of the S-ESC algorithm was found to be robust across a wide cross-section of properties with a common parameterization utilized throughout. This was not the case for the comparator algorithms. Specifically, performance could be sensitive to the particular data distribution or parameter sweeps might be necessary to provide comparable performance. An additional evaluation is performed against a non-symbiotic GA, with S-ESC still returning superior clustering solutions."
      }
    ]
  },
  {
    "tid":"899",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Timely and energy-efficient communications in rural infostation systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The infostation communication paradigm represents a good trade-off between the size of investments in infrastructure and the connectivity needs in rural areas. In fact, exploiting the emerging mesh networking technologies, infostations can be deployed with low cost. In infostation systems, mobile nodes achieve network connectivity when they are in the radio coverage of the infostation. Nevertheless, preliminary to the communication, there is a phase in which discovery must be achieved between mobile nodes and the infostation. This is an energy consuming phase that has significant impact on the performance experienced by mobile nodes. In this article the trade-off between energy efficiency and performance experienced by mobile users is discussed. This is an important problem in rural areas since infostations will often be supplied by photovoltaic or eolic generators, which have a cost that increases as the need for energy increases. To carry the study, analytical results previously obtained for general mobile wireless networks are tailored to infostation systems deployed in rural areas. Such derivations have been utilized to gain insight into the design of energy-efficient protocols for the above application scenario."
      }
    ]
  },
  {
    "tid":"900",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Replica compensated linear regulators for supply-regulated phase-locked loops' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Supply-regulated phase-locked loops rely upon the VCO voltage regulator to maintain a low sensitivity to supply noise and hence low overall jitter. By analyzing regulator supply rejection, we show that in order to simultaneously meet the bandwidth and low dropout requirements, previous regulator implementations used in supply-regulated PLLs suffer from unfavorable tradeoffs between power supply rejection and power consumption. We therefore propose a compensation technique that places the regulator's amplifier in a local replica feedback loop, stabilizing the regulator by increasing the amplifier bandwidth while lowering its gain. Even though the forward gain of the amplifier is reduced, supply noise affects the replica output in addition to the actual output, and therefore the amplifier's gain to reject supply noise is effectively restored. Analysis shows that for reasonable mismatch between the replica and actual loads, regulator performance is uncompromised, and experimental results from a 90 nm SOI test chip confirm that with the same power consumption, the proposed regulator achieves at least 4 dB higher supply rejection than the previous regulator design. Furthermore, simulations show that if not for other supply rejection-limiting components in the PLL, the supply rejection improvement of the proposed regulator is greater than 15 dB."
      }
    ]
  },
  {
    "tid":"901",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Study and Optimize the Process of Batch Small Files Replication' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"I/O performance is always the traditional criterion for the evaluation of storage system. Many researches have been being carried on how to improve the storage system performance, mainly focusing on the storage architecture and I/O optimization for the storage devices. In many application systems, the phenomenon of replicating batch small files between two locations widely exists and always represents poor performance in systems. This paper analyzes and optimizes replication process for batch small files in Linux file system. In local case, six algorithms are achieved by using parallel, consecutive and aggregating polices in different stages of the whole process. In network case, achieve and compress strategies are also introduced and compared with aggregating algorithm. Moreover, the average latency of basic operations in each stage of file I/O can be estimated accurately, which is helpful for future research of file system. The experiment shows that the algorithm of consecutive reading source files and parallel writing target files have the best performance in local replication, and aggregating algorithm also do in network replication."
      }
    ]
  },
  {
    "tid":"902",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Bi-Modal Person Recognition on a Mobile Phone: Using Mobile Phone Data' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a novel fully automatic bi-modal, face and speaker, recognition system which runs in real-time on a mobile phone. The implemented system runs in real-time on a Nokia N900 and demonstrates the feasibility of performing both automatic face and speaker recognition on a mobile phone. We evaluate this recognition system on a novel publicly-available mobile phone database and provide a well defined evaluation protocol. This database was captured almost exclusively using mobile phones and aims to improve research into deploying biometric techniques to mobile devices. We show, on this mobile phone database, that face and speaker recognition can be performed in a mobile environment and using score fusion can improve the performance by more than 25% in terms of error rates."
      }
    ]
  },
  {
    "tid":"903",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Unified Analysis of Discontinuous Galerkin Methods for Elliptic Problems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We provide a framework for the analysis of a large class of discontinuous methods for second-order elliptic problems. It allows for the understanding and comparison of most of the discontinuous Galerkin methods that have been proposed over the past three decades for the numerical treatment of elliptic problems."
      }
    ]
  },
  {
    "tid":"904",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A MapReduce approach to Gi*(d) spatial statistic' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Managing and analyzing massive spatial datasets as supported by GIS and spatial analysis is becoming crucial to geospatial problem-solving and decision-making. MapReduce provides a data-centric computational model through which highly scalable spatial analysis computation can be achieved. However, it is challenging to leverage multi-dimensional spatial characteristics on the horizontally-partitioned and transparently managed MapReduce data system for improving the computational performance of spatial analysis. This paper tackles this challenge through the development of MapReduce-based computation of Gi*(d) -- a spatial statistic for detecting local clustering. Without exploiting spatial characteristics, Gi*(d) computation for a particular location requires pair-wise distance calculation for all points of a given dataset. A spatial locality-based storage and indexing strategy is developed to associate spatial locality with storage locality on MapReduce platform. Based on a spatial indexing method, unnecessary map tasks can be eliminated for a MapReduce job, thus significantly improving the overall computation performance. To leverage underlying parallelism on storage nodes, an application-level load balancing mechanism is developed to produce even loads among map tasks based on adaptive spatial domain decomposition. Experiments show the effectiveness of the developed storage and indexing strategy with different distance parameter settings. Significant reduction on execution time for all-point computation is observed through the use of the application-level load balancing mechanism."
      }
    ]
  },
  {
    "tid":"905",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Recovering Transparent Shape From Time-Of-Flight Distortion' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a method for recovering shape and normal of a transparent object from a single viewpoint using a Time-of-Flight (ToF) camera. Our method is built upon the fact that the speed of light varies with the refractive index of the medium and therefore the depth measurement of a transparent object with a ToF camera may be distorted. We show that, from this ToF distortion, the refractive light path can be uniquely determined by estimating a single parameter. We estimate this parameter by introducing a surface normal consistency between the one determined by a light path candidate and the other computed from the corresponding shape. The proposed method is evaluated by both simulation and real-world experiments and shows faithful transparent shape recovery."
      }
    ]
  },
  {
    "tid":"906",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Distributed In-Memory Computing on Binary RRAM Crossbar.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The recently emerging resistive random-access memory (RRAM) can provide nonvolatile memory storage but also intrinsic computing for matrix-vector multiplication, which is ideal for the low-power and high-throughput data analytics accelerator performed in memory. However, the existing RRAM crossbar--based computing is mainly assumed as a multilevel analog computing, whose result is sensitive to process nonuniformity as well as additional overhead from AD-conversion and I/O. In this article, we explore the matrix-vector multiplication accelerator on a binary RRAM crossbar with adaptive 1-bit-comparator--based parallel conversion. Moreover, a distributed in-memory computing architecture is also developed with the according control protocol. Both memory array and logic accelerator are implemented on the binary RRAM crossbar, where the logic-memory pair can be distributed with the control bus protocol. Experimental results have shown that compared to the analog RRAM crossbar, the proposed binary RRAM crossbar can achieve significant area savings with better calculation accuracy. Moreover, significant speedup can be achieved for matrix-vector multiplication in neural network--based machine learning such that the overall training and testing time can be both reduced. In addition, large energy savings can be also achieved when compared to the traditional CMOS-based out-of-memory computing architecture."
      }
    ]
  },
  {
    "tid":"907",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'SpySMAC: Automated Configuration and Performance Analysis of SAT Solvers' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Most modern SAT solvers expose a range of parameters to allow some customization for improving performance on specific types of instances. Performing this customization manually can be challenging and time-consuming, and as a consequence several automated algorithm configuration methods have been developed for this purpose. Although automatic algorithm configuration has already been applied successfully to many different SAT solvers, a comprehensive analysis of the configuration process is usually not readily available to users. Here, we present SpySMAC to address this gap by providing a lightweight and easy-to-use toolbox for (i) automatic configuration of SAT solvers in different settings, (ii) a thorough performance analysis comparing the best found configuration to the default one, and (iii) an assessment of each parameter's importance using the fANOVA framework. To showcase our tool, we apply it to Lingeling and probSAT, two state-of-the-art solvers with very different characteristics."
      }
    ]
  },
  {
    "tid":"908",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Blurred persistence in transactional persistent memory' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Persistent memory provides data persistence at main memory level and enables memory-level storage systems. To ensure consistency of the storage systems, memory writes need to be transactional and are carefully moved across the boundary between the volatile CPU cache and the persistent memory. Unfortunately, the CPU cache is hardware-controlled, and it incurs high overhead for programs to track and move data blocks from being volatile to persistent. In this paper, we propose a software-based mechanism, Blurred Persistence, to blur the volatility-persistence boundary, so as to reduce the overhead in transaction support. Blurred Persistence consists of two techniques. First, Execution in Log executes a transaction in the log to eliminate duplicated data copies for execution. It allows the persistence of volatile uncommitted data, which can be detected by reorganizing the log structure. Second, Volatile Checkpoint with Bulk Persistence allows the committed data to aggressively stay volatile by leveraging the data durability in the log, as long as the commit order across threads is kept. By doing so, it reduces the frequency of forced persistence and improves cache efficiency. Evaluations show that our mechanism improves system performance by 56.3% to 143.7% for a variety of workloads."
      }
    ]
  },
  {
    "tid":"909",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Potential Game Approach For Spectrum Sharing In Distributed Cognitive Radio Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In a spectrum sharing system lower priority users are allowed to spatially reuse the spectrum allocated to higher priority users as long as they do not disrupt communications of the latter Therefore to improve spectrum utilization an important requirement for the former users is to manage the interference and ensure that the latter users can maintain reliable communications In the present paper a game theoretic framework of joint channel selection and power allocation for spectrum sharing in distributed cognitive radio networks is proposed First a utility function that captures the cooperative behavior to manage the interference and the satisfaction level to improve the throughput of the lower priority users is de fined Next based on the defined utility function the proposed framework can be formulated as a potential game thus it is guaranteed to converge to a Nash equilibrium when the best response dynamic is performed Simulation results show the convergence of the proposed potential game and reveal that performance improvements in terms of network throughput of the lower priority users and outage probability of the higher priority users can be achieved by the introduction of an adaptive coefficient adjustment scheme in the proposed utility function at the expense of the convergence to the Nash equilibrium"
      }
    ]
  },
  {
    "tid":"910",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Simple classification of walking activities using commodity smart phones' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"People interact with mobile computing devices everywhere, while sitting, walking, running or even driving. Adapting the interface to suit these contexts is important, thus this paper proposes a simple human activity classification system. Our approach uses a vector magnitude recognition technique to detect and classify when a person is stationary (or not walking), casually walking, or jogging, without any prior training. The user study has confirmed the accuracy."
      }
    ]
  },
  {
    "tid":"911",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Cross-modal Moment Localization in Videos.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we address the temporal moment localization issue, namely, localizing a video moment described by a natural language query in an untrimmed video. This is a general yet challenging vision-language task since it requires not only the localization of moments, but also the multimodal comprehension of textual-temporal information (e.g., \"first\" and \"leaving\") that helps to distinguish the desired moment from the others, especially those with the similar visual content. While existing studies treat a given language query as a single unit, we propose to decompose it into two components: the relevant cue related to the desired moment localization and the irrelevant one meaningless to the localization. This allows us to flexibly adapt to arbitrary queries in an end-to-end framework. In our proposed model, a language-temporal attention network is utilized to learn the word attention based on the temporal context information in the video. Therefore, our model can automatically select \"what words to listen to\" for localizing the desired moment. We evaluate the proposed model on two public benchmark datasets: DiDeMo and Charades-STA. The experimental results verify its superiority over several state-of-the-art methods.\n\n"
      }
    ]
  },
  {
    "tid":"912",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Towards a Framework for Mapping Between UML/OCL and XML/XQuery' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The Unified Modeling Language is the standard language for modeling systems. UML has been extended to model web applications. At the same time, Web technology has become largely relying on XML documents. The structure of XML documents, namely the XML schema or DTD for these documents can be modeled using UML data structures. UML tools are usually concerned with the generation of the structure and behavior of the system that is captured by models in their equivalents in the selected platform. In this paper we introduce a novel approach for the integration between UML and XML families of technologies. We model the structure of XML using UML class diagrams and based on this, we study how queries on XML documents, namely XQuery expressions can be described using UML techniques. Here we show that modeling of XML documents and its queries represented by XQuery expressions is possible using the querying capabilities of UML Class diagram and the Object Constraint Language (OCL). As a result, we see how these two technologies compare, what the advantages of both technologies are and how they can be combined."
      }
    ]
  },
  {
    "tid":"913",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Mixed Deterministic Model for Coreference Resolution.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a mixed deterministic model for coreference resolution in the CoNLL-2012 shared task. We separate the two main stages of our model, mention detection and coreference resolution, into several sub-tasks which are solved by machine learning method and deterministic rules based on multi-filters, such as lexical, syntactic, semantic, gender and number information. We participate in the closed track for English and Chinese, and also submit an open result for Chinese using tools to generate the required features. Finally, we reach the average F1 scores 58.68, 60.69 and 61.02 on the English closed task, Chinese closed and open tasks."
      }
    ]
  },
  {
    "tid":"914",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Interfaces, attributes, and use cases: A compass for SDN.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The term Software Defined Networking (SDN) is prevalent in today&#39;s discussion about future communication networks. As with any new term or paradigm, however, no consistent definition regarding this technology has formed. The fragmented view on SDN results in legacy products being passed off by equipment vendors as SDN, academics mixing up the attributes of SDN with those of network virtualization,..."
      }
    ]
  },
  {
    "tid":"915",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Increasing task consolidation efficiency by using more accurate resource estimations' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cloud providers aim to provide computing services for a wide range of applications, such as web applications, emails, web searches, and map-reduce jobs. These applications are commonly scheduled to run on multi-sites clusters that nowadays are becoming larger and more heterogeneous. A major challenge is to efficiently utilize the cluster's available resources, in particular to maximize overall machine utilization levels while minimizing the application waiting time. We propose a methodology for achieving an efficient utilization of the cluster's resources while providing the users with fast and reliable computing services. The methodology consists of three main modules: (i) a prediction module that forecasts the maximum resource requirement of a task; (ii) a scheduling module that efficiently allocates tasks to machines; and (iii) a monitoring module that tracks the levels of utilization of the machines and tasks, and can evict one or more tasks from the machines for rescheduling if required.There are multiple ways of predicting task requirements, scheduling tasks on machines and evicting task from machines. The decisions made in each module can have significant impact on not only the objective function but also on the efficiency of the decisions made in other components. We therefore study these different combinations and analyze their interaction in order to determine a configuration that meets the objective of the problem. To test our methodology we have developed a simulator and provide a detailed analysis of these interactions between different modules by using a publicly available trace from a large Google cluster ( ~ 12,000 machines). Our results show that the impact of more accurate resource estimations for the scheduling of tasks and evicting lower priority tasks in case of over-utilization can lead to an increase in the average utilization of the cluster, a reduction in the number of tasks being evicted, and a reduction in task waiting time. We proposed an algorithm that can balance two conflicting criteria.We described three prediction techniques and a scalable scheduling heuristic.We investigate the behavior of several eviction strategies.We described a simulation framework to test the effectiveness of our approach."
      }
    ]
  },
  {
    "tid":"916",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Cyclostationary noise modeling in narrowband powerline communication for Smart Grid applications' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A Smart Grid intelligently monitors and controls energy flows in an electric grid. Having up-to-date distributed readings of grid conditions helps utilities efficiently scale generation up or down to meet demand. Narrowband powerline communication (PLC) systems can provide these up-to-date readings from subscribers to the local utility over existing power lines. A key challenge in PLC systems is overcoming additive non-Gaussian noise. In this paper, we propose to use a cyclostationary model for the dominant component of additive non-Gaussian noise. The key contributions are (1) fitting measured data from outdoor narrowband PLC system field trials to a cyclostationary model, and (2) developing a cyclostationary noise generation model that fits measured data. We found that the period in the cyclostationary model matched half of the period of the main powerline frequency, which is consistent with previous work in indoor PLC additive noise modeling."
      }
    ]
  },
  {
    "tid":"917",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Effective and efficient similarity searching in motion capture data.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Motion capture data describe human movements in the form of spatio-temporal trajectories of skeleton joints. Intelligent management of such complex data is a challenging task for computers which requires an effective concept of motion similarity. However, evaluating the pair-wise similarity is a difficult problem as a single action can be performed by various actors in different ways, speeds or starting positions. Recent methods usually model the motion similarity by comparing customized features using distance-based functions or specialized machine-learning classifiers. By combining both these approaches, we transform the problem of comparing motions of variable sizes into the problem of comparing fixed-size vectors. Specifically, each rather-short motion is encoded into a compact visual representation from which a highly descriptive 4,096-dimensional feature vector is extracted using a fine-tuned deep convolutional neural network. The advantage is that the fixed-size features are compared by the Euclidean distance which enables efficient motion indexing by any metric-based index structure. Another advantage of the proposed approach is its tolerance towards an imprecise action segmentation, the variance in movement speed, and a lower data quality. All these properties together bring new possibilities for effective and efficient large-scale retrieval."
      }
    ]
  },
  {
    "tid":"918",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Proportional fairness of call blocking probability' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"To guarantee proportional fairness (PF) is an important way to realize predictable and controllable service differentiation. In this paper, a proportional differentiation call admission control (CAC) algorithm, named PBS-PF, is proposed to guarantee the network services' PF in terms of call blocking probability (CBP). PBS-PF is based on the partial bandwidth sharing CAC scheme due to this scheme's high bandwidth utilization rate. PBS-PF dynamically calculates the bandwidth admission threshold of each service class to control the blocking call number for guaranteeing the CBP ratio of high and low priority services to accord with a given value. The simulations show that the CBP of each service class changes with the network load, but their ratio approximately keeps unchanged, which proves the effectiveness of PBS-PF. At last, we analyze the time complexity of PBS-PF."
      }
    ]
  },
  {
    "tid":"919",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An efficient broadcast relay scheme for MANETs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper focuses on the upper bound of broadcast coverage area to achieve high delivery rate and low number of rebroadcasts. In order to improve the performance of geometry-based protocols, a novel relative position-based broadcast relay (RPBR) approach is proposed, in which the forwarding hosts are able to rebroadcast packets effectively, especially for heavy host density. The numerical results show that the proposed RPBR scheme is able to enhance the distance-based protocol to rebroadcast packets effectively by increasing the packet delivery ratio up to 10%. On the other hand, for the angle-based protocol associated with RPBR, the number of forwarding hosts can be reduced by 30%. A performance comparison of the topology-based broadcast protocols and the RPBR scheme combined with geometry-based in dynamic ad hoc network environment is presented."
      }
    ]
  },
  {
    "tid":"920",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Object-oriented Design and Push Web Server based Framework for Physical Object Interactions and Services' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"One of the substantial issues in ubiquitous computing is the automatic processing of information from real world objects and enabling their interactions in the background. This scenario requires a framework on which such information processing and object interaction can be supported. This article presents our research progress in developing a framework based on the object-oriented design approach and the use of a push web server. With the concept of object abstraction, an object can hide its internal structure from the outside world, which can make the object secure. Moreover, object interaction can be conducted via message exchanges, which makes the interface simple and standardized and the heterogeneous objects can be easily handled as well. Instead of using the traditional pull web server, a push web server, i.e., Comet, which runs on top of HTTP protocol is used to exchange messages. To this, object interactions can be operated smoothly and seamlessly in real time with shorter delay."
      }
    ]
  },
  {
    "tid":"921",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Experimental Evaluation of Point-of-interest Recommendation in Location-based Social Networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Point-of-interest (POI) recommendation is an important service to Location-Based Social Networks (LBSNs) that can benefit both users and businesses. In recent years, a number of POI recommender systems have been proposed, but there is still a lack of systematical comparison thereof. In this paper, we provide an all-around evaluation of 12 state-of-the-art POI recommendation models. From the evaluation, we obtain several important findings, based on which we can better understand and utilize POI recommendation models in various scenarios. We anticipate this work to provide readers with an overall picture of the cutting-edge research on POI recommendation."
      }
    ]
  },
  {
    "tid":"922",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Issues in the classification of disease instances with ontologies.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Ontologies define classes of entities and their interrelations. They are. used to organize data according to a theory, of the domain. Towards that end, ontologies provide class definitions (i.e., the necessary and sufficient conditions for defining class membership). In medical ontologies, it is often difficult to establish such definitions for diseases. We use three examples (anemia, leukemia and schizophrenia) to illustrate the limitations of ontologies as classification resources. We show that eligibility criteria are often more useful than the Aristotelian definitions traditionally, used in ontologies. Examples of eligibility criteria for diseases include complex predicates such as 'x is an instance of the class C when at least n criteria among in are verified' and symptoms must last at least one month if not treated, but less than one month, if effectively, treated'. References to normality, and abnormality are often found in disease definitions, but the operational definition of these references (i.e., the statistical and contextual information necessary to define them) is rarefy provided. We conclude that knowledge bases that include probabilistic and statistical know-ledge as well as rule-based criteria are more useful that? Aristotelian definitions for representing the predicates defined by necessary and sufficient conditions. Rich knowledge bases are needed to clarify the relations between individuals and classes it? various studies and applications. However, as ontologies represent relations among classes, they can play, a supporting role in disease classification services built primarily, on knowledge bases."
      }
    ]
  },
  {
    "tid":"923",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'ARCS: an aggregated related column scoring scheme for aligned sequences.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Biologists frequently align multiple biological sequences to determine consensus sequences and/or search for predominant residues and conserved regions. Particularly, determining conserved regions in an alignment is one of the most important activities. Since protein sequences are often several-hundred residues or longer, it is difficult to distinguish biologically important conserved regions (motifs or domains) from others. The widely used tools, Logos, Al2co, Confind, and the entropy-based method, often fail to highlight such regions. Thus a computational tool that can highlight biologically important regions accurately will be highly desired.This paper presents a new scoring scheme ARCS (Aggregated Related Column Score) for aligned biological sequences. ARCS method considers not only the traditional character similarity measure but also column correlation. In an extensive experimental evaluation using 533 PROSITE patterns, ARCS is able to highlight the motif regions with up to 77.7% accuracy corresponding to the top three peaks.The source code is available on http://bio.informatics.indiana.edu/projects/arcs and http://goldengate.case.edu/projects/arcs"
      }
    ]
  },
  {
    "tid":"924",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Physikit: Data Engagement Through Physical Ambient Visualizations in the Home.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Internet of things (IoT) devices and sensor kits have the potential to democratize the access, use, and appropriation of data. Despite the increased availability of low cost sensors, most of the produced data is \\\"black box\\\" in nature: users often do not know how to access or interpret data. We propose a \\\"human-data design\\\" approach in which end-users are given tools to create, share, and use data through tangible and physical visualizations. This paper introduces Physikit, a system designed to allow users to explore and engage with environmental data through physical ambient visualizations. We report on the design and implementation of Physikit, and present a two-week field study which showed that participants got an increased sense of the meaning of data, embellished and appropriated the basic visualizations to make them blend into their homes, and used the visualizations as a probe for community engagement and social behavior."
      }
    ]
  },
  {
    "tid":"925",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Quality of Service Support for Fine-Grained Sharing on GPUs.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"GPUs have been widely adopted in data centers to provide acceleration services to many applications. Sharing a GPU is increasingly important for better processing throughput and energy efficiency. However, quality of service (QoS) among concurrent applications is minimally supported. Previous efforts are too coarse-grained and not scalable with increasing QoS requirements. We propose QoS mechanisms for a fine-grained form of GPU sharing. Our QoS support can provide control over the progress of kernels on a per cycle basis and the amount of thread-level parallelism of each kernel. Due to accurate resource management, our QoS support has significantly better scalability compared with previous best efforts. Evaluations show that, when the GPU is shared by three kernels, two of which have QoS goals, the proposed techniques achieve QoS goals 43.8% more often than previous techniques and have 20.5% higher throughput."
      }
    ]
  },
  {
    "tid":"926",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Dual-Consistency Cache Coherence Protocol' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Weak memory consistency models can maximize system performance by enabling hardware and compiler optimizations, but increase programming complexity since they do not match programmers' intuition. The design of an efficient system with an intuitive memory model is an open challenge. This paper proposes SPEL, a dual-consistency cache coherence protocol which simultaneously guarantees the strongest memory consistency model provided by the hardware and yields improvements in both performance and energy consumption. The design of the protocol exploits a compile-time identification of code regions which can be executed under a less restrictive, thus optimized protocol, without harming correctness. Outside these regions, code is executed under a more restrictive protocol which enforces sequential consistency. Compared to a standard directory protocol, we show improvements in performance of 24% and reductions in energy consumption of 32%, on average, for a 64-core chip multiprocessor."
      }
    ]
  },
  {
    "tid":"927",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Human Centric Recognition Of 3d Ear Models' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Comparing ear photographs is considered to be an important aspect of disaster victim identification and other forensic and security applications. An interesting approach concerns the construction of 3D ear models by fitting the parameters of a 'standard' ear shape, in order to transform it into an optimal approximation of a 3D ear image. A feature list is then extracted from each 3D ear model and used in the recognition process. In this paper, we study how the quality and usability of a recognition process can be improved by computational intelligence techniques. More specifically, we study and illustrate how bipolar data modelling and aggregation techniques can be used for improving the representation and handling of data imperfections. A novel bipolar measure for computing the similarity between corresponding feature lists is proposed. This measure is based on the Minkowski distance, but explicitly deals with hesitation that is caused by bad image quality. Moreover, we investigate how forensic expert knowledge can be adequately reflected in the recognition process. For that reason, a hierarchically structured comparison technique for feature sets and other characteristics is proposed. Comparison results are expressed by bipolar satisfaction degrees and properly aggregated to an overall result. The benefits and added value of the novel technique are discussed and demonstrated by an illustrative example."
      }
    ]
  },
  {
    "tid":"928",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Understanding and Predicting Graded Search Satisfaction' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Understanding and estimating satisfaction with search engines is an important aspect of evaluating retrieval performance. Research to date has modeled and predicted search satisfaction on a binary scale, i.e., the searchers are either satisfied or dissatisfied with their search outcome. However, users' search experience is a complex construct and there are different degrees of satisfaction. As such, binary classification of satisfaction may be limiting. To the best of our knowledge, we are the first to study the problem of understanding and predicting graded (multi-level) search satisfaction. We ex-amine sessions mined from search engine logs, where searcher satisfaction was also assessed on multi-point scale by human annotators. Leveraging these search log data, we observe rich and non-monotonous changes in search behavior in sessions with different degrees of satisfaction. The findings suggest that we should predict finer-grained satisfaction levels. To address this issue, we model search satisfaction using features indicating search outcome, search effort, and changes in both outcome and effort during a session. We show that our approach can predict subtle changes in search satisfaction more accurately than state-of-the-art methods, affording greater insight into search satisfaction. The strong performance of our models has implications for search providers seeking to accu-rately measure satisfaction with their services."
      }
    ]
  },
  {
    "tid":"929",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An adaptive stabilization framework for distributed hash tables' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Distributed Hash Tables (DHT) algorithms obtain good lookup performance bounds by using deterministic rules to organize peer nodes into an overlay network. To preserve the invariants of the overlay network, DHTs use stabilization procedures that reorganize the topology graph when participating nodes join or fail. Most DHTs use periodic stabilization, in which peers perform stabilization at fixed intervals of time, disregarding the rate of change in overlay topology; this may lead to poor performance and large stabilization-induced communication overhead. We propose a novel adaptive stabilization framework that takes into consideration the continuous evolution in network conditions. Each peer collects statistical data about the network and dynamically adjusts its stabilization rate based on the analysis of the data. The objective of our scheme is to maintain nominal network performance and to minimize the communication overhead of stabilization."
      }
    ]
  },
  {
    "tid":"930",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Depth-Based Hand Pose Estimation: Data, Methods, and Challenges' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Hand pose estimation has matured rapidly in recent years. The introduction of commodity depth sensors and a multitude of practical applications have spurred new advances. We provide an extensive analysis of the state-of-the-art, focusing on hand pose estimation from a single depth frame. To do so, we have implemented a considerable number of systems, and will release all software and evaluation code. We summarize important conclusions here: (1) Pose estimation appears roughly solved for scenes with isolated hands. However, methods still struggle to analyze cluttered scenes where hands may be interacting with nearby objects and surfaces. To spur further progress we introduce a challenging new dataset with diverse, cluttered scenes. (2) Many methods evaluate themselves with disparate criteria, making comparisons difficult. We define a consistent evaluation criteria, rigorously motivated by human experiments. (3) We introduce a simple nearest-neighbor baseline that outperforms most existing systems. This implies that most systems do not generalize beyond their training sets. This also reinforces the under-appreciated point that training data is as important as the model itself. We conclude with directions for future progress."
      }
    ]
  },
  {
    "tid":"931",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Case-based reasoning in comparative effectiveness research' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A medical case, as defined by the medical field, is the fundamental building block of medical knowledge. We combine the concept of medical case with indirect evidence from similar patients so as to enable the performance of CER (comparative effectiveness research) treatment comparisons for personalization of medical care. We propose that the vehicle by which this goal is achieved is provided by the integration of case-based reasoning and CER considerations. We discuss the challenges involved and propose a roadmap for creating scientifically sound real-world evidence for assisting personalized treatment decisions."
      }
    ]
  },
  {
    "tid":"932",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Predicting success or failure of brace treatment for adolescents with idiopathic scoliosis.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Adolescent idiopathic scoliosis (AIS) is a three-dimensional spinal deformity. Brace treatment is a common non-surgical treatment, intended to prevent progression (worsening) of the condition during adolescence. Estimating a braced patient's risk of progression is an essential part of planning treatment, so method for predicting this risk would be a useful decision support tool for practitioners. This work attempts to discover whether failure of brace treatment (progression) can be predicted at the start of treatment. Records were obtained for 62 AIS patients who had completed brace treatment. Subjects were labeled as &amp;quot;progressive&amp;quot; if their condition had progressed despite brace treatment and &amp;quot;non-progressive&amp;quot; otherwise. Wrapper-based feature selection selected two useful predictor variables from a list of 14 clinical measurements taken from the records. A logistic regression model was trained to classify patients as &amp;quot;progressive&amp;quot; or &amp;quot;non-progressive&amp;quot; using these two variables. The logistic regression model's simplicity and interpretability should facilitate its clinical acceptance. The model was tested on data from an additional 28 patients and found to be 75\u00a0% accurate. This accuracy is sufficient to make the predictions clinically useful. It can be used online: http://www.ece.ualberta.ca/~dchalmer/SimpleBracePredictor.html ."
      }
    ]
  },
  {
    "tid":"933",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Robot with a Decentralized Consensus-making Mechanism Based on the Immune System' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In recent years much attention has been focused on behavior-based artificial intelligence(AI), which has already demonstrated its robustness and flexibility against dynamically changing world. However, in this approach, the followings have not yet been resolved: how do we construct an appropriate arbitration mechanism, and how do we prepare appropriate competence modules. In this paper, to overcome these problems, we propose a new decentralized consensus-making system inspired by the biological immune system. And we apply our proposed method to behavior arbitration for an autonomous mobile robot, namely garbage collecting problem that takes into account of the concept of self-sufficiency. To verify the feasibility of our method, we carry out some simulations. In addition, we investigate two types of adaptation mechanisms, and try to evolve the proposed artificial immune network using reinforcement signals."
      }
    ]
  },
  {
    "tid":"934",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Comparative Study on Two Types of Automated Container Terminal Systems.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper introduces a new type of automated container terminal system, which utilizes multistory frame bridges and rail-mounted trolleys to transport containers between the quay and the yard. For this new design concept, this paper makes an explorative study to identify the challenges and opportunity for it to be applied in transshipment hubs. We compare the transport efficiency and stacking cap..."
      }
    ]
  },
  {
    "tid":"935",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Hybrid estimation for stochastic piecewise linear systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose a hybrid estimation algorithm based on Bayesian inference approach for stochastic piecewise linear systems. We derive the discrete state (or mode) transition probabilities based on a system of inequalities that describe the probabilistic state space partition of the system. Simulation results are presented to demonstrate the performance of the algorithm."
      }
    ]
  },
  {
    "tid":"936",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'pBMDS: a behavior-based malware detection system for cellphone devices' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Computing environments on cellphones, especially smartphones, are becoming more open and general-purpose, thus they also become attractive targets of malware. Cellphone malware not only causes privacy leakage, extra charges, and depletion of battery power, but also generates malicious traffic and drains down mobile network and service capacity. In this work we devise a novel behavior-based malware detection system named pBMDS, which adopts a probabilistic approach through correlating user inputs with system calls to detect anomalous activities in cellphones. pBMDS observes unique behaviors of the mobile phone applications and the operating users on input and output constrained devices, and leverages a Hidden Markov Model (HMM) to learn application and user behaviors from two major aspects: process state transitions and user operational patterns. Built on these, pBDMS identifies behavioral differences between malware and human users. Through extensive experiments on major smartphone platforms, we show that pBMDS can be easily deployed to existing smartphone hardware and it achieves high detection accuracy and low false positive rates in protecting major applications in smartphones."
      }
    ]
  },
  {
    "tid":"937",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Benefit based cache data placement and update for mobile peer to peer networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mobile Peer to Peer (MP2P) networks provide decentralization, self-organization, scalability characters, but suffer from high latency and link break problems. In this paper, we study the cache/replication placement and cache update problems arising in such kind of networks. While researchers have proposed various replication placement algorithms to place data across the network to address the problem, it was proven as NP-hard. As a result, many heuristic algorithms have been brought forward for solving the problem. In our paper, we propose an effective and low cost cache placement strategy combined with an update scheme which can be easily implemented in a decentralized way. Extensive experiments are conducted to demonstrate the efficiency of the cache placement and update scheme."
      }
    ]
  },
  {
    "tid":"938",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Statistically efficient methods for pitch and DOA estimation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Traditionally, direction-of-arrival (DOA) and pitch estimation of multichannel, periodic sources have been considered as two separate problems. Separate estimation may render the task of resolving sources with similar DOA or pitch impossible, and it may decrease the estimation accuracy. Therefore, it was recently considered to estimate the DOA and pitch jointly. In this paper, we propose two novel methods for DOA and pitch estimation. They both yield maximum-likelihood estimates in white Gaussian noise scenarios, where the SNR may be different across channels, as opposed to state-of-the-art methods. The first method is a joint estimator, whereas the latter use a cascaded approach, but with a much lower computational complexity. The simulation results confirm that the proposed methods outperform state-of-the-art methods in terms of estimation accuracy in both synthetic and real-life signal scenarios."
      }
    ]
  },
  {
    "tid":"939",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Non-Rigid Metric Shape and Motion Recovery from Uncalibrated Images Using Priors' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we focus on the estimation of the 3D Euclidean shape and motion of a non-rigid object which is moving rigidly while deforming and is observed by a perspective camera. Our method exploits the fact that it is often a reasonable assumption that some of the points are deforming throughout the sequence while others remain rigid. First we use an automatic segmentation algorithm to identify the set of rigid points which in turn is used to estimate the internal camera calibration parameters and the overall rigid motion. Finally we formalise the problem of non-rigid shape estimation as a constrained non-linear minimization adding priors on the degree of deformability of each point. We perform experiments on synthetic and real data which show firstly that even when using a minimal set of rigid points it is possible to obtain reliable metric information and secondly that the shape priors help to disambiguate the contribution to the image motion caused by the deformation and the perspective distortion."
      }
    ]
  },
  {
    "tid":"940",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Approximate regional sequence matching for genomic databases' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recent advances in computational biology have raised sequence matching requirements that result in new types of sequence database problems. In this work, we introduce an important class of such problems, the approximate regional sequence matching (ARSM) problem. Given a data and a pattern sequence, an ARSM result is an approximate occurrence of a region of the pattern in the data sequence under two conditions. First, the region must contain a predetermined area of the pattern sequence, termed core. Second, the allowable deviation between the region of the pattern and its occurrence in the data sequence depends on the length of the region. We propose the PS-ARSM method that processes holistically the regions of a pattern, taking advantage of their overlaps to efficiently identify the ARSM results. Its performance is evaluated with respect to existing techniques adapted to the ARSM problem."
      }
    ]
  },
  {
    "tid":"941",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Static strands: safely collapsing dependence chains for increasing embedded power efficiency' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Modern embedded processors are designed to maximize execution efficiency--the amount of performance achieved per unit of energy dissipated while meeting minimum performance levels. To increase this efficiency we propose utilizing static strands, dependence chains without fan-out which are exposed by a compiler pass. These dependent instructions are resequenced to be sequential and annotated to communicate their location to the hardware. Importantly, this modified application is binary compatible and functionally identical to the original, allowing transparent execution on a baseline processor. However, these static strands can be easily collapsed and optimized by simple processor modifications, significantly reducing the workload energy. Results show that over 30% of MediaBench and Spec2000int dynamic instructions can be collapsed, reducing issue logic energy by 16 to 24%, bypass energy 17 to 20%, and register file energy 13 to 14%. Additionally, by increasing the effective capactity of pipeline resources by almost a third, average IPC can be improved up to 15%. This performance gain can then be traded in for a lower clock frequency to maintain a basline level of performance, reducing energy further."
      }
    ]
  },
  {
    "tid":"942",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Enabling public verifiability and data dynamics for storage security in cloud computing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cloud Computing has been envisioned as the next-generation architecture of IT Enterprise. It moves the application software and databases to the centralized large data centers, where the management of the data and services may not be fully trustworthy. This unique paradigm brings about many new security challenges, which have not been well understood. This work studies the problem of ensuring the integrity of data storage in Cloud Computing. In particular, we consider the task of allowing a third party auditor (TPA), on behalf of the cloud client, to verify the integrity of the dynamic data stored in the cloud. The introduction of TPA eliminates the involvement of client through the auditing of whether his data stored in the cloud is indeed intact, which can be important in achieving economies of scale for Cloud Computing. The support for data dynamics via the most general forms of data operation, such as block modification, insertion and deletion, is also a significant step toward practicality, since services in Cloud Computing are not limited to archive or backup data only. While prior works on ensuring remote data integrity often lacks the support of either public verifiability or dynamic data operations, this paper achieves both. We first identify the difficulties and potential security problems of direct extensions with fully dynamic data updates from prior works and then show how to construct an elegant verification scheme for seamless integration of these two salient features in our protocol design. In particular, to achieve efficient data dynamics, we improve the Proof of Retrievability model [1] by manipulating the classic Merkle Hash Tree (MHT) construction for block tag authentication. Extensive security and performance analysis show that the proposed scheme is highly efficient and provably secure."
      }
    ]
  },
  {
    "tid":"943",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Multi-objective Perspective for Operator Scheduling using Fine-grained DVS Architecture' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"  The stringent power budget of fine grained power managed digital integrated circuits have driven chip designers to optimize power at the cost of area and delay, which were the traditional cost criteria for circuit optimization. The emerging scenario motivates us to revisit the classical operator scheduling problem under the availability of DVFS enabled functional units that can trade-off cycles with power. We study the design space defined due to this trade-off and present a branch-and-bound(B/B) algorithm to explore this state space and report the pareto-optimal front with respect to area and power. The scheduling also aims at maximum resource sharing and is able to attain sufficient area and power gains for complex benchmarks when timing constraints are relaxed by sufficient amount. Experimental results show that the algorithm that operates without any user constraint(area/power) is able to solve the problem for most available benchmarks, and the use of power budget or area budget constraints leads to significant performance gain. "
      }
    ]
  },
  {
    "tid":"944",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On the DoF Region of the MIMO Gaussian Two-User Interference Channel With an Instantaneous Relay.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper studies inner and outer bounds on the degrees of freedom (DoF) region of the multi-antenna two-user Gaussian interference channel with an instantaneous relay (IR) or relay without delay. It is assumed that the two transmitters and the two receivers have M antennas, while the IR receives through Nr antennas and transmits through Nt antennas. In the proposed achievable scheme, which gener..."
      }
    ]
  },
  {
    "tid":"945",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Design and strain selection criteria for bacterial communication networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we discuss data and methodological challenges for building bacterial communication networks using two examples: E. coli as a flagellate bacterium and G. sulfurreducens as a biofilm forming bacterium. We first highlight the link between the bacterial network communication design with respect to metabolic information processing design. The potentialities of designing routing network schemes described previously in the literature and based on bacteria motility and genetic message exchanges will depend on the genes coding for the intracellular and intercellular signalling pathways. In bacteria, the \u201cmobilome\u201d is related to horizontal gene transfer. Bacteria trade-off the acquisition of new genes which could improve their survival (and often their communication bandwidth), keeping their genome small enough to ensure quick DNA replication and fast increase of the biomass to speed up cell division. First, by using a multi-objective optimisation procedure, we search for the optimal trade-off between energy production, which is a requirement for the motility, and biomass growth, which is related to the overall survival and fitness of the bacterium. We use flux balance analysis of the genome-scale biochemical network of E. coli k-13 MG1655. Then, as a second case study, we analyse the electric properties and biomass trade-off of the bacterium G. sulfurreducens, which constructs an electric biofilm where electrons move across the nanowires. Here we discuss the potentialities of optimisation methodologies to design and select bacterial strains with desiderata properties. The optimisation methodologies establish also a relation between metabolism, network communication and computing. Moreover, we point to genetic design and synthetic biology as key areas to develop bacterial nano communication networks."
      }
    ]
  },
  {
    "tid":"946",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Dynamic Clusters Graph for Detecting Moving Targets Using WSNs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Efficient target tracking applications require active sensor nodes to track a cluster of moving targets. Clustering could lead to significant cost improvement as compared to tracking individual targets. This paper presents accurate clustering of targets for both coherent and incoherent movement patterns. We propose a novel clustering algorithm that utilises an implicit dynamic time frame to assess the relational history of targets in creating a weighted graph of connected components. The proposed algorithm employs key features of localisation algorithms in target tracking, namely, estimated current and predicted locations to determine the relational directions and distances of moving targets. Our simulation results show a significant improvement on the clustering accuracy and computation time by dynamically adjusting the history-window size and predicting the relationships among targets."
      }
    ]
  },
  {
    "tid":"947",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Combined Motion Estimation And Tracking Control For Autonomous Navigation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents an assisting system for autonomous vehicle in outdoor environment. The system consists of several modules, which includes localization estimation, tracking control for navigation. In order to provide the position of a vehicle, which supports for future navigation, a motion estimation method based on nonholonomic constraint is presented. The method simplifies solution to 3D motion estimation using minimum set of parameters of geometric constraint. To reduce the number of parameters for accelerating computational speed, it is supposed that the vehicle moves following the model of Ackermann steering constraint, which requires constraints between rotation and translation components. An edge matching method based on omnidirectional vision is used to estimate the oriented heading of the vehicle motion. The advantage of the omnidirectional camera is that allows tracking landmarks in large rotation angle, which is utilized for high accuracy estimating. Finally, a stable and robust control method is used for motion tracking and control to navigate vehicle. The simulation and experimental results demonstrate the effectiveness in accuracy of the proposed method under variety of terrains in outdoor environments."
      }
    ]
  },
  {
    "tid":"948",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Two-Level Additive Schwarz Methods for a Discontinuous Galerkin Approximation of Second Order Elliptic Problems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present some two-level nonoverlapping and overlapping additive Schwarz methods for a discontinuous Galerkin method for solving second order elliptic problems. It is shown that the condition numbers of the preconditioned systems are of the order $O(\\frac{H}{h})$ for the nonoverlapping Schwarz methods and of the order $O(\\frac{H}{\\delta})$ for the overlapping Schwarz methods, where h and H stand for the fine-mesh size and the coarse-mesh size, respectively, and $\\delta$ denotes the size of the overlaps between subdomains. Numerical experiments are provided to gauge the efficiency of the methods and to validate the theory."
      }
    ]
  },
  {
    "tid":"949",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Towards a BPEL model-driven approach for Web services security' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"By handling the orchestration, composition and interaction of Web services, the Business Process Execution Language (BPEL) has gained tremendous interest. However, such process-based language does not assure a secure environment for Web services composition. The key solution cannot be seen as a simple embed of security properties in the source code of the business logic since the dynamism of the BPEL process will be affected when the security measures get updated. In this context, several approaches have emerged to tackle such issue by offering the ability to specify the security properties independently from the business logic based on policy languages. Nevertheless, these languages are complex, verbose and require programming expertise. Owing to these difficulties, specifying and the enforcing BPEL security policies become very tedious tasks. To mitigate these challenges, we propose in this paper, a novel approach that takes advantage of both the Unified Modeling Language (UML) and the Aspect Oriented Paradigm (AOP). By elaborating a UML extension mechanism, called UML Profile, our approach provides the users with model-based capabilities to specify aspects that enforce the required security policies. On the other hand, it offers a high level of flexibility when enforcing security hardening solutions in the BPEL process by exploiting the AOP approach. We illustrate our approach through an example of the dynamic generation and integration of model-based security aspects in a BPEL process."
      }
    ]
  },
  {
    "tid":"950",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'p2-Cycles: p-Cycles with Parasitic Protection Links' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The p-cycle and its Failure Independent Path Protection (FIPP) extension are known to be efficient and agile protection strategies. The p-cycle is preconfigured such that if there is a failure, only the switches at two end nodes need to be reconfigured. In this paper, we extend the p-cycle by allowing cycles to have attached links, called Parasitic Protection Links (PPL), in order to protect paths whose source and destination nodes are not only located on the cycle but also connected by the PPL to the cycle. A p-cycle with PPL is named p2-cycle. We address the unicast service protection problems against single-link failures by using p2-cycle in mesh networks and the problem is formulated as an Integer Linear Program (ILP). The numerical results show that the p2-cycle scheme provides better capacity efficiency than the FIPP p-cycle scheme in all the traffic scenarios considered and consumes 2.7%-14.8% extra total cost over the optimum, provided by Shared Backup Path Protection (SBPP) approach. Moreover, we study the failure recovery performance by comparing it to FIPP and SBPP in terms of average number of reconfigurations (NOR). The results achieved by the p2-cycle is less than that of SBPP in every scenario considered and the number reduces from 2.48 to 2.31 comparing to 2, achieved by FIPP, as the traffic demands increases in NSFNET. In conclusion, the p2-cycle provides greater overall performance over existent p-cycle-based and path-based protection schemes, especially when the number of traffic demands is large."
      }
    ]
  },
  {
    "tid":"951",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Surface material recognition through haptic exploration using an intelligent contact sensing finger' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Object surface properties are among the most important information which a robot requires in order to effectively interact with an unknown environment. This paper presents a novel haptic exploration strategy for recognizing the physical properties of unknown object surfaces using an intelligent finger. This developed intelligent finger is capable of identifying the contact location, normal and tangential force, and the vibrations generated from the contact in real time. In the proposed strategy, this finger gently slides along the surface with a short stroke while increasing and decreasing the sliding velocity. By applying a dynamic friction model to describe this contact, rich and accurate surface physical properties can be identified within this stroke. This allows different surface materials to be easily distinguished even if when they have very similar texture. Several supervised learning algorithms have been applied and compared for surface recognition based on the obtained surface properties. It has been found that the nai\u0308ve Bayes classifier is superior to radial basis function network and k-NN method, achieving an overall classification accuracy of 88.5% for distinguishing twelve different surface materials."
      }
    ]
  },
  {
    "tid":"952",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Does Wireless Sensor Network Scale? A Measurement Study on GreenOrbs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Sensor networks are deemed suitable for large-scale deployments in the wild for a variety of applications. In spite of the remarkable efforts the community put to build the sensor systems, an essential question still remains unclear at the system level, motivating us to explore the answer from a point of real-world deployment view. Does the wireless sensor network really scale? We present findings from a large-scale operating sensor network system, GreenOrbs, with up to 330 nodes deployed in the forest. We instrument such an operating network throughout the protocol stack and present observations across layers in the network. Based on our findings from the system measurement, we propose and make initial efforts to validate three conjectures that give potential guidelines for future designs of large-scale sensor networks. 1) A small portion of nodes bottlenecks the entire network, and most of the existing network indicators may not accurately capture them. 2) The network dynamics mainly come from the inherent concurrency of network operations instead of environment changes. 3) The environment, although the dynamics are not as significant as we assumed, has an unpredictable impact on the sensor network. We suggest that an event-based routing structure can be trained and thus better adapted to the wild environment when building a large-scale sensor network."
      }
    ]
  },
  {
    "tid":"953",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'RFIPad: Enabling Cost-Efficient and Device-Free In-air Handwriting Using Passive Tags' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"An important function of smart environments is the ubiquitous access of computing devices. In public areas such as hospitals, libraries, and airports, people may want to interact with nearby computing systems to get information, such as directions to a hospital room, locations of books, and flight departure/arrival information. Touch screen based displays and kiosks, which are commonly used today, may incur extra hardware cost or even possible germ and bacteria infection. This work provides a new solution: users can make queries and inputs by performing in-air handwriting to an array of passive RFID tags, named RFIPad. This input method does not require human hands to carry any device and hence is convenient for applications in public areas. Besides the mobile and contactless property, this system is a cost-efficient extension to current RFID systems: an existing reader can monitor multiple RFIPads while performing its regular applications such as identification and tracking. We implement a prototype of RFIPad using commercial off-the-shelf UHF RFID devices. Experimental results show that RFIPad achieves >91% accuracy in recognizing basic touch-screen operations and English letters."
      }
    ]
  },
  {
    "tid":"954",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Diversity improvement of solutions in multiobjective genetic algorithms using pseudo function inverses' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Diversity improvement methods generally implement niching and fitness sharing schemes. In this work we propose a general principle based on using the inverse mapping from objective space to decision space that allows for the creation of diverse solutions in a direct manner. When analytical forms of objective functions are known, we propose a method of generating set-valued inverse maps, in functional or algorithmic form, which when restricted to the feasible search range yield pseudo inverses of objectives. In the absence of analytical functional forms we propose the use of artificial neural networks (ANNs) in a novel configuration to directly learn the inverse map without network inversion procedures. We implement two diversity creation operators and use them in a standard binary multi-objective genetic algorithm (MOGA) to solve standard bi-objective optimization problems. We also propose a parameter less approach of fixing the number and desirable locations of solutions in sparse regions. Proposed algorithms are compared with NSGA-II and it is shown that the proposed algorithms achieve desired level of diversity in fewer function evaluations compared to NSGA-II."
      }
    ]
  },
  {
    "tid":"955",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Users, Queries and Documents: A Unified Representation for Web Mining' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The collective feedback of the users of an Information Retrieval system has been proved to be useful in many tasks. A popular approach in the literature is to process the logs stored by Internet Service Providers (ISP), Intranet proxies or Web search engines to extract a query-document bi-partite graph. In this paper, we propose to use a richer data structure which is able to preserve most of the information available in the logs including query refinements, page visits and search activity. In particular, we represent the query refinements as separate transitions between the corresponding query nodes in the graph and we augment the graph by associating one node to each single user. Users are linked to the queries which they have issued and to the documents they have visited. The resulting data structure is a complete representation of the collective search activity performed by the users of a search engine or of an Intranet. The experimental results show that this more powerful representation can be successfully used to improve the quality of query clustering and to discover query suggestions."
      }
    ]
  },
  {
    "tid":"956",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Topological local-metric framework for mobile robots navigation: a long term perspective.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Long term mapping and localization are the primary components for mobile robots in real world application deployment, of which the crucial challenge is the robustness and stability. In this paper, we introduce a topological local-metric framework (TLF), aiming at dealing with environmental changes, erroneous measurements and achieving constant complexity. TLF organizes the sensor data collected by the robot in a topological graph, of which the geometry is only encoded in the edge, i.e. the relative poses between adjacent nodes, relaxing the global consistency to local consistency. Therefore the TLF is more robust to unavoidable erroneous measurements from sensor information matching since the error is constrained in the local. Based on TLF, as there is no global coordinate, we further propose the localization and navigation algorithms by switching across multiple local metric coordinates. Besides, a lifelong memorizing mechanism is presented to memorize the environmental changes in the TLF with constant complexity, as no global optimization is required. In experiments, the framework and algorithms are evaluated on 21-session data collected by stereo cameras, which are sensitive to illumination, and compared with the state-of-art global consistent framework. The results demonstrate that TLF can achieve similar localization accuracy with that from global consistent framework, but brings higher robustness with lower cost. The localization performance can also be improved from sessions because of the memorizing mechanism. Finally, equipped with TLF, the robot navigates itself in a 1\u00a0km session autonomously."
      }
    ]
  },
  {
    "tid":"957",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Wireless LAN load balancing with genetic algorithms' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In recent years IEEE 802.11 wireless local area networks (WLANs) have become increasingly popular. Consequently, there has also been a surge in the number of end-users. The IEEE 802.11 standards do not provide any mechanism for load distribution and as a result user quality of service (QoS) degrades significantly in congested networks where large numbers of users tend to congregate in the same area. The objective of this paper is to provide load balancing techniques that optimise network throughput in areas of user congestion, thereby improving user QoS. Specifically, we develop micro-genetic and standard genetic algorithm approaches for the WLAN load balancing problem, and we analyse their strengths and weaknesses. We also compare the performance of these algorithms with schemes currently in use in IEEE 802.11 WLANs. The results demonstrate that the proposed genetic algorithms give a significant improvement in performance over current techniques. We also show that this improvement is achieved without penalising any class of user."
      }
    ]
  },
  {
    "tid":"958",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Throw your photos: an intuitive approach for sharing between mobile phones and interactive tables' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many approaches have been proposed to connect mobile phones with interactive tables. Most rely on having the phone placed on table all times, which may hinder the overall user experience with applications on phones in general and with photo sharing ones in particular: privacy, intuitiveness of use, and technology limits are on stake. We introduce in this paper an approach which allows users to have the phone in hand when interacting with photo manipulation applications on tables, supported with natural gestures of throwing photos off the phone onto the table and dragging them into it to enhance the connected relationship between the two physical entities even when placed apart."
      }
    ]
  },
  {
    "tid":"959",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Machine Learning Approach Against a Masked AES.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Side-channel attacks challenge the security of cryptographic devices. One of the widespread countermeasures against these attacks is the masking approach. In 2012, Nassar et al. [21] presented a new lightweight (low-cost) Boolean masking countermeasure to protect the implementation of the AES block-cipher. This masking scheme represents the target algorithm of the DPAContest V4 [30]. In this article, we present the first machine learning attack against a masking countermeasure, using the dataset of the DPAContest V4. We succeeded to extract each targeted byte of the key of the masked AES with 26 traces during the attacking phase. This number of traces represents roughly twice the number of traces needed compared to an unmasked AES on the same cryptographic device. Finally, we compared our proposal to a stochastic attack and to a strategy based on template attack. We showed that an attack based on a machine learning model reduces the number of traces required during the attacking step with a factor two and four compared respectively to template attack and to stochastic attack when analyzing the same leakage information. A new strategy based on stochastic attack reduces this number to 27.8 traces (in average) during the attack but requires a larger execution time in our setting than a learning model."
      }
    ]
  },
  {
    "tid":"960",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Architecture for the Design of Context-Aware Conversational Agents' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we present a architecture for the development of conversational agents that provide a personalized service to the user. The different agents included in our architecture facilitate an adapted service by taking into account context information and users specific requirements and preferences. This functionality is achieved by means of the introduction of a context manager and the definition of user profiles. We describe the main characteristics of our architecture and its application to develop and evaluate an information system for an academic domain."
      }
    ]
  },
  {
    "tid":"961",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A novel approach to automated cell counting for studying human corneal epithelial cells.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A novel automated cell counting technique for cell sample images used to study the side-effects of lens cleaning solutions on human corneal epithelial cells is developed. The proposed multi-step approach integrates non-maximum suppression, seeded region growing, connected component analysis, and adaptive thresholding to produce segmentation and classification results that are robust to background illumination variation and clustering of cells. The proposed algorithm is computationally efficient, and experimental results show that the average detection rate of nucleated cells is greater than 90% with the proposed technique as opposed to the state-of-the-art level set method which gives an accuracy of less than 65%."
      }
    ]
  },
  {
    "tid":"962",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'netCSI: A Generic Fault Diagnosis Algorithm for Large-Scale Failures in Computer Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we present a framework and a set of algorithms for determining faults in networks when large scale outages occur. The design principles of our algorithm, netCSI, are motivated by the fact that failures are geographically clustered in such cases. We address the challenge of determining faults with incomplete symptom information due to a limited number of reporting nodes in the network. netCSI consists of two parts: hypotheses generation algorithm, and ranking algorithm. When constructing the hypotheses list of potential causes, we make novel use of the positive and negative symptoms to improve the precision of the results. The ranking algorithm is based on conditional failure probability models that account for the geographic correlation of the network objects in clustered failures. We evaluate the performance of netCSI for networks with both random and realistic topologies. We compare the performance of netCSI with an existing fault diagnosis algorithm, MAX-COVERAGE, and achieve an average gain of 128\\% in accuracy for realistic topologies."
      }
    ]
  },
  {
    "tid":"963",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Bounding the Blocking Probabilities in Multirate CDMA Networks Supporting Elastic Services' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Several previous contributions have proposed calculation methods that can be used to determine the steady state (and from it the blocking probabilities) of code-division multiple-access (CDMA) systems. This present work extends the classical Kaufman-Roberts formula such that it becomes applicable in CDMA systems in which elastic services with state-dependent instantaneous bit rate and average-bit-rate-dependent residency time are supported. Our model captures the effect of soft blocking, that is, an arriving session may be blocked in virtually all system states but with a state dependent probability. The core of this method is to approximate the original irreversible Markov chain with a reversible one and to give lower and upper bounds on the so-called partially blocking macro states of the state space. We employ this extended formula to establish lower and upper bounds on the steady state and the classwise blocking probabilities."
      }
    ]
  },
  {
    "tid":"964",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Monitoring Aquatic Debris Using Smartphone-Based Robots.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Monitoring aquatic debris is of great interest to the ecosystems, marine life, human health, and water transport. This paper presents the design and implementation of SOAR-a vision-based surveillance robot system that integrates an off-the-shelf Android smartphone and a gliding robotic fish for debris monitoring in relatively calm waters. SOAR features real-time debris detection and coverage-based rotation scheduling algorithms. The image processing algorithms for debris detection are specifically designed to address the unique challenges in aquatic environments. The rotation scheduling algorithm provides effective coverage for sporadic debris arrivals despite camera's limited angular view. Moreover, SOAR is able to dynamically offload compute-intensive processing tasks to the cloud for battery power conservation. We have implemented a SOAR prototype and conducted extensive experimental evaluation. The results show that SOAR can accurately detect debris in the presence of various environment and system dynamics, and the rotation scheduling algorithm enables SOAR to capture debris arrivals with reduced energy consumption."
      }
    ]
  },
  {
    "tid":"965",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Scalable scheduling for symmetric multiprocessors (SMP)' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present a task duplication-based scalable scheduling algorithm for Symmetric Multiprocessors (SMP), called S3MP (Scalable Scheduling for SMP), to address the problem of task scheduling. The algorithm pre-allocates network communication resources so as to avoid potential communication conflicts, and generates a schedule for the number of processors available in a SMP. This algorithm employs heuristics to select duplication of tasks so that schedule length is reduced/minimized. The algorithm can schedule the tasks of a directed acyclic graph (DAG) on to the processors of SMP with a worst case time complexity O(V2), where V is the number nodes of the DAG. The performance of the S3MP algorithm has been observed by comparing the schedule length under various number of processors and the ratio of communication to computation cost. This algorithm also has been applied to some practical DAGs."
      }
    ]
  },
  {
    "tid":"966",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Social Personalized Ranking Embedding for Next POI Recommendation.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"As the increasing popularity of the applications of location-based services, points-of-interest (POI) recommendation has become a great value part to help users explore their surrounding living environment and improve the quality of life. Recently, some researchers proposed next POI recommendation, which not only exploiting the users personal interests but also considers the sequential information of users check-ins. There are some next POI recommendation models exploit Metric Embedding method to improve recommendation performance and efficiency. However, these approaches not consider social relations in next POI recommendation, which is challenging due to social relations are noisy and sparse. To this end, in this paper, we proposed a Social Personalized Ranking Embedding (SPRE) model, which integrates user personalization and social relations into consideration, to learn the social relations by social embedding for next POI recommendation. Our experiments on a real-world large-scale dataset (Foursquare) results show that our model outperforms the state-of-the-art next POI recommendation methods."
      }
    ]
  },
  {
    "tid":"967",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Opportunities and Challenges of Digital Signal Processing in Deeply Technology-Scaled Transceivers' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The ever improving cost advantages and processing capabilities of the technology have been happening according to the so-called Moore's Law. Although digital circuits can significantly benefit from the aggressive scaling, it is very controversial for analog circuit. However, analog circuit still has to follow the scaling trend because a single chip integration offers key commercial advantages. To optimally achieve the best performance/power/cost tradeoff with deeply scaled technology nodes, there is a clear trend and paradigm shift towards digital intensive and digitally assisted transceivers. Successes of such transceivers have been proven for individual transceiver components and narrow band systems. When targeting emerging communication standards, higher carrier frequencies, further technology scaling and reconfigurable radios, required signal processing design and implementation are orders of magnitudes more challenging but potential gains are promising. Based on a variety of transceiver designs implementing emerging architectures for different sub-6 GHz and 60 GHz communication systems, we will highlight the key challenges and opportunities experienced using 40 nm and 28 nm technology nodes."
      }
    ]
  },
  {
    "tid":"968",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A comprehensive cardiac motion estimation framework using both untagged and 3-D tagged MR images based on nonrigid registration.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we present a novel technique based on nonrigid image registration for myocardial motion estimation using both untagged and 3-D tagged MR images. The novel aspect of our technique is its simultaneous usage of complementary information from both untagged and 3-D tagged MR images. To estimate the motion within the myocardium, we register a sequence of tagged and untagged MR images duri..."
      }
    ]
  },
  {
    "tid":"969",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Large scale real-life action recognition using conditional random fields with stochastic training' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Action recognition is usually studied with limited lab settings and a small data set. Traditional lab settings assume that the start and the end of each action are known. However, this is not true for the real-life activity recognition, where different actions are present in a continuous temporal sequence, with their boundaries unknown to the recognizer. Also, unlike previous attempts, our study is based on a large-scale data set collected from real world activities. The novelty of this paper is twofold: (1) Large-scale non-boundary action recognition; (2) The first application of the averaged stochastic gradient training with feedback (ASF) to conditional random fields. We find the ASF training method outperforms a variety of traditional training methods in this task."
      }
    ]
  },
  {
    "tid":"970",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Normalization-Based Task Scheduling Algorithms for Heterogeneous Multi-Cloud Environment.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cloud computing is one of the most successful technologies that offer on-demand services through the Internet. However, datacenters of the clouds may not have unlimited capacity which can fulfill the demanded services in peak hours. Therefore, scheduling workloads across multiple clouds in a federated manner has gained a significant attention in the recent years. In this paper, we present four task scheduling algorithms, called CZSN, CDSN, CDN and CNRSN for heterogeneous multi-cloud environment. The first two algorithms are based on traditional normalization techniques, namely z-score and decimal scaling respectively which are hired from data mining. The next two algorithms are based on two newly proposed normalization techniques, called distribution scaling and nearest radix scaling respectively. All the proposed algorithms are shown to work on-line. We perform rigorous experiments on the proposed algorithms using various synthetic as well as benchmark datasets. Their performances are evaluated through simulation run by measuring two performance metrics, namely makespan and average cloud utilization. The experimental results are compared with that of existing algorithms to show the efficacy of the proposed algorithms."
      }
    ]
  },
  {
    "tid":"971",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Context-Aware Ubiquitous Web Services Recommendation Based on User Location Update.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a novel ubiquitous Web service recommendation approach to context-aware recommendation based on user location update (CASR-ULU). First, we model the influence of user location update based on user preference expansion. Second, we perform the context-aware similarity mining for updated location. Third, we predict the Quality of Service by Bayesian inference, and thus recommend the ideal Web service for the specific user subsequently. Furthermore, a calendar Android mobile application is implemented to testify the CASR-ULU algorithm in a ubiquitous environment. Finally, we evaluate the CASR-ULU method on WS-Dream dataset with evaluation matrices such as RMSE and MAE. Experimental results show that our method achieves competitive recommendation performance in a ubiquitous environment, compared to several state-of-the-art methods."
      }
    ]
  },
  {
    "tid":"972",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Provide context-aware advertisements with interactivity' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Interactivity and context-awareness are two salient features of mobile advertising, but their actual influences on advertising effectiveness have not been studied. This study attempts to understand expectations and concerns of Chinese mobile users hold for context aware advertising, and to investigate the influences of interactivity and context-awareness. A web-based survey and an experiment were conducted for the two purposes respectively. Results suggested that Chinese users generally hold a modestly positive attitude towards context aware advertisements; advertisements adapt to physical environment, geographic context, and purchasing history are desired; young people are more open to context aware advertisements. Their major concerns were irrelevant messages and information flood. The interaction between perceived interactivity and context awareness has a big effect on advertising effectiveness. For high interactivity advertisements, contextual offers of the advertisements result in more responses and better advertisement attitude. But for low interactivity advertisements, contextual offers incur more negative attitude and lead to lower purchase intention."
      }
    ]
  },
  {
    "tid":"973",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Metrics For Best Practice Networks Analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Best practice networks are thematic social networks focused on sharing common area works and goals. The analysis of user behavior on social network is fundamental to tune services and stimulate the network growth. This is even more relevant for best practice collaborative networks where details about collaboration may lead to understand the effective activities and role of people in the several groups. In this paper, a set of metrics for analyzing user behavior related to download on a best practice network is presented. The metrics are currently active, as administrative tools for the Mobile Medicine best practice network."
      }
    ]
  },
  {
    "tid":"974",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Stabilization of singular Markovian jump systems with time-varying switchings.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper studies the stabilization problem for a class of singular Markovian jump systems (SMJSs) with time-varying transition rates by using the quantized method firstly. By proposing a quantization principle, the time-varying transition rate matrix (TRM) is quantized into a series of finite TRMs with norm bounded uncertainties. Then, the difficulties of the time-varying TRM confronted in system analysis and synthesis can be overcome. New criteria for stochastic admissibility analysis as well as controller synthesis are established in terms of linear matrix inequalities (LMIs). Finally, the validity of the theoretical results is shown by numerical examples."
      }
    ]
  },
  {
    "tid":"975",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title '\u201cMask-Bot 2i\u201d: An active customisable robotic head with interchangeable face' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper describes the development of \u201cMask-Bot 2i\u201d (codenamed: Kabuto), a robotic head designed for research into Human-Robot-Interactions. The uniqueness of our new robotic head is that the appearance of its face can be alter on-the-fly. Different faces can be projected onto the active head system. The head is fully active with a 3-DOF neck, with support for biannual hearing as well as video camera for seeing. An interchangeable face is the main feature of this new Mask-Bot, the head can be equipped with an average face mask as well as a highly customised individualised face that can be easily exchanged. Additionally, the actuation of the head has been designed to match the natural head movements of an average human. Thus, enabling the head and face to be articulated synchronously to the speech production while the natural head movement matches that of the animated face. The design and realisation of this new system is presented in details in this paper."
      }
    ]
  },
  {
    "tid":"976",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Enhanced collision arbitration protocol utilizing multiple antennas in RFID systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we present an enhanced collision arbitration protocol that operates over multiple antennas in RFID systems. An RFID system consists of a reader and tags, and all tags have to be quickly recognized by a reader. Due to a contention based tag transmission, collision arbitration is a very important issue in tag identification. However, collision always happens in the existing single antenna based collision arbitration scheme when more than one tag simultaneously transmit message. In order to reduce the number of collisions, we propose to use multiple antennas at the reader such that the reader can recognize multiple tags simultaneously by utilizing multi-user receiver at the reader and multiple orthogonal preambles used by each tag. In this scheme, only the tags that are not identified retransmit in the next time slot while the identified tags keep silent. Then the identification delay significantly decreases because the number of retransmissions decreases in comparison with the single antenna scheme. We analyze the performance of this proposed scheme and conduct simulations to confirm the validity of the mathematical modeling. According to the analytical and simulation results, the proposed scheme outperforms the existing single antenna scheme."
      }
    ]
  },
  {
    "tid":"977",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Music meter and tempo tracking from raw polyphonic audio' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a method for the extraction of music meter and tempo from raw polyphonic audio recordings, assuming that music meter remains constant throughout the recoding. Although this assumption can be restrictive for certain musical genres, it is acceptable for a large cor- pus of folklore eastern music styles, including Greek tra- ditional dance music. Our approach is based on the self- similarity analysis of the audio recording and does not as- sume the presence of percussive instruments. Its novelty lies in the fact that music meter and tempo are jointly de- termined. The method has been applied to a variety of musical genres, in the context of Greek traditional music where music meter can be 24, 3 4, 4 4, 5 4, 7 8, 9 8, 12 8 and tempo ranges from 40bpm to 330bpm. Experiments have, so far, demonstrated the efficiency of our method (music meter and tempo were successfully extracted for over 95% of the recordings)."
      }
    ]
  },
  {
    "tid":"978",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Improving completeness of electronic problem lists through clinical decision support: a randomized, controlled trial.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Background Accurate clinical problem lists are critical for patient care, clinical decision support, population reporting, quality improvement, and research. However, problem lists are often incomplete or out of date. Objective To determine whether a clinical alerting system, which uses inference rules to notify providers of undocumented problems, improves problem list documentation. Study Design and Methods Inference rules for 17 conditions were constructed and an electronic health record-based intervention was evaluated to improve problem documentation. A cluster randomized trial was conducted of 11 participating clinics affiliated with a large academic medical center, totaling 28 primary care clinical areas, with 14 receiving the intervention and 14 as controls. The intervention was a clinical alert directed to the provider that suggested adding a problem to the electronic problem list based on inference rules. The primary outcome measure was acceptance of the alert. The number of study problems added in each arm as a pre-specified secondary outcome was also assessed. Data were collected during 6-month pre-intervention (11/2009-5/2010) and intervention (5/2010-11/2010) periods. Results 17 043 alerts were presented, of which 41.1% were accepted. In the intervention arm, providers documented significantly more study problems (adjusted OR=3.4, p<0.001), with an absolute difference of 6277 additional problems. In the intervention group, 70.4% of all study problems were added via the problem list alerts. Significant increases in problem notation were observed for 13 of 17 conditions. Conclusion Problem inference alerts significantly increase notation of important patient problems in primary care, which in turn has the potential to facilitate quality improvement."
      }
    ]
  },
  {
    "tid":"979",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Building a secure star schema in data warehouses by an extension of the relational package from CWM' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Data Warehouses (DWs) are widely accepted as the core of current decision support systems. Therefore, it is vital to incorporate security requirements from the early stages of the DWs projects and enforce them in the further design phases. Very few approaches specify security and audit measures in the conceptual modeling of DWs. Furthermore, these security measures are specified in the final implementation on top of commercial systems as there is not a standard relational representation of security measures for DWs (i.e. the well-known star schema does not allow us to specify security and audit measures on its multidimensional representation of data; instead, they must be specified on top of the implemented relational tables). On the other hand, the Common Warehouse Metamodel (CWM) has been accepted as the standard for the exchange and the interoperability of metadata. Nevertheless, it does not allow us to specify security measures for DWs. In this paper, we make use of the own extension mechanisms provided by the CWM to extend the relational package in order to build a star schema that represents the security and audit rules captured during the conceptual modeling phase of DWs. Finally, in order to show the benefits of our extension, we apply it to a case study related to the management of the pharmacy consortium business."
      }
    ]
  },
  {
    "tid":"980",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Loop Transformations to Reduce the Dynamic FPGA Recon?guration Overhead' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Dynamic hardware generation reduces the number of FPGA resources needed and speeds up an application by optimizing the FPGA configuration at run-time for the exact problem at hand. Because of the large overhead associated with dynamic hardware generation, it is important to minimize the number of reconfigurations. In this work, we present a technique to maximize the reuse of a configuration by means of loop transformations. Our approach builds on similar work on temporal data locality optimization. Our experiments on a matrix multiplication benchmark show that we can reduce the number of reconfigurations by an order of magnitude, making dynamic hardware generation techniques much more useful in practice. When we combine our approach with a dynamic hardware generation tool with a very low overhead, so called parameterizable configurations, we can obtain a significant speed up over generic counterparts."
      }
    ]
  },
  {
    "tid":"981",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient packet classification for network intrusion detection using FPGA' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Using FPGA technology for real-time network intrusion detection has gained many research efforts recently. In this paper, a novel packet classification architecture called BV-TCAM is presented, which is implemented for an FPGA-based Network Intrusion Detection System (NIDS). The classifier can report multiple matches at gigabit per second network link rates. The BV-TCAM architecture combines the Ternary Content Addressable Memory (TCAM) and the Bit Vector (BV) algorithm to effectively compress the data representations and boost throughput. A tree-bitmap implementation of the BV algorithm is used for source and destination port lookup while a TCAM performs the lookup of the other header fields, which can be represented as a prefix or exact value. The architecture eliminates the requirement for prefix expansion of port ranges. With the aid of a small embedded TCAM, packet classification can be implemented in a relatively small part of the available logic of an FPGA. The design is prototyped and evaluated in a Xilinx FPGA XCV2000E on the FPX platform. Even with the most difficult set of rules and packet inputs, the circuit is fast enough to sustain OC48 traffic throughput. Using larger and faster FPGAs, the system can work at speeds greater than OC192."
      }
    ]
  },
  {
    "tid":"982",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Routine high-return human-competitive automated problem-solving by means of genetic programming' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Genetic programming is a systematic method for getting computers to automatically solve problems. Genetic programming starts from a high-level statement of what needs to be done and automatically creates a computer program to solve the problem by means of a simulated evolutionary process. The paper demonstrates that genetic programming (1) now routinely delivers high-return human-competitive machine intelligence; (2) is an automated invention machine; (3) can automatically create a general solution to a problem in the form of a parameterized topology and (4) has delivered a progression of qualitatively more substantial results in synchrony with five approximately order-of-magnitude increases in the expenditure of computer time. These points are illustrated by a group of recent results involving the automatic synthesis of the topology and sizing of analog electrical circuits, the automatic synthesis of placement and routing of circuits, and the automatic synthesis of controllers as well as references to work involving the automatic synthesis of antennas, networks of chemical reactions (metabolic pathways), genetic networks, mathematical algorithms, and protein classifiers."
      }
    ]
  },
  {
    "tid":"983",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Investigation of partial query proximity in web search' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Proximity of query terms in a document is an important criterion in IR. However, no investigation has been made to determine the most useful term sequences for which proximity should be considered. In this study, we test the effectiveness of using proximity of partial term sequences (n-grams) for Web search. We observe that the proximity of sequences of 3 to 5 terms is most effective for long queries, while shorter or longer sequences appear less useful. This suggests that combinations of 3 to 5 terms can best capture the intention in user queries. In addition, we also experiment with weighing the importance of query sub-sequences using query log frequencies. Our preliminary tests show promising empirical results."
      }
    ]
  },
  {
    "tid":"984",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The QoSbox: quantitative service differentiation in BSD routers' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We describe the design and implementation of the QoSbox, a configurable IP router that provides per-hop service differentiation on loss, delays and throughput to classes of traffic. The novel aspects of the QoSbox are that (1) the QoSbox does not rely on any external component (e.g., no traffic shaping and no admission control) to provide the desired service differentiation, but instead, (2) dynamically adapts packet forwarding and dropping decisions as a function of the instantaneous traffic arrivals and allows for temporary relaxation of some service objectives; also, (3) the QoSbox can enforce both absolute and proportional service differentiation on queuing delays, loss rates, and throughput at the same time. We focus on a publicly available implementation of the QoSbox in BSD-based PC-routers. We evaluate our implementation in a testbed of BSD routers over a FastEthernet network, and we sketch how the QoSbox can be implemented in high speed architectures."
      }
    ]
  },
  {
    "tid":"985",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Automated hypothesis generation based on mining scientific literature.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Keeping up with the ever-expanding flow of data and publications is untenable and poses a fundamental bottleneck to scientific progress. Current search technologies typically find many relevant documents, but they do not extract and organize the information content of these documents or suggest new scientific hypotheses based on this organized content. We present an initial case study on KnIT, a prototype system that mines the information contained in the scientific literature, represents it explicitly in a queriable network, and then further reasons upon these data to generate novel and experimentally testable hypotheses. KnIT combines entity detection with neighbor-text feature analysis and with graph-based diffusion of information to identify potential new properties of entities that are strongly implied by existing relationships. We discuss a successful application of our approach that mines the published literature to identify new protein kinases that phosphorylate the protein tumor suppressor p53. Retrospective analysis demonstrates the accuracy of this approach and ongoing laboratory experiments suggest that kinases identified by our system may indeed phosphorylate p53. These results establish proof of principle for automated hypothesis generation and discovery based on text mining of the scientific literature."
      }
    ]
  },
  {
    "tid":"986",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Scalable Load Balanced Birkhoff-Von Neumann Symmetric Tdm Switch Ic For High-Speed Networking Applications' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"For the first time, we implemented a scalable load balanced Birkhoff-von Neumann TDM switch IC with SERDES interface circuits for high speed networking applications. Any NxN Birkhoff-von Neumann TDM switch could be constructed recursively from the designed TDM switch IC to achieve switching capacity of hundred gigabits per second or higher. The TDM switch IC contained a digital 8x8 TDM switch core with 8B10B CODECs and analog SERDES I/O interfaces. In the I/O interfaces, eight 2.56/3.2Gbps dual-mode 16/20:1 SERDES with CML buffers were developed. The 16/20:1 instead of 8/10:1 serializer and deserializer were used to reduce the required operating frequency in the switch core by half. New half-rate architectures and all static CMOS gates were used in the 16/20:1 serializer and deserializer for the low power consumption. A wide-band CML I/O buffer with our patented PMOS active load scheme was developed. All the implementations were based on the 0.18 mu m CMOS technology. Our test results showed a 20 Gbps switching capacity for the 8x8 TDM switch IC."
      }
    ]
  },
  {
    "tid":"987",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Effective QoS Differentiation Scheme for Wireless Mesh Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Wireless mesh networking is emerging as an important architecture for future-generation wireless communications systems. Quality of service provisioning is a challenging issue in WMNs. In this article we study an effective QoS differentiation scheme for IEEE 802.16 WiMAX mesh networks. Both collocated and general topologies are exploited. Illustrative numerical examples are presented to demonstrate the effectiveness of the proposed strategy. The impact of key parameters on performance is discussed for differentiating various services. Moreover, with the proposed scheme, WMN scalability can be greatly improved. The challenges with respect to the integration of WMN and cooperative transmission are discussed, and the fairness problem is addressed with potential solutions."
      }
    ]
  },
  {
    "tid":"988",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Answering why-not and why questions on reverse top-k queries.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Why-not and why questions can be posed by database users to seek clarifications on unexpected query results. Specifically, why-not questions aim to explain why certain expected tuples are absent from the query results, while why questions try to clarify why certain unexpected tuples are present in the query results. This paper systematically explores the why-not and why questions on reverse top-k queries, owing to its importance in multi-criteria decision making. We first formalize why-not questions on reverse top-k queries, which try to include the missing objects in the reverse top-k query results, and then, we propose a unified framework called WQRTQ to answer why-not questions on reverse top-k queries. Our framework offers three solutions to cater for different application scenarios. Furthermore, we study why questions on reverse top-k queries, which aim to exclude the undesirable objects from the reverse top-k query results, and extend the framework WQRTQ to efficiently answer why questions on reverse top-k queries, which demonstrates the flexibility of our proposed algorithms. Extensive experimental evaluation with both real and synthetic data sets verifies the effectiveness and efficiency of the presented algorithms under various experimental settings."
      }
    ]
  },
  {
    "tid":"989",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Dynamic Detection and Tolerance of Attacks in Storage Area Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Due to the increasing amount of data handled by business applications and the need of an easy way to access data by multiple servers, storage area networks (SANs) have been proposed as a solution to be deployed in enterprises networks. Despite their advantages, a set of security issues are related to these next generation high speed network architectures and that may be a threat to the performance and the availability of such networks. In this paper, we propose an intrusion detection and tolerance system that ensures protection of the SANs against attacks. The proposed solution is based on: (a) the management of two areas (virtual area and protected area) at each storage node; (b) the cooperation of detection modules running on each SAN component; and (c) the use of distributed set of rules that are updated and managed in a secure manner. A case study is given to illustrate the proposed system capabilities in terms of intrusion detection and tolerance."
      }
    ]
  },
  {
    "tid":"990",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Wireless-Powered Communication Under Statistical Quality Of Service Constraints' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we study the performance of wireless information and power transfer in the presence of statistical queuing constraints. We consider harvest-then-transmit protocol in which users first harvest energy from a dedicated source and then transmit information through an uplink multiple access channel (MAC). Each user is subject to limitations on the buffer overflow probability, specified by the quality of service (QoS) exponent., and the optimal time allocation for energy harvesting and information decoding operations depends on these constraints in addition to the channel characteristics. We formulate optimization problems to maximize the throughput with and without QoS constraints. In both cases, the problems are convex, and hence Karush-KuhnTucker (KKT) conditions are necessary and sufficient for global optimality. However, it is difficult to obtain closed-form expressions for optimal time interval since we assume that operating intervals are independent of each fading state realization. Hence, we develop an algorithm to obtain optimal solutions numerically. Simulation results justify that QoS constraints primarily affect achievable rate distribution among the users, and override the channel conditions."
      }
    ]
  },
  {
    "tid":"991",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Automatic Layer Separation using Light Field Imaging' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"  We propose a novel approach that jointly removes reflection or translucent layer from a scene and estimates scene depth. The input data are captured via light field imaging. The problem is couched as minimizing the rank of the transmitted scene layer via Robust Principle Component Analysis (RPCA). We also impose regularization based on piecewise smoothness, gradient sparsity, and layer independence to simultaneously recover 3D geometry of the transmitted layer. Experimental results on synthetic and real data show that our technique is robust and reliable, and can handle a broad range of layer separation problems. "
      }
    ]
  },
  {
    "tid":"992",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An efficient VLSI dictionary machine' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A binary tree machine which can handle all the dictionary machine and priority queue operations as well as some other data queries is designed in this paper. This machine supports operations like Insert, Delete, Extract-Min/Max, Membership and Near and their redundant forms. If the number of keys present in the tree is n, then each of the operations takes O(log n) steps, and can be fed into the tree in a pipeline manner at a constant rate. A machine with N&equil;2**s-1 processors can store upto N-s data elements. The output is generated at a fixed interval. It does not use any horizontal wires as in some previous designs. Moreover, this approach does not require any sorted-order, is potentially applicable to non-binary tree structures, and is significantly simpler to implement than all the previous designs."
      }
    ]
  },
  {
    "tid":"993",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On the Quality of Service of Failure Detectors Based on Control Theory' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The detection of failures is a fundamental issue for fault tolerance in distributed systems. Recently, many people have come to realize that failure detection ought to be provided as some form of generic service, similar to IP address lookup. However, this has not been successful so far; one of the reasons being the fact that classical failure detectors were not designed to satisfy several application requirements simultaneously. More specifically, traditional implementations of failure detectors are often tuned for running over local networks and fail to address some important problems found in wide-area distributed systems with a large number of monitored components.In this paper, we study the quality of service (QoS) of failure detectors. We first present a novel failure detector scheme combined with control theory that can help in solving or optimizing some problems. Furthermore, this paper discusses the design and analysis of implementing a scalable failure detection service for such large wide-area distributed systems considering dynamically adjusting the heartbeat streams, so that it satisfies the bottleneck router requirements. We further show how the online failure detector control algorithm can be used to design a controller, analyze the theoretical aspects of the proposed algorithm and verify its agreement. Simulation results show the efficiency of our scheme in terms of high utilization of the bottleneck link, fast response and good stability of the bottleneck router buffer occupancy as well as of the controlled sending rates. In conclusion, the new failure detector algorithm provides a better QoS than an algorithm that is proposed by Stelling et al [1,2]."
      }
    ]
  },
  {
    "tid":"994",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A proportionate fair scheduling rule with good worst-case performance' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we consider the following scenario. A set of n jobs with different threads is being run concurrently. Each job has an associated weight, which gives the proportion of processor time that it should be allocated. In a single time quantum, p threads of (not necessarily distinct) jobs receive one unit of service, and we require a rule that selects those p threads, at each quantum. Proportionate fairness means that over time, each job will have received an amount of service that is proportional to its weight. That aim cannot be achieved exactly due to the discretisation of service provision, but we can still hope to bound the extent to which service allocation deviates from its target. It is important that any scheduling rule be simple since the rule will be used frequently.We consider a variant of the Surplus Fair Scheduling (SFS) algorithm of Chandra, Adler, Goyal, and Shenoy. Our variant, which is appropriate for scenarios where jobs consist of multiple threads, retains the properties that make SFS empirically attractive but allows the first proof of proportionate fairness in a multiprocessor context. We show that when the variant is run, no job lags more than p H(n)-p+1 steps below its target number of services, where H(n) is the Harmonic function. Also, no job is over-supplied by more than O(1) extra services. This analysis is tight and it also extends to an adversarial setting, which models some situations in which the relative weights of jobs change over time."
      }
    ]
  },
  {
    "tid":"995",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Particle Swarm Optimization with Extremal Optimization for the Prediction of CpG Islands in the Mammal Genome' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Regions with abundant GC nucleotides in a genome, which are often referred to as CpG islands, have been used in methylation analysis and the prediction of promoter regions. In this study, we propose PSOEO (Particle Swarm Optimization with Extremal Optimization), a method for the prediction of CpG islands in the mammal genome. This method adopts the GGF criteria (GC content \u2265 50%, observed/expected (O/E) ratio \u22650.6 and length \u2265200 bp) for the search of CpG islands. First, we used the PSO algorithm to predict CpG islands. In a second stage, we used EO to search for various output states (local search) in order to find a better result. Extremal optimization is a developed heuristic local search method. Finally, we used five evaluation criteria, namely the sensitivity (SN), specificity (SP), accuracy (ACC), correlation coefficient (CC) and performance coefficient (PC) to compare other methods in the literature. PSOEO method provided better SN and CC predictions for the locations of CpG islands than the other methods it was compared to."
      }
    ]
  },
  {
    "tid":"996",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Blind Zone Alert System Based on Intra-Vehicular Wireless Sensor Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Due to the increasing number of sensors deployed in modern vehicles, intra-vehicular wireless sensor networks (IVWSNs) have recently received a lot of attention in the automotive industry, as they can reduce the amount of wiring harness inside a vehicle. By removing the wires, car manufacturers can reduce the weight of a vehicle and improve engine performance, fuel economy, and reliability. In addition to these direct benefits, an IVWSN is a versatile platform that can support other vehicular applications as well. An example application, known as a side blind zone alert (SBZA) system, which monitors the blind zone of the vehicle and alerts the driver in a timely manner to prevent collisions, is discussed in this paper. The performance of the IVWSN-based SBZA system is evaluated via real experiments conducted on two test vehicles. Our results show that the proposed system can achieve approximately 95%-99% detection rate with less than 15% false alarm rate. Compared with commercial systems using radars or cameras, the main benefit of the IVWSN-based SBZA is substantially lower cost."
      }
    ]
  },
  {
    "tid":"997",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Deep Variational Canonical Correlation Analysis.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present deep variational canonical correlation analysis (VCCA), a deep multi-view learning model that extends the latent variable model interpretation of linear CCA~citep{BachJordan05a} to nonlinear observation models parameterized by deep neural networks (DNNs). Computing the marginal data likelihood, as well as inference of the latent variables, are intractable under this model. We derive a variational lower bound of the data likelihood by parameterizing the posterior density of the latent variables with another DNN, and approximate the lower bound via Monte Carlo sampling. Interestingly, the resulting model resembles that of multi-view autoencoders~citep{Ngiam_11b}, with the key distinction of an additional sampling procedure at the bottleneck layer. We also propose a variant of VCCA called VCCA-private which can, in addition to the ``common variablesu0027u0027 underlying both views, extract the ``private variablesu0027u0027 within each view. We demonstrate that VCCA-private is able to disentangle the shared and private information for multi-view data without hard supervision."
      }
    ]
  },
  {
    "tid":"998",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Modelling the impact of high level leakage optimization techniques on the delay of RT-components' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"To adress the problem of static power consumption, approaches as ABB and AVS have been proposed to reduce runtime leakage in integrated circuits. Applying these techniques is a trade off between power and delay, which is best decided early in the design flow. Therefore high level power and delay estimation is needed. In our work, we present a fast RT Level delay macro model considering supply and bias voltages and temperature. Errors below 5% combined with only few characterization data enables this approach to be used by high level design tools to support leakage optimization by e.g. ABB and AVS."
      }
    ]
  },
  {
    "tid":"999",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A framework for providing adaptive sports video to mobile devices' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we present a novel framework that serves adaptive sports video to mobile users. Our framework combines content-based sports highlights extraction and quality-domain video compression technologies in video server, capable of reducing wireless bandwidth consumption more effectively. We develop a robust replay-based highlights extraction method, and propose a content-based video streaming coding scheme to handle the problems of bandwidth and capacity of computation. To validate the practicality and effectiveness of our system, we conduct the experiments on several real soccer videos. The experimental results demonstrate the robustness of our highlights extraction method and more than 77.5% of the bandwidth consumption could be reduced."
      }
    ]
  },
  {
    "tid":"1000",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Co-clustering on manifolds' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Co-clustering is based on the duality between data points (e.g. documents) and features (e.g. words), i.e. data points can be grouped based on their distribution on features, while features can be grouped based on their distribution on the data points. In the past decade, several co-clustering algo- rithms have been proposed and shown to be superior to tra- ditional one-side clustering. However, existing co-clustering algorithms fail to consider the geometric structure in the data, which is essential for clustering data on manifold. To address this problem, in this paper, we propose a Dual Regularized Co-Clustering (DRCC) method based on semi- nonnegative matrix tri-factorization. We deem that not only the data points, but also the features are sampled from some manifolds, namely data manifold and feature manifold re- spectively. As a result, we construct two graphs, i.e. data graph and feature graph, to explore the geometric struc- ture of data manifold and feature manifold. Then our co- clustering method is formulated as semi-nonnegative matrix tri-factorization with two graph regularizers, requiring that the cluster labels of data points are smooth with respect to the data manifold, while the cluster labels of features are smooth with respect to the feature manifold. We will show that DRCC can be solved via alternating minimization, and its convergence is theoretically guaranteed. Experiments of clustering on many benchmark data sets demonstrate that the proposed method outperforms many state of the art clus- tering methods."
      }
    ]
  },
  {
    "tid":"1001",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A rate separation mechanism for performance improvements of multi-rate WLANs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The fundamental access mode of the IEEE 802.11 MAC protocol is contention based. If the traffic load is heavy or the number of contending station is large, the number of collisions is increased and it leads to the performance degradation. In this paper, we propose a mechanism that tries to reduce the number of collisions by separating and grouping the contending stations and distributing those groups over time in multi-rate WLANs. For this, we issue the trade-off relationship between the throughput fairness and temporal fairness in multi-rate WLANs. Considering the trade-off relationship, we propose a Rate Separation (RS) mechanism in which the grouping is done based on the current transmission rates of contending stations. From our simulation study, we show that the proposed mechanism reduces the number of collisions and achieves improved performance over the IEEE 802.11b WLANs."
      }
    ]
  },
  {
    "tid":"1002",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Lowering overhead in sampling-based execution monitoring and tracing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Debugging is an important phase in the embedded software development cycle because of its high proportion in the overall cost in the product development. Debugging is difficult for real-time applications as such programs are time-sensitive and must meet deadlines in often a resource constrained environment. A common approach for real-time systems is to monitor the execution instead of stepping through the program, because stepping will usually violate all deadline constraints. We consider a sampling-based approach for monitoring, because of its predictable overhead for the system compared to traditional monitoring. However, the sampling-based approach can easily have high overhead depending on the length of branches and the granularity of the monitoring effort. To reduce this overhead, we instrument the program with markers that will permit us to sample less frequently and thus reduce the overhead. This leads to the interesting problems of (a) where to place the markers in the code and (b) how to manipulate the markers. While related work investigates the first part, in this work, we investigate the second component of the problem. We investigate different instrumentation schemes and propose two new schemes based on bitvectors that significantly reduce the overhead for sampling-based execution monitoring."
      }
    ]
  },
  {
    "tid":"1003",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Bacterial foraging algorithm with varying population' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Most of evolutionary algorithms (EAs) are based on a fixed population. However, due to this feature, such algorithms do not fully explore the potential of searching ability and are time consuming. This paper presents a novel nature-inspired heuristic optimization algorithm: bacterial foraging algorithm with varying population (BFAVP), based on a more bacterially-realistic model of bacterial foraging patterns, which incorporates a varying population framework and the underlying mechanisms of bacterial chemotaxis, metabolism, proliferation, elimination and quorum sensing. In order to evaluate its merits, BFAVP has been tested on several benchmark functions and the results show that it performs better than other popularly used EAs, in terms of both accuracy and convergency."
      }
    ]
  },
  {
    "tid":"1004",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Template match of buildings and results analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper expounds the important application of the transform thought to solve building template matching problems based on building skeleton matching in large-scale topography map. A new building template matching algorithm via their substitution is proposed. The algorithm workflow is iterative to match pre-defined templates. Skeleton of building polygon is extracted based on point set Voronoi diagram. Trimming and pre-simplification of skeletons is invoked then. Skeleton graph is constructed and skeleton nodes whose degree is below 2 are merged in the next step. Building template is represented as node degree array and matched through array operations. If a building is matched with a template, the algorithm stops; else simplification is needed to forward the simplified skeleton into next template matching circle. Given a building layer of a large scale topographic map sheet, statistical results of building template matching are analyzed for potential application scenarios of building templates."
      }
    ]
  },
  {
    "tid":"1005",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Iterative data detection and decoding using list channel estimation and Markov Chain Monte Carlo' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we study joint iterative data detection and channel decoding under imperfect channel state information (CSI). We apply the Markov Chain Monte Carlo technique to generate a list of channel estimates (LCE) that maximizes the a posteriori probabilities of the transmitted data, given the received signal and the soft feedback from the channel decoder. The LCE is refined over each iteration of data detection and decoding to facilitate improved channel estimation and thus yields superior detection performance. It is shown that, even with a small list size, the proposed MCMC-LCE detector outperforms the coherent detector in which data detection is performed based on a single channel estimate (SCE). As opposed to the noncoherent detectors which impose stringent constraints on the fading distribution, the MCMC-LCE detector is applicable to general fading distributions. It also offers a low complexity that is linear in the coherent length of the channel and the list size."
      }
    ]
  },
  {
    "tid":"1006",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Example-Based Motion Cloning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we pose a motion cloning problem of retargeting the motion of a source character to a target character with a different structure. Based on scattered data interpolation, an example-based approach to motion cloning is proposed. Provided with a set Of example motions, our method automatically extracts a small number of representative postures called source key-postures. The animator then creates the corresponding key-postures of the target character, breathing his/her imagination and creativity into the output animation. Exploiting this correspondence, each input posture is cloned frame by frame to the target character to produce an initial animation, which is further adjusted in space and time for retargeting and timewarping and then finalized with some interactive fine tuning. With rich animation data available, our motion cloning method aims at rapid prototyping of an animation to verify an animator's concept at an early stage. Copyright (C) 2004 John Wiley Sons, Ltd."
      }
    ]
  },
  {
    "tid":"1007",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Semi-supervised sequence classification using abstraction augmented Markov models' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Supervised methods for learning sequence classifiers rely on the availability of large amounts of labeled data. However, in many applications because of the high cost and effort involved in labeling the data, the amount of labeled data is quite small compared to the amount of unlabeled data. Hence, there is a growing interest in semi-supervised methods that can exploit large amounts of unlabeled data together with small amounts of labeled data. In this paper, we introduce a novel Abstraction Augmented Markov Model (AAMM) based approach to semi-supervised learning. We investigate the effectiveness of AAMMs in exploiting unlabeled data. We compare semi-supervised AAMMs with: (i) the Markov models (MMs) (which do not take advantage of unlabeled data); and (ii) an expectation maximization (EM) based approach to semi-supervised training of MMs (that make use of unlabeled data). The results of our experiments on three protein subcellular localization prediction tasks show that semi-supervised AAMMs: (i) can effectively exploit unlabeled data; and (ii) are more accurate than both the MMs and the EM based semi-supervised MMs."
      }
    ]
  },
  {
    "tid":"1008",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'QARI: Quality Aware Software Deployment for Wireless Sensor Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"If we are to deploy sensor applications in a realistic business context, we must provide innovative middleware services to control and enforce required system behavior; in order to correctly interpret collected temperature data, for example, sensor applications require guarantees about minimal coverage and the number of available sensors. The extreme dynamism, scale and unreliability of wireless sensor networks represent major challenges in contemporary software management. This paper presents QARI, a middleware service for decentralized and quality aware software deployment, which offers a simple yet flexible way to define, enforce, and maintain software deployment specifications. We have evaluated QARI on the LooCI component model and the SunSPOT platform; results confirm that QARI enables quality aware software deployment for a single as well as multiple applications, and even in the presence of node failure and mobility."
      }
    ]
  },
  {
    "tid":"1009",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'New results on robust finite-time boundedness of uncertain switched neural networks with time-varying delays.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper investigates the finite-time boundedness (FTB) problem for a class of uncertain switched neural networks with time-varying delays. By exploring the mode-dependent properties of each subsystem, all the subsystems could be categorized into stable and unstable ones under the Lyapunov-like function framework. The sufficient conditions and a set of unified switching signals with average dwell time (ADT) are first derived with a known limit to the total activation time ratio between unstable and stable subsystems. Then, the obtained results are extended to a new switching approach with mode-dependent average dwell time (MDADT). Compared with general results, our proposed approach distinguishes the stable and unstable subsystems rather than viewing all subsystems as being unstable, thus getting less conservative switching criteria. Finally, a numerical example is provided to show the validity and the advantages of the finding techniques."
      }
    ]
  },
  {
    "tid":"1010",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Low-power embedded system for real-time correction of fish-eye automotive cameras' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The design and the implementation of a flexible and cost-effective embedded system for real-time correction of fish-eye automotive cameras is presented. Nowadays many car manufacturers already introduced on-board video systems, equipped with fish-eye lens, to provide the driver a better view of the so-called blind zones. A fish-eye lens achieves a larger field of view (FOV) but, on the other hand, causes distortion, both radial and tangential, of the images projected on the image sensor. Since radial distortion is noticeable and dangerous, a real-time system for its correction is presented, whose low-power, low-cost and flexibility features are suitable for automotive applications."
      }
    ]
  },
  {
    "tid":"1011",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Raptor: a network tool for mitigating the impact of spatially correlated failures in infrastructure networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Current practices of fault-tolerant network design ignore the fact that most network infrastructure faults are localized or spatially correlated (i.e., confined to geographic regions). Network operators require new tools to mitigate the impact of such region-based faults on their infrastructures. Utilizing the support from the U.S. Department of Defense, and by consolidating a wide range of theories and solutions developed in the last few years, the authors of this paper have developed Raptor, an advanced Network Planning and Management Tool that facilitates the design and provisioning of robust and resilient networks. The tool provides multi-faceted network design, evaluation, and simulation capabilities for network planners. Future extensions of the tool currently being worked upon not only expand the tool\u2019s capabilities, but also extend these capabilities to heterogeneous interdependent networks such as communication, power, water, and satellite networks."
      }
    ]
  },
  {
    "tid":"1012",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Evolutionary game-based data aggregation model for wireless sensor networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Data aggregation has been emerged as a basic approach in wireless sensor networks (WSNs) in order to reduce the number of transmissions of sensor nodes. Since multi-source data obtained from different nodes represent redundancy or complement property, as an effective tool to deal with the conflicts, the use of game theory for WSNs is provided. The authors propose a common aggregation model, which ..."
      }
    ]
  },
  {
    "tid":"1013",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On the realization of high-order current-mode filter employing current controlled conveyors' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a general structure for realizing high-order current-mode analog filter responses with single-input and three-outputs is presented. The proposed circuit uses dual-output second-generation current controlled conveyors (DO-CCCIIs) and only grounded capacitors (Active-C structure), and can simultaneously realize low-pass, band-pass (if order is even number) and high-pass filter responses. The developed circuit does not require any component matching constraints thus it is suitable for fully integrated circuit (IC) technology. Simulations are performed for the fourth-order Butterworth filter to illustrate the frequency domain performance of the circuit."
      }
    ]
  },
  {
    "tid":"1014",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Rsp-Ql Semantics: A Unifying Query Model To Explain Heterogeneity Of Rdf Stream Processing Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"RDF and SPARQL are established standards for data interchange and querying on the Web. While they have been shown to be useful and applicable in many scenarios, they are not sufficiently adequate for dealing with streams of data and their intrinsic continuous nature. In the last years data and query languages have been proposed to extend both RDF and SPARQL for streams and continuous processing, under the name of RDF Stream Processing - RSP. These efforts resulted in several models and implementations that, at a first look, appear to propose alternative syntaxes but equivalent semantics. However, when asked to continuously answer the same queries on the same data streams, they provide different answers at disparate moments due to the heterogeneity of their operational semantics. These discrepancies render the process of understanding and comparing continuous query results complex and misleading. In this work, the authors propose RSP-QL, a comprehensive model that formally defines the semantics of an RSP system. RSP-QL makes explicit the hidden assumptions of currently available RSP systems, allows defining a formal notion of correctness for RSP query results and, thus, explains why available implementations provide different answers at disparate moments."
      }
    ]
  },
  {
    "tid":"1015",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient Auto-Increment Keys Generation for Distributed Log-Structured Storage Systems.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recent years, writing-intensive workloads on big data make log-structured style storage popular in distributed data storage systems, which provides both large-volume storage capacity and high-performance data updates. Rapidly generating valid keys for append records can significantly improve the data write performance of log-structured storage systems. In distributed and high concurrency environment, however, both the huge disk IO and the interaction overhead of a traditional lock manager limit the transactional throughput for generating auto-increment keys. In this paper, we design an efficient auto-increment keys generation manager (AKGM), a memory management structure that cannot only avoid disk IO but also eliminate the interaction overhead of traditional lock manager for transactions of generating auto-increment keys. We also propose a protocol called adaptive batch processing (ABP), which enables systems implementing AKGM to achieve high transactional throughput even under high contention workloads. We implement these protocols in an open-source database based on log-structured storage, and our experimental results show the superior performance of AKGM and ABP."
      }
    ]
  },
  {
    "tid":"1016",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Image Classification With Tailored Fine-Grained Dictionaries.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a novel fine-grained dictionary learning method for image classification. To learn a high-quality discriminative dictionary, three types of multispecific subdictionaries, i.e., class-specific dictionaries (CSDs), universal dictionary (UD), and family-specific dictionaries (FSDs), are simultaneously uncovered. Here, CSDs and UD, respectively, model the patterns for each cl..."
      }
    ]
  },
  {
    "tid":"1017",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Forensic Signature Verification Competition 4NSigComp2010 - Detection of Simulated and Disguised Signatures' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This competition scenario aims at a performance comparison of several automated systems for the task of signature verification. The systems have to rate the probability of authorship and non-authorship of signatures. In particular they have to determine whether questioned signatures are simulated disguised or the normal signature of the reference writer. Furthermore, the results will be compared to forensic handwriting examiners (FHEs) opinions on the same tasks. As such, to the best of the authors\u2019 knowledge, this scenario will be the first attempt in literature to relate system performances to the performance of FHEs who gave their opinion on exactly the the same signatures."
      }
    ]
  },
  {
    "tid":"1018",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Modulo-type Viterbi decoder for high-speed digital transmission systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Abstract\u2014In this letter, we present a modulo-type Viterbi de- coder (VD) which is a combination of modulo operation and VD at the receiver. We describe design rules by which the modulo-type VD and the conventional VD have the same functionality regard- less of the shape of the folded-modulo area or the constellation. The modulo-type VD in conjunction with the Tomlinson\u2010Harashima precoder at the transmitter reduces the system complexity signifi- cantly as compared to a conventional VD. Index Terms\u2014Folded-modulo area (FMA), modulo-type Viterbi"
      }
    ]
  },
  {
    "tid":"1019",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Creative Virtual Environments: Effect of Supraliminal Priming on Team Brainstorming' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Much of human behavior involves non-conscious cognition. Research has shown that it is possible to manipulate non-conscious cognition and behavior through \"priming\", a well-known phenomenon from cognitive psychology. 3-D Virtual Environments (VEs) have shown potential as platforms for virtual collaboration. Visually oriented, VEs represent team members as avatars. We posit that incorporating visual elements designed to improve creativity through priming in VEs can influence team-brainstorming performance. Using Open Wonderland, we developed two environments for virtual team brainstorming: one that looked like a traditional open office environment and one designed to prime team members for improved creativity. Participants worked in teams to generate ideas on a topic. Results show that when teams generated ideas in creativity primed environment, they generated significantly more ideas that were of better quality than when they worked in a neutral priming environment."
      }
    ]
  },
  {
    "tid":"1020",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'High-Rate, Double-Symbol-Decodable Stbcs From Clifford Algebras' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"For the number of transmit antennas N = 2(a) the maximum rate (in complex symbols per channel use) of all the Quasi-Orthogonal Designs (QODs) reported in the literature is a/2(a)-1. In this paper, we report double-symbol-decodable Space-Time Block Codes with rate a-1/2(a)-2 for N = 2(a) transmit antennas. In particular, our code for 8 and 16 transmit antennas offer rates 1 and 3/4 respectively, the known QODs offer only 3/4 and 1/2 respectively. Our construction is based on the representations of Clifford algebras and applicable for any number of transmit antennas. We study the diversity sum and diversity product of our codes. We show that our diversity sum is larger than that of all known QODs and hence our codes perform better than the comparable QODs at low SNRs for identical spectral efficiency. We provide simulation results for various spectral efficiencies."
      }
    ]
  },
  {
    "tid":"1021",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On portable macrocell FPU generators for division and square root operators complying to the full IEEE-754 standard' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we investigate the design of macrocell generators of division and square root floating-point operators. The number representation used in our operators is the IEEE- 754-1985 standard for binary floating-point numbers. The design and implementation of the generators rely on a powerful multiview macroblock generator tool called GenOptim. This computeraided design (CAD) tool is able to output a set of different descriptions for several VLSI technologies as well as field programmable gate arrays (FPGA's). The division and square root operators described in this paper use the signed-binary-digit representation. We start first by describing the operators for the significand, then we investigate the IEEE floating-point operators. All along this paper, and wherever appropriate, we present the implementation results using the GenOptim environment."
      }
    ]
  },
  {
    "tid":"1022",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Extra gain: Improved sparse channel estimation using reweighted \u21131-norm penalized LMS/F algorithm' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The channel estimation is one of important techniques to ensure reliable broadband signal transmission. Broadband channels are often modeled as a sparse channel. Comparing with traditional dense-assumption based linear channel estimation methods, e.g., least mean square/fourth (LMS/F) algorithm, exploiting sparse structure information can get extra performance gain. By introducing \u21131-norm penalty, two sparse LMS/F algorithms, (zero-attracting LMSF, ZA-LMS/F and reweighted ZA-LMSF, RZA-LMSF), have been proposed [1]. Motivated by existing reweighted \u21131-norm (RL1) sparse algorithm in compressive sensing [2], we propose an improved channel estimation method using RL1 sparse penalized LMS/F (RL1-LMS/F) algorithm to exploit more efficient sparse structure information. First, updating equation of RL1-LMS/F is derived. Second, we compare their sparse penalize strength via figure example. Finally, computer simulation results are given to validate the superiority of proposed method over than conventional two methods."
      }
    ]
  },
  {
    "tid":"1023",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Image Segmentation Using Graph Representations and Local Appearance and Shape Models' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A generic model-based segmentation algorithm is presented. Based on a set of training data, consisting of images with corresponding object segmentations, a local appearance and local shape model is build. The object is described by a set of landmarks. For each landmark a local appearance model is build. This model describes the local intensity values in the image around each landmark. The local shape model is constructed by considering the landmarks to be vertices in an undirected graph. The edges represent the relations between neighboring landmarks. By implying the markovianity property on the graph, every landmark is only directly dependent upon its neighboring landmarks, leading to a local shape model. The objective function to be minimized is obtained from a maximum a-posteriori approach. To minimize this objective function, the problem is discretized by considering a finite set of possible candidates for each landmark. In this way the segmentation problem is turned into a labeling problem. Mean field annealing is used to optimize this labeling problem. The algorithm is validated for the segmentation of teeth from cone beam computed tomography images and for automated cephalometric analysis."
      }
    ]
  },
  {
    "tid":"1024",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Handling Domain Specific Document Repositories for Application of Query Languages' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Domain specific information is increasingly available on the Web in form of document repositories. In specialized domains such as agriculture, bio-medical sciences and health-care, this information is required by various domain experts. Health-care experts such as researchers and practitioners require it during health-care delivery and for educational purposes. These users differ from the Web users and database users. Most of the existing document repositories on the Web have alphabetical and keyword based searches. These are not sufficient for the expert users with precise and complex queries, who require in-depth results within time constraints. Their information needs can be supported by providing user-level schema. Such a schema can support database-style high-level query languages over these repositories. Seeking specialized domain-specific information through queries is gaining importance. In this paper, a model for on-line document repositories is proposed. Queries can be performed with in-depth results. The model can be replicated to similarly structured document repositories in any given domain."
      }
    ]
  },
  {
    "tid":"1025",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'genBRDF: discovering new analytic BRDFs with genetic programming' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present a framework for learning new analytic BRDF models through Genetic Programming that we call genBRDF. This approach to reflectance modeling can be seen as an extension of traditional methods that rely either on a phenomenological or empirical process. Our technique augments the human effort involved in deriving mathematical expressions that accurately characterize complex high-dimensional reflectance functions through a large-scale optimization. We present a number of analysis tools and data visualization techniques that are crucial to sifting through the large result sets produced by genBRDF in order to identify fruitful expressions. Additionally, we highlight several new models found by genBRDF that have not previously appeared in the BRDF literature. These new BRDF models are compact and more accurate than current state-of-the-art alternatives."
      }
    ]
  },
  {
    "tid":"1026",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Privacy Leakage in Smart Homes and Its Mitigation: IFTTT as a Case Study' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The combination of an appified smart home platform and third-party apps have enabled developers to contribute their novel ideas to bring more convenience to their users. However, this also brings the potential of privacy leakage. If a third-party app is permitted to monitor a user day and night, then it will learn the behavior pattern of this user before long. In this paper, we exploited how IFTTT monitors the daily life of a user in several ways that are hardly noticeable. We propose the \u201cSpecific-fuzzification\u201d to protect the privacy of a user in two steps: filter the unnecessary events to the IFTTT, then fuzz the value of the events that must be uploaded. We evaluated the \u201cSpecific-fuzzification\u201d on event records of seven users, the result showed comparing the original IFTTT, the modified IFTTT patched with \u201cSpecific-fuzzification\u201d only gained rare events and thus could no longer recognize any behavior patterns of a user."
      }
    ]
  },
  {
    "tid":"1027",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An LTE Uplink Receiver PHY benchmark and subframe-based power management' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With the proliferation of mobile phones and other mobile internet appliances, the application area of baseband processing continues to grow in importance. Much academic research addresses the underlying mathematics, but little has been published on the design of systems to execute baseband workloads. Most systems research is conducted within companies who go to great lengths to protect their intellectual property. We present an open-source LTE Uplink Receiver PHY benchmark with a realistic representation of the baseband processing of an LTE base station, and we demonstrate its usefulness in investigating resource management strategies to conserve power on a TILEPro64. By estimating the workload of each subframe and using these estimates to control power-gating, we reduce power consumption by more than 24% (11% on average) compared to executing the benchmark with no estimation-guided resource management. By making available a benchmark containing no proprietary algorithms, we enable a broader community to conduct research both in baseband processing and on the systems that are used to execute such workloads."
      }
    ]
  },
  {
    "tid":"1028",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Riemannian metric optimization on surfaces (RMOS) for intrinsic brain mapping in the Laplace-Beltrami embedding space.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"\u2022A novel framework for intrinsic surface mapping in the Laplace\u2013Beltrami embedding space.\u2022Riemannian metric optimization for feature-driven brain surface mapping.\u2022Generally applicable for both cortical and sub-cortical structures.\u2022Holistic mapping of striatal surfaces."
      }
    ]
  },
  {
    "tid":"1029",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Detecting & avoiding interference between goals in intelligent agents' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Pro-active agents typically have multiple simultaneous goals. These may interact with each other both positively and negatively. In this paper we provide a mechanism allowing agents to detect and avoid a particular kind of negative interaction where the effects of one goal undo conditions that must be protected for successful pursuit of another goal. In order to detect such interactions we maintain summary information about the definite and potential conditional requirements and resulting effects of goals and their associated plans. We use these summaries to guard protected conditions by scheduling the execution of goals and plan steps. The algorithms and data structures developed allow agents to act rationally instead of blindly pursuing goals that will conflict."
      }
    ]
  },
  {
    "tid":"1030",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Temporal UID Matrix Strategy for Indexing Video Databases.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In video databases, each video contains temporal and spatial relationships between content objects. One of the well-known video indexing strategies is the 3D C-string strategy. However, it cannot deal with the condition that an object appears and then disappears for more than one time. To solve this problem, in this paper, we propose an indexing strategy, called Temporal UID Matrix1. Based on the original 13 spatial relationships proposed by 2D C-string and our three new spatial relationships, we can derive the temporal relationships from the sequence of spatial relationships. Therefore, in our proposed strategy, although we build only index for spatial relationships, and we still can answer the video queries, i.e., spatial, temporal, and spatio-temporal queries. From our simulation study, we show that our proposed strategy is more efficient for video searching than the 3D C-string strategy."
      }
    ]
  },
  {
    "tid":"1031",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Value-based manufacturing optimisation in serverless clouds for industry 4.0.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"There is increasing impetus towards 'Industry 4.0', a recently proposed roadmap for process automation across a broad spectrum of manufacturing industries. The proposed approach uses Evolutionary Computation to optimise real-world metrics. Features of the proposed approach are that it is generic (i.e. applicable across multiple problem domains) and decentralised, i.e. hosted remotely from the physical system upon which it operates. In particular, by virtue of being serverless, the project goal is that computation can be performed 'just in time' in a scalable fashion. We describe a case study for value-based optimisation, applicable to a wide range of manufacturing processes. In particular, value is expressed in terms of Overall Equipment Effectiveness (OEE), grounded in monetary units. We propose a novel online stopping condition that takes into account the predicted utility of further computational effort. We apply this method to scheduling problems in the (max, +) algebra, and compare against a baseline stopping criterion with no prediction mechanism. Near optimal profit is obtained by the proposed approach, across multiple problem instances.\n\n"
      }
    ]
  },
  {
    "tid":"1032",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multi-Agent Distributed Optimization via Inexact Consensus ADMM' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Multi-agent distributed consensus optimization problems arise in many signal processing applications. Recently, the alternating direction method of multipliers (ADMM) has been used for solving this family of problems. ADMM based distributed optimization method is shown to have faster convergence rate compared with classic methods based on consensus subgradient, but can be computationally expensive, especially for problems with complicated structures or large dimensions. In this paper, we propose low-complexity algorithms that can reduce the overall computational cost of consensus ADMM by an order of magnitude for certain large-scale problems. Central to the proposed algorithms is the use of an inexact step for each ADMM update, which enables the agents to perform cheap computation at each iteration. Our convergence analyses show that the proposed methods converge well under some convexity assumptions. Numerical results show that the proposed algorithms offer considerably lower computational complexity than the standard ADMM based distributed optimization methods."
      }
    ]
  },
  {
    "tid":"1033",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Drawing euler diagrams with circles' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Euler diagrams are a popular and intuitive visualization tool which are used in a wide variety of application areas, including biological and medical data analysis. As with other data visualization methods, such as graphs, bar charts, or pie charts, the automated generation of an Euler diagram from a suitable data set would be advantageous, removing the burden of manual data analysis and the subsequent task of drawing an appropriate diagram. Various methods have emerged that automatically draw Euler diagrams from abstract descriptions of them. One such method draws some, but not all, abstract descriptions using only circles. We extend that method so that more abstract descriptions can be drawn with circles, allowing sets to be represented by multiple curves. Furthermore, we show how to transform any 'undrawable' abstract description into a drawable one by adding in extra zones. Thus, given any abstract description, our method produces a drawing using only circles. A software implementation of the method is available for download."
      }
    ]
  },
  {
    "tid":"1034",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Incentive-aware routing in DTNs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Disruption tolerant networks (DTNs) are a class of networks in which no contemporaneous path may exist between the source and destination at a given time. In such a network, routing takes place with the help of relay nodes and in a store-and-forward fashion. If the nodes in a DTN are controlled by rational entities, such as people or organizations, the nodes can be expected to behave selfishly and attempt to maximize their utilities and conserve their resources. Since routing is an inherently cooperative activity, system operation will be critically impaired unless cooperation is somehow incentivized. The lack of end-to-end paths, high variation in network conditions, and long feedback delay in DTNs imply that existing solutions for mobile ad-hoc networks do not apply to DTNs. In this paper, we propose the use of pair-wise tit-for-tat (TFT) as a simple, robust and practical incentive mechanism for DTNs. Existing TFT mechanisms often face bootstrapping problems or suffer from exploitation. We propose a TFT mechanism that incorporates generosity and contrition to address these issues. We then develop an incentive-aware routing protocol that allows selfish nodes to maximize their own performance while conforming to TFT constraints. For comparison, we also develop techniques to optimize the system-wide performance when all nodes are cooperative. Using both synthetic and real DTN traces, we show that without an incentive mechanism, the delivery ratio among selfish nodes can be as low as 20% as what is achieved under full cooperation; in contrast, with TFT as a basis of cooperation among selfish nodes, the delivery ratio increases to 60% or higher as under full cooperation. We also address the practical challenges involved in implementing the TFT mechanism. To our knowledge, this is the first practical incentive-aware routing scheme for DTNs."
      }
    ]
  },
  {
    "tid":"1035",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A note on the convergence of barrier algorithms to second-order necessary points' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"\u00a0\u00a0"
      }
    ]
  },
  {
    "tid":"1036",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Security in Programmable Netword Infrastructures: The Integration of Network and Application Solutions' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Programming the network infrastructure significantly enhances its flexibility and favors fast deployment of new protocols, but also introduces serious security risks. It is crucial to protect the whole distributed infrastructure, especially its availability in case of denial-of-service attacks. A security framework for programmable networks may provide security solutions at different levels of abstraction. Active networks mainly propose a network-layer approach, by extending the packet format to include security information. Mobile code technologies tend to provide security tools at the application layer to integrate with standard external infrastructures, such as public key ones. The paper describes the security frameworks of several programmable network proposals and points out the dis/advantages related to the adopted abstraction level. This comparison suggests to consider an integrated security framework capable of choosing the service-specific balance between application-layer flexibility and network efficiency. To this purpose, the paper presents the architecture of a Programmable Network Component (PNC) that integrates security solutions at different layers and that has been implemented by using a mobile agent programming environment."
      }
    ]
  },
  {
    "tid":"1037",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Supporting Interoperability And Context-Awareness In E-Learning Through Situation-Driven Learning Processes' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Current E-Learning technologies primarily follow a data and metadata-centric paradigm by providing the learner with composite content containing the learning resources and the learning process description, usually based on specific metadata standards such as ADL SCORM or IMS Learning Design. Due to the design-time binding of learning resources, the actual learning context cannot be considered appropriately at runtime, what limits the reusability and interoperability of learning resources. This paper proposes Situation-driven Learning Processes (SDLP) which describe learning processes semantically from two perspectives: the user perspective considers a learning process as a course of learning goals which lead from an initial situation to a desired situation, whereas the system perspective utilizes Semantic Web Services (SWS) technology to semantically describe necessary resources for each learning goal within a specific learning situation. Consequently, a learning process is composed dynamically and accomplished in terms of SWS goal achievements by automatically allocating learning resources at runtime. Moreover, metadata standard-independent SDLP are mapped to established standards such as ADL SCORM and IMS LD. As a result, dynamic adaptation to specific learning contexts as well as interoperability across different metadata standards and application environments is achieved. To prove the feasibility, a prototypical application is described finally."
      }
    ]
  },
  {
    "tid":"1038",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Collaborative QoS-Aware Service Evaluation Method Among Multi-Users for a Shared Service' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In service selection, an end user often has his or her personal preferences imposing on a candidate service's non-functional properties. For a service selection process promoted by a group of users, candidate services are often evaluated by a group of end users who may have different preferences or priorities. In this situation, it is often a challenging effort to make a tradeoff among various preferences or priorities of the users. In view of this challenge, a multi-criteria decision-making method, named AHP Analytic Hierarchy Process, is introduced to transform both qualitative personal preferences and users' priorities into numeric weights. Furthermore, a QoS-aware service evaluation method is presented for a shared service's co-selection taking advantage of AHP theory. At last, a case study is presented to demonstrate the feasibility of the method."
      }
    ]
  },
  {
    "tid":"1039",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mobile spatial interaction' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mobile phones are starting to become the major platform for interaction with spatial information. Recent research has yielded promising applications and approaches for exploring, accessing and augmenting information related to the user's immediate surroundings. The CHI workshop \"Mobile Spatial Interaction\" (MSI) aims at gathering researchers working on this emerging and multifaceted, but quickly evolving topic. A forum for open dialogue is needed to enable researchers to obtain a picture of the facets and the benefits of mobile spatial interaction as well as its challenges. Potential ways to combine the various approaches will be examined and discussed."
      }
    ]
  },
  {
    "tid":"1040",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Agent societies and social networks for ubiquitous computing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In ubiquitous computing environments, providing appropriate services and information to users at the right place in the right way is challenging for many reasons: different user interests, heterogeneous devices and services, dynamic networks, information overload, or differing privacy levels, for example. Agent technology is a paradigm expected to play an increasing role in complex computing environments, and due to the increasing popularity of social networking services, we expect to see the convergence of agent and social web technologies. The goal of this theme issue is to bring together state-of-the-art research contributions that examine the convergence of agent technologies and social networks for ubiquitous computing."
      }
    ]
  },
  {
    "tid":"1041",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Novel approaches to crawling important pages early.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Abstract Web crawlers are essential to many Web applications, such as Web search engines, Web archives, and Web directories, which maintain Web pages in their local repositories. In this paper, we study the problem of crawl scheduling that biases crawl ordering toward important pages. We propose a set of crawling algorithms for effective and efficient crawl ordering by prioritizing important pages with the well-known PageRank as the importance metric. In order to score URLs, the proposed algorithms utilize various features, including partial link structure, inter-host links, page titles, and topic relevance. We conduct a large-scale experiment using publicly available data sets to examine the effect of each feature on crawl ordering and evaluate the performance of many algorithms. The experimental results verify the efficacy of our schemes. In particular, compared with the representative RankMass crawler, the FPR-title-host algorithm reduces computational overhead by a factor as great as three in running time while improving effectiveness by 5 % in cumulative PageRank."
      }
    ]
  },
  {
    "tid":"1042",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Speech watermarking through parametric modeling' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A general formulation for speech watermarking through para- metric modeling is suggested, then the paper focuses on a watermarking technique based on linear-predictive (LP) modeling of speech. In the particular strategy employed here, information is embedded by modifying the autocorre- lation values of the original speech. The amount of informa- tion that can be embedded is subject to fidelity constraints. The modified LP coefficients derived from the new set of au- tocorrelation values are used for reconstructing the water- marked speech. The perceptual quality of the watermarked speech depends on the relative energy of the embedded wa- termark, the watermark sequence used, and the LP model order. Robustness of the technique to various signal pro- cessing operations and attacks like compression, cropping, and additive noise are studied via experiments on a small speech application. Factors affecting watermark robustness and related security issues are also discussed."
      }
    ]
  },
  {
    "tid":"1043",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Training a Ranking Function for Open-Domain Question Answering.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In recent years, there have been amazing advances in deep learning methods for machine reading. In machine reading, the machine reader has to extract the answer from the given ground truth paragraph. Recently, the state-of-the-art machine reading models achieve human level performance in SQuAD which is a reading comprehension-style question answering (QA) task. The success of machine reading has inspired researchers to combine information retrieval with machine reading to tackle open-domain QA. However, these systems perform poorly compared to reading comprehension-style QA because it is difficult to retrieve the pieces of paragraphs that contain the answer to the question. In this study, we propose two neural network rankers that assign scores to different passages based on their likelihood of containing the answer to a given question. Additionally, we analyze the relative importance of semantic similarity and word level relevance matching in open-domain QA."
      }
    ]
  },
  {
    "tid":"1044",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title '3-D human body tracking from depth images using analysis by synthesis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A general method to estimate MPEG-4 body animation parameters (BAPs) from depth images by using an analysis-by-synthesis technique is presented. A generic body model is first adapted to the specific person geometry. Then, the mean square error between the synthetic depth image, produced by model rendering, and the original depth image is minimized using the downhill simplex minimization method. Use of this error norm is seen to yield improved results, when compared to two alternative error norms, which were also evaluated. Results are presented for the specific application, where six animation parameters of the arm and the palm are estimated. In this application, an initial estimate is obtained by applying an expectation-maximization (EM) algorithm, which identifies three arm parts and two joint positions (elbow, wrist). A significant advantage of this procedure is that the resulting information can be exploited for reducing the search space and for automatic scale adaptation of each body part"
      }
    ]
  },
  {
    "tid":"1045",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Towards scalable location-aware services: requirements and research issues' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The emergence of location-aware services calls for new real time spatio-temporal query processing algorithms that deal with large numbers of mobile objects and queries. Online query response is an important characterization of location-aware services. A delay in the answer to a query gives invalid and obsolete results, simply because moving objects can change their locations before the query responds. To handle large numbers of spatio-temporal queries efficiently, we propose the idea of sharing as a means to achieve scalability. In this paper, we introduce several types of sharing in the context of continuous spatio-temporal queries. Examples of sharing in the context of real-time spatio-temporal database systems include sharing the execution, sharing the underlying space, sharing the sliding time windows, and sharing the objects of interest. We demonstrate how sharing can be integrated into query predicates, e.g., selection and spatial join processing. The goal of this paper is to outline research directions and approaches that will lead to scalable and efficient location-aware services."
      }
    ]
  },
  {
    "tid":"1046",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fast and Accurate Floating Point Summation with Application to Computational Geometry' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present several simple algorithms for accurately computing the sum of n floating point numbers using a wider accumulator. Let f and F be the number of significant bits in the summands and the accumulator, respectively. Then assuming gradual underflow, no overflow, and round-to-nearest arithmetic, up to ?2F-f/(1-2-f)?+1 numbers can be accurately added by just summing the terms in decreasing order of exponents, yielding a sum correct to within about 1.5 units in the last place. In particular, if the sum is zero, it is computed exactly. We apply this result to the floating point formats in the IEEE floating point standard, and investigate its performance. Our results show that in the absence of massive cancellation (the most common case) the cost of guaranteed accuracy is about 30\u201340% more than the straightforward summation. If massive cancellation does occur, the cost of computing the accurate sum is about a factor of ten. Finally, we apply our algorithm in computing a robust geometric predicate (used in computational geometry), where our accurate summation algorithm improves the existing algorithm by a factor of two on a nearly coplanar set of points."
      }
    ]
  },
  {
    "tid":"1047",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient Rebuilding of Large Java Heaps from Event Traces' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Understanding and tracking down memory-related performance problems, such as long garbage collection times and memory leaks, is a tedious task in large and complex applications. Memory profilers can support developers in this task by recording detailed traces of the application's memory behavior which can then be analyzed offline. Unfortunately, these traces can become huge, and processing them is a real challenge. If the goal is to rebuild the heap from a trace in order to analyze it, most state-of-the-art tools perform badly, because they are either too slow or provide only a coarse-grained view of the monitored application. In this paper we present novel techniques and data structures for efficiently processing large event traces and for reconstructing the heap with only a fraction of the monitored application's memory. The reconstructed heap contains vital information about all objects, such as their addresses, their types, their allocation sites, and their allocating threads. We also provide a detailed evaluation of our approach on 33 benchmarks, showing that we can rebuild the heap from a trace efficiently using only about 18% of the memory that was used in the monitored application."
      }
    ]
  },
  {
    "tid":"1048",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A System Prototype with Multiple Robots for Finding u-Objects in a Smart Space' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A smart space that can offer automatic services mainly relies on sensing devices to acquire contextual information, and on actuation devices to take responsive actions. Because such devices are usually fixed in some locations in the physical environment and can\u2019t move by themselves, they can only sense information or act only in specific locations/directions. Different from these devices, are robots which can carry sensors and/or actuators, and are capable of moving from one location to another in a space. Because of their capability to move, robots can therefore be utilized to add flexible location-related service functions to smart spaces. To realize these new functions, robots must collaboratively work with a smart space manager and other devices/u-objects in the space. This paper presents the design and development of a system prototype with multiple robots with focus on how to manage the multiple robots and their collaborative work in finding RFID tagged u-objects in a smart room. Some experiments have been conducted and the result verifies and evaluates the aforementioned functionalities."
      }
    ]
  },
  {
    "tid":"1049",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Category-Aware API Clustering and Distributed Recommendation for Automatic Mashup Creation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mashup has emeraged as a promising way to allow developers to compose existed APIs (services) to create new or value-added services. With the rapid increasing number of services published on the Internet, service recommendation for automatic mashup creation gains a lot of momentum. Since mashup inherently requires services with different functions, the recommendation result should contain services from various categories. However, most existing recommendation approaches only rank all candidate services in a single list, which has two deficiencies. First, ranking services without considering to which categories they belong may lead to meaningless service ranking and affect the recommendation accuracy. Second, mashup developers are not always clear about which service categories they need and services in which categories cooperate better for mashup creation. Without explicitly recommending which service categories are relevant for mashup creation, it remains difficult for mashup developers to select proper services in a mixed ranking list, which lower the user friendliness of recommendation. To overcome these deficiencies, a novel category-aware service clustering and distributed recommending method is proposed for automatic mashup creation. First, a Kmeans variant (vKmeans) method based on topic model Latent Dirichlet Allocation is introduced for enhancing service categorization and providing a basis for recommendation. Second, on top of vKmeans, a service category relevance ranking (SCRR) model, which combines machine learning and collaborative filtering, is developed to decompose mashup requirements and explicitly predict relevant service categories. Finally, a category-aware distributed service recommendation (CDSR) model, which is based on a distributed machine learning framework, is developed for predicting service ranking order within each category. Experiments on a real-world dataset have proved that the proposed approach not only gains significant improvement at precision rate but also enhances the diversity of recommendation results."
      }
    ]
  },
  {
    "tid":"1050",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A 124 Mpixels/s VLSI design for histogram-based joint bilateral filtering.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents an efficient and scalable design for histogram-based bilateral filtering (BF) and joint BF (JBF) by memory reduction methods and architecture design techniques to solve the problems of high memory cost, high computational complexity, high bandwidth, and large range table. The presented memory reduction methods exploit the progressive computing characteristics to reduce the memory cost to 0.003%-0.020%, as compared with the original approach. Furthermore, the architecture design techniques adopt range domain parallelism and take advantage of the computing order and the numerical properties to solve the complexity, bandwidth, and range-table problems. The example design with a 90-nm complementary metal-oxide-semiconductor process can deliver the throughput to 124 Mpixels/s with 356-K gate counts and 23-KB on-chip memory."
      }
    ]
  },
  {
    "tid":"1051",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fast and efficient training of RBF networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Radial basis function (RBF) networks are used in many applications, e.g. for pattern classification or nonlinear regression. Typically, either stochastic, iterative training algorithms (e.g. gradient-based or second-order techniques) or clustering methods in combination with a linear optimisation technique (e.g. c-means and singular value decomposition for a linear least-squares problem) are applied to find the parameters (centres, radii and weights) of an RBF network. This article points out the advantages of a combination of the two approaches and describes a modification of the standard c-means algorithm that leads to a linear least-squares problem for which solvability can be guaranteed. The first idea may lead to significant improvements concerning the training time as well as the approximation and generalisation properties of the networks. In the particular application problem investigated here (intrusion detection in computer networks), the overall training time could be reduced by about 29% and the error rate could be reduced by about 74%. The second idea rises the reliability of the training procedure at no additional costs (regarding both, run time and quality of results)."
      }
    ]
  },
  {
    "tid":"1052",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Conditional entropy maximization for PET image reconstruction using adaptive mesh model.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Iterative image reconstruction algorithms have been widely used in the field of positron emission tomography (PET). However, such algorithms are sensitive to noise artifacts so that the reconstruction begins to degrade when the number of iterations is high. In this paper, we propose a new algorithm to reconstruct an image from the PET emission projection data by using the conditional entropy maximization and the adaptive mesh model. In a traditional tomography reconstruction method, the reconstructed image is directly computed in the pixel domain. Unlike this kind of methods, the proposed approach is performed by estimating the nodal values from the observed projection data in a mesh domain. In our method, the initial Delaunay triangulation mesh is generated from a set of randomly selected pixel points, and it is then modified according to the pixel intensity value of the estimated image at each iteration step in which the conditional entropy maximization is used. The advantage of using the adaptive mesh model for image reconstruction is that it provides a natural spatially adaptive smoothness mechanism. In experiments using the synthetic and clinical data, it is found that the proposed algorithm is more robust to noise compared to the common pixel-based MLEM algorithm and mesh-based MLEM with a fixed mesh structure."
      }
    ]
  },
  {
    "tid":"1053",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Beacon-Less Location Discovery Scheme for Wireless Sensor Networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In wireless sensor networks (WSNs), sensor location plays a critical role in many applications. Having a GPS receiver on every sensor node is costly. In the past, a number of location discovery schemes have been proposed. Most of these schemes share a common feature: they use some special nodes, called beacon nodes, which are assumed to know their own locations (e.g., through GPS receivers or manual configuration). Other sensors discover their locations based on the information provided by these beacon nodes. In this paper, we show that efficient location discovery can be achieved in sensor networks without using beacons. We propose a beacon-less location discovery scheme. based on the following observations: in practice, it is quite common that sensors are deployed in groups, i.e., sensors are put into n groups, and sensors in the same group are deployed together at the same deployment point (the deployment point is different from the sensors' final resident location). Sensors from the same group can land in different locations, and those locations usually follow a probability distribution that can be known a priori. With this prior deployment knowledge, we show that sensors can discover their locations by observing the group memberships of its neighbors. We model the location discovery problem as a statistical estimation problem, and we use the Maximum Likelihood Estimation method to estimate the location. We have conducted experiments to evaluate our scheme."
      }
    ]
  },
  {
    "tid":"1054",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Intelligent facial emotion recognition using a layered encoding cascade optimization model' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A layered cascade optimization model is developed for facial emotion recognition.Two layered cascade-based evolutionary algorithms are proposed for feature selection.They focus on within-class and between-class variations for feature optimization.Both a neural network and an adaptive ensemble classifier are employed for expression recognition.Superior performance is shown in both frontal and 90\u00bf side-view expression recognition. In this research, we propose a facial expression recognition system with a layered encoding cascade optimization model. Since generating an effective facial representation is a vital step to the success of facial emotion recognition, a modified Local Gabor Binary Pattern operator is first employed to derive a refined initial face representation and we then propose two evolutionary algorithms for feature optimization including (i) direct similarity and (ii) Pareto-based feature selection, under the layered cascade model. The direct similarity feature selection considers characteristics within the same emotion category that give the minimum within-class variation while the Pareto-based feature optimization focuses on features that best represent each expression category and at the same time provide the most distinctions to other expressions. Both a neural network and an ensemble classifier with weighted majority vote are implemented for the recognition of seven expressions based on the selected optimized features. The ensemble model also automatically updates itself with the most recent concepts in the data. Evaluated with the Cohn-Kanade database, our system achieves the best accuracies when the ensemble classifier is applied, and outperforms other research reported in the literature with 96.8% for direct similarity based optimization and 97.4% for the Pareto-based feature selection. Cross-database evaluation with frontal images from the MMI database has also been conducted to further prove system efficiency where it achieves 97.5% for Pareto-based approach and 90.7% for direct similarity-based feature selection and outperforms related research for MMI. When evaluated with 90\u00bf side-view images extracted from the videos of the MMI database, the system achieves superior performances with 80% accuracies for both optimization algorithms. Experiments with other weighting and meta-learning combination methods for the construction of ensembles are also explored with our proposed ensemble showing great adpativity to new test data stream for cross-database evaluation. In future work, we aim to incorporate other filtering techniques and evolutionary algorithms into the optimization models to further enhance the recognition performance."
      }
    ]
  },
  {
    "tid":"1055",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Performance evaluation of two concurrency control protocols for distributed databases with multiversioned entities' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The authors evaluate and compare the performance of two concurrency control protocols for distributed databases with multiversioned entities, assuming that each transaction incrementally declares its access set from the successive parts of the preordered entities. The first protocol is called protocol proposed (PP). The second is a variant of the protocol proposed by D.P. Reed (1978), here called RP1. Performance results for these protocols are collected using simulations. Key performance issues of PP are studied and relative performance of PP and RP1 is compared. Extra memory requirement is the most important cost for PP, while the cost associated with abortion of transactions are most important for RP1. For slow communication networks, at all workloads except some range of low workloads, PP performs better than RP1. For fast networks, between low to very high workloads RP1 performs better for a range of parameters. At extremely high workloads, both perform poorly, but the higher memory requirement of PP is more tolerable than the high abortion rate of RP1. A protocol similar to PP is proposed that permits universioned entities and so does not have extra memory cost and has the advantages of PP"
      }
    ]
  },
  {
    "tid":"1056",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Anonymous spatial query on non-uniform data' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Location and local service is one of the hottest bunches of applications in recent years, due to the proliferation of Global Position System (GPS) and mobile web search technology. Spatial queries retrieving neighboring Point-Of-Interests (POI) require actual user locations for services. However, exposing the physical location of querier to service system may pose privacy threat to users, if malicious adversary has access to the system. To hinder the service system from obtaining the \"true\" location of querier, current obfuscation-based approach requires a trusted third party anonymizer. As for the data-encryption-based and cPIR-based approaches, they incur costly computation overheads. Although the secure hardware-aided PIR-based technique has been shown to be superior to formers, it did not consider the characteristics of data distribution of searching domain. To deal with the problem of non-uniform data distribution and efficient retrieval, we propose a scheme, MHBL, based on Hilbert space-filling curve and flexible multi-layer grids for efficient storage and retrieval of POI data, so that improved performance of PIR-based techniques could be achieved. Numerical experiments demonstrate that the proposed technique indeed deliver better efficiency under various criteria."
      }
    ]
  },
  {
    "tid":"1057",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Dynamic picking system for 3D seismic data: Design and evaluation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In the framework of data interpretation for petroleum exploration, this paper contributes two contributions for visual exploration aiming to manually segment surfaces embedded in volumetric data. Resulting from a user-centered design approach, the first contribution, dynamic picking, is a new method of viewing slices dedicated to surface tracking, i.e. fault-picking, from 3D large seismic data sets. The proposed method establishes a new paradigm of interaction breaking with the conventional 2D slices method usually used by geoscientists. Based on the 2D+time visualization method, dynamic picking facilitates localizing of faults by taking advantage of the intrinsic ability of the human visual system to detect dynamic changes in textured data. The second, projective slice, is a focus+context visualization technique that offers the advantage of facilitating the anticipation of upcoming slices over the sloping 3D surface. From the reported experimental results, dynamic picking leads to a good compromise between fitting precision and completeness of picking while the projective slice significantly reduces the amount of workload for an equivalent level of precision."
      }
    ]
  },
  {
    "tid":"1058",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mining Productive-Associated Periodic-Frequent Patterns in Body Sensor Data for Smart Home Care.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The understanding of various health-oriented vital sign data generated from body sensor networks (BSNs) and discovery of the associations between the generated parameters is an important task that may assist and promote important decision making in healthcare. For example, in a smart home scenario where occupants' health status is continuously monitored remotely, it is essential to provide the required assistance when an unusual or critical situation is detected in their vital sign data. In this paper, we present an efficient approach for mining the periodic patterns obtained from BSN data. In addition, we employ a correlation test on the generated patterns and introduce productive-associated periodic-frequent patterns as the set of correlated periodic-frequent items. The combination of these measures has the advantage of empowering healthcare providers and patients to raise the quality of diagnosis as well as improve treatment and smart care, especially for elderly people in smart homes. We develop an efficient algorithm named PPFP-growth (Productive Periodic-Frequent Pattern-growth) to discover all productive-associated periodic frequent patterns using these measures. PPFP-growth is efficient and the productiveness measure removes uncorrelated periodic items. An experimental evaluation on synthetic and real datasets shows the efficiency of the proposed PPFP-growth algorithm, which can filter a huge number of periodic patterns to reveal only the correlated ones."
      }
    ]
  },
  {
    "tid":"1059",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Support vector machine learning from heterogeneous data: an empirical analysis using protein sequence and structure' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Motivation: Drawing inferences from large, heterogeneous sets of biological data requires a theoretical framework that is capable of representing, e.g. DNA and protein sequences, protein structures, microarray expression data, various types of interaction networks, etc. Recently, a class of algorithms known as kernel methods has emerged as a powerful framework for combining diverse types of data. The support vector machine (SVM) algorithm is the most popular kernel method, due to its theoretical underpinnings and strong empirical performance on a wide variety of classification tasks. Furthermore, several recently described extensions allow the SVM to assign relative weights to various datasets, depending upon their utilities in performing a given classification task. Results: In this work, we empirically investigate the performance of the SVM on the task of inferring gene functional annotations from a combination of protein sequence and structure data. Our results suggest that the SVM is quite robust to noise in the input datasets. Consequently, in the presence of only two types of data, an SVM trained from an unweighted combination of datasets performs as well or better than a more sophisticated algorithm that assigns weights to individual data types. Indeed, for this simple case, we can demonstrate empirically that no solution is significantly better than the naive, unweighted average of the two datasets. On the other hand, when multiple noisy datasets are included in the experiment, then the naive approach fares worse than the weighted approach. Our results suggest that for many applications, a naive unweighted sum of kernels may be sufficient. Availability: http://noble.gs.washington.edu/proj/seqstruct Contact: noble@gs.washington.edu Supplementary information: Supplementary Data are available at Bioinformatics online."
      }
    ]
  },
  {
    "tid":"1060",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Low cost RFID real lightweight binding proof protocol for medication errors and patient safety.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"An Institute of Medicine Report stated there are 98,000 people annually who die due to medication related errors in the United States, and hospitals and other medical institutions are thus being pressed to use technologies to reduce such errors. One approach is to provide a suitable protocol that can cooperate with low cost RFID tags in order to identify patients. However, existing low cost RFID tags lack computational power and it is almost impossible to equip them with security functions, such as keyed hash function. To address this issue, a so a real lightweight binding proof protocol is proposed in this paper. The proposed protocol uses only logic gates (e.g. AND, XOR, ADD) to achieve the goal of proving that two tags exist in the field simultaneously, without the need for any complicated security algorithms. In addition, various scenarios are provider to explain the process of adopting this binding proof protocol with regard to guarding patient safety and preventing medication errors."
      }
    ]
  },
  {
    "tid":"1061",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An identifier-based approach to internet mobility: a survey.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This article surveys one Internet mobility approach that is highlighted by the employment of an identifier namespace. This approach uses identifiers other than traditional IP addresses to name mobile hosts, contents, or other entities, and introduces a mapping function to locate the entities in the global scope. Although this approach has been adopted by many Internet mobility solutions and is also considered as a promising way to support mobility in the future Internet, there lacks a comprehensive review and analysis of this approach together with related solutions, especially the ones proposed in recent years. This article describes the emergence, evolution, and state of the art of the approach, and gives a classification, a review, and a comparison of typical solutions related to the approach. This article also presents discussions of open issues, challenges, and research trends on designing future Internet mobility solutions."
      }
    ]
  },
  {
    "tid":"1062",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'FPGA dynamic power minimization through placement and routing constraints' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Field-programmable gate arrays (FPGAs) are pervasive in embedded systems requiring low-power utilization. A novel power optimization methodology for reducing the dynamic power consumed by the routing of FPGA circuits by modifying the constraints applied to existing commercial tool sets is presented. The power optimization techniques influence commercial FPGA Place and Route (PAR) tools by translating power goals into standard throughput and placement-based constraints. The Low-Power Intelligent Tool Environment (LITE) is presented, which was developed to support the experimentation of power models and power optimization algorithms. The generated constraints seek to implement one of four power optimization approaches: slack minimization, clock tree paring, N-terminal net colocation, and area minimization. In an experimental study, we optimize dynamic power of circuitsmapped into 0.12 \u00b5m Xilinx Virtex-II FPGAs. Results show that several optimization algorithms can be combined on a single design, and power is reduced by up to 19.4%, with an average power savings of 10.2%."
      }
    ]
  },
  {
    "tid":"1063",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficiently processing deterministic approximate aggregation query on massive data.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In actual applications, aggregation is an important operation to return statistical characterizations of subset of the data set. On massive data, approximate aggregation often is preferable for its better timeliness and responsiveness. This paper focuses on deterministic approximate aggregation to return running aggregate within progressive deterministic error interval. The existing methods either return approximate results with fixed accuracy, or return online approximate aggregate with probabilistic confidence interval, or incur a high I/O cost on massive data. This paper proposes LDA algorithm to compute deterministic approximate aggregate on massive data efficiently. LDA utilizes selection attribute lattice of hierarchical structure to distribute tuples and obtain a horizontal partitioning of the table. In each partition, each selection attribute is kept in column file and each ranking attribute is transposed to bit-slices. Given the selection condition, only relevant partitions are involved to compute the running aggregate. The compact storage scheme based on Z-order space filling curve is proposed to reduce the management cost of the partitions. An error reduction method is devised to reduce the error interval when computing running aggregate. The extensive experimental results on synthetic and real data sets show that LDA has a significant performance advantage over the existing algorithms."
      }
    ]
  },
  {
    "tid":"1064",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Designing Scalable Out-of-core Sorting with Hybrid MPI+PGAS Programming Models' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"While Hadoop holds the current Sort Benchmark record, previous research has shown that MPI-based solutions can deliver similar performance. However, most existing MPI-based designs rely on two-sided communication semantics. The emerging Partitioned Global Address Space (PGAS) programming model presents a flexible way to express parallelism for data-intensive applications. However, not all portions of the data analytics applications are amenable to conversion using PGAS models. In this study, we propose a novel design of the out-of-core, k-way parallel sort algorithm that takes advantage of the features of both MPI and OpenSHMEM PGAS models. To the best of our knowledge, this is the first design of any data intensive computing application using Hybrid MPI + PGAS models. Our experimental evaluation indicates that our proposed framework outperforms existing MPI-based design by up to 45% at 8,192 processes. It also achieves 7X improvement over Hadoop-based sort using the same amount of resources at 1,024 cores."
      }
    ]
  },
  {
    "tid":"1065",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multi-polyhedron reconstruction in a three-view system using relaxation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Nous proposons dans cet articles deux algorithmes d'appariement st\u00e9r\u00e9o bas\u00e9s sur la relaxation visant \u00e0 trouver les correspondances entre des objets multi-polyh\u00e9driques dans un syst\u00e8me \u00e0 trois vues. Dans le premier algorithme, les points attributs sur la seconde image proches de l'intersection des deux lignes \u00e9pipolaires d\u00e9riv\u00e9es de la premi\u00e8re et troisi\u00e8me images sont consid\u00e9r\u00e9s comme des correspondances possibles. Dans le second algorithme, les attributs proches de l'une ou l'autre des lignes \u00e9pipolaries, au lieu des attributs proches de l'intersection des deux lignes \u00e9pipolaires se croisant comme dans le premier algorithme, sont consid\u00e9r\u00e9s comme des correspondances possibles. La probabilit\u00e9 initiale pour chaque triplet est assign\u00e9e selon la distance entre l'intersection des deux lignes \u00e9pipolaires se croisant (ou l'une des deux lignes \u00e9pipolaires) et l'emplacement de l'attribut. La relation de connexion entre les points attributs est alors utilis\u00e9e dans le relaxation pour r\u00e9soudre le probl\u00e8me des triplets de correspondance ambigus. Les r\u00e9sultats exp\u00e9rimentaux montrent que le premier algorithme d'appariement est plus rapide et requiert moins de m\u00e9moire, tandis que le second est plus lent mais plus fiable. Pour des angles aigus inf\u00e9rieurs \u00e0 60\u00b0, le premier algorithme devient non fiable et il faut faire appel au deuxi\u00e8me algorithme. Un choix judicieux de l'un des deux algorithmes propos\u00e9s permet une reconstruction de polyh\u00e8dres 3D multiples dans une sc\u00e8ne complexe avec configurations de cam\u00e9ra arbitraires dans un syst\u00e8me \u00e0 trois vues."
      }
    ]
  },
  {
    "tid":"1066",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimal Control of Towing Kites' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we present a challenging application of periodic optimal control. A kite that is towing a ship into a given target direction should fly optimal loops. We show how to find the maximum average tractive force by controlling the roll angle of the towing kite taking into account that the wind is increasing with the altitude over the sea. The optimal control problem for this highly nonlinear and unstable system has periodicity constraints, free initial values, and a free cycle duration. For its solution, we use MUSCOD-II, an optimal control package based on the direct multiple shooting method. Finally, we discuss the influence of an important design parameter, the effective glide ratio of the kite"
      }
    ]
  },
  {
    "tid":"1067",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Humanoid arm with the integrated serial elastic actuator' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes the design and development of an integrated serial elastic actuator (SEA) which is used as the elbow joint in the soft humanoid arm. First, requirements of SEA are illustrated. And six different types of elastic elements (e.g. the discoid element, the cylindrical) are analyzed using the design of experiment (DOE) method. And then, a discoid elastic element with larger flexibility is developed. Based on this, an integrated SEA containing the position sensors and the torque sensor is developed. And a 7-degree-of-freedom humanoid arm with joint flexibilities is designed. Finally, experiments of SEA demonstrate the approximation of the stiffness between the desired SEA and the prototype."
      }
    ]
  },
  {
    "tid":"1068",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Category-Aware API Clustering and Distributed Recommendation for Automatic Mashup Creation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mashup has emeraged as a promising way to allow developers to compose existed APIs (services) to create new or value-added services. With the rapid increasing number of services published on the Internet, service recommendation for automatic mashup creation gains a lot of momentum. Since mashup inherently requires services with different functions, the recommendation result should contain services from various categories. However, most existing recommendation approaches only rank all candidate services in a single list, which has two deficiencies. First, ranking services without considering to which categories they belong may lead to meaningless service ranking and affect the recommendation accuracy. Second, mashup developers are not always clear about which service categories they need and services in which categories cooperate better for mashup creation. Without explicitly recommending which service categories are relevant for mashup creation, it remains difficult for mashup developers to select proper services in a mixed ranking list, which lower the user friendliness of recommendation. To overcome these deficiencies, a novel category-aware service clustering and distributed recommending method is proposed for automatic mashup creation. First, a Kmeans variant (vKmeans) method based on topic model Latent Dirichlet Allocation is introduced for enhancing service categorization and providing a basis for recommendation. Second, on top of vKmeans, a service category relevance ranking (SCRR) model, which combines machine learning and collaborative filtering, is developed to decompose mashup requirements and explicitly predict relevant service categories. Finally, a category-aware distributed service recommendation (CDSR) model, which is based on a distributed machine learning framework, is developed for predicting service ranking order within each category. Experiments on a real-world dataset have proved that the proposed approach not only gains significant improvement at precision rate but also enhances the diversity of recommendation results."
      }
    ]
  },
  {
    "tid":"1069",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'HONOR: Hybrid Optimization for NOn-convex Regularized problems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice. However, due to the non-convexity and non-smoothness of the regularizer, how to efficiently solve the non-convex optimization problem for large-scale data is still quite challenging. In this paper, we propose an efficient Hybrid Optimization algorithm for NOn-convex Regularized problems (HONOR). Specifically, we develop a hybrid scheme which effectively integrates a Quasi-Newton (QN) step and a Gradient Descent (GD) step. Our contributions are as follows: (1) HONOR incorporates the second-order information to greatly speed up the convergence, while it avoids solving a regularized quadratic programming and only involves matrix-vector multiplications without explicitly forming the inverse Hessian matrix. (2) We establish a rigorous convergence analysis for HONOR, which shows that convergence is guaranteed even for non-convex problems, while it is typically challenging to analyze the convergence for non-convex problems. (3) We conduct empirical studies on large-scale data sets and results demonstrate that HONOR converges significantly faster than state-of-the-art algorithms."
      }
    ]
  },
  {
    "tid":"1070",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multiband Subspace Tracking Speech Enhancement for In-Car Human Computer Speech Interaction' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a new subspace-based speech enhancement algorithm for in-car human computer speech interaction is presented. We first incorporate a perceptual filter-bank which is derived from psycho-acoustic model with signal subspace approach to effectively suppress in-car noises of engine. Second, for real-time applications, a new subspace tracking algorithm is derived by modifying PASTd algorithm to solve the data dependent hazard of tacking algorithm. Six different types of in-car noises in TAICAR database are used in our evaluation. The experimental results demonstrate that our approach is superior to conventional subspace and spectral subtraction methods."
      }
    ]
  },
  {
    "tid":"1071",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On Convergence of Discriminative Training Algorithm for Speaker Recognition' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we present a discriminative training algorithm for GMM based speaker verification, and develop a convergence proof for the same. During training of the models, instead of performing MAP adaptation of the UBM, the model parameters of the GMM are estimated such that target scores are maximized while impostor scores are minimized. The focus of the algorithm is to estimate more accurately, the parameters of the elements of the mixture that are unique to a speaker. The algorithm uses an Expectation-Maximisation-like framework for estimation of parameters. It is shown that the algorithm converges with appropriate choice of a regularization parameter ($\\alpha$). $\\alpha$ balances target and impostor data during each iteration of the algorithm. Fast convergence can be obtained by modifying the regularization parameter after each iteration. It is shown that incrementing $\\alpha$ is a sufficient condition for convergence. Further, an impostor data selection technique for fast implementation is addressed. The performance of the system is evaluated on NIST 2003 benchmark dataset. A relative improvement 9.5% is obtained using Mel Frequency Slope as feature."
      }
    ]
  },
  {
    "tid":"1072",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Managing QoS Acceptability for Service Selection: A Probabilistic Description Logics Based Approach.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Quality of Service (QoS) guarantees are usually regulated in a Service Level Agreement (SLA) between provider and consumer of services. Such guarantees are often violated, it may however be the case where the available services do not match exactly all required QoS, leading the system to grind to a halt. It would be better to look for an approximation for acceptable QoS and avoid the complete stopping of the running services. This paper aims at making dynamic QoS acceptability easy for service selection. The proposed model is based on an extension of existing probabilistic description logics reacting to QoS variations. The contributions made are twofold (1) a query description language to express the required QoS by means of a probabilistic description logic (2) a reasoning algorithm for decision making about the acceptability of QoS w.r.t the probabilistic description."
      }
    ]
  },
  {
    "tid":"1073",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multiobjective prototype optimization with evolved improvement steps' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recently, a new iterative optimization framework utilizing an evolutionary algorithm called \"Prototype Optimization with Evolved iMprovement Steps\" (POEMS) was introduced, which showed good performance on hard optimization problems - large instances of TSP and real-valued optimization problems. Especially, on discrete optimization problems such as the TSP the algorithm exhibited much better search capabilities than the standard evolutionary approaches. In many real-world optimization problems a solution is sought for multiple (conflicting) optimization criteria. This paper proposes a multiobjective version of the POEMS algorithm (mPOEMS), which was experimentally evaluated on the multiobjective 0/1 knapsack problem with alternative multiobjective evolutionary algorithms. Major result of the experiments was that the proposed algorithm performed comparable to or better than the alternative algorithms."
      }
    ]
  },
  {
    "tid":"1074",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Automated segmentation of mammary gland regions in non-contrast X-ray CT images.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The identification of mammary gland regions is a necessary processing step during the anatomical structure recognition of human body and can be expected to provide useful information for breast tumor diagnosis. This paper proposes a fully automated scheme for segmenting the mammary gland regions in non-contrast torso CT images. This scheme calculates the probability of each voxel belonging to the mammary gland or chest muscle in CT images as the reference of the segmentation, and decides the mammary gland regions based on CT number automatically. The probability is estimated from the location of the mammary glands and chest muscles in CT images. The location is investigated from a knowledge base that stores pre-recognized anatomical structures using a number of different CT scans. We applied this scheme to 66 patient cases (female, age: 20\u201380) and evaluated the accuracy by using the Jaccard similarity coefficient (JSC) between the segmented results and two gold standards that were generated manually by 2 medical experts independently for each CT case. The result showed that the mean value of the JSC score was 0.83 with the standard deviation of 0.09 for 66 CT cases. The proposed scheme was applied to investigate the breast density distributions in normal mammary gland regions so as to demonstrate the effect and usefulness of the proposed scheme."
      }
    ]
  },
  {
    "tid":"1075",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Object tracking using SIFT features and mean shift' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A scale invariant feature transform (SIFT) based mean shift algorithm is presented for object tracking in real scenarios. SIFT features are used to correspond the region of interests across frames. Meanwhile, mean shift is applied to conduct similarity search via color histograms. The probability distributions from these two measurements are evaluated in an expectation-maximization scheme so as to achieve maximum likelihood estimation of similar regions. This mutual support mechanism can lead to consistent tracking performance if one of the two measurements becomes unstable. Experimental work demonstrates that the proposed mean shift/SIFT strategy improves the tracking performance of the classical mean shift and SIFT tracking algorithms in complicated real scenarios."
      }
    ]
  },
  {
    "tid":"1076",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A large-scale study of cultural differences using urban data about eating and drinking preferences.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"\u2022Study of spatio-temporal properties of user\u2019s food and drink preferences worldwide.\u2022Proposition of a new methodology for identifying cultural boundaries.\u2022Extensive evaluation of our proposed methodology in several aspects."
      }
    ]
  },
  {
    "tid":"1077",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Enabling stereoscopic high dynamic range video' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Stereoscopic and high dynamic range (HDR) imaging are two methods that enhance video content by respectively improving depth perception and light representation. A large body of research has looked into each of these technologies independently, but very little work has attempted to combine them due to limitations in capture and display; HDR video capture (for a wide range of exposure values over 20 f-stops) is not yet commercially available and few prototype HDR video cameras exist. In this work we propose techniques which facilitate stereoscopic high dynamic range (SHDR) video capture by using an HDR and LDR camera pair. Three methods are proposed: one based on generating the missing HDR frame by warping the existing one using a disparity map; increasing the range of LDR video using a novel expansion operator; and a hybrid of the two where expansion is used for pixels within the LDR range and warping for the rest. Generated videos were compared to the ground truth SHDR video captured using two HDR video cameras. Results show little overall error and demonstrate that the hybrid method produces the least error of the presented methods."
      }
    ]
  },
  {
    "tid":"1078",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fault-Tolerant and scalable protocols for replicated services in mobile agent systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"To enhance scalability of replicated services a large number of mobile agents attempt to access in mobile agent systems, we present a new strategy to apply an appropriate passive replication protocol to each replicated service according to its execution behavior because deterministic services require weaker constraints to ensure their consistency than non-deterministic ones. For this goal, two passive replication protocols are introduced for non-deterministic services and for deterministic services respectively. They both allow visiting mobile agents to be forwarded to and execute on any node performing a service agent, not necessarily the primary agent. Especially, in case of the protocol for deterministic services, after a backup service agent has received a mobile agent request and obtained the delivery sequence number of the request from the primary service agent, the backup agent, not the primary one, is responsible for processing the request and coordinating with the other replica service agents."
      }
    ]
  },
  {
    "tid":"1079",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Privacy-Preserving Set Operations in the Presence of Rational Parties' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Privacy-preserving set operations are useful for many data mining algorithms as building tools. Protocols for privacy-preserving set operations have considered semi-honest and malicious adversarial models in cryptographic settings, whereby an adversary is assumed to follow or arbitrarily deviate from the protocol. Semi-honest model provides weak security requiring small amount of computation, on the other hand, malicious model provides strong security requiring expensive computations like homomorphic encryption. However, efficient computation of such set operations are desirable for practical implementations. In this paper, we build efficient and private set operations avoiding the use of expensive tools like homomorphic encryption, zero knowledge proof, and oblivious transfer. Our protocol is constructed in game-theoretic model. In other words, instead of being semi-honest or malicious, the parties are viewed as rational and are assumed (only) to act in their self-interest. We show that our protocol satisfies computational Nash equilibrium."
      }
    ]
  },
  {
    "tid":"1080",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Dynamic collaborative robotic platform - A brief introduction' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a design for a platform of collaborative robots and electronic devices. The platform design is based on the similarity between the type, functionality and characteristics of the robot or device. We categorize the participating devices by functionality rather than by architecture, therefore making it easy to support new robots or devices with similar functions but different architectures. This approach also allows users to develop, implement and port applications quickly and easily. We demonstrate the efficacy and correctness of our platform through a variety of robotic applications ranging from research to teaching."
      }
    ]
  },
  {
    "tid":"1081",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On the Performance of Reconfigurable Distributed MIMO in Mobile Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose a new distributed Multi-Input Multi-Output (MIMO) architecture for mobile networks, which we refer to as reconfigurable distributed MIMO (RD-MIMO), where the communicating mobile nodes temporarily recruit adjacent nodes to operate as distributed antenna arrays. To best serve the communicating nodes, the node clusters are continuously reconfigured due to the node mobility and varying cha..."
      }
    ]
  },
  {
    "tid":"1082",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A digital signature mechanism and authentication scheme for group communication in grid' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Group communication is one of the most important strategies to realize large-scale information resource sharing in grid. However, it is very difficult to ensure the security of group communication in grid. In this paper, we propose a digital signature mechanism with one time secret key. In the mechanism, the trust center only needs to issue the partial secret key one time for each group member; and each group member can generate its different secret key each time. Moreover, the size of the signature is independent of the number of group members and the group public key is constant. According to this mechanism, we present a new authentication scheme. In order to verify the efficiency of the scheme, a series of simulation experiments are presented. The results show that the scheme is efficient for group communication in grid."
      }
    ]
  },
  {
    "tid":"1083",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An energy efficient and integrity-preserving aggregation protocol in wireless sensor networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Wireless sensor networks(WSNs) is a data-centric network where the querier is mostly concerned about the statistical aggregates (MAX/MIN, SUM, AVERAGE). A few protocols have been proposed for provably secure tree-based in-network data aggregation in WSN. However, these protocols all suffer from high communication overhead or long network delay when sending off-path values to each sensor node to independently verify that its own data was added into the final aggregation result. Since the off-path dissemination phase is the dominating factor, it is crucial to optimize this phase and save energy to increase the lifetime of network. In this paper, we propose a novel integrity preserving protocol Energy Efficient and Integrity-Preserving Aggregation Protocol (E2IPAP) for tree-based sensor network-s, aiming to provide a new approach for result-checking and reduce communication overhead. Analysis shows that, E2IPAP significantly reduces the overhead in disseminating off-path values and overall communication. E2IPAP has O(? log n) node congestion and is suitable for similar secure data aggregation protocols."
      }
    ]
  },
  {
    "tid":"1084",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Situational assessment of intrusion alerts: a multi attack scenario evaluation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this research study, we focus on intrusion alerts and the burden of analyzing numerous security events by network administrators. We present Avisa2, a network security visualization system that can assist in the comprehension of IDS alerts and detection of abnormal pattern activities. The quantity of security events triggered by modern day intrusion systems, accompanied by the level of complexity and lack of correlation between events, limits the human cognitive process in identifying anomalous behavior. This shortcoming induces the need for an automated process that would project critical situations and prioritize network hosts encountering peculiar behaviors. At the heart of Avisa2 lies a collection of heuristic functions that are utilized to score, rank, and prioritize internal hosts of the monitored network. We believe this contribution elevates the practicality of Avisa2 in identifying critical situations and renders it to be far superior to traditional security systems that solely focus on visualization. The effectiveness of Avisa2 is evaluated on two multi-stage attack scenarios; each intentionally focused on a particular attack type, network service, and network range. Avisa2 proved effective and accurate in prioritizing hosts under attack or hosts in which attacks were performed from."
      }
    ]
  },
  {
    "tid":"1085",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On Stable Dynamic Background Generation Technique Using Gaussian Mixture Models for Robust Object Detection' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Gaussian mixture models (GMM) is used to represent the dynamic background in a surveillance video to detect the moving objects automatically. All the existing GMM based techniques inherently use the proportion by which a pixel is going to observe the background in any operating environment. In this paper we first show that such a proportion not only varies widely across different scenarios but also forbids using very fast learning rate. We then propose a dynamic background generation technique in conjunction with basic background subtraction which detected moving objects with improved stability and superior detection quality on a wide range of operating environments in two sets of benchmark surveillance sequences."
      }
    ]
  },
  {
    "tid":"1086",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Gated2Depth: Real-time Dense Lidar from Gated Images.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present an imaging framework which converts three images from a gated camera into high-resolution depth maps with depth resolution comparable to pulsed lidar measurements. Existing scanning lidar systems achieve low spatial resolution at large ranges due to mechanically-limited angular sampling rates, restricting scene understanding tasks to close-range clusters with dense sampling. In addition, todayu0027s lidar detector technologies, short-pulsed laser sources and scanning mechanics result in high cost, power consumption and large form-factors. We depart from point scanning and propose a learned architecture that recovers high-fidelity dense depth from three temporally gated images, acquired with a flash source and a high-resolution CMOS sensor. The proposed architecture exploits semantic context across gated slices, and is trained on a synthetic discriminator loss without the need of dense depth labels. The method is real-time and essentially turns a gated camera into a low-cost dense flash lidar which we validate on a wide range of outdoor driving captures and in simulations."
      }
    ]
  },
  {
    "tid":"1087",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Application-Driven Study of Parallel System Overheads and Network Bandwidth Requirements' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Evaluating and analyzing the performance of a parallel application on an architecture to explain the disparity between projected and delivered performance is an important aspect of parallel systems research. However, conducting such a study is hard due to the vast design space of these systems. In this paper, we study two important aspects related to the performance of parallel applications on shared memory parallel architectures. Fist, we quantify overheads observed during the execution of these applications on three different simulated architectures. We next use these results to synthesize the bandwidth requirements for the applications with respect to different network topologies. This study is performed using an execution-driven simulation tool called SPASM, which provides a way of isolating and quantifying the different parallel system overheads in a nonintrusive manner. The first exercise shows that in shared memory machines with private caches, as long as the applications are well-structured to exploit locality, the key determinant that impacts performance is network connection. The second exercise quantifies the network bandwidth needed to minimize the effect of network connection. Specifically, it is shown that for the applications considered, as long as the problem sizes are increased commensurate with the system size, current network technologies supporting 200-300 MBytes/sec link bandwidth are sufficient to keep the network overheads (such as the latency and contention) within acceptable bounds."
      }
    ]
  },
  {
    "tid":"1088",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Empirical study on fusion methods using ensemble of RBFNN for network intrusion detection' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The network security problem has become a critical issue and many approaches have been proposed to tackle the information security problems, especially the Denial of Service (DoS) attacks. Multiple Classifier System (MCS) is one of the approaches that have been adopted in the detection of DoS attacks recently. Fusion strategy is crucial and has great impact on the classification performance of an MCS. However the selection of the fusion strategy for an MCS in DoS problem varies widely. In this paper, we focus on the comparative study on adopting different fusion strategies for an MCS in DoS problem."
      }
    ]
  },
  {
    "tid":"1089",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title '\"We've bin watching you\": designing for reflection and social persuasion to promote sustainable lifestyles' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"BinCam is a social persuasive system to motivate reflection and behavioral change in the food waste and recycling habits of young adults. The system replaces an existing kitchen refuse bin and automatically logs disposed of items through digital images captured by a smart phone installed on the underside of the bin lid. Captured images are uploaded to a BinCam application on Facebook where they can be explored by all users of the BinCam system. Engagement with BinCam is designed to fit into the existing structure of users' everyday life, with the intention that reflection on waste and recycling becomes a playful and shared group activity. Results of a user study reveal an increase in both users' awareness of, and reflection about, their waste management and their motivation to improve their waste-related skills. With BinCam, we also explore informational and normative social influences as a source of change (e.g., socially evoked feelings of 'guilt' for non-recycling or food disposal), which has to date been underexplored in persuasive HCI. Design implications for reflection and social persuasion are proposed."
      }
    ]
  },
  {
    "tid":"1090",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimizing ballast design of wave energy converters using evolutionary algorithms' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Wave energy converters promise to be a viable alternative to current electrical generation methods. However, these generators must become more efficient before wide-scale industrial use can become cost-effective. The efficiency of these devices is primarily dependent upon their geometry and ballast configuration which are both difficult to evaluate, due to slow computation time and high computation cost of current models. In this paper, we use evolutionary algorithms to optimize the ballast geometry of a wave energy generator using a two step process. First, we generate a function approximator (neural network) to predict wave energy converter power output with respect to key geometric design variables. This is a critical step as the computation time of using a full model (e.g., AQWA) to predict energy output prohibits the use of an evolutionary algorithm for design optimization. The function approximator reduced the computation time by over 99% while having an average error of only 1.5%. The evolutionary algorithm then optimized the weight distribution of a wave energy generator, resulting in an 84% improvement in power output over a ballast-free wave energy converter."
      }
    ]
  },
  {
    "tid":"1091",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Limited Scale-Free Overlay Topologies for Unstructured Peer-to-Peer Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In unstructured peer-to-peer (P2P) networks, the overlay topology (or connectivity graph) among peers is a crucial component in addition to the peer/data organization and search. Topological characteristics have profound impact on the efficiency of a search on such unstructured P2P networks, as well as other networks. A key limitation of scale-free (power-law) topologies is the high load (i.e., high degree) on a very few number of hub nodes. In a typical unstructured P2P network, peers are not willing to maintain high degrees/loads as they may not want to store a large number of entries for construction of the overlay topology. Therefore, to achieve fairness and practicality among all peers, hard cutoffs on the number of entries are imposed by the individual peers, which limits scale-freeness of the overall topology, hence limited scale-free networks. Thus, it is expected that the efficiency of the flooding search reduces as the size of the hard cutoff does. We investigate the construction of scale-free topologies with hard cutoffs (i.e., there are not any major hubs) and the effect of these hard cutoffs on the search efficiency. Interestingly, we observe that the efficiency of normalized flooding and random walk search algorithms increases as the hard cutoff decreases."
      }
    ]
  },
  {
    "tid":"1092",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A semi-analytic porosity evolution scheme for simulating wormhole propagation with the Darcy-Brinkman-Forchheimer model.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we consider numerical simulation of wormhole propagation with the Darcy\u2013Brinkman\u2013Forchheimer model. Matrix acidization in carbonate reservoirs is a widely practiced technique in the product enhancement of the oil and gas reservoir. A wormhole, i.e. a flow channel with high porosity, is generated during reactive dissolution of carbonates by the action of the injected acid. In the wormhole forming process, the porosity changes non-uniformly in space, and it even becomes close to unity in the central regions of a wormhole. The Darcy\u2013Brinkman\u2013Forchheimer model accounts for both the porous media and clear fluid area, so it can be used to model wormhole propagation perfectly. This model, however, strongly depends on porosity. Therefore, the time schemes for solving the evolutionary equation of porosity have a significant effect on accuracy and stability of the numerical simulation for wormhole propagation. We propose a semi-analytic time scheme, which solves the porosity equation analytically at each time step for given acid concentration. The proposed numerical method can improve the accuracy and stability of numerical simulation significantly. For theoretical analysis of the proposed time scheme for the wormhole simulation, we first reconstruct the analytical functions of porosity to analyze the time error of the porosity, and on the basis of error estimates of porosity, we employ a coupled analysis approach to achieve the estimates of pressure, velocity and solute concentration. The time error estimates for velocity, pressure, concentration and porosity are obtained in different norms. Finally, numerical results are provided to verify the effectiveness of the proposed scheme."
      }
    ]
  },
  {
    "tid":"1093",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Trace Norm Regularization: Reformulations, Algorithms, and Multi-Task Learning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We consider a recently proposed optimization formulation of multi-task learning based on trace norm regularized least squares. While this problem may be formulated as a semidefinite program (SDP), its size is beyond general SDP solvers. Previous solution approaches apply proximal gradient methods to solve the primal problem. We derive new primal and dual reformulations of this problem, including a reduced dual formulation that involves minimizing a convex quadratic function over an operator-norm ball in matrix space. This reduced dual problem may be solved by gradient-projection methods, with each projection involving a singular value decomposition. The dual approach is compared with existing approaches and its practical effectiveness is illustrated on simulations and an application to gene expression pattern analysis."
      }
    ]
  },
  {
    "tid":"1094",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Structural similarity-based object tracking in multimodality surveillance videos' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper addresses the problem of object tracking in video sequences for surveillance applications by using a recently proposed structural similarity-based image distance measure. Multimodality surveillance videos pose specific challenges to tracking algorithms, due to, for example, low or variable light conditions and the presence of spurious or camouflaged objects. These factors often cause undesired luminance and contrast variations in videos produced by infrared sensors (due to varying thermal conditions) and visible sensors (e.g., the object entering shadowy areas). Commonly used colour and edge histogram-based trackers often fail in such conditions. In contrast, the structural similarity measure reflects the distance between two video frames by jointly comparing their luminance, contrast and spatial characteristics and is sensitive to relative rather than absolute changes in the video frame. In this work, we show that the performance of a particle filter tracker is improved significantly when the structural similarity-based distance is applied instead of the conventional Bhattacharyya histogram-based distance. Extensive evaluation of the proposed algorithm is presented together with comparisons with colour, edge and mean-shift trackers using real-world surveillance video sequences from multimodal (infrared and visible) cameras."
      }
    ]
  },
  {
    "tid":"1095",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An interactive neural network-based approach for solving multiple criteria decision-making problems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a new approach for solving multiple criteria decision-making (MCDM) problems is proposed based on Decision Neural Network (DNN). The DNN is used to capture and represent the Decision Maker's (DM's) preference. Then, with DNN, an optimization problem is solved to search for the most desirable solution. Procedures of model modification through an interactive procedure, and model optimization are also discussed. An example is given to illustrate the approach and the result is compared with the Interactive FFANN Procedure [Manage. Sci. 42 (1996)]. The result shows that the DNN approach is an encouraging and robust method for solving MCDM problems."
      }
    ]
  },
  {
    "tid":"1096",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Enabling Parallel TMA Image Analysis in a Grid Environment' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The Grid represents a great opportunity in the scientific community to solve data/compute intensive problems and to share data. Using the Grid, different application domains could achieve great advantages; however, the exploitation of the Grid is not a trivial pursuit for many users. The key issue is to enable a simplified use of Grid resources and tools that scientists usually employ. In this paper, we investigate these topics in bioinformatics community to allow the efficient elaboration on the Grid of images obtained through the Tissue MicroArray technique. We present a Grid Framework for Tissue Microarray Analysis, GF4TMA, that allows the selection of TMA images and their efficient analysis by the exploitation of the Grid architecture. In particular, we exploited PIMA(GE)^2 Lib, the Parallel IMAGE processing GEnoa Library. A critical point was to enable parallel computations on the Grid without compromising the efficiency and the user-friendliness of the native library. We tackled this issue by encapsulating the library in a Grid service."
      }
    ]
  },
  {
    "tid":"1097",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Robust Modular Artmap For Multi-Class Shape Recognition' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a Fuzzy ARTMAP (FAM) based modular architecture for multi-class pattern recognition known as Modular Adaptive Resonance Theory Map (MARTMAP). The prediction of class membership is made collectively by combining outputs from multiple novelty detectors. Distance-based familiarity discrimination is introduced to improve the robustness of MARTMAP in the presence of noise. The effectiveness of the proposed architecture is analyzed and compared with ARTMAP-FD network, FAM network, and One-Against-One Support Vector Machine (OAO-SVM). Experimental results show that MARTMAP is able to retain effective familiarity discrimination in noisy environment, and yet less sensitive to class imbalance problem as compared to its counterparts."
      }
    ]
  },
  {
    "tid":"1098",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Harvesting Caregiving Knowledge: Design Considerations for Integrating Volunteer Input in Dementia Care' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"ABSTRACTImproving volunteer performance leads to better caregiving in dementia care settings. However, caregiving knowledge systems have been focused on eliciting and sharing expert, primary caregiver knowledge, rather than volunteer-provided knowledge. Through the use of an experience prototype, we explored the content of volunteer caregiver knowledge and identified ways in which such non-expert knowledge can be useful to dementia care. By using lay language, sharing information specific to the client and collaboratively finding strategies for interaction, volunteers were able to boost the effectiveness of future volunteers. Therapists who reviewed the content affirmed the reliability of volunteer caregiver knowledge and placed value on its recency, variety and its ability to help bridge language and professional barriers. We discuss how future systems designed for eliciting and sharing volunteer caregiver knowledge can be used to promote better dementia care."
      }
    ]
  },
  {
    "tid":"1099",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The Index-Permutation Graph Model for Hierarchical Interconnection Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we present the index-permutation (IP) graph model, and apply it to the systematic development of efficient hierarchical networks. We derive several classes of interconnection networks based on IP graphs to achieve desired properties; the results compare favorably with popular interconnection networks, as measured by topological (e.g., node degree and diameter) and algorithmic properties, and are particularly efficient in view of their sparse inter-module communication patterns. In particular, the diameters of suitably-constructed super-IP graphs, a subclass of IP graphs, are optimal within a factor of 1 +o(1), given their node degrees. The IP graph model can also be used as a common platform that unifies the architectures and algorithms for a vast variety of interconnection networks."
      }
    ]
  },
  {
    "tid":"1100",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Interactive Trip Planning Using Activity Trajectories.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present an interactive trip planning system called @FINDER which uses an exemplar trajectory query to find the most related top-k spatial-textual trajectories. @FINDER is implemented to support various degrees of user information needs for trip planning. For users with zero knowledge about places to travel, @FINDER provides a heatmap of popular points of interest (POIs) as well as popular activities from a trajectory database. The system helps users quickly explore the places, and helps formulate an exemplar trajectory query, which specifies preferred places to go and activities of interest. Then @FINDER provides efficient query processing of the top-k related spatial-textual trajectories using a new approach to spatial-textual trajectory indexing recently developed at RMIT University. For each of the top-k results found in the form of a set of POIs and activities, @FINDER further computes the optimal route (in term of the travel time) covering all of the POIs, and returns an album to the user. Lastly, users can further interact with @FINDER by adding or deleting POIs/activities in the original exemplar query, and the system will update the results in a timely manner."
      }
    ]
  },
  {
    "tid":"1101",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On the Impact of Evolving Requirements-Architecture Dependencies: An Exploratory Study' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Architecture design plays a significant role in the evolution of software systems, as it provides the prime realization of the driving requirements and their inter-dependencies. With the increasing volatility of software requirements nowadays, it is necessary to understand the correlation between evolving classical requirements dependencies and their impact on the architectural decomposition. In the context of this analysis, two questions arise: (i) what are the conventional categories of requirements dependencies that are more architecturally significant in terms of change impact? and (ii) to what extent those evolving dependencies tend to generate ripple effects through architectural modules and interfaces. In order to address these two questions, this paper first presents an analysis model that categorizes requirements dependencies. Second, we have performed an exploratory study, based on the change history analysis of a real-life Web-based information system, in order to gather the most architecturally-significant requirements dependencies from our model. We have systematically analyzed ten system releases, based on some qualitative and quantitative indicators, with respect to how the requirements- architecture dependencies and compositions evolved."
      }
    ]
  },
  {
    "tid":"1102",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Requirements for Integrating Defect Prediction and Risk-Based Testing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Defect prediction is a powerful method that provides information about the likely defective parts in software system and is applicable to improve effectiveness and efficiency of software quality assurance. This makes defect prediction a perfect candidate to be combined with risk-based testing to optimally guide testing activities towards risky parts of software. As a first step towards a successful combination, this paper presents requirements that have to be fulfilled for enabling the synergies between defect prediction and risk-based testing."
      }
    ]
  },
  {
    "tid":"1103",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On the Capacity of Multiple Access and Broadcast Fading Channels with Full Channel State Information at Low SNR.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We study the throughput capacity region of the Gaussian multi-access (MAC) fading channel with perfect channel state information (CSI) at the receiver and at the transmitters, at low power regime. We show that it has a multidimensional rectangle structure and thus is simply characterized by single user capacity points. More specifically, we show that at low power regime, the boundary surface of th..."
      }
    ]
  },
  {
    "tid":"1104",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Architecture for Programming Distributed Applications on Fog to Cloud Systems.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a framework to develop and execute applications in distributed and highly dynamic computing systems composed of cloud resources and fog devices such as mobile phones, cloudlets, and micro-clouds. The work builds on the COMPSs programming framework, which includes a programming model and a runtime already validated in HPC and cloud environments for the transparent execution of parallel applications. As part of the proposed contribution, COMPSs has been enhanced to support the execution of applications on mobile platforms that offer GPUs and CPUs. The scheduling component of COMPSs is under design to be able to offload the computation to other fog devices in the same level of the hierarchy and to cloud resources when more computational power is required. The framework has been tested executing a sample application on a mobile phone offloading task to a laptop and a private cloud."
      }
    ]
  },
  {
    "tid":"1105",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Comparing Learning Attention Control in Perceptual and Decision Space' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The first question answered in this paper is whether or not learning attention control in the decision space is feasible and how to develop an online as well as interactive learning approach for such control in this space, in case of feasibility. Here, decision space is formed by the decision vector of the agents each has allowed to dynamically observe just a subset of all available sensors. Attention control in this new space means active and dynamic selection of these decision agents to contribute in making final decision. The second debate is verifying the advantages of attention control in decision space over that in perceptual space. According to the tight coupling of attention control and motor action selection, in order to answer above mentioned questions, attention control and motor action selection are formulated in a unified optimization problem and reinforcement learning is utilized to solve it. In addition to the theoretic comparison of learning attention control in perceptual and decision space in terms of computational complexity, two proposed approaches are tested on a simple traffic sign recognition task."
      }
    ]
  },
  {
    "tid":"1106",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Priority-Based Medium Access Control Protocol for Providing QoS in Wireless Sensor Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this letter, we propose PQ-MAC: a priority-based medium access control (MAC) protocol for providing quality of service (QoS) in wireless sensor networks (WSNs) which minimizes the energy consumption with traffic-based sleep-wakeup scheduling and supports QoS using differentiating channel access policy, packet scheduling, and queue management. The PQ-MAC utilizes the advantages of time division multiple access (TDMA) and slotted carrier-sense multiple access (CSMA). The proposed protocol is an energy-efficient, priority-based, and QoS compatible MAC protocol. It consists of multi-level queue management, sleep-wakeup scheduling, and an ordered contention period (CP) scheme. It also guarantees time-bounded delivery of QoS packets. Performance evaluation is conducted between PQ-MAC and S-MAC with respect to three performance metrics: energy consumption, throughput, and average latency. The simulation results show that the performance of PQ-MAC is better than that of S-MAC."
      }
    ]
  },
  {
    "tid":"1107",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'IRA Code Design for Iterative Detection and Decoding: A Setpoint-Based Approach' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a novel setpoint-based design approach for Irregular Repeat Accumulate (IRA) codes in iterative detection and decoding structures is presented. In contrast to conventional IRA code design in which the convolutional decoder is combined with the detector, the goal behind this approach is to keep the IRA decoding structure consisting of convolutional decoder and repetition decoder intact, i.e. to consider it as an inner loop of the overall detection structure. The outer loop is then composed of the IRA decoder and the system specific detector. This approach requires to adapt the irregular repetition code jointly to the convolutional decoder as well as to the detector which is achieved by formulating setpoints for the inner and outer code characteristic. As will be shown, the presented code design approach, although starting from a completely different viewpoint as the conventional approach, leads to an irregular repetition code with a very similar transfer characteristic and code rate than the conventional approach."
      }
    ]
  },
  {
    "tid":"1108",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Discovering containment: from infants to machines.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Abstract   Rapid developments in the fields of learning and object recognition have been obtained by successfully developing and using methods for learning from a large number of labeled image examples. However, such current methods cannot explain infants\u2019 learning of new concepts based on their visual experience, in particular, the ability to learn complex concepts without external guidance, as well as the natural order in which related concepts are acquired. A remarkable example of early visual learning is the category of u0027containersu0027 and the notion of \u2018containment\u2019. Surprisingly, this is one of the earliest spatial relations to be learned, starting already around 3\u202fmonth of age, and preceding other common relations (e.g., \u2018support\u2019, \u2018in-between\u2019). In this work we present a model, which explains infants\u2019 capacity of learning \u2018containment\u2019 and related concepts by \u2018just looking\u2019, together with their empirical development trajectory. Learning occurs in the model fast and without external guidance, relying only on perceptual processes that are present in the first months of life. Instead of labeled training examples, the system provides its own internal supervision to guide the learning process. We show how the detection of so-called \u2018paradoxical occlusion\u2019 provides natural internal supervision, which guides the system to gradually acquire a range of useful containment-related concepts. Similar mechanisms of using implicit internal supervision can have broad application in other cognitive domains as well as artificial intelligent systems, because they alleviate the need for supplying extensive external supervision, and because they can guide the learning process to extract concepts that are meaningful to the observer, even if they are not by themselves obvious, or salient in the input."
      }
    ]
  },
  {
    "tid":"1109",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title '\nEnergy harvesting aware relay node addition for power-efficient coverage in wireless sensor networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper deals with power-efficient coverage in wireless sensor networks (WSN) by taking advantage of energy-harvesting capabilities. A general scenario is considered for deployed networks with two types of sensor nodes, harvesting enabled nodes (HNs), and none-harvesting nodes (NHNs). The aim is to use only the HNs for relaying packets, while NHNs use will be limited to sensing and transmitting their own readings. The problem is modeled using graph theory and reduced to finding the minimum weighted connected dominating set in a vertex weighted graph. A limited number of relay nodes is added at the positions close to the NHNs in the resulted set. The weight function ensures minimizing the number of NHNs in the set, and thus reducing the relay nodes to be added. Our contribution is to consider relay node placement (addition) in energy harvesting WSN, where only HNs are used to forward packets. This is to preserve the limited energy of NHNs. Extensive simulation results show that the proposed relay node addition strategy prolongs the network lifetime, from the double, to factors of several tens of times. This is at a reasonable cost in terms of the number of relay nodes added, which is compared to a lower-bound derived in the paper."
      }
    ]
  },
  {
    "tid":"1110",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Nonlinear self-interference cancellation in MIMO full-duplex transceivers under crosstalk.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a novel digital self-interference canceller for an inband multiple-input-multiple-output (MIMO) full-duplex radio. The signal model utilized by the canceller is capable of modeling the in-phase quadrature (IQ) imbalance, the nonlinearity of the transmitter power amplifier, and the crosstalk between the transmitters, thereby being the most comprehensive signal model presented thus far within the full-duplex literature. Furthermore, it is also shown to be valid for various different radio frequency (RF) cancellation solutions. In addition to this, a novel complexity reduction scheme for the digital canceller is also presented. It is based on the widely known principal component analysis, which is used to generate a transformation matrix for controlling the number of parameters in the canceller. Extensive waveform simulations are then carried out, and the obtained results confirm the high performance of the proposed digital canceller under various circuit imperfections. The complexity reduction scheme is also shown to be capable of removing up to 65% of the parameters in the digital canceller, thereby significantly reducing its computational requirements."
      }
    ]
  },
  {
    "tid":"1111",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Dealing with uncertainty in the principal directions of tensors' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The variability of tensor fields is usually analyzed with multivariate statistical distributions. Multivariate distributions model every component of the tensor, which are not invariant under rotation. They therefore tell very little information about the true shape of the tensor. A statistical analysis on the eigenvalues of the tensor would be more revealing. The eigenvalues determine if a tensor is uniaxial, i.e.: only one eigenvalue is different from zero, isotropic, volume preserving or purely anisotropic. However, the eigenvalues of a normally distributed tensor are not, in general, normally distributed. In this paper, we solve this problem directly for small sizes of samples by determining the probability that the maximum error is within a reasonable bound. When the error is likely to be within a reasonable bound, we consider the eigenvalues of a tensor to be normally distributed along a mean eigendirection. Monte Carlo simulation shows that the computed bound is tight and becomes tighter when the number of sample increases. An application of the method, analysis of deformations on the cortical surface, is presented in this paper. On this data, we found that 80% of the anisotropic deformations could be analyzed by modeling the eigenvalues directly. Thus, the proposed method allows formulating statistical hypothesis directly on eigenvalues in many cases of measured deformations. Although the method was used in only one application, the method could be extended to application involving diffusion MRI or other imaging technique involving tensors."
      }
    ]
  },
  {
    "tid":"1112",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Tractable reasoning with vague knowledge using fuzzy $\\mathcal{EL}^{++}$' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Fuzzy Description Logics (fuzzy DLs) are extensions of classic DLs that are capable of representing and reasoning with imprecise and vague knowledge. Though reasoning algorithms for very expressive fuzzy DLs have been developed and optimizations have started to be explored, the efficiency of such systems is still questionable, and the study of tractable languages is an interesting open issue. In this paper we introduce a tractable fuzzy extension of $\\mathcal{EL}^{++}$ . We present its syntax and semantics together with a reasoning algorithm for the fuzzy concept subsumption problem to which other problems can be reduced."
      }
    ]
  },
  {
    "tid":"1113",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On-the-fly elimination of dynamic irregularities for GPU computing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The power-efficient massively parallel Graphics Processing Units (GPUs) have become increasingly influential for general-purpose computing over the past few years. However, their efficiency is sensitive to dynamic irregular memory references and control flows in an application. Experiments have shown great performance gains when these irregularities are removed. But it remains an open question how to achieve those gains through software approaches on modern GPUs. This paper presents a systematic exploration to tackle dynamic irregularities in both control flows and memory references. It reveals some properties of dynamic irregularities in both control flows and memory references, their interactions, and their relations with program data and threads. It describes several heuristics-based algorithms and runtime adaptation techniques for effectively removing dynamic irregularities through data reordering and job swapping. It presents a framework, G-Streamline, as a unified software solution to dynamic irregularities in GPU computing. G-Streamline has several distinctive properties. It is a pure software solution and works on the fly, requiring no hardware extensions or offline profiling. It treats both types of irregularities at the same time in a holistic fashion, maximizing the whole-program performance by resolving conflicts among optimizations. Its optimization overhead is largely transparent to GPU kernel executions, jeopardizing no basic efficiency of the GPU application. Finally, it is robust to the presence of various complexities in GPU applications. Experiments show that G-Streamline is effective in reducing dynamic irregularities in GPU computing, producing speedups between 1.07 and 2.5 for a variety of applications."
      }
    ]
  },
  {
    "tid":"1114",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Reducing Memory Traffic and Accelerting Prolog Execution in a Superscalar Prolog System' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Memory access operations constitute about 32.7% of all the operations executed in a typical Prolog program. Among these memory accesses, 75% are to the program control structures (environments and choice points). These memory accesses plus possible data cache misses greatly impair system performance, and the problem is even more severe in a VLIW, superscalar, or superpipelined Prolog system. This paper describes an innovative windowed register file management technique called SORWT (splittable overlapping register window technique). With SORWT, environments, choice points, and arguments can be stored in a windowed register file. SORWT reduces the number of memory accesses to only 25% of the number made when a conventional stack scheme is used. This paper describes in detail how Warren and PLM instructions can be implemented using SORWT and it presents a register file overflow/underflow handling mechanism called the memory window matrix (MWM) and a mapping function for use between register windows and the MWM. Thirty benchmark programs are used to study performance issues, the overhead of SORWT, optimal register file and window sizes, and the argument overflow rate."
      }
    ]
  },
  {
    "tid":"1115",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Network signaling channel for improving ZigBee performance in dynamic cluster-tree networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"ZigBee is one of the most potential standardized technologies for wireless sensor networks (WSNs). Yet, sufficient energy-efficiency for the lowest power WSNs is achieved only in rather static networks. This severely limits the applicability of ZigBee in outdoor and mobile applications, where operation environment is harsh and link failures are common. This paper proposes a network channel beaconing (NCB) algorithm for improving ZigBee performance in dynamic cluster-tree networks. NCB reduces the energy consumption of passive scans by dedicating one frequency channel for network beacon transmissions and by energy optimizing their transmission rate. According to an energy analysis, the power consumption of network maintenance operations reduces by 70%-76% in dynamic networks. In static networks, energy overhead is negligible. Moreover, the service time for data routing increases up to 37%. The performance of NCB is validated by ns-2 simulations. NCB can be implemented as an extension on MAC and NWK layers and it is fully compatible with ZigBee."
      }
    ]
  },
  {
    "tid":"1116",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A neural network document classifier with linguistic feature selection' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this article, a neural network document classifier with linguistic feature selection and multi-category output is presented. It consists of a feature selection unit and a hierarchical neural network classification unit. In feature selection unit, we extract terms from some original documents by text processing, and then we analyze the conformity and uniformity of each term by entropy function which is characterized to measure the significance of term. Terms with high significance will be selected as input features for neural network document classifiers. In order to reduce the input dimension, we perform a mechanism to merge synonyms. According to the uniformity analysis, we obtain a term similarity matrix by fuzzy relation operation. By this method, we can construct a synonym thesaurus to reduce input dimension. In the hierarchical neural network classification unit, we adopt the well-known back-propagation learning model to build some proper hierarchical classification units. In our experiments, a product description database from an electronic commercial company is employed. The experimental results show that this classifier achieves sufficient accuracy to help human classification. It can save much manpower and working time for classifying a large database."
      }
    ]
  },
  {
    "tid":"1117",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Analysis Methods of Multi-state Systems Partially Having Dependent Components Using Multiple-Valued Decision Diagrams' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In a large system, such as a water, gas, or electrical distribution system, degraded performance due to failures of components can be modeled as a set of discrete states interconnected by edges with weights that represent conditional probabilities. To establish such a model, we compute the conditional probabilities with multi-valued decision diagrams (MDDs). Since a typical decision diagram is large, the computation time is also large. In this paper, we propose an edge-valued MDD (EVMDD) based method to avoid unnecessary path traversals. The proposed method is a hybrid method of a path traversal method and a bottom-up method that visits each node only once. By effectively combining both methods, we achieve a speed-up of the analysis by about 2:3 times for large systems compared to an existing method."
      }
    ]
  },
  {
    "tid":"1118",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Improving Retrieval Performance for Verbose Queries via Axiomatic Analysis of Term Discrimination Heuristic' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Number of terms in a query is a query-specific constant that is typically ignored in retrieval functions. However, previous studies have shown that the performance of retrieval models varies for different query lengths, and it usually degrades when query length increases. A possible reason for this issue can be the extraneous terms in longer queries that makes it a challenge for the retrieval models to distinguish between the key and complementary concepts of the query. As a signal to understand the importance of a term, inverse document frequency (IDF) can be used to discriminate query terms. In this paper, we propose a constraint to model the interaction between query length and IDF. Our theoretical analysis shows that current state-of-the-art retrieval models, such as BM25, do not satisfy the proposed constraint. We further analyze the BM25 model and suggest a modification to adapt BM25 so that it adheres to the new constraint. Our experiments on three TREC collections demonstrate that the proposed modification outperforms the baselines, especially for verbose queries."
      }
    ]
  },
  {
    "tid":"1119",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient and Optimistic Fair Exchanges Based on Standard RSA with Provable Security' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we introduce a new and natural paradigm for fair exchange protocols, called verifiable probabilis- tic signature scheme. A security model with precise and formal definitions is presented, and an RSA-based efficient and provably secure verifiable probabilistic signature scheme is proposed. Our scheme works well with standard RSA signature schemes, and the proposed optimistic fair exchange protocol is much concise and efficient, and suitable for practical applications."
      }
    ]
  },
  {
    "tid":"1120",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Scalable parallel OPTICS data clustering using graph algorithmic techniques' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"OPTICS is a hierarchical density-based data clustering algorithm that discovers arbitrary-shaped clusters and eliminates noise using adjustable reachability distance thresholds. Parallelizing OPTICS is considered challenging as the algorithm exhibits a strongly sequential data access order. We present a scalable parallel OPTICS algorithm (Poptics) designed using graph algorithmic concepts. To break the data access sequentiality, POPTICS exploits the similarities between the OPTICS algorithm and Prim's Minimum Spanning Tree algorithm. Additionally, we use the disjoint-set data structure to achieve a high parallelism for distributed cluster extraction. Using high dimensional datasets containing up to a billion floating point numbers, we show scalable speedups of up to 27.5 for our OpenMP implementation on a 40-core shared-memory machine, and up to 3,008 for our MPI implementation on a 4,096-core distributed-memory machine. We also show that the quality of the results given by POPTICS is comparable to those given by the classical OPTICS algorithm."
      }
    ]
  },
  {
    "tid":"1121",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Selectivity estimation for string predicates: overcoming the underestimation problem' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Queries with (equality or LIKE) selection predicatesover string attributes are widely used in relationaldatabases. However, state-of-the-art techniques forestimating selectivities of string predicates are often biasedtowards severely underestimating selectivities. In thispaper, we develop accurate selectivity estimators for stringpredicates that adapt to data and query characteristics,and which can exploit and build on a variety of existingestimators. A thorough experimental evaluation over realdata sets demonstrates the resilience of our estimators tovariations in both data and query characteristics."
      }
    ]
  },
  {
    "tid":"1122",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A new class of distributed optimization algorithms: application to regression of distributed data.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In a distributed optimization problem, the complete problem information is not available at a single location but is rather distributed among different agents in a multi-agent system. In the problems studied in the literature, each agent has an objective function and the network goal is to minimize the sum of the agents' objective functions over a constraint set that is globally known. In this paper, we study a generalization of the above distributed optimization problem. In particular, the network objective is to minimize a function of the sum of the individual objective functions over the constraint set. The 'outer' function and the constraint set are known to all the agents. We discuss an algorithm and prove its convergence, and then discuss extensions to more general and complex distributed optimization problems. We provide a motivation for our algorithms through the example of distributed regression of distributed data."
      }
    ]
  },
  {
    "tid":"1123",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Leveraging a Compound Graph-Based DHT for Multi-Attribute Range Queries with Performance Analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Resource discovery is critical to the usability and accessibility of grid computing systems. Distributed Hash Table (DHT) has been applied to grid systems as a distributed mechanism for providing scalable range-query and multi-attribute resource discovery. Multi-DHT-based approaches depend on multiple DHT networks with each network responsible for a single attribute. Single-DHT-based approaches keep the resource information of all attributes in a single node. Both classes of approaches lead to high overhead. In this paper, we propose a Low-Overhead Range-query Multi-attribute (LORM) DHT-based resource discovery approach. Unlike other DHT-based approaches, LORM relies on a single compound graph-based DHT network and distributes resource information among nodes in balance by taking advantage of the compound graph structure. Moreover, it has high capability to handle the large-scale and dynamic characteristics of resources in grids. Experimental results demonstrate the efficiency of LORM in comparison with other resource discovery approaches. LORM dramatically reduces maintenance and resource discovery overhead. In addition, it yields significant improvements in resource location efficiency. We also analyze the performance of the LORM approach rigorously by comparing it with other multi-DHT-based and single-DHT-based approaches with respect to their overhead and efficiency. The analytical results are consistent with experimental results, and prove the superiority of the LORM approach in theory."
      }
    ]
  },
  {
    "tid":"1124",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Adaptive Content-Based Tile Partitioning Algorithm For The Hevc Standard' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes a content-based Tile partitioning algorithm designed to exploit video properties in the tiling process, aiming at the reduction of the coding losses generated by the use of Tiles. In the proposed algorithm two steps are performed to define the vertical and horizontal Tile boundaries willing to group the high correlated samples into the same Tile partition. The definition of the Tiles' boundaries is performed by analyzing the variance map extracted from the raw picture information. Experimental results have shown that the proposed algorithm is able to reduce the inherent coding efficiency losses of using Tiles when compared to the conventional uniform spaced Tile partitions."
      }
    ]
  },
  {
    "tid":"1125",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Text Cube: Computing IR Measures for Multidimensional Text Database Analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Since Jim Gray introduced the concept of \u201ddata cube\u201d in 1997, data cube, associated with online analytical processing (OLAP), has become a driving engine in data warehouse industry. Because the boom of Internet has given rise to an ever increasing amount of text data associated with other multidimensional information, it is natural to propose a data cube model that integrates the power of traditional OLAP and IR techniques for text. In this paper, we propose a Text-Cube model on multidimensional text database and study effective OLAP over such data. Two kinds of hierarchies are distinguishable inside: dimensional hierarchy and term hierarchy. By incorporating these hierarchies, we conduct systematic studies on efficient text-cube implementation, OLAP execution and query processing. Our performance study shows the high promise of our methods."
      }
    ]
  },
  {
    "tid":"1126",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Grain segmentation of 3d superalloy images using multichannel EWCVT under human annotation constraints' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Grain segmentation on 3D superalloy images provides superalloy's micro-structures, based on which many physical and mechanical properties can be evaluated. This is a challenging problem in senses of (1) the number of grains in a superalloy sample could be thousands or even more; (2) the intensity within a grain may not be homogeneous; and (3) superalloy images usually contains carbides and noises. Recently, the Multichannel Edge-Weighted Centroid Voronoi Tessellation (MCEWCVT) algorithm [1] was developed for grain segmentation and showed better performance than many widely used image segmentation algorithms. However, as a general-purpose clustering algorithm, MCEWCVT does not consider possible prior knowledge from material scientists in the process of grain segmentation. In this paper, we address this issue by defining an energy minimization problem which subject to certain constraints. Then we develop a Constrained Multichannel Edge-Weighted Centroid Voronoi Tessellation (CMEWCVT) algorithm to effectively solve this constrained minimization problem. In particular, manually annotated segmentation on a very small set of 2D slices are taken as constraints and incorporated into the whole clustering process. Experimental results demonstrate that the proposed CMEWCVT algorithm significantly improve the previous grain-segmentation performance."
      }
    ]
  },
  {
    "tid":"1127",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Reconfigurable three dimensional photovoltaic panel architecture for solar-powered time extension' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Photovoltaic (PV) power generation systems are usually accompanied by battery to bridge the gap between the generation and load demand. Solar tracking is also used to enhance the power stability and increase the amount of collected energy from the Sun. However, battery and tracking devices significantly increase the system cost, and they are subject to wear and tear, which makes maintenance-free installation challenging. In this work, we conduct the design optimization of a twofold three dimensional PV panel for solar-powered systems. With the proposed three dimensional arrangement, we extend the solar-powered time of the target application that is powered only with solar power. Experimental results show that the proposed architecture and control method extend the service time of the target system by up to 23% compared to a non-reconfigurable flat panel with the same PV panel area."
      }
    ]
  },
  {
    "tid":"1128",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Bangla and English City Name Recognition for Indian Postal Automation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Because of multi-lingual behavior destination address block of a postal document of an Indian state may be written in two or more scripts. From a statistical analysis of Indian postal document we noted that about 22.04% of Indian postal documents are written in two scripts. Because of inter-mixing of these scripts in postal address writings, it is very difficult to identify the script by which a city name is written. To avoid such identification difficulties, in this paper we proposed a lexicon-driven bi-lingual (English and Bangla) city name recognition scheme for Indian postal automation. We obtained 93.19% accuracy when tested on 11875 city name samples."
      }
    ]
  },
  {
    "tid":"1129",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Vehicle Trajectory Estimation Based on Monocular Vision' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes a system to estimate the 3D position and velocity of vehicles, from images acquired with a monocular camera. Given image regions where vehicles are detected, Gaussian distributions are estimated detailing the most probable 3D road regions where vehicles lay. This is done by combining an assumed image formation model with the Unscented Transform mechanism. These distributions are then fed into a Multiple Hypothesis Tracking algorithm, which constructs trajectories coherent with an assumed model of dynamics. This algorithm not only characterizes the dynamics of detected vehicles, but also discards false detections, as they do not find spatio-temporal support. The proposals is tested in synthetic sequences, evaluating how noisy observations and miss-detections affect the accuracy of recovered trajectories."
      }
    ]
  },
  {
    "tid":"1130",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Learn while you earn: two approaches to learning auction parameters in take-it-or-leave-it auctions' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Much of the research in auction theory assumes that the auctioneer knows the distribution of participants' valuations with complete certainty. However, this is unrealistic. Thus, we analyse cases in which the auctioneer is uncertain about the valuation distributions; specifically, we consider a repeated auction setting in which the auctioneer can learn these distributions. Using take-it-or-leave-it auctions (Sandholm and Gilpin, 2006) as an exemplar auction format, we consider two auction design criteria. Firstly, an auctioneer could maximise expected revenue each time the auction is held. Secondly, an auctioneer could maximise the information gained in earlier auctions (as measured by the Kullback-Liebler divergence between its posterior and prior) to develop good estimates of the unknowns, which are later exploited to improve the revenue earned in the long-run. Simulation results comparing the two criteria indicate that setting offers to maximise revenue does not significantly detract from learning performance, but optimising offers for information gain substantially reduces expected revenue while not producing significantly better parameter estimates."
      }
    ]
  },
  {
    "tid":"1131",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Evaluating modeling techniques based on models of learning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"To compare modeling techniques, combine grammar-based and cognitive-based approaches and test domain understanding."
      }
    ]
  },
  {
    "tid":"1132",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Analysis of firewall policy rules using traffic mining techniques' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The firewall is usually the first line of defence in ensuring network security. However, the management of manually configured firewall rules has proven to be complex, error-prone and costly for large networks. Even with error-free rules, presence of defects in the firewall implementation or device may make the network insecure. Evaluation of effectiveness of policy and correctness of implementation requires a thorough analysis of network traffic data. We present a set of algorithms that simplify this analysis. By analysing only the firewall log files using aggregation and heuristics, we regenerate the effective firewall rules, i.e., what the firewall is really doing. By comparing these with the original rules, we can easily find if there is any anomaly in the original rules, and if there is any defect in the implementation. Our experiments show that the effective firewall rules can be regenerated to a high degree of accuracy from a small amount of data."
      }
    ]
  },
  {
    "tid":"1133",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A secure routing scheme based on social network analysis in wireless mesh networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"As an extension of wireless ad hoc and sensor networks, wireless mesh networks (WMNs) are employed as an emerging key solution for wireless broadband connectivity improvement. Due to the lack of physical security guarantees, WMNs are susceptible to various kinds of attack. In this paper, we focus on node social selfish attack, which decreases network performance significantly. Since this type of attack is not obvious to detect, we propose a security routing scheme based on social network and reputation evaluation to solve this attack issue. First, we present a dynamic reputation model to evaluate a nodeu0027s routing behavior, from which we can identify selfish attacks and selfish nodes. Furthermore, a social characteristic evaluation model is studied to evaluate the social relationship among nodes. Groups are built based on the similarity of node social status and we can get a secure routing based on these social groups of nodes. In addition, in our scheme, nodes are encouraged to enter into multiple groups and friend nodes are recommended to join into groups to reduce the possibility of isolated nodes. Simulation results demonstrate that our scheme is able to reflect node security status, and routings are chosen and adjusted according to security status timely and accurately so that the safety and reliability of routing are improved."
      }
    ]
  },
  {
    "tid":"1134",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The ViP2P Platform: XML Views in P2P' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"  The growing volumes of XML data sources on the Web or produced by enterprises, organizations etc. raise many performance challenges for data management applications. In this work, we are concerned with the distributed, peer-to-peer management of large corpora of XML documents, based on distributed hash table (or DHT, in short) overlay networks. We present ViP2P (standing for Views in Peer-to-Peer), a distributed platform for sharing XML documents based on a structured P2P network infrastructure (DHT). At the core of ViP2P stand distributed materialized XML views, defined by arbitrary XML queries, filled in with data published anywhere in the network, and exploited to efficiently answer queries issued by any network peer. ViP2P allows user queries to be evaluated over XML documents published by peers in two modes. First, a long-running subscription mode, when a query can be registered in the system and receive answers incrementally when and if published data matches the query. Second, queries can also be asked in an ad-hoc, snapshot mode, where results are required immediately and must be computed based on the results of other long-running, subscription queries. ViP2P innovates over other similar DHT-based XML sharing platforms by using a very expressive structured XML query language. This expressivity leads to a very flexible distribution of XML content in the ViP2P network, and to efficient snapshot query execution. ViP2P has been tested in real deployments of hundreds of computers. We present the platform architecture, its internal algorithms, and demonstrate its efficiency and scalability through a set of experiments. Our experimental results outgrow by orders of magnitude similar competitor systems in terms of data volumes, network size and data dissemination throughput. "
      }
    ]
  },
  {
    "tid":"1135",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Task Swapping for Schedule Improvement: A Broader Analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we analyze and extend a recently developed \"task-swapping procedure\" for improving schedules in over- subscribed situations. In such situations, there are tasks which cannot be directly added to the current schedule with- out introducing capacity conflicts. A schedule is improved if one or more of these tasks can be feasibly included, and the goal of task swapping is to rearrange some portion of the current schedule to make this possible. Key to effective task swapping is an ability to exploit the scheduling flexibility in- herent in the constraints associated with various scheduled tasks, and previous work has shown that the use of retraction heuristics that favor tasks with greater rescheduling flexibil- ity can give rise to strong schedule improvement capabilities. We extend this work by developing and evaluating several improvements to the core task swapping procedure. We in- troduce three pruning techniques and show that each signif- icantly improves computational efficiency while maintaining solution quality. We then investigate the possibility of im- proving the \"end\" solutions by stochastically exploring the \"neighborhood\" around them, and demonstrate that improved solutions are possible given the ability to spend additional time."
      }
    ]
  },
  {
    "tid":"1136",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Topology-based on-board data dissemination approach for sensor network' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In a previous paper, we introduced a hybrid scalable approach for data-gathering and dissemination in sensor networks we called On Board Data Dissemination OBDD [6]. The approach was validated and implemented on well-structured network topologies where virtual boards carry the queries are constructed guided by well-defined trajectories. In this paper we extend our previous work to include networks with irregular topologies. In such networks boards guided by well-defined trajectories are no longer enough to construct the dissemination structure. We have adapted and modified an underlying topology detection algorithm to fit our protocol. The resulting topology-based on-board data dissemination TOBDD efficiently works with any network topology. It also maintains the desirable features of OBDD: although the approach adapts both pull and push strategies, it synchronizes both phases to maintain the communications mainly on-demand and to prohibit problems that could be inherited from the push strategy. Both analysis and simulation show that TOBDD promises an efficient and scalable paradigm for data-gathering and dissemination in sensor networks that reduces the communication cost and leads to load balancing."
      }
    ]
  },
  {
    "tid":"1137",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Design and Experimental Evaluation of Distributed Heterogeneous Graph-Processing Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Graph processing is increasingly used in a variety of domains, from engineering to logistics and from scientific computing to online gaming. To process graphs efficiently, GPU-enabled graph-processing systems such as TOTEM and Medusa exploit the GPU or the combined CPU+GPU capabilities of a single machine. Unlike scalable distributed CPU-based systems such as Pregel and GraphX, existing GPU-enabled systems are restricted to the resources of a single machine, including the limited amount of GPU memory, and thus cannot analyze the increasingly large-scale graphs we see in practice. To address this problem, we design and implement three families of distributed heterogeneous graph-processing systems that can use both the CPUs and GPUs of multiple machines. We further focus on graph partitioning, for which we compare existing graph-partitioning policies and a new policy specifically targeted at heterogeneity. We implement all our distributed heterogeneous systems based on the programming model of the single-machine TOTEM, to which we add (1) a new communication layer for CPUs and GPUs across multiple machines to support distributed graphs, and (2) a workload partitioning method that uses offline profiling to distribute the work on the CPUs and the GPUs. We conduct a comprehensive real-world performance evaluation for all three families. To ensure representative results, we select 3 typical algorithms and 5 datasets with different characteristics. Our results include algorithm run time, performance breakdown, scalability, graph partitioning time, and comparison with other graph-processing systems. They demonstrate the feasibility of distributed heterogeneous graph processing and show evidence of the high performance that can be achieved by combining CPUs and GPUs in a distributed environment."
      }
    ]
  },
  {
    "tid":"1138",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'How did university departments interweave the Web: A study of connectivity and underlying factors' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents two studies of the use of the WWW in Scottish universities and American land-grant universities. First, we investigated the relationship between the organisational profile of a university department in Scotland and its structural connectivity on the WWW. A Spearman rank order correlation analysis revealed a number of strong correlation relationships between structural connectiv..."
      }
    ]
  },
  {
    "tid":"1139",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Betterrelations: using a game to rate linked data triples' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"While associations between concepts in our memory have different strengths, explicit strengths of links (edge weights) are missing in Linked Data. In order to build a collection of such edge weights, we created a web-game prototype that ranks triples by importance. In this paper we briefly describe the game, Linked Data preprocessing aspects, and the promising results of an evaluation of the game."
      }
    ]
  },
  {
    "tid":"1140",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mapit: A Case Study For Location Driven Knowledge Discovery And Mining' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In the present world scenario, everybody is on the lookout for suitable housing options, each having different needs (e.g., the elderly are looking for safe, quiet neighbourhood, while students are looking for affordable apartments close to the university/school). For e.g., Craigslist currently does not have a map version, making the process of apartment searching a very long and laborious process. This creates a need for software that is significantly superior to current web search tools. We demonstrate the development of a tool which takes the Craigslist apartment listings on Google Maps. MapIt then integrates this functionality with the information collected from location based extraction of various web sources such as the city police blotter which makes apartment searching simpler and faster, helping the user to make a better decision. The paper also discusses the challenges that are faced in the development process, the raw and unstructured nature of the documents, the existence of geo/non-geo and geo-geo disambiguities and our approach in identifying the location of the apartment from informal text (geo-parsing and geo-tagging of content) to ensure maximum coverage of the listings."
      }
    ]
  },
  {
    "tid":"1141",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Self-Timed Charge Recycling Search-Line Drivers In Content-Addressable Memories' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes a novel scheme of self-timed charge recycling search-line (SL) drivers for content-addressable memories. In the conventional charge recycling SL driving scheme [11], an additional clock needs to be generated from the system clock with stringent requirements for phase and pulse width to control the charge sharing course. In contrast, the proposed scheme can self-sense the end of the charge sharing. Besides, by entering the high-resistant driving state in advance, the proposed scheme reduces the delay overhead of the control logic from 8 to 6 gates delay. Simulation is carried out in a 0.35-mu m 3.3-V CMOS process, and the results show the proposed SL driver with load of 512 bits TCAM cells achieves 31.6 fJ/bit/search energy index and 1.19 ns delay time. The achievement illustrates 27.4% energy reduction over buffered driver and 22.2% delay time reduction over the conventional charge recycling SL driver."
      }
    ]
  },
  {
    "tid":"1142",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Non-negative multilinear principal component analysis of auditory temporal modulations for music genre classification' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Motivated by psychophysiological investigations on the human auditory system, a bio-inspired two-dimensional auditory representation of music signals is exploited, that captures the slow temporal modulations. Although each recording is represented by a second-order tensor (i.e., a matrix), a third-order tensor is needed to represent a music corpus. Non-negative multilinear principal component analysis (NMPCA) is proposed for the unsupervised dimensionality reduction of the third-order tensors. The NMPCA maximizes the total tensor scatter while preserving the non-negativity of auditory representations. An algorithm for NMPCA is derived by exploiting the structure of the Grassmann manifold. The NMPCA is compared against three multilinear subspace analysis techniques, namely the non-negative tensor factorization, the high-order singular value decomposition, and the multilinear principal component analysis as well as their linear counterparts, i.e., the non-negative matrix factorization, the singular value decomposition, and the principal components analysis in extracting features that are subsequently classified by either support vector machine or nearest neighbor classifiers. Three different sets of experiments conducted on the GTZAN and the ISMIR2004 Genre datasets demonstrate the superiority of NMPCA against the aforementioned subspace analysis techniques in extracting more discriminating features, especially when the training set has small cardinality. The best classification accuracies reported in the paper exceed those obtained by the state-of-the-art music genre classification algorithms applied to both datasets."
      }
    ]
  },
  {
    "tid":"1143",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An energy-efficient sensor routing scheme for home automation networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In the emerging ubiquitous home, sensors are placed everywhere in the house and collect various physical data such as temperature, humidity, and light to provide information to consumer electronics devices. The devices are then automatically activated if necessary. For example, the ventilator works when the air is foul and the heating system performs according to the weather and the existence of people in the house. Because sensors have limited battery power, energy-efficient routing is important. In this paper, we present a new sensor routing scheme that provides energy-efficient data delivery from sensors to the home base station. The proposed scheme divides the home area into sectors and locates a manager node to each sector. The manager node receives collected data from sensors and delivers the data to the base station through the shortest path of the 2-dimensional (x, y) coordinates. Performance results show that the proposed scheme reduces energy consumption significantly compared with conventional sensor routing schemes."
      }
    ]
  },
  {
    "tid":"1144",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Gateway discovery algorithm based on multiple QoS path parameters between mobile node and gateway node.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Several gateway selection schemes have been proposed that select gateway nodes based on a single Quality of Service (QoS) path parameter, for instance path availability period, link capacity or end-to-end delay, etc. or on multiple non-QoS parameters, for instance the combination of gateway node speed, residual energy, and number of hops, for Mobile Ad hoc NETworks (MANETs). Each scheme just focuses on the ment of improve only a single network performance, i.e., network throughput, packet delivery ratio, end-to-end delay, or packet drop ratio. However, none of these schemes improves the overall network performance because they focus on a single QoS path parameter or on set of non-QoS parameters. To improve the overall network performance, it is necessary to select a gateway with stable path, a path with the maximum residual load capacity and the minimum latency. In this paper, we propose a gateway selection scheme that considers multiple QoS path parameters such as path availability period, available capacity and latency, to select a potential gateway node. We improve the path availability computation accuracy, we introduce a feedback system to updated path dynamics to the traffic source node and we propose an efficient method to propagate QoS parameters in our scheme. Computer simulations show that our gateway selection scheme improves throughput and packet delivery ratio with less per node energy consumption. It also improves the end-to-end delay compared to single QoS path parameter gateway selection schemes. In addition, we simulate the proposed scheme by considering weighting factors to gateway selection parameters and results show that the weighting factors improve the throughput and end-to-end delay compared to the conventional schemes."
      }
    ]
  },
  {
    "tid":"1145",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Routing with off-network control: a novel paradigm for scalable design in very large sensor networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a novel architectural solution to address the problem of scalable routing in very large sensor networks. The control complexities of the existing sensor routing protocols, both flat and with traditional hierarchy, do not scale very well for large networks with potentially hundreds of thousands of embedded sensor devices. This paper develops a novel routing solution Off-Network Control Processing (ONCP) that achieves control scalability in large sensor networks by shifting certain amount of routing functions \"off-network\". This routing approach, consisting of \"coarse grain\" global routing, and distributed \"fine grain\" local routing is proposed for achieving scalability by avoiding network-wide control message dissemination. We present the ONCP architectural concepts and analytically characterize its performance in relation to both flat and traditional hierarchical sensor routing architectures. We also present ns2-based experimental results which indicate that for very large networks, the packet drop, latency and energy performance of ONCP can be significantly better than those for flat sensor routing protocols such as Directed Diffusion and cluster-based traditional hierarchical protocols such as CBRP."
      }
    ]
  },
  {
    "tid":"1146",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Joint Multi-Field Siamese Recurrent Neural Network For Entity Resolution' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Entity resolution which deals with determining whether two records refer to the same entity has a wide range of applications in both data cleaning and integration. Traditional approaches focus on using string metrics to calculate the matching scores of recorded pairs or employing the machine learning technique with hand-crafted features. However, the effectiveness of these methods largely depends on designing good domain-specific metric methods or extracting discriminative features with rich domain knowledge. Also, traditional learning-based methods usually ignore the discrepancy between citation's fields. In this paper, to decrease the impact of information gaps between different fields and fully take advantage of semantical and contextual information in each field, we present a novel joint multi-field siamese recurrent architecture. In particular, our method employs word-based Long Short-Term Memory (LSTM) for the fields with the strong relevance between each word and character-based Recurrent Neural Network (RNN) for the fields with the weak relevance between each word, which can exploit each field's temporal information effectively. Experimental results on three datasets demonstrate that our model can learn discriminative features and outperforms several baseline methods and other RNN-based methods."
      }
    ]
  },
  {
    "tid":"1147",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Novel Prototype Reduction Method for the K-Nearest Neighbor Algorithm with K >= 1' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a novel prototype reduction algorithm is proposed, which aims at reducing the storage requirement and enhancing the online speed while retaining the same level of accuracy for a K-nearest neighbor (KNN) classifier. To achieve this goal, our proposed algorithm learns the weighted similarity function for a KNN classifier by maximizing the leave-one-out cross-validation accuracy. Unlike the classical methods PW, LPD and WDNN which can only work with K = 1 our developed algorithm can work with K >= 1. This flexibility allows our lean it algorithm to have superior classification accuracy and noise robustness. The proposed approach is assessed through experiments with twenty real world benchmark data sets. In all these experiments, the proposed approach shows it can dramatically reduce the storage requirement and online time for KNN while having equal or better accuracy than KNN, and it also shows comparable results to several prototype reduction methods proposed in literature."
      }
    ]
  },
  {
    "tid":"1148",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fast segment-wise DC coding for 3D video compression' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A Fast Segment-wise DC Coding (SDC) algorithm is proposed for 3D-HEVC depth intra coding. Since SDC is a supplementary option for intra prediction modes, it is not necessary to check SDC option for all intra candidate modes during full Rate-Distortion (RD) cost calculation. Based on the observation that most Prediction Units (PUs) choose the same intra coding mode with and without SDC option, we propose a fast SDC coding algorithm to avoid unnecessary full-RD cost calculation. The performance evaluation is based on HTM-11.0 and the experimental result for the proposed fast algorithm shows up to 25% encoding time saving with only 0.19% BD-bitrate increase in coded and synthesized view for All-Intra test case."
      }
    ]
  },
  {
    "tid":"1149",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Generic Subclass Ensemble: A Novel Approach to Ensemble Classification' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Multiple classifier systems, also known as classifier ensembles, have received great attention in recent years because of their improved classification accuracy in different applications. In this paper, we propose a new general approach to ensemble classification, named generic subclass ensemble, in which each base classifier is trained with data belonging to a subset of classes, and thus discriminates among a subset of target categories. The ensemble classifiers are then fused using a combination rule. The proposed approach differs from existing methods that manipulate the target attribute, since in our approach individual classification problems are not restricted to two-class problems. We perform a series of experiments to evaluate the efficiency of the generic subclass approach on a set of benchmark datasets. Experimental results with multilayer perceptrons show that the proposed approach presents a viable alternative to the most commonly used ensemble classification approaches."
      }
    ]
  },
  {
    "tid":"1150",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A forward-backward voice packet loss concealment algorithm for multimedia over IP network services' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a voice packet loss concealment algorithm in order to improve voice quality for both multimedia over IP and voice over IP services. The proposed algorithm estimates the coding parameters of lost frames by combining forward and backward prediction from the good frames before and after the lost frames. The performance of the proposed algorithm is evaluated on the ITU-T G.729 coder, and it is compared with the performance of the conventional algorithms in terms of objective and subjective quality measures. From the PESQ score comparison and the listening test, it is found that the proposed algorithm provides better voice quality than the conventional ones."
      }
    ]
  },
  {
    "tid":"1151",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'PolarScheduler: Dynamic Transmission Control for Floating LoRa Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"LoRa is widely deploying in aquatic environments to support various Internet of Things applications. However, floating LoRa networks suffer from serious performance degradation due to the polarization loss caused by the swaying antenna. Existing methods that only control the transmission starting from the aligned attitude have limited improvement due to the ignorance of aligned period length. In this paper, we propose PolarScheduler, a dynamic transmission control method for floating LoRa networks. PolarScheduler actively controls transmission configurations to match polarization aligned periods. We propose a V-zone model to capture diverse aligned periods under different configurations. We also design a low-cost model establishment method and an efficient optimal configuration searching algorithm to make full use of aligned periods. We implement PolarScheduler on commercial LoRa platforms and evaluate its performance in a deployed network. Extensive experiments show that PolarScheduler can improve the packet delivery rate and throughput by up to 20.0% and 15.7%, compared to the state-of-the-art method."
      }
    ]
  },
  {
    "tid":"1152",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mining from open answers in questionnaire data' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Surveys are an important part of marketing and customer relationship management, and open answers (i.e., answers to open questions) in particular may contain valuable information and provide an important basis for making business decisions. We have developed a text mining system that provides a new way for analyzing open answers in questionnaire data. The product is able to perform the following two functions: (A) accurate extraction of characteristics for individual analysis targets, (B) accurate extraction of the relationships among characteristics of analysis targets. In this paper, we describe the working of our text mining system. It employs two statistical learning techniques: rule analysis and Correspondence Analysis for performing the two functions. Our text mining system has already been put into use by a number of large corporations in Japan in the performance of text mining on various types of survey data, including open answers about brand images, open answers about company images, complaints about products, comments written on home pages, business reports, and help desk records. In this it has been found to be useful in forming a basis for effective business decisions."
      }
    ]
  },
  {
    "tid":"1153",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'TABOR: A Graphical Model-based Approach for Anomaly Detection in Industrial Control Systems.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Industrial Control Systems (ICS) such as water and power are critical to any society. Process anomaly detection mechanisms have been proposed to protect such systems to minimize the risk of damage or loss of resources. In this paper, a graphical model-based approach is proposed for profiling normal operational behavior of an operational ICS referred to as SWaT (Secure Water Treatment). Timed automata are learned as a model of regular behaviors shown in sensors signal like fluctuations of water level in tanks. Bayesian networks are learned to discover dependencies between sensors and actuators. The models are used as a one-class classifier for process anomaly detection, recognizing irregular behavioral patterns and dependencies. The detection results can be interpreted and the abnormal sensors or actuators localized due to the interpretability of the graphical models. This approach is applied to a dataset collected from SWaT. Experimental results demonstrate the model's superior performance on both precision and run-time over methods including support vector machine and deep neural networks. The underlying idea is generic and applicable to other industrial control systems such as power and transportation.\n\n"
      }
    ]
  },
  {
    "tid":"1154",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'High performance reconfigurable architecture for double precision floating point division' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Floating point arithmetic (FPA) are very crucial and critical domain for the hardware acceleration. FPA are widely used in the vast field of application. The division operation of the FPA is a very intensive operation, in terms of complexity, area requirement and performance speed. This paper presents an efficient FPGA implementation of double-precision FPA divisions on Virtex-2pro FPGA platform, for the ease of comparing with prior works. The proposed method is based on the method of binomial expansion, which uses look-up tables and partial block multipliers (PBM). Compared with previously reported work, the proposed design occupies smaller area (in terms of number slices, number of multipliers and the BRAM usage) with a higher performance gain and less latency. By using over 5 million unique random test cases, our results show that the proposed design gives an average error of less than 0.5 ULP (unit at last place), and a maximum error of 2 ULP without using any rounding scheme. However, rounding can also be added to the design to restore some accuracy at a slight cost in area."
      }
    ]
  },
  {
    "tid":"1155",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Moving big data to the cloud' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cloud computing, rapidly emerging as a new computation paradigm, provides agile and scalable resource access in a utility-like fashion, especially for the processing of big data. An important open issue here is how to efficiently move the data, from different geographical locations over time, into a cloud for effective processing. The de facto approach of hard drive shipping is not flexible, nor secure. This work studies timely, cost-minimizing upload of massive, dynamically-generated, geodispersed data into the cloud, for processing using a MapReducelike framework. Targeting at a cloud encompassing disparate data centers, we model a cost-minimizing data migration problem, and propose two online algorithms, for optimizing at any given time the choice of the data center for data aggregation and processing, as well as the routes for transmitting data there. The first is an online lazy migration (OLM) algorithm achieving a competitive ratio of as low as 2.55, under typical system settings. The second is a randomized fixed horizon control (RFHC) algorithm achieving a competitive ratio of 1+ 1/l+\u03bb \u03ba/\u03bb with a lookahead window of l, where \u03ba and \u03bb are system parameters of similar magnitude."
      }
    ]
  },
  {
    "tid":"1156",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Structural optimization by genetic algorithm with degeneration (GAd)' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Genetic Algorithms (GAs) are models of biological evolution. But the concept of degeneration had not previously been introduced to GA. Degeneration is the evolutionary decline or loss of a function or characteristic, and it had not been thought that degeneration could be used for optimization. In this paper, GA with degeneration (GAd), which introduces the concept of degeneration to GA, is proposed. It is assumed that damaged genes and irreversible mutations cause degeneration and that if degeneration occurs, some characteristics of an individual are lost. If the characteristics are regarded as parameters in a model, the number of parameters can be reduced and the optimal parameter structure can be discovered. It is shown that the optimal parameter structure of some models such as neural networks can be discovered by GAd. \u00a9 2004 Wiley Periodicals, Inc. Syst Comp Jpn, 35(5): 32\u201343, 2004; Published online in Wiley InterScience (). DOI 10.1002&sol;scj.10552"
      }
    ]
  },
  {
    "tid":"1157",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Path quality aware fair load distribution for data aggregation in unreliable wireless sensor networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose a sophisticated load distribution scheme using linear programming that considers the path quality from each node to a base station in unreliable wireless sensor networks. The fair load distribution in WSNs is very critical for extending the lifetime of the network. Many load distribution methods by using linear programming have been presented under noiseless channel condition, so the load for retransmissions is neglected. Although some link quality based load distribution methods reflect the load of retransmissions, they also have limitations to delay and energy consumption. Path quality based load distribution proposes a method to improve load balance by measuring the number of successful sends in traffic without retransmissions. We improve it up to about 64% when comparing the link quality based method, and up to 83% without even considering link quality method."
      }
    ]
  },
  {
    "tid":"1158",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Energy-efficient communication protocols' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Wireless networking has experienced a great deal of popularity, and significant advances have been made in wireless technology. However, energy efficiency of radio communication systems is still a critical issue due to the limited battery capacity of portable devices. In this paper, we deal with the charge recovery effect that takes place in electrochemical cells and show how we can take advantage of this mechanism to increase the energy delivered by a battery. Then, we present energy-aware traffic shaping techniques, as well as scheduling and routing algorithms, which exploit the battery recovery effect."
      }
    ]
  },
  {
    "tid":"1159",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'BioScout: a life-science query monitoring system' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Scientific data are available through an increasing number of heterogeneous, independently evolving, sources. Although the sources themselves are independently evolving, the data stored in them are not. There exist inherent and intricate relationships between the distributed data-sets and scientists are routinely required to write distributed queries in this setting. Being non-experts in computer science, the scientists are faced with two major challenges: (i) How to express such distributed queries. This is a non-trivial task, even if we assume that scientists are familiar with query languages like SQL. Such queries can get arbitrarily complex as more sources are considered; (ii) How to efficiently evaluate such distributed queries. An efficient evaluation must account for batches of hundreds (or even thousands) of submitted queries and must optimize all of them as a whole. In this demo, we focus on the biological domain for illustration purposes (our solutions are applicable to other scientific domains) and we present a system, called BioScout, that offers solutions in both of the above challenges. In more detail, we demonstrate the following functionality: (i) in BioScout, scientists draw their queries graphically, resulting in a query graph. The scientist is unaware of the query language used or of any optimization issues. Given the query graph, the system is able to generate, as a first step, an optimal query plan for the submitted query; (ii) BioScout uses four different strategies to combine the optimal query plans of individual queries to generate a global query plan for all the submitted queries. In the demo, we illustrate graphically how each of the four strategies works."
      }
    ]
  },
  {
    "tid":"1160",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'seBoost: Selective boosting for heterogeneous manycores' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Boosting techniques have been widely adopted in commercial multicore and manycore systems, mainly because they provide means to satisfy performance requirements surges, for one or more cores, at run-time. Current boosting techniques select the boosting levels (for boosted cores) and the throttle-down levels (for non-boosted cores) either arbitrarily or through step-wise control approaches. These methods might result in unnecessary performance losses for the non-boosted cores, in short boosting intervals, in failing to satisfy the required performance surges, or in unnecessary high power and energy consumption. This paper presents an efficient and lightweight run-time boosting technique based on transient temperature estimation, called seBoost. Our technique guarantees meeting the performance requirements surges at run-time, thus maximizing the boosting time with a minimum loss of performance for the non-boosted cores.\n\n"
      }
    ]
  },
  {
    "tid":"1161",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Efficient Ciphertext-Policy Attribute-Based Encryption Scheme Supporting Collaborative Decryption With Blockchain' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In the last few decades, ciphertext-policy attribute-based encryption (CP-ABE) technology has attracted great interest, since it can provide fine-grained, flexible, and access control for sensitive data to implement a high secure and efficient data-sharing mechanism. In this article, based on the linear secret sharing scheme (LSSS), an efficient scheme is proposed to realize a collaborative decryption function. For any user group, when the user\u2019s attribute set cannot access the ciphertext alone, the private key of other users in the same group can be used for collaborative decryption with the permission of the data owner. Our scheme uses the LSSS matrix that can significantly reduce the computation and storage overhead when comparing with the existing schemes. Then, a multiauthorization model is created based on the Bohen\u2013Lynn\u2013Shacham technology in order to solve the key-management issue. Finally, we implemented the specific functions of the framework through JAVA, and built a private chain to verify the feasibility of data transfer between users."
      }
    ]
  },
  {
    "tid":"1162",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Local L2-Thresholding Based Data Mining in Peer-to-Peer Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In a large network of computers, wireless sensors, or mobile devices, each of the components (hence, peers) has some data about the global status of the system. Many of the functions of the system, such as routing decisions, search strategies, data cleansing, and the assignment of mutual trust, depend on the global status. Therefore, it is essential that the system be able to detect, and react to, changes in its global status. Computing global predicates in such systems is usually very costly. Mainly because of their scale, and in some cases (e.g., sensor networks) also because of the high cost of communication. The cost further increases when the data changes rapidly (due to state changes, node failure, etc.) and computation has to follow these changes. In this paper we describe a two step approach for dealing with these costs. First, we describe a highly efficient local algorithm which detect when the L2 norm of the average data surpasses a threshold. Then, we use this algorithm as a feedback loop for the monitoring of complex predicates on the data such as the data's k-means clustering. The efficiency of the L2 algorithm guarantees that so long as the clustering results represent the data (i.e., the data is stationary) few resources are required. When the data undergoes an epoch change a change in the underlying distribution and the model no longer represents it, the feedback loop indicates this and the model is rebuilt. Furthermore, the existence of a feedback loop allows using approximate and \"best-effort\" methods for constructing the model; if an ill-fit model is built the feedback loop would indicate so, and the model would be rebuilt."
      }
    ]
  },
  {
    "tid":"1163",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Reordering of Location Identifiers for Indexing an RFID Tag Object Database' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The query performance for tracing tags depends upon the distribution of tag trajectories in the data space. We examine a more efficient representation of tag trajectories by means of ordering the set of domain values. Our analysis shows that the order of Location IDentifiers (LIDs) can give a great impact on the efficiency of query processing. To find out the optimal ordering of LIDs, we propose a new LID proximity functionto rearrange an arbitrary order of LIDs. This function enables logically adjacent tag trajectories, which are frequently accessed together, to be stored in close proximity on the disk. To determine the optimal sequence of LIDs in the domain, we also propose a reordering scheme of LIDs. The experimental results show that the proposed reordering scheme achieves better query processing than the previous methods of assigning LIDs."
      }
    ]
  },
  {
    "tid":"1164",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'CDNs Content Outsourcing via Generalized Communities' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Content Distribution Networks (CDNs) balance costs and quality in services related to content delivery. Devising an efficient content outsourcing policy is crucial since, based on such policies, CDN providers can provide client-tailored content, improve performance, and result in significant economical gains. Earlier content outsourcing approaches may often prove ineffective since they drive prefetching decisions by assuming knowledge of content popularity statistics, which are not always available and are extremely volatile. This work addresses this issue, by proposing a novel self-adaptive technique under a CDN framework on which outsourced content is identified with no a-priori knowledge of (earlier) request statistics. This is employed by using a structure-based approach identifying coherent clusters of \"correlated\" Web server content objects, the so-called Web page communities. These communities are the core outsourcing unit and in this paper a detailed simulation experimentation has shown that the proposed technique is robust and effective in reducing user-perceived latency as compared with competing approaches, i.e., two communities-based approaches, Web caching, and non-CDN."
      }
    ]
  },
  {
    "tid":"1165",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Research on classification method of high-dimensional class-imbalanced datasets based on SVM' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"High-dimensional problems result in bad classification results because some combinations of features have an adverse effect on classification; while class-imbalanced problems make the classifier to concern the majority class more but the minority less, because the number of samples of majority class is more than minority class. The problem of both high-dimensional and class-imbalanced classification is found in many fields such as bioinformatics, healthcare and so on. Many researchers study either the high-dimensional problem or class-imbalanced problem and come up with a series of algorithms, but they ignore the above new problem, which indicates high-dimensional problems affect sampling process while class-imbalanced problems interfere feature selection. Firstly, this paper analyses the new problem arising from the mutual influence of the two problems, and then introduces SVM and analyses its advantages in dealing high-dimensional problem and class-imbalanced problem. Next, this paper proposes a new algorithm named BRFE-PBKS-SVM aimed at high-dimensional class-imbalanced datasets, which improves SVM-RFE by considering the class-imbalanced problem in the process of feature selection, and it also improves SMOTE so that the procedure of over-sampling could work in the Hilbert space with an adaptive over-sampling rate by PSO. Finally, the experimental results show the performance of this algorithm."
      }
    ]
  },
  {
    "tid":"1166",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A novel rate control technique for asymmetric-quality stereoscopic video' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Three-dimensional (3D) video technology is becoming increasingly popular, as it can provide stereoscopic perception and immersive experience to end users. Some asymmetric stereoscopic video coding methods had been developed by utilizing binocular psycho-visual redundancy. However, rate control problem had not been well considered in these methods to further control the quality of stereoscopic video. In this paper, a novel rate control method is proposed for asymmetric-quality stereoscopic video. In order to model the asymmetric-quality stereoscopic video, we use a fixed threshold to quantize the binocular psycho-visual redundancy and establish the relationship between the distortion and quantization for the asymmetric-quality stereoscopic video. Then, a rate control method is engineered in stereoscopic video coding to control the rate and quality of left and right views. Experimental results show that the proposed method can accurately control the rate and quality of stereoscopic video, while having a lower computational complexity1."
      }
    ]
  },
  {
    "tid":"1167",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'SUSTain: Scalable Unsupervised Scoring for Tensors and its Application to Phenotyping.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a new method, which we call SUSTain, that extends real-valued matrix and tensor factorizations to data where values are integers. Such data are common when the values correspond to event counts or ordinal measures. The conventional approach is to treat integer data as real, and then apply real-valued factorizations. However, doing so fails to preserve important characteristics of the original data, thereby making it hard to interpret the results. Instead, our approach extracts factor values from integer datasets as scores that are constrained to take values from a small integer set. These scores are easy to interpret: a score of zero indicates no feature contribution and higher scores indicate distinct levels of feature importance. At its core, SUSTain relies on: a) a problem partitioning into integer-constrained subproblems, so that they can be optimally solved in an efficient manner; and b) organizing the order of the subproblems' solution, to promote reuse of shared intermediate results. We propose two variants, SUSTain_M and SUSTain_T, to handle both matrix and tensor inputs, respectively. We evaluate SUSTain against several state-of-the-art baselines on both synthetic and real Electronic Health Record (EHR) datasets. Comparing to those baselines, SUSTain shows either significantly better fit or orders of magnitude speedups that achieve a comparable fit (up to 425\u00d7 faster). We apply SUSTain to EHR datasets to extract patient phenotypes (i.e., clinically meaningful patient clusters). Furthermore, 87% of them were validated as clinically meaningful phenotypes related to heart failure by a cardiologist.\n\n"
      }
    ]
  },
  {
    "tid":"1168",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Time-aware Analysis and Ranking of Lurkers in Social Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mining the silent members of an online community, also called lurkers, has been recognized as an important problem that accompanies the extensive use of online social networks (OSNs). Existing solutions to the ranking of lurkers can aid understanding the lurking behaviors in an OSN. However, they are limited to use only structural properties of the static network graph, thus ignoring any relevant information concerning the time dimension. Our goal in this work is to push forward research in lurker mining in a twofold manner: (1) to provide an in-depth analysis of temporal aspects that aims to unveil the behavior of lurkers and their relations with other users, and (2) to enhance existing methods for ranking lurkers by integrating different time-aware properties concerning information production and information consumption actions. Network analysis and ranking evaluation performed on Flickr, FriendFeed and Instagram networks allowed us to draw interesting remarks on both the understanding of lurking dynamics and on transient and cumulative scenarios of time-aware ranking."
      }
    ]
  },
  {
    "tid":"1169",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Beyond Overlay: Reaping Mutual Benefits for Primary and Secondary Networks Through Node-Level Cooperation.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Existing spectrum sharing paradigms have set clear boundaries between the primary and secondary networks. There is either no or very limited node-level cooperation between the primary and secondary networks. In this paper, we develop a new and bold spectrum-sharing paradigm beyond the state of the art for future wireless networks. We explore network cooperation as a new dimension for spectrum sharing between the primary and secondary users. Such network cooperation can be defined as a set of policies under which different degrees of cooperation are to be achieved. The benefits of this paradigm are numerous, as they allow integrating resources from two networks. There are many possible node-level cooperation policies that one can employ under this paradigm. For the purpose of performance study, we consider a specific policy called United cooperation of Primary and Secondary (UPS) networks. UPS allows a complete cooperation between the primary and secondary networks at the node level to relay each other's traffic. As a case study, we consider a problem with the goal of supporting the rate requirement of the primary network traffic while maximizing the throughput of the secondary sessions. For this problem, we develop an optimization model and formulate a combinatorial optimization problem. We also develop an approximation solution based on a piece-wise linearization technique. Simulation results show that UPS offers significantly better throughput performance than that under the interweave paradigm."
      }
    ]
  },
  {
    "tid":"1170",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Finding influential communities in massive networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Community search is a problem of finding densely connected subgraphs that satisfy the query conditions in a network, which has attracted much attention in recent years. However, all the previous studies on community search do not consider the influence of a community. In this paper, we introduce a novel community model called -influential community based on the concept of -core to capture the influence of a community. Based on this community model, we propose a linear time online search algorithm to find the top- -influential communities in a network. To further speed up the influential community search algorithm, we devise a linear space data structure which supports efficient search of the top- -influential communities in optimal time. We also propose an efficient algorithm to maintain the data structure when the network is frequently updated. Additionally, we propose a novel I/O-efficient algorithm to find the top- -influential communities in a disk-resident graph under the assumption of , where  and  denote the size of the main memory and the number of nodes, respectively. Finally, we conduct extensive experiments on six real-world massive networks, and the results demonstrate the efficiency and effectiveness of the proposed methods."
      }
    ]
  },
  {
    "tid":"1171",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Robust text segmentation using graph cut' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Text segmentation provides important clues for the accurate identification of character locations and the analysis of character properties such as shape estimation and texture synthesis. In this paper, we propose a robust text segmentation method that employs Markov Random Field (MRF) and use graph cut algorithms to solve the energy minimization problem. To effectively select accurate seeds to boost the text segmentation performance, stroke feature transform is adopted to robustly identify text seeds and text edges. Background seeds are obtained near the text edges in order to well preserve the text boundaries. The energy functions are defined as an MRF consisting of data energy and smoothness energy which can be efficiently solved by graph cut algorithms. One distinctive property of the proposed technique is that it can identify more distinctive seeds so that only one cut is needed to well separate the text regions from the background, hence much faster than the existing iterative graph cut approach. Experiments on ICDAR 2003 and ICDAR 2011 datasets show that the proposed technique obtains superior performance on both pixel level and atom level segmentation."
      }
    ]
  },
  {
    "tid":"1172",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Addressing failures in exascale computing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present here a report produced by a workshop on 'Addressing failures in exascale computing' held in Park City, Utah, 4-11 August 2012. The charter of this workshop was to establish a common taxonomy about resilience across all the levels in a computing system, discuss existing knowledge on resilience across the various hardware and software layers of an exascale system, and build on those results, examining potential solutions from both a hardware and software perspective and focusing on a combined approach.The workshop brought together participants with expertise in applications, system software, and hardware; they came from industry, government, and academia, and their interests ranged from theory to implementation. The combination allowed broad and comprehensive discussions and led to this document, which summarizes and builds on those discussions."
      }
    ]
  },
  {
    "tid":"1173",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Random effects model for the reliability management of modules of a fighter aircraft' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The operational availability of fighter aircrafts plays an important role in the national defense. Low operational availability of fighter aircrafts can cause many problems and ROKA (Republic of Korea Airforce) needs proper strategies to improve the current practice of reliability management by accurately forecasting both MTBF (mean time between failure) and MTTR (mean time to repair). In this paper, we develop a random effects model to forecast both MTBF and MTTR of installed modules of fighter aircrafts based on their characteristics and operational conditions. Advantage of using such a random effects model is the ability of accommodating not only the individual characteristics of each module and operational conditions but also the uncertainty caused by random error that cannot be explained by them. Our study is expected to contribute to ROKA in improving operational availability of fighter aircrafts and establishing effective logistics management."
      }
    ]
  },
  {
    "tid":"1174",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Recognizing Daily Life Context Using Web-Collected Audio Data' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This work presents an approach to model daily life contexts from web-collected audio data. Being available in vast quantities from many different sources, audio data from the web provides heterogeneous training data to construct recognition systems. Crowd-sourced textual descriptions (tags) related to individual sound samples were used in a configurable recognition system to model 23 sound context categories. We analysed our approach using different outlier filtering techniques with dedicated recordings of all 23 categories and in a study with 230 hours of full-day recordings of 10 participants using smart phones. Depending on the outlier technique, our system achieved recognition accuracies between 51% and 80%."
      }
    ]
  },
  {
    "tid":"1175",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Expanding neighborhood search-GRASP for the probabilistic traveling salesman problem' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The Probabilistic Traveling Salesman Problem is a variation of the classic traveling salesman problem and one of the most\n significant stochastic routing problems. In probabilistic traveling salesman problem only a subset of potential customers\n need to be visited on any given instance of the problem. The number of customers to be visited each time is a random variable.\n In this paper, a variant of the well-known Greedy Randomized Adaptive Search Procedure (GRASP), the Expanding Neighborhood\n Search\u2013GRASP, is proposed for the solution of the probabilistic traveling salesman problem. expanding neighborhood search\u2013GRASP\n has been proved to be a very efficient algorithm for the solution of the traveling salesman problem. The proposed algorithm\n is tested on a numerous benchmark problems from TSPLIB with very satisfactory results. Comparisons with the classic GRASP\n algorithm and with a Tabu Search algorithm are also presented. Also, a comparison is performed with the results of a number\n of implementations of the Ant Colony Optimization algorithm from the literature and in six out of ten cases the proposed algorithm\n gives a new best solution."
      }
    ]
  },
  {
    "tid":"1176",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Energy-Efficient Full Diversity Collaborative Unitary Space-Time Block Code Designs via Unique Factorization of Signals' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a novel concept called a uniquely factorable constellation pair (UFCP) is proposed for the systematic design of a noncoherent full diversity collaborative unitary space-time block code by normalizing two Alamouti codes for a wireless communication system having two transmitter antennas and a single receiver antenna. It is proved that such a unitary UFCP code assures the unique identification of both channel coefficients and transmitted signals in a noise-free case as well as full diversity for the noncoherent maximum likelihood receiver in a noise case. To further improve error performance, an optimal unitary UFCP code is designed by appropriately and uniquely factorizing a pair of energy-efficient cross quadrature amplitude modulation (QAM) constellations to maximize the coding gain subject to a transmission bit rate constraint. After a deep investigation of the fractional coding gain function, a technical approach developed in this paper to maximizing the coding gain is to carefully design an energy scale to compress the first three largest energy points in the corner of the QAM constellations in the denominator of the objective as well as carefully design a constellation triple forming two UFCPs, with one collaborating with the other two so as to make the accumulated minimum Euclidean distance along the two transmitter antennas in the numerator of the objective as large as possible, and at the same time, to avoid as many corner points of the QAM constellations with the largest energy as possible to achieve the minimum of the numerator. In other words, the optimal coding gain is attained by intelligent constellations collaboration and efficient energy compression. Computer simulations demonstrate that error performance of the optimal unitary UFCP code presented in this paper outperforms those of the differential code and the signal-to-noise-ratio-efficient training code."
      }
    ]
  },
  {
    "tid":"1177",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Power and Rate Adaptation Based on CSI and Velocity Variation for OFDM Systems Under Doubly Selective Fading Channels.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a novel joint continuous power and rate adaptation scheme is proposed fordoubly selective fading channels in orthogonal frequency division multiplexing (OFDM) systems, based on terminal velocity and perfect or imperfect channel state information (CSI). The analysis and simulation results show that the continuous power and rate adaptation scheme is very effective and improve the performance of OFDM systems substantially under time-varying fading channels, as compared with the traditional adaptation schemes operating without a priori knowledge of velocity and mobility adaptation without CSI."
      }
    ]
  },
  {
    "tid":"1178",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Design and Implementation of Open MPI over Quadrics/Elan4' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Open MPI is a project recently initiated to provide a fault-tolerant, multi-network capable implementation of MPI-2, based on experiences gained from FT-MPI, LA-MPI, LAM/MPI, and MVAPICH projects. Its initial communication architecture is layered on top of TCP/IP. In this paper, we have designed and implemented Open MPI point-to-point layer on top of a high-end interconnect, Quadrics/Elan4. The restriction of Quadrics static process model has been overcome to accommodate the requirement of MPI-2 dynamic process management. Quadrics Queued-based Direct Memory Access (QDMA) and Remote Direct Memory Access (RDMA) mechanisms have been integrated to form a low-overhead, high-performance transport layer. Lightweight asynchronous progress is made possible with a combination of Quadrics chained event and QDMA mechanisms. Experimental results indicate that the resulting point-to-point transport layer is able to achieve comparable performance to Quadrics native QDMA operations, from which it is derived. Our implementation provides an MPI-2 compliant message passing library over Quadrics/Elan4 with a performance comparable to MPICH-Quadrics."
      }
    ]
  },
  {
    "tid":"1179",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Robust Output Fuzzy Control: Application to Vehicle Lateral Dynamics' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper deals with the robust control for Four Wheels Steering (4WS) vehicle dynamics when the road adhesion conditions change and the sideslip angle is unavailable for measurement. The nonlinear model of the vehicle, is firstly represented by a uncertain Takagi-Sugeno (T-S) model. Next, robust output stabilization of vehicle is considered. The controller and the observer are designed in terms of Bilinear Matrix Inequality (BMI) problem. The obtained simulation results indicate that considerable improvements in the vehicle handling can be achieved when the vehicle is governed by the proposed fuzzy observer-based controller."
      }
    ]
  },
  {
    "tid":"1180",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Verifying Atomicity Specifications for Concurrent Object-Oriented Software Using Model-Checking' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In recent work, Flanagan and Qadeer proposed atomicity declarations as a light-weight mechanism for specifying non-interference properties in concurrent programming languages such as Java, and they provided a type and eect system to verify atomicity properties. While verification of atomicity specifications via a static type system has several advantages (scalability, compositional checking), we show that verifica- tion via model-checking also has several advantages (fewer unchecked annotations, greater coverage of Java idioms, stronger verification). In particular, we show that by adapting the Bogor model-checker, we nat- urally address several properties that are dicult to check with a static type system."
      }
    ]
  },
  {
    "tid":"1181",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Exploring Heterogeneity for Multi-Domain Recommendation with Decisive Factors Selection' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"To address the recommendation problems in the scenarios of multiple domains, in this paper, we propose a novel method, HMRec, which models both consistency and heterogeneity of users' multiple behaviors in a unified framework. Moreover, the decisive factors of each domain can also be captured by our approach successfully. Experiments on the real multi-domain dataset demonstrate the effectiveness of our model."
      }
    ]
  },
  {
    "tid":"1182",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Action recognition with multi-scale trajectory-pooled 3D convolutional descriptors.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Hand-crafted and learning-based features are two main types of video representations in the field of video understanding. How to integrate their merits to design good descriptors has been the research hotspot recently. Motivated by TDD (Wang et al. 2015), we combine trajectory pooling method and 3D ConvNets (Tran et al. 2015) and put forward a novel multi-scale trajectory-pooled 3D convolutional descriptor (MTC3D) for action recognition in this paper. Specifically, we calculate multi-scale dense trajectories from the input video and perform trajectory pooling on feature maps of 3D CNN. The proposed descriptor has two advantages: 3D CNN has the ability to extract high-level semantic information from videos and multi-scale trajectory pooling method utilizes the temporal information of videos subtly. The experiments on the datasets of HMDB51 and UCF101 demonstrate that the proposed descriptor achieves state-of-the-art results."
      }
    ]
  },
  {
    "tid":"1183",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Shortest paths with arbitrary clearance from navigation meshes' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper addresses the problem of efficiently computing optimal paths of arbitrary clearance from a polygonal representation of a given virtual environment. Key to the proposed method is a new type of triangulated navigation mesh, called a Local Clearance Triangulation, which enables the efficient and correct determination if a disc of arbitrary size can pass through any narrow passages of the mesh. The proposed approach uniquely balances speed of computation and optimality of paths by first computing high-quality locally shortest paths efficiently in optimal time. Only in case global optimality is needed, an extended search will gradually improve the current path (if not already the global optimal) until the globally shortest one is determined. The presented method represents the first solution correctly extracting shortest paths of arbitrary clearance directly from a triangulated environment."
      }
    ]
  },
  {
    "tid":"1184",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient Video Text Detection Using Edge Features' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we explore new edge features such as straightness for the elimination of non significant edges from the segmented text portion of a video frame to detect accurate boundary of the text lines in video images. To segment the complete text portions, the method introduces candidate text block selection from a given image. Heuristic rules are formed based on combination of filters and edge analysis for identifying a candidate text block in the image. Furthermore, the same rules are extended to grow boundary of candidate text block in order to segment complete text portions in the image. The experimental results of the proposed method show that the method outperforms an existing method in terms of a number of metrics."
      }
    ]
  },
  {
    "tid":"1185",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Social and Cognitive System for Learning Negotiation Strategies with Incomplete Information' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Finding adequate (win-win solutions for both parties) negotiation strategy with incomplete information for autonomous agents, even in one-to-one negotiation, is a complex problem. First part of this paper aims to develop negotiation strategies for autonomous agents with incomplete information, where negotiation behaviors, based on time-dependent behaviors, are suggested to be used in combination (inspired from empirical human negotiation research). Suggested combination allows agents to improve negotiation process in terms of agent utilities, round number to reach an agreement, and percentage of agreements. Second part aims to develop a social and cognitive system for learning negotiation strategies from interaction, where characters conciliatory, neutral, or aggressive, are suggested to be integrated in negotiation behaviors (inspired from research works aiming to analyze human behavior and those on social negotiation psychology). Suggested strategy displays ability to provide agents, through a basic buying strategy, with a first intelligence level in social and cognitive system to learn from interaction (human-agent or agent-agent)."
      }
    ]
  },
  {
    "tid":"1186",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Rss Gradient-Assisted Frontier Exploration And Radio Source Localization' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We consider the combined problem of frontier exploration in a complex indoor environment while seeking a radio source. To do this in an efficient manner, we incorporate radio signal strength (RSS) information into the exploration algorithm by locally sampling the RSS and estimating the 2-D RSS gradient. The algorithm exploits the local motion to collect RSS samples for gradient estimation and seeks to explore in a way that brings the robot to the signal source. This strategy avoids random or exhaustive exploration. An indoor experiment demonstrates the exploration algorithm that uses this information to dynamically prioritize candidate frontiers and traverse to a radio source. Simulations, including radio propagation modeling with a ray-tracing algorithm, enable study of control algorithm tradeoffs and statistical performance."
      }
    ]
  },
  {
    "tid":"1187",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Function Approximation through Fuzzy Systems Using Taylor Series Expansion-Based Rules: Interpretability and Parameter Tuning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we present a new approach for the problem of approximating a function from a training set of I/O points using fuzzy logic and fuzzy systems. Such approach, as we will see, will provide us a number of advantages comparing to other more-limited systems. Among these advantages, we may highlight the considerable reduction in the number of rules needed to model the underlined function of this set of data and, from other point of view, the possibility of bringing interpretation to the rules of the system obtained, using the Taylor Series concept. This work is reinforced by an algorithm able to obtain the pseudo-optimal polynomial consequents of the rules. Finally the performance of our approach and that of the associated algorithm are shown through a significant example."
      }
    ]
  },
  {
    "tid":"1188",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A clustering approach to corner point analysis in hand drawn images' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Drawing tasks are used widely within neuropsychology for the assessment and monitoring of a variety of conditions. Automated assessment of these tasks improves their repeatability and accuracy in use alongside a reduction in trained therapist resource loading. Using images from test responses collected in an assessment of the condition of visuo-spatial neglect, this paper presents a method utilising a standard corner detection routine (SUSAN) to establish the presence and location of corner points within an image of a drawn geometric shape. It is demonstrated that by applying a series of standard clustering systems to these located corners, the accuracy in terms of number and spatial position of corner points found assessed against actual corner position is improved."
      }
    ]
  },
  {
    "tid":"1189",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Personalizing Search on Shared Devices' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Search personalization tailors the search experience to individual searchers. To do this, search engines construct interest models comprising signals from observed behavior associated with ma-chines, often via Web browser cookies or other user identifiers. However, shared device usage is common, meaning that the activities of multiple searchers may be interwoven in the interest models generated. Recent research on activity attribution has led to methods to automatically disentangle the histories of multiple searchers and correctly ascribe newly-observed search activity to the correct per-son. Building on this, we introduce attribution-based personalization (ABP), a procedure that extends traditional personalization to target individual searchers on shared devices. Activity attribution may improve personalization, but its benefits are not yet fully understood. We present an oracle study (with perfect knowledge of which searchers perform each action on each machine) to under-stand the effectiveness of ABP in predicting searchers' future interests. We utilize a large Web search log dataset containing both per-son identifiers and machine identifiers to quantify the gain in personalization performance from ABP, identify the circumstances under which ABP is most effective, and develop a classifier to determine when to apply it that yields sizable gains in personalization performance. ABP allows search providers to personalize experiences for individuals rather than targeting all users of a device collectively."
      }
    ]
  },
  {
    "tid":"1190",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The Complexity of Channel Scheduling in Multi-Radio Multi-Channel Wireless Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The complexity of channel scheduling in Multi- Radio Multi-Channel (MR-MC) wireless networks is an open research topic. This problem asks for the set of edges that can support maximum amount of simultaneous traffic over orthogonal channels under a certain interference model. There exist two major interference models for channel scheduling, with one under the physical distance constraint, and one under the hop distance constraint. The complexity of channel scheduling under these two interference models serves as the foundation for many problems related to network throughput maximization. However, channel scheduling was proved to be NP-Hard only under the hop distance constraint for SR-SC wireless networks. In this paper, we fill the void by proving that channel scheduling is NP-Hard under both models in MR-MC wireless networks. In addition, we propose a polynomial-time approximation scheme (PTAS) framework that is applicable to channel scheduling under both interference models in MR-MC wireless networks. Furthermore, we conduct a comparison study on the two interference models and identify conditions under which these two models are equivalent for channel scheduling."
      }
    ]
  },
  {
    "tid":"1191",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Two Simulation Tools for Testing ATM Resource Allocation Strategies.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Two event-driven simulation tools aimed at testing access control and routing mechanisms in an Asynchronous Transfer Mode (ATM) environment are presented. The first one is dedicated to the behavioural description of an ATM Virtual Circuit (VC) switch. The purpose is to test resource allocation and call Admission Control (CAC) strategies, so only the relevant events for this objective are considered and some important functions (e.g., the switching element) for a complete description of an ATM switch are not explicitly modeled. The second simulator has been designed to test routing strategies for an ATM network. In such a case, a higher level of abstraction than in the previous one is necessary and, again, only the meaningful events to the aim are taken into account. Some resource allocation, CAC, and routing schemes are also reported, along with the description of the simulation tools. Several simulation results are discussed, in order to assess their performance."
      }
    ]
  },
  {
    "tid":"1192",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Graph Theory Algorithms For Mobile Ad Hoc Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Various simulators (e.g., ns-2 and GloMoSim) are available to implement and study the behavior of the routing protocols for mobile ad hoc networks (MANETs). But, students and investigators who are new to this area often get perplexed in the complexity of these simulators and lose the focus in designing and analyzing the characteristics of the network and the protocol. Most of the time would be spent in learning the existing code modules of the simulator and the logical flow between the different code modules. The purpose of this paper would be to illustrate the applications of Graph Theory algorithms to study, analyze and simulate the behavior of routing protocols for MANETs. Specifically, we focus on the applications of Graph Theory algorithms to determine paths, trees and connected dominating sets for simulating and analyzing respectively unicast (single-path and multi-path), multicast and broadcast communication in mobile ad hoc networks (MANETs). We will discuss the (i) Dijkstra's shortest path algorithm and its modifications for finding stable paths and bottleneck paths; (ii) Prim's minimum spanning tree algorithm and its modification for finding all pairs smallest and largest bottleneck paths; (iii) Minimum Steiner tree algorithm to connect a source node to all the receivers of a multicast group; (iv) A node-degree based algorithm to construct an approximate minimum connected dominating set (CDS) for sending information from one node to all other nodes in the network; and (v) Algorithms to find a sequence of link-disjoint, node-disjoint and zone-disjoint multi-path routes in MANETs."
      }
    ]
  },
  {
    "tid":"1193",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The sway compensation trajectory for a biped robot' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"From 1970's, legged robots have attracted much attention of many researchers. In spite of this, it has been regarded that dynamically stable walking is very difficult to be tack- led for any types of legged robots. For a trot gait for quadruped walking robots, we have proposed \"the sway compensation trajectory\". This method utilizes a lateral, longitudinal, and vertical motion of a robot body to keep a zero moment point (ZMP) on a diagonal line between sup- port legs. In this paper, we develop the sway compensation trajectory for a biped robot, and show that dynamically stable walking is realized. This method makes it quite easy to design stable ZMP and COG (center of gravity) trajecto- ries, which have been regarded as a very complicated and delicate problem. The effectiveness of the proposed method is verified through computer simulations and walking ex- periments by a humanoid robot, HOAP-1."
      }
    ]
  },
  {
    "tid":"1194",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Self-configurable border landmark selection in wireless networks: Algorithms and applications' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The nodes at the border of the self-configurable wireless network are commonly employed as landmarks for many applications, including infrastructureless localization, border detection, and routing. However, how to identify the best set of nodes as such landmarks is still an open problem. In this paper, we propose three algorithms for border landmark selection, namely: the Convex Hull-Based (CHB) algorithm, the Center Node Elimination (CNE) algorithm, and the Hierarchy-Structured (HS) algorithm. CHB works perfectly in theory and provides a deep insight into the landmark selection problem. At the same time, it is noticed that CHB is centralized and sensitive to errors in distance estimation. The CNE algorithm is a distributed approach, devised to gradually exclude the nodes in the \u201ccenter\u201d of the network until the desired number of nodes are left, which are employed as landmarks. While CNE works effectively in a small network, its high order computation complexity and communication overhead may eventually lead to scalability problem when it is applied in very large networks. To address this problem, we propose the HS algorithm for striking the balance between accuracy and complexity/overhead. In HS, we establish a hierarchical structure with multiple layers, and apply the CNE algorithm in an appropriate layer to identify an initial set of candidate nodes. The outcomes are then rectified through a recursive process, yielding the final landmarks. Three applications, including coordinates establishment, border detection, and landmark-based routing in general networks without location information, are introduced based on the selected landmarks. We carry out extensive simulations to compare the performance of our landmark selection algorithms and demonstrate their effectiveness in all of the applications."
      }
    ]
  },
  {
    "tid":"1195",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Low latency timing synchronization scheme for IEEE 802.11a/n/ac systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper addresses the problem of timing synchronization at the receiver of an IEEE 802.11a/n/ac communications system. In such systems, typically with multiple antennas at the transmitter, cyclic delay diversity (CDD) is implemented in the transmitted signal to provide diversity and hence, performance improvement in the error rates. However, the use of CDD creates pseudo-multipaths which degrade the performance of conventional timing synchronization algorithms, due to the increased delay spread of the channel. This paper proposes a method that achieves fine timing synchronization with low latency in two concurrent steps. The proposed scheme first achieves a coarse synchronization point so that important receiver operations can first take place. A fine timing synchronization algorithm operates concurrently to achieve a more accurate synchronization point and, to avoid incurring additional latency, post-processing is performed on the frequency domain samples of the preambles to compensate for the difference in synchronization points. The proposed scheme is shown to achieve excellent packet error rate performance with low latency requirements."
      }
    ]
  },
  {
    "tid":"1196",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Measurement-based guidance of software projects using explicit project plans' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"As first steps towards establishing software engineering as an engineering discipline, we need to create explicit models of its building blocks, i.e., projects, processes, products, and various quality perspectives; organize these models for effective reuse across project boundaries, and establish measurable criteria for project guidance. The paper investigates the possibilities of providing measurement-based project guidance using explicit project plans. Following a summary of technologies developed by the process modelling and measurement subcommunities of software engineering, a method for integrating these technologies is suggested, and the potential benefits for project guidance are discussed. Examples from the MVP Project at the University of Kaiserslautern are used throughout for illustration purposes."
      }
    ]
  },
  {
    "tid":"1197",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Adaptive Gait For Quadruped Robots To Walk On A Slope' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes an adaptive intermittent crawl gait for quadruped robots to walk on a slope. In the proposed method, the inclination of a floor is estimated in real time by the displacement of the projection of the center of gravity from the nominal position. Depending upon the estimated inclination, the gait is changed in order to gain the longitudinal stability margin. Experiments and dynamic simulations are performed to validate the proposed method. The results are presented and discussed."
      }
    ]
  },
  {
    "tid":"1198",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'All-terminal reliability analysis of wireless networks of redundant radio modules' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Due to the low cost of easy-to-use components of open-source hardware and software, the Internet of Things may be pervading through everything everywhere. The dependability of the Internet of Things is critical because the services will be concerned with real life. However, the inexpensive hardware provides limited resources for communication. In this paper, we present exact all-terminal reliability of the wireless networks of which the number of links of each node is bounded. First, we analyze the all-terminal reliability of the regular wireless network, where any two nodes are connected by a wireless link consisting of a pair of radio modules. The reliability polynomial is derived from the Tutte polynomial that is the seminal work of the algebraic graph theory. Second, we analyze the all-terminal reliability of the wireless network including the random mesh network under the assumption that if the module fails, all radio links connected through a radio module are cut off. We consider both the fault of the communication module and the fading of the wireless channel in the indoor channel condition. For improving the reliability, we also propose a fault-tolerant method giving redundant radio modules into each node. We also analyze cost-effectiveness. We also calculate the mean-time-to-failure (MTTF)\u00a0for wireless networks. We show that the random mesh network of which each node includes three or more the radio-broadcast modules is more dependable than any other regular network of which each node comprises four point-to-point modules if it is the connected graph in the static environment."
      }
    ]
  },
  {
    "tid":"1199",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Effective capacity region of two-user opportunistic spectrum access in fading channel under discrete transmission rate policy' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper considers effective capacity region of two-user opportunistic spectrum access in fading channel under discrete transmission rate policy. Although opportunistic spectrum access has been regarded as a promising solution to the the problem of spectrum inefficiency and scarcity, little attention is given to capacity analysis under QoS constraint. Since discrete transmission rate policy is more preferable in practice, we firstly extend the finite state channel model to the opportunistic spectrum access by taking the occupancy of primary user into consideration. Then by means of effective capacity, we investigate the capacity region for the two-user opportunistic spectrum access under QoS constraint. Finally, we present numeric results and draw conclusions."
      }
    ]
  },
  {
    "tid":"1200",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Latent Semantic Indexing-based approach to multilingual document clustering' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The creation and deployment of knowledge repositories for managing, sharing, and reusing tacit knowledge within an organization has emerged as a prevalent approach in current knowledge management practices. A knowledge repository typically contains vast amounts of formal knowledge elements, which generally are available as documents. To facilitate users' navigation of documents within a knowledge repository, knowledge maps, often created by document clustering techniques, represent an appealing and promising approach. Various document clustering techniques have been proposed in the literature, but most deal with monolingual documents (i.e., written in the same language). However, as a result of increased globalization and advances in Internet technology, an organization often maintains documents in different languages in its knowledge repositories, which necessitates multilingual document clustering (MLDC) to create organizational knowledge maps. Motivated by the significance of this demand, this study designs a Latent Semantic Indexing (LSI)-based MLDC technique capable of generating knowledge maps (i.e., document clusters) from multilingual documents. The empirical evaluation results show that the proposed LSI-based MLDC technique achieves satisfactory clustering effectiveness, measured by both cluster recall and cluster precision, and is capable of maintaining a good balance between monolingual and cross-lingual clustering effectiveness when clustering a multilingual document corpus."
      }
    ]
  },
  {
    "tid":"1201",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Assessing Software Quality by Program Clustering and Defect Prediction' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many empirical studies have shown that defect prediction models built on product metrics can be used to assess the quality of software modules. So far, most methods proposed in this direction predict defects by class or file. In this paper, we propose a novel software defect prediction method based on functional clusters of programs to improve the performance, especially the effort-aware performance, of defect prediction. In the method, we use proper-grained and problem-oriented program clusters as the basic units of defect prediction. To evaluate the effectiveness of the method, we conducted an experimental study on Eclipse 3.0. We found that, comparing with class-based models, cluster-based prediction models can significantly improve the recall (from 31.6% to 99.2%) and precision (from 73.8% to 91.6%) of defect prediction. According to the effort-aware evaluation, the effort needed to review code to find half of the total defects can be reduced by 6% if using cluster-based prediction models."
      }
    ]
  },
  {
    "tid":"1202",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On Spatial Selectivity and Prediction across Conditions with fMRI.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Researchers in functional neuroimaging mostly use activation coordinates to formulate their hypotheses. Instead, we propose to use the full statistical images to define regions of interest (ROIs). This paper presents two machine learning approaches, transfer learning and selection transfer, that are compared upon their ability to identify the common patterns between brain activation maps related to two functional tasks. We provide some preliminary quantification of these similarities, and show that selection transfer makes it possible to set a spatial scale yielding ROIs that are more specific to the context of interest than with transfer learning. In particular, selection transfer outlines well known regions such as the Visual Word Form Area when discriminating between different visual tasks."
      }
    ]
  },
  {
    "tid":"1203",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Reduced-order dynamic output feedback control of continuous-time T--S fuzzy systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The problem of reduced-order dynamic output feedback control design for continuous-time Takagi-Sugeno fuzzy systems with H\"~ guaranteed cost is addressed. The proposed approach is based on a line-integral fuzzy Lyapunov function with arbitrary polynomial dependence on the premise variables, allowing the membership functions, modeled as multi-simplexes, to vary arbitrarily. The controller is obtained by transforming the problem into a static output feedback control design problem solved by means of a two-stages linear matrix inequality procedure. The main appeal of the approach is that the order of the output feedback controller as well as the dependence of the premise variables are customizable for control implementation purposes. Examples illustrate the flexibility of the method and also that the proposed strategy can provide less conservative results when compared to other methods for full order dynamic output feedback controllers from the literature of continuous-time Takagi-Sugeno fuzzy systems."
      }
    ]
  },
  {
    "tid":"1204",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Profiling-driven multi-cycling in FPGA high-level synthesis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Multi-cycling is a well-known strategy to improve performance in digital design, wherein the required time for selected combinational paths is lengthened to multiple clock cycles (rather than just one). The approach can be applied to paths associated with computations whose results are not needed immediately -- such paths are allowed multiple clock cycles to \\\"complete\\\", reducing the opportunity for them to form the critical path of the circuit. In this paper, we consider multi-cycling in the high-level synthesis context (HLS) and use software profiling to guide multi-cycling optimizations. Specifically, prior to HLS, we execute the program in software with typical datasets to gather data on the number of times each code segment executes. During HLS, we then extend the schedule for infrequently executed code segments and apply multi-cycling to the dilated schedules, which exhibit greater opportunities for multi-cycling. In essence, our approach ensures that non-frequently executed code segments will not form the critical path of the HLS-generated circuit. In an experimental study targeting the Altera Stratix IV FPGA, we evaluate the impact on speed performance and area for both traditional multi-cycling, as well as the proposed software profiling-driven multi-cycling, and show that profiling-driven multi-cycling leads to an average speedup of over 10% across 13 benchmark circuits, with some circuit speedups in excess of 30%. Circuit area is reduced by 11%, yielding a mean 20% improvement in area-delay product."
      }
    ]
  },
  {
    "tid":"1205",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Searching in Cooperative Patent Classification: Comparison between keyword and concept-based search' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"International patent corpus is a gigantic source containing today about 80million of documents. Every patent is manually analyzed by patent officers and then classified by a specific code called Patent Class (PC). Cooperative Patent Classification CPC is the new classification system introduced since January 2013 in order to standardize the classification systems of all major patent offices. Like keywords for papers, PCs point to the core of the invention, describing concisely what they contain inside. Most of patents strategies are based on PC as filter for results therefore the selection of relevant PCs is often a primary and crucial activity. This task is considered particularly challenging and only few tools have been specially developed for this purpose. The most efficient tools are provided by patent offices of EPO and WIPO. This paper analyzes their PCs search strategy (mainly based on keyword-based engines) in order to identify main limitations in terms of missing relevant PCs (recall) and non-relevant results (precision). Patents have been processed by KOM, a semantic patent search tool developed by the authors. Unlike all other PC search tools, KOM uses semantic parser and many knowledge bases for carrying out a conceptual patent search. Its functioning is described step by step through a detailed analysis pointing out the benefits of a concept-based search vis-a-vis a keyword-based search. An exemplary case is proposed dealing with CPCs describing the sterilization of contact lenses. Comparison could be likewise conducted on other PCs such as International (IPC), European (ECLA) or United States (USPC) patent classification codes."
      }
    ]
  },
  {
    "tid":"1206",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Predicting User Physiological Response for Interactive Environments: An Inductive Approach' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Affective reasoning holds great potential for interactive digital entertainment, education, and training. Incorporating affective reasoning into the decision-making capabilities of interactive environments could enable them to create customized experiences that are dynamically tailored to individual users' ever changing levels of engagement, interest, and emotional state. Because physiological responses are directly triggered by changes in affect, biofeedback data such as heart rate and galvanic skin response can be used to infer affective changes. However, biofeedback hardware is intrusive and cumbersome in deployed applications. This paper proposes an inductive framework for automatically learning models of users' physiological response from observations of user behaviors in interactive environments. These models can be used at runtime without biofeedback hardware to continuously predict users' physiological state directly from situational context in the interactive environment. Empirical studies with induced decision tree, na\u00efve Bayes, and Bayesian Network physiological response models suggest that they may be sufficiently accurate for practical use."
      }
    ]
  },
  {
    "tid":"1207",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Nonpreemptive priority scheme for the S-MAC protocol in multimedia mobile sensor networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"S-MAC is a well-known medium access control (MAC) protocol designed for wireless sensor networks (WSNs). S-MAC uses a few novel techniques to reduce energy consumption and support self-configuration. Besides, S-MAC also achieves good scalability and collision avoidance by utilizing a combined scheduling and contention scheme. However, S-MAC does not have any priority scheme, making it unsuitable for multimedia applications and suffering from varying degree of starvation and fairness problems. In this paper, we propose a simple, efficient, but yet well-performing sliding contention window control mechanism to modify the S-MAC protocol such that priority can be supported along with the multimedia applications in multimedia mobile sensor networks. Furthermore, the proposed scheme could solve the starvation problem and alleviate the fairness problem. An analytic model is carried out to study the saturation throughput of proposed scheme. In addition, simulations were also conducted to analyze the proposed scheme and the results show that the proposed scheme is able to carry the prioritized traffic and mitigate the starvation and fairness problems."
      }
    ]
  },
  {
    "tid":"1208",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Debug determinism: the sweet spot for replay-based debugging' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Deterministic replay tools offer a compelling approach to debugging hard-to-reproduce bugs. Recent work on relaxed-deterministic replay techniques shows that replay debugging with low in-production overhead is possible. However, despite considerable progress, a replay-debugging system that offers not only low in-production runtime overhead but also high debugging utility, remains out of reach. To this end, we argue that the research community should strive for debug determinism-- a new determinism model premised on the idea that effective debugging entails reproducing the same failure and the same root cause as the original execution. We present ideas on how to achieve and quantify debug determinism and give preliminary evidence that a debug-deterministic system has potential to provide both low in-production overhead and high debugging utility."
      }
    ]
  },
  {
    "tid":"1209",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'New techniques for data fusion in multimodal FMT-CT imaging' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We examine approaches to the incorporation of anatomic structural information into the inverse problem of fluorescence molecular tomography (FMT). Using an appropriate relationship between anatomic and reconstruction image resolution, we build an inverse problem parameterized along the anatomical segmentation. These values serve as the basis for two new regularization techniques. The first regularizes individual voxels in proportion to the importance of the underlying segments in reducing the residual error. The second is based on a well known statistical interpretation of Tikhonov-type regularization in which the statistical prior is defined implicitly as the solution to a PDE whose structure is based on the anatomical segmentation. Results are shown using both techniques for a simulated experiment within the chest cavity of a mouse."
      }
    ]
  },
  {
    "tid":"1210",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Framework for a NetFPGA-based Snort NIDS' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Network's speed continues to increase at a high rate resulting in massive network traffic. This results in a need to have a high-speed network intrusion detection system (NIDS) to detect malicious traffic. Snort is a software-based NIDS that can run as a single threaded application. However, it may not be able to detect intrusions in real-time especially in networks with high traffic. This paper proposes a two layer framework where Snort will run as a second line of defense and will be executed only when deep payload analysis is needed. To accelerate the efficiency of Snort, the proposed system will dynamically offload the most frequent rules or signatures to a NetFPGA based hardware. The NetFPGA will work as a first line of defense that accelerates the detection by filtering all the traffic looking for intrusions. This will be done by analyzing the captured packet header to match the offloaded rules or signatures."
      }
    ]
  },
  {
    "tid":"1211",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Parcae: a system for flexible parallel execution' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Workload, platform, and available resources constitute a parallel program's execution environment. Most parallelization efforts statically target an anticipated range of environments, but performance generally degrades outside that range. Existing approaches address this problem with dynamic tuning but do not optimize a multiprogrammed system holistically. Further, they either require manual programming effort or are limited to array-based data-parallel programs. This paper presents Parcae, a generally applicable automatic system for platform-wide dynamic tuning. Parcae includes (i) the Nona compiler, which creates flexible parallel programs whose tasks can be efficiently reconfigured during execution; (ii) the Decima monitor, which measures resource availability and system performance to detect change in the environment; and (iii) the Morta executor, which cuts short the life of executing tasks, replacing them with other functionally equivalent tasks better suited to the current environment. Parallel programs made flexible by Parcae outperform original parallel implementations in many interesting scenarios."
      }
    ]
  },
  {
    "tid":"1212",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Algorithm design for a class of base station location problems in sensor networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Base station placement has significant impact on sensor network performance. Despite its significance, results on this problem remain limited, particularly theoretical results that can provide performance guarantee. This paper proposes a set of procedure to design (1 - \u025b) approximation algorithms for base station placement problems under any desired small error bound \u025b 0. It offers a general framework to transform infinite search space to a finite-element search space with performance guarantee. We apply this procedure to solve two practical problems. In the first problem where the objective is to maximize network lifetime, an approximation algorithm designed through this procedure offers 1/\u025b2 complexity reduction when compared to a state-of-the-art algorithm. This represents the best known result to this problem. In the second problem, we apply the design procedure to address base station placement problem when the optimization objective is to maximize network capacity. Our (1 - \u025b) approximation algorithm is the first theoretical result on this problem."
      }
    ]
  },
  {
    "tid":"1213",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Structural topic model for latent topical structure analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Topic models have been successfully applied to many document analysis tasks to discover topics embedded in text. However, existing topic models generally cannot capture the latent topical structures in documents. Since languages are intrinsically cohesive and coherent, modeling and discovering latent topical transition structures within documents would be beneficial for many text analysis tasks. In this work, we propose a new topic model, Structural Topic Model, which simultaneously discovers topics and reveals the latent topical structures in text through explicitly modeling topical transitions with a latent first-order Markov chain. Experiment results show that the proposed Structural Topic Model can effectively discover topical structures in text, and the identified structures significantly improve the performance of tasks such as sentence annotation and sentence ordering."
      }
    ]
  },
  {
    "tid":"1214",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Process Mining Online Assessment Data' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Traditional data mining techniques have been extensively applied to find interesting patterns, build descriptive and predictive models from large volumes of data accumulated through the use of different information systems. The results of data mining can be used for getting a better understanding of the underlying educational processes, for generating recommendations and advice to students, for improving management of learning objects, etc. However, most of the traditional data mining techniques focus on data dependencies or simple patterns and do not provide a visual representation of the complete educational (assessment) process ready to be analyzed. To allow for these types of analysis (in which the process plays the central role), a new line of data-mining research, called process mining, has been initiated. Process mining focuses on the development of a set of intelligent tools and techniques aimed at extracting process-related knowledge from event logs recorded by an information system. In this paper we demonstrate the applicability of process mining, and the ProM framework in particular, to educational data mining context. We analyze assessment data from recently organized online multiple choice tests and demonstrate the use of process discovery, conformance checking and performance analysis techniques."
      }
    ]
  },
  {
    "tid":"1215",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Generating Impact-Based Summaries for Scientific Literature' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we present a study of a novel summarization problem, i.e., summarizing the impact of a scientic publication. Given a pa- per and its citation context, we study how to extract sentences that can represent the most inuential content of the paper. We propose language modeling methods for solving this problem, and study how to incorporate fea- tures such as authority and proximity to ac- curately estimate the impact language model. Experiment results on a SIGIR publication collection show that the proposed methods are effective for generating impact-based sum- maries."
      }
    ]
  },
  {
    "tid":"1216",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Feedback effects between similarity and social influence in online communities' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A fundamental open question in the analysis of social networks is to understand the interplay between similarity and social ties. People are similar to their neighbors in a social network for two distinct reasons: first, they grow to resemble their current friends due to social influence; and second, they tend to form new links to others who are already like them, a process often termed selection by sociologists. While both factors are present in everyday social processes, they are in tension: social influence can push systems toward uniformity of behavior, while selection can lead to fragmentation. As such, it is important to understand the relative effects of these forces, and this has been a challenge due to the difficulty of isolating and quantifying them in real settings. We develop techniques for identifying and modeling the interactions between social influence and selection, using data from online communities where both social interaction and changes in behavior over time can be measured. We find clear feedback effects between the two factors, with rising similarity between two individuals serving, in aggregate, as an indicator of future interaction -- but with similarity then continuing to increase steadily, although at a slower rate, for long periods after initial interactions. We also consider the relative value of similarity and social influence in modeling future behavior. For instance, to predict the activities that an individual is likely to do next, is it more useful to know the current activities of their friends, or of the people most similar to them?"
      }
    ]
  },
  {
    "tid":"1217",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'SIR-optimized DFT-bank transceivers for multipath fading channels' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we study DFT-bank transceivers with filter length longer than the block size. We show how to optimize the DFT-bank transceiver so that both the intra-band and cross-band interferences are minimized for unknown multipath channels. No costly post processing technique is needed and the only channel dependent part is a set of one-tap equalizers at the receiver. The optimization problem is formulated as a Rayleigh-Ritz ratio whose solution is well-known. Experiments are carried out for transmission over random multipath channels and the results show that satisfactory signal-to-interference ratio performance can be obtained."
      }
    ]
  },
  {
    "tid":"1218",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Smart blueprints: automatically generated maps of homes and the devices within them' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Off-the-shelf home automation technology is making it easier than ever for people to convert their own homes into \\\"smart homes\\\". However, manual configuration is tedious and error-prone. In this paper, we present a system that automatically generates a map of the home and the devices within it. It requires no specialized deployment tools, 3D scanners, or localization hardware, and infers the floor plan directly from the smart home sensors themselves, e.g. light and motion sensors. The system can be used to automatically configure home automation systems or to automatically produce an intuitive map-like interface for visualizing sensor data and interacting with controllers. We call this system Smart Blueprints because it is automatically customized to the unique configuration of each home. We evaluate this system by deploying in four different houses. Our results indicate that, for three out of the four home deployments, our system can automatically narrow the layout down to 2-4 candidates per home using only one week of collected data."
      }
    ]
  },
  {
    "tid":"1219",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Endoscopic scene labelling and augmentation using intraoperative pulsatile motion and colour appearance cues with preoperative anatomical priors.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A new technique for localizing both visible and occluded structures in an endoscopic view was proposed and tested. This method leverages both preoperative data, as a source of patient-specific prior knowledge, as well as vasculature pulsation and endoscopic visual cues in order to accurately segment the highly noisy and cluttered environment of an endoscopic video. Our results on in vivo clinical cases of partial nephrectomy illustrate the potential of the proposed framework for augmented reality applications in minimally invasive surgeries."
      }
    ]
  },
  {
    "tid":"1220",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Scalable TCAM-based regular expression matching with compressed finite automata' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Regular expression (RegEx) matching is a core function of deep packet inspection in modern network devices. Previous TCAM-based RegEx matching algorithms a priori assume that a deterministic finite automaton (DFA) can be built for a given set of RegEx patterns. However, practical RegEx patterns contain complex terms like wildcard closure and repeat character, and it may be impossible to build a DFA with a reasonable number of states. This results in prior work to being infeasible in practice. Moreover, TCAM-based RegEx matching is required to scale to a large-scale set of RegEx patterns. In this paper, we propose a compressed finite automaton implementation called (CFA) for scalable TCAM-based RegEx matching. CFA is designed to reduce TCAM space by using three compression techniques: transition, character, and state compressions. Experiments on realistic RegEx pattern sets show CFA highly outperforms previous solutions in terms of TCAM space, matching throughput, and TCAM power consumption.\n\n"
      }
    ]
  },
  {
    "tid":"1221",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Entity Consolidation: The Golden Record Problem.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Four key processes in data integration are: data preparation (i.e., extracting, transforming, and cleaning data), schema integration (i.e., identifying attribute correspondences), entity resolution (i.e., finding clusters of records that represent the same entity) and entity consolidation (i.e., merging each cluster into a u0027golden recordu0027, which contains the canonical value for each attribute). In real-world scenarios, Master Data Management (MDM) is often employed to perform the last two steps using a u0027match-mergeu0027 module, which is driven by a collection of user-written match and merge rules. However, it is well understood that such solutions often fail to scale to the sizes and complexity of problems currently being addressed. Scalable solutions to entity resolution have been previously developed to generate a collection of clusters of records representing the same entity. In this paper, we propose a scalable entity consolidation algorithm to merge these clusters into u0027golden recordsu0027. We first automatically generate matching rules from the clusters and then group these rules into sets with common characteristics to cut down on the number which must be verified by a human. Next, for the human-approved rule groups, we apply them to merge the duplicate attribute values in the clusters. Finally, we employ existing truth discovery methods, such as majority consensus (MC), to resolve remaining conflicts in the clusters. We applied our methods on three real-world datasets. In a dataset with 31,023 clusters and 80,451 duplicate records, 72,239 matching rules were generated. By having a human con rm only 100 algorithm-generated rule groups, we achieved a recall of 75% and a precision of 98% for merging duplicate attribute values. When we invoked our algorithm prior to running MC, we improved the precision of golden record construction by 40%."
      }
    ]
  },
  {
    "tid":"1222",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Community Dynamics and Analysis of Decadal Trends in Climate Data' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The application of complex networks to study complex phenomena, including the Internet, social networks, food networks, and others, has seen a growing interest in recent years. In particular, the use of complex networks and network theory to analyze the behavior of the climate system is an emerging topic. This newfound interest is due to the difficulty of analyzing climate data -- this analysis is notoriously difficult due to the strong spatio-temporal dependencies, multivariate nature, seasonal behavior, and nonlinear phenomena inherent in the climate system. Network-based approaches model the complex long-term dependencies of weather attributes (such as temperature or air pressure) between locations on the Earth as a network of relationships and analyze these networks to gather insights about the emergent behavior of the system as a whole. In this paper, we describe our work-in-progress on a methodology for capturing and characterizing the evolution of the climate network. We do this by splitting the climate data into a set of overlapping decadal time windows and forming a network for each of these datasets representing the complex interdependencies in the climate system over the particular decade. We can then use this sequence of networks to characterize major patterns and anomalies in the data. We validate our methodology by identifying nontrivial events and trends in the evolution of the decadal networks and correlating these with known climatological phenomena."
      }
    ]
  },
  {
    "tid":"1223",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Evidential Reasoning Approach to Attribute Value Conflict Resolution in Database Integration' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Resolving domain incompatibility among independently developed databases often involves uncertain information. DeMichiel [1] showed that uncertain information can be generated by the mapping of conflicting attributes to a common domain, based on some domain knowledge. In this paper, we show that uncertain information can also arise when the database integration process requires information not directly represented in the component databases, but can be obtained through some summary of data. We therefore propose an extended relational model based on Dempster-Shafer theory of evidence [2] to incorporate such uncertain knowledge about the source databases. The extended relation uses evidence sets to represent uncertainty in information, which allow probabilities to be attached to subsets of possible domain values. We also develop a full set of extended relational operations over the extended relations. In particular, an extended union operation has been formalized to combine two extended relations using Dempster's rule of combination. The closure and boundedness properties of our proposed extended operations are formulated. We also illustrate the use of extended operations by some query examples."
      }
    ]
  },
  {
    "tid":"1224",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On the limitations of query obfuscation techniques for location privacy' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A promising approach to location privacy is query obfuscation, which involves reporting k -- 1 false locations along with the real location. In this paper, we examine the level of privacy protection provided by the current query obfuscation techniques against adversarial location service providers. As a representative and realistic implementation of query obfuscation, we focus on SybilQuery. We present two types of attacks depending upon whether or not a short-term query history is available. When history is available, using machine learning, we were able to identify 93.67% of user trips, with only 2.02% of fake trips misclassified, for the security parameter k = 5. In the absence of history, we used trip correlations to form a smaller set of trips effectively increasing the user query identification probability from 20% to about 40%. Our work demonstrates that the use of aggregate statistical information alone is not sufficient to generate simulated trips. We identify areas for improvement in the existing query obfuscation techniques."
      }
    ]
  },
  {
    "tid":"1225",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'WWW recycling for a better world' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Web 2.0 is affecting the structure of our society by creating new spaces of freedom, giving voice to any opinion, easing interpersonal relationships, and encouraging the creation of collaborating collectivities. Technologies such as blog, podcasting, wiki, and news feeds have the power to transform every user from a mere information consumer to a potential producer, from a spectator to an actor. The success of YouTube, MySpace, Facebook, and Wikipedia is under everybody's eyes. Generally speaking, Web 2.0 is seen as a powerful tool that, by exploiting Internet technologies, supplies services to the society, helping the entire socio-cultural system to develop and to move toward a democratic direction. Unfortunately, this imagery is partially distorted: first, Web 2.0 and Internet technologies are differently accessible by different parts of the society and, second, as many sociologists pointed out, their applications often appeal to users' egoistic purposes and self-celebration spirit. Although we cannot dismiss the value of current Web 2.0 applications, we are convinced there is a much more valuable potential that has not been exploited yet. By comparing the immense benefits that Web 2.0 could bring to the whole society, with its factual employment, one could provocatively change the meaning of the acronym WWW into World Wide Waste. We are hence convinced that it is necessary to redesign the utilization paradigm of Web 2.0 and, in general, of the Internet in order to recycle unused parts of Web 2.0 into altruistic bricks that could be appropriately rerouted and composed for alternative, unselfish employment."
      }
    ]
  },
  {
    "tid":"1226",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On the randomness requirements of rumor spreading' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We investigate the randomness requirements of the classical rumor spreading problem on fully connected graphs with n vertices. In the standard random protocol, where each node that knows the rumor sends it to a randomly chosen neighbor in every round, each node needs O((log n)2) random bits in order to spread the rumor in O(log n) rounds with high probability (w.h.p.). For the simple quasirandom rumor spreading protocol proposed by Doerr, Friedrich, and Sauerwald (2008), [log n] random bits per node are sufficient. A lower bound by Doerr and Fouz (2009) shows that this is asymptotically tight for a slightly more general class of protocols, the so-called gate-model. In this paper, we consider general rumor spreading protocols. We provide a simple push-protocol that requires only a total of O(n log log n) random bits (i.e., on average O(log log n) bits per node) in order to spread the rumor in O(log n) rounds w.h.p. We also investigate the theoretical minimal randomness requirements of efficient rumor spreading. We prove the existence of a (non-uniform) push-protocol for which a total of 2 log n + log log n + o(log log n) random bits suffice to spread the rumor in log n + ln n + O(1) rounds with probability 1 \u2212 o(1). This is contrasted by a simple time-randomness tradeoff for the class of all rumor spreading protocols, according to which any protocol that uses log n \u2212 log log n \u2212 \u03c9(1) random bits requires \u03c9(log n) rounds to spread the rumor."
      }
    ]
  },
  {
    "tid":"1227",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Leveraging Identity-Based Cryptography for Node ID Assignment in Structured P2P Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Structured peer-to-peer (P2P) systems have grown enormously because of their scalability, efficiency, and reliability. These systems assign a unique identifier to each user and object. However, current assignment schemes allow an adversary to carefully select user IDs and/or simultaneously obtain many pseudo-identities\u2014ultimately leading to an ability to disrupt the P2P system in very targeted and dangerous ways. In this paper, we propose novel ID assignment protocols based on identity-based cryptography. This approach permits the acquisition of node IDs to be tightly regulated without many of the complexities and costs associated with traditional certificate solutions. We broadly consider the security requirements of ID assignment and present three protocols representing distinct threat and trust models. A detailed empirical study of the protocols is given. Our analysis shows that the cost of our identity-based protocols is nominal, and that the associated identity services can scale to millions of users using a limited number of servers."
      }
    ]
  },
  {
    "tid":"1228",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Opportunistic Optical Hyperchannel and Its Distributed QoS Assuring Access Control' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Light-trail is proposed as a candidate to carry IP traffic over wavelength-division multiplexing optical networks given its capability of enabling high-speed provisioning and accommodating multigranularity traffic. In a light-trail, the optical shutters at the start node and the end node are configured to be in OFF state and the optical shutters at the intermediate nodes are configured to be in ON state. Thus, an optical bus is formed, allowing traffic multiplexing without the state change of any optical shutter. This, however, limits the system throughput and also makes it impossible to implement a fully distributed medium access control (MAC) protocol to assure quality of service (QoS) in a light-trail. With the recent development on ultrafast optical shutter, we propose an improved light-trail architecture, called opportunistic hyperchannel in this paper. In an opportunistic hyperchannel, an intermediate node can dynamically control its optical shutter which makes it possible to design a fully distributed QoS assuring MAC protocol. We then present a QoS assuring distributed dynamic scheduling protocol, namely, minimum source round robin (minSrcRR) protocol, for opportunistic hyperchannels. Theoretical analysis on the effectiveness of the proposed QoS assuring protocol and the worst-case delay bound are also derived in this paper. The simulation results quantitatively demonstrate the advantage of opportunistic hyperchannels and the effectiveness of minSrcRR protocol."
      }
    ]
  },
  {
    "tid":"1229",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Data acquisition with RF-based energy harvesting sensor: From information theory to green system' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Harvesting energy from ambient environment is a new promising solution to free electronic devices from electric wire or limited-lifetime battery, which is significant in sensor networks and body-area networks. This paper investigates the fundamental limits of information transmission in data acquisition system with RF-based energy harvesting sensor node, in which the host node acts not only as an information source but also as an energy source for the sensor node while only information is transmitted back from sensor to host node. From a view of system level, achievable capacity-rate region and capacity-rate function are proposed as metrics to measure the transmission performance. The tradeoff relationship of two way information rates between sensor and host node in a time division duplex system is obtained and the corresponding optimal transmission policy is also given. At last, a typical application in terms of minimizing required transmit power, namely green system, is introduced to validate the results developed in this paper."
      }
    ]
  },
  {
    "tid":"1230",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'HLLS: A History information based Light Location Service for MANETs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In mobile ad hoc networks, a location service (LS) provides the fundamental service for geographical routing. However, most existing schemes for location service incur a high overhead because of the periodic updates of location information. To address this issue, we propose a novel History information based Light Location Service (HLLS), using the temporal relationship among historical locations of mobile nodes in the network. In HLLS, location information of mobile nodes is propagated by Hello beacons locally, and the location query is performed with the aid of historical locations. In this way, HLLS can eliminate the tremendous periodical location updates and significantly reduce the overhead for location service. Using a combination of analytical modeling and extensive simulations, we show the effectiveness of HLLS in comparison with other approaches."
      }
    ]
  },
  {
    "tid":"1231",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient and flexible inter-overlay scheduling of media streams for multi-channel P2P streaming' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Existing studies on channel bandwidth imbalance in P2P multi-channel streaming systems have been exclusively focused on inter-overlay bandwidth allocation. However, an efficient inter-overlay scheduling algorithm is still in lack for benefactors. To this end, this paper presents an inter-overlay substream scheduling algorithm compatible with various overlay meshes for active inter-overlay cooperation, through which the outbound bandwidth of benefactors can be efficiently utilized and bandwidth-deprived channels receiving benefactions can attain a better streaming quality."
      }
    ]
  },
  {
    "tid":"1232",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Continuous-Time Analog Two-Dimensional IIR Beam Filters' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Analog filter circuits that are realized in continuous-time discrete-space domain are proposed for implementing 2-D infinite impulse response (IIR) spatiotemporal transfer functions having beam-shaped passbands. These filters may be used for analog beamforming applications including seismic, audio, sonar, and ultrasonic signal processing. Unlike digital implementations, the proposed 2-D analog beam filters avoid the need for A/D conversions and digital circuitry. Importantly, the beam distortion (beam squint) due to bilinear transformation that is unavoidable in digital 2-D IIR beam filters is significantly reduced. A demonstrative example using operational amplifiers for first-order analog filters is provided."
      }
    ]
  },
  {
    "tid":"1233",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Empirical Investigation into Social Productivity of a Software Process: An Approach by Using the Structural Equation Modeling.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The actual and expected benefits of fostering the alignment of people factors and cooperation among software development; teams enables software development organization to improve software development productivity. Furthermore, software development productivity presents a significant challenge for both understanding and quantifying the performance characteristics of software development organizations. This paper introduces an approach to model software development productivity by using structural equation modeling (SEM), a technique that can be used for testing and estimating relationships using empirical data. We also present preliminary results from an exploratory study about the enabling social factors that affect software development productivity. Our quantitative analysis involves grouping productivity and social productivity factors for studying and identifying their implicit relationship. To this end, we issue questionnaires to test our hypothesis and to gather sample data. The paper concludes by showing initial results, limitations, and directions for future research."
      }
    ]
  },
  {
    "tid":"1234",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A process variability control chart' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this study a Shewhart type control chart namely the V t chart, is proposed for improved monitoring of the process variability of a quality characteristic of interest Y. The proposed control chart is based on the ratio type estimator of the variance using a single auxiliary variable X. It is assumed that (Y, X) follows a bivariate normal distribution. The design structure of the V t chart is developed for Phase-I quality control and its comparison is made with those of the S 2 chart (a well-known Shewhart control chart) and the V r chart (a Shewhart type control chart proposed by Riaz (Comput Stat, 2008a) used for the same purpose. It is observed that the proposed V t chart outperforms the S 2\u00a0and V r charts, in terms of discriminatory power, for detecting moderate to large shifts in the process variability. It is observed that the performance of the V t chart keeps improving with an increase in |\u03c1 yx | , where \u03c1 yx is the correlation between Y and X."
      }
    ]
  },
  {
    "tid":"1235",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Person Re-identification by Saliency Learning.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Human eyes can recognize person identities based on small salient regions, i.e., person saliency is distinctive and reliable in pedestrian matching across disjoint camera views. However, such valuable information is often hidden when computing similarities of pedestrian images with existing approaches. Inspired by our user study result of human perception on person saliency, we propose a novel per..."
      }
    ]
  },
  {
    "tid":"1236",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Effective Communication Model For Collaborative Commerce Of Web-Based Surveillance Services' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we design and implement a Web-based information system for providing remote surveillance services. The surveillance data can be treated as a kind of digitized product in E-market. By collaborating different value-added services with the same surveillance data, diverse surveillance applications can be introduced. For example, by collaborating with companies (i.e. virtual enterprises) that supply Web-services such as face-recognition, moving object tracking and abandoned object identification, we can create new and diverse digitized products for supporting security systems, intelligent transportation system (ITS) and biometric system in airports, parking lots and offices. For supporting ubiquitous services, we collaborate with the auto-dialup service of PSTN networks (B2B E-commerce) to send messages to the subscribed user (B2C E-commerce) through his/her phone, pager or fax machine. By collaborating the media streaming service, users can watch live/recorded surveillance videos immediately through wired/wireless Web-devices. To reduce network workload, we apply multicast communication for the surveillance video delivery. Furthermore, an effect and efficient motion detection method is proposed to reduce redundant traffic and storage. Experiments show that our proposed schemes can help the system remit the performance penalty from network congestion."
      }
    ]
  },
  {
    "tid":"1237",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Novel Quantitative Measure Of Redundancy For Kinematically Redundant Manipulators' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a novel quantitative measure of redundancy, called Relined Redundancy Index (RRI), which is measured in the joint-rate level, is proposed. Based on the concept that a larger solution space of the inverse kinematics problem represents larger redundancy, the RRI is defined as the normalized magnitude of the solution spaces, The value of the RRI varies from 0 to 1. and larger RRI corresponds to larger redundancy, Unlike joint-angle-level approaches, our method does not have the annoying problem that different regions of joint angles correspond to the same primary task. The proposed RRI is computationally efficient and easy to apply for real-time applications. Simulation results show that with RRI the manipulator can reduce the execution time of a given task significantly, compared to the conventional approach. The results also illustrate that motion failures can he avoided with RRI. RRI is potentially useful. It is believed that RRI can be applied to a variety of applications of redundant robots in the future."
      }
    ]
  },
  {
    "tid":"1238",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Complementary assistance mechanisms for end user mashup composition' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Despite several efforts for simplifying the composition process, learning efforts required for using existing mashup editors to develop mashups remain still high. In this paper, we describe how this barrier can be lowered by means of an assisted development approach that seamlessly integrates automatic composition and interactive pattern recommendation techniques into existing mashup platforms for supporting easy mashup development by end users. We showcase the use of such an assisted development environment in the context of an open-source mashup platform Apache Rave. Results of our user studies demonstrate the benefits of our approach for end user mashup development."
      }
    ]
  },
  {
    "tid":"1239",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Sparse Multimodal Manifold-Regularized Transfer Learning for MCI Conversion Prediction' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Effective prediction of conversion of mild cognitive impairment (MCI) to Alzheimer's disease (AD) is important for early diagnosis of AD, as well as for evaluating AD risk pre-symptomatically. Different from most traditional methods for MCI conversion prediction, in this paper, we propose a novel sparse multimodal manifold-regularized transfer learning classification (SM2TLC) method, which can simultaneously use other related classification tasks (e.g., AD vs. normal controls (NC) classification) and also the unlabeled data for improving the MCI conversion prediction. Our proposed method includes two key components: (1) a criterion based on the maximum mean discrepancy (MMD) for eliminating the negative effect related to the distribution differences between the auxiliary (i.e., AD/NC) and the target (i.e., MCI converters/MCI non-converters) domains, and (2) a sparse semisupervised manifold-regularized least squares classification method for utilization of unlabeled data. Experimental results on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database show that the proposed method can significantly improve the classification performance between MCI converters and MCI non-converters, compared with the state-of-the-art methods."
      }
    ]
  },
  {
    "tid":"1240",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Best upgrade plans for single and multiple source-destination pairs.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we study Resource Constrained Best Upgrade Plan (BUP) computation in road network databases. Consider a transportation network (weighted graph) G where a subset of the edges are upgradable, i.e., for each such edge there is a cost, which if spent, the weight of the edge can be reduced to a specific new value. In the single-pair version of BUP, the input includes a source and a destination in G, and a budget B (resource constraint). The goal is to identify which upgradable edges should be upgraded so that the shortest path distance between source and destination (in the updated network) is minimized, without exceeding the available budget for the upgrade. In the multiple-pair version of BUP, a set Q of source-destination pairs is given, and the problem is to choose for upgrade those edges that lead to the smallest sum of shortest path distances across all pairs in Q, subject to budget constraint B. In addition to transportation networks, the BUP query arises in other domains too, such as telecommunications. We propose a framework for BUP processing and evaluate it with experiments on large, real road networks."
      }
    ]
  },
  {
    "tid":"1241",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A visual analysis of the process of process modeling' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The construction of business process models has become an important requisite in the analysis and optimization of processes. The success of the analysis and optimization efforts heavily depends on the quality of the models. Therefore, a research domain emerged that studies the process of process modeling. This paper contributes to this research by presenting a way of visualizing the different steps a modeler undertakes to construct a process model, in a so-called process of process modeling Chart. The graphical representation lowers the cognitive efforts to discover properties of the modeling process, which facilitates the research and the development of theory, training and tool support for improving model quality. The paper contains an extensive overview of applications of the tool that demonstrate its usefulness for research and practice and discusses the observations from the visualization in relation to other work. The visualization was evaluated through a qualitative study that confirmed its usefulness and added value compared to the Dotted Chart on which the visualization was inspired."
      }
    ]
  },
  {
    "tid":"1242",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'First in, best dressed: The presence of order-effect bias in journal ranking surveys.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The purpose of this study is to test for the presence of order-effect bias in journal ranking surveys. Data were obtained from 379 active knowledge management and intellectual capital researchers who rated 25 journals on a 7-point scale. Five different versions of the survey instrument were utilized. Consistent with the cognitive elaboration model, the satisficing theory, and the Gricean maxim of orderliness, order-effect bias was observed in journal ranking surveys. Journals that appear in the beginning of the ranking list delivered to survey respondents consistently receive higher scores than journals at the end of the list. Overall, the position of the journal in the list explains over 10% of its score. Therefore, authors of journal ranking studies are recommended to use multiple versions of the survey instrument with randomized journal orders. (C) 2012 Elsevier Ltd. All rights reserved."
      }
    ]
  },
  {
    "tid":"1243",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'State-Clustering Based Multiple Deep Neural Networks Modeling Approach for Speech Recognition' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The hybrid deep neural network (DNN) and hidden Markov model (HMM) has recently achieved dramatic performance gains in automatic speech recognition (ASR). The DNN-based acoustic model is very powerful but its learning process is extremely time-consuming. In this paper, we propose a novel DNN-based acoustic modeling framework for speech recognition, where the posterior probabilities of HMM states are computed from multiple DNNs (mDNN), instead of a single large DNN, for the purpose of parallel training towards faster turnaround. In the proposed mDNN method all tied HMM states are first grouped into several disjoint clusters based on data-driven methods. Next, several hierarchically structured DNNs are trained separately in parallel for these clusters using multiple computing units (e.g. GPUs). In decoding, the posterior probabilities of HMM states can be calculated by combining outputs from multiple DNNs. In this work, we have shown that the training procedure of the mDNN under popular criteria, including both frame-level cross-entropy and sequence-level discriminative training, can be parallelized efficiently to yield significant speedup. The training speedup is mainly attributed to the fact that multiple DNNs are parallelized over multiple GPUs and each DNN is smaller in size and trained by only a subset of training data. We have evaluated the proposed mDNN method on a 64-hour Mandarin transcription task and the 320-hour Switchboard task. Compared to the conventional DNN, a 4-cluster mDNN model with similar size can yield comparable recognition performance in Switchboard (only about 2% performance degradation) with a greater than 7 times speed improvement in CE training and a 2.9 times improvement in sequence training, when 4 GPUs are used."
      }
    ]
  },
  {
    "tid":"1244",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Tailoring Data Quality Models Using Social Network Preferences' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"To succeed in their tasks, users need to manage data with the most adequate quality levels possible according to specific data quality models. Typically, data quality assessment consists of calculating a synthesizing value by means of a weighted average of values and weights associated with each data quality dimension of the data quality model. We shall study not only the overall perception of the level of importance for the set of users carrying out similar tasks, but also the different issues that can influence the selection of the data quality dimensions for the model. The core contribution of this paper is a framework for representing and managing data quality models using social networks. The framework includes a proposal for a data model for social networks centered on data quality (DQSN), and an extensible set of operators based on soft-computing theories for corresponding operations."
      }
    ]
  },
  {
    "tid":"1245",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Search Result Prefetching on Desktop and Mobile.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Search result examination is an important part of searching. High page load latency for landing pages (clicked search results) can reduce the efficiency of the search process. Proactively prefetching landing pages in advance of clickthrough can save searchers valuable time. However, prefetching consumes resources (primarily bandwidth and battery) that are wasted unless the prefetched results are requested by searchers. Balancing the costs in prefetching particular results against the benefits in reduced latency to searchers represents the search result prefetching challenge. In this article, we introduce this challenge and present methods to address it in both desktop and mobile settings. Our methods leverage searchers\u2019 cursor movements (on desktop) and viewport-based viewing behavior (on mobile) on search engine result pages (SERPs) in real time to dynamically estimate the result that searchers will request next. We demonstrate through large-scale log analysis that our approach significantly outperforms three strong baselines that prefetch results based on (i) the search engine result ranking (prefetch top-ranked results), (ii) past SERP clicks from all searchers for the query (prefetch popular results), or (iii) past SERP clicks from the current searcher for the query (prefetch results that the searcher prefers). Our promising findings have implications for the design of search support in desktop and mobile settings that makes the search process more efficient."
      }
    ]
  },
  {
    "tid":"1246",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Convergence properties of distributed estimators for heterogeneous wireless sensor networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Wireless sensor networks (WSNs) are usually deployed for monitoring systems with the distributed detection and estimation of sensors. A consensus based estimation method is proposed in this paper for heterogeneous WSNs with two types of sensors. The convergence properties of the proposed estimators are analyzed under time-varying inputs. It is shown that the kind of sensors with high processing ability can exponentially converge to time-varying ambience signals with a bounded error ball. The radius of the ball can be obtained by a minimization problem. The results also imply that network topology has a great impact on the convergence of the distributed estimators. The simulation study illustrates the effectiveness of the proposed estimation method. \u00a9 2010 IEEE."
      }
    ]
  },
  {
    "tid":"1247",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On approximation of real-world influence spread' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"To find the most influential nodes for viral marketing, several models have been proposed to describe the influence propagation process. Among them, the Independent Cascade (IC) Model is most widely-studied. However, under IC model, computing influence spread (i.e., the expected number of nodes that will be influenced) for each given seed set has been proved to be #P-hard. To that end, in this paper, we propose GS algorithm for quick approximation of influence spread by solving a linear system, based on the fact that propagation probabilities in real-world social networks are usually quite small. Furthermore, for better approximation, we study the structural defect problem existing in networks, and correspondingly, propose enhanced algorithms, GSbyStep and SSSbyStep, by incorporating the Maximum Influence Path heuristic. Our algorithms are evaluated by extensive experiments on four social networks. Experimental results show that our algorithms can get better approximations to the IC model than the state-of-the-arts."
      }
    ]
  },
  {
    "tid":"1248",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A probabilistic semantic approach for discovering web services' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Service discovery is one of challenging issues in Service-Oriented computing. Currently, most of the existing service discovering and matching approaches are based on keywords-based strategy. However, this method is inefficient and time-consuming. In this paper, we present a novel approach for discovering web services. Based on the current dominating mechanisms of discovering and describing Web Services with UDDI and WSDL, the proposed approach utilizes Probabilistic Latent Semantic Analysis (PLSA) to capture semantic concepts hidden behind words in the query and advertisements in services so that services matching is expected to carry out at concept level. We also present related algorithms and preliminary experiments to evaluate the effectiveness of our approach."
      }
    ]
  },
  {
    "tid":"1249",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Data mining techniques for effective and scalable traffic analysis' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper describes a novel approach to traffic analysis in high speed networks based on data mining techniques. Data mining techniques are here applied as a means to effectively process the significant amount of captured data. The paper provides a first evaluation of the proposed approach in terms of its ability of extracting relevant information and its computational requirements. Such evaluation is based on experiments run on a prototypal implementation of the proposed approach."
      }
    ]
  },
  {
    "tid":"1250",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Content-Based Dynamic 3D Mosaics' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose a content-based 3D mosaic (CB3M) representation for long video sequences of 3D and dynamic scenes captured by a camera on a mobile platform. After a set of parallel-perspective (pushbroom) mosaics with varying viewing directions is generated, a multi-view, segmentation-based stereo matching algorithm is applied to extract parametric representations of the color, structure and motion of the dynamic and/or 3D objects in urban scenes. We use the fact that all the static objects obey the epipolar geometry of pushbroom stereo, whereas an independent moving object either violates the epipolar geometry if the motion is not in the direction of sensor motion or exhibits unusual 3D structures. The CB3M is a highly compressed visual representation for a very long video sequence of a dynamic 3D scene. More importantly, the CB3M representation has object contents of both 3D and motion. Experimental results are given for the CB3M construction for both simulated and real video sequences."
      }
    ]
  },
  {
    "tid":"1251",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A lightweight electronic signature scheme using Twitter.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In many application contexts, qualified electronic signature are difficult to adopt due to cost and technological reasons. As the European legislation admits the use of (non-qualified) electronic signatures in several cases, the design of new signature protocols with cheapness and usability features is a challenging issue. In this paper, we propose a new lightweight electronic signature protocol that does not require any public key infrastructure, cryptography and dedicated device, yet guaranteeing a good level of security. The protocol is conceived for closed domains of users, such as the case of documents exchanged between employees of a company. Signature and verification processes rely on the Twitter social network and do not require any changes of its features. A system prototype has been also designed and implemented to show that the adoption of our solution is both realistic and effective."
      }
    ]
  },
  {
    "tid":"1252",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A mobile butterfly-watching learning system for supporting independent learning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we describe the development of a mobile butterfly-watching learning (BWL) system which supports independent learners by offering a new pattern of outdoor mobile learning (or called as m-learning) activities. The proposed BWL system was designed using a wireless mobile ad-hoc learning environment. In our designed system, each individual learner has a wireless handheld device, which is a PDA (personal digital assistant) with an IEEE 802.11 wireless network card and a small-sized CCD camera. One instructor has a notebook computer with a wi-fi wireless LAN card which serves as the local server. The notebook has a complete butterfly database. All learners' wireless handheld devices and the notebook constitute a mobile ad-hoc learning environment. During the butterfly-watching activity, each learner takes a distinct butterfly picture, and wirelessly transmits the picture to the local server. A content-based butterfly-image retrieval technique is applied to search for the most closely matching butterfly information, which is returned in real time to the learner's wireless handheld device. To illustrate the effect of the BWL system, an outdoor BWL activity was actually performed at elementary school in Taiwan. This article reports on our attempts to develop a new m-learning system to support an independent learning model. We deeply believe that it is worth developing a mobile learning system to provide a possible independent-learning environment for outdoor learners."
      }
    ]
  },
  {
    "tid":"1253",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Parallel Bandit-Based Approach for Autotuning FPGA Compilation.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mainstream FPGA CAD tools provide an extensive collection of optimization options that have a significant impact on the quality of the final design. These options together create an enormous and complex design space that cannot effectively be explored by human effort alone. Instead, we propose to search this parameter space using autotuning, which is a popular approach in the compiler optimization domain. Specifically, we study the effectiveness of applying the multi-armed bandit (MAB) technique to automatically tune the options for a complete FPGA compilation flow from RTL to bitstream, including RTL/logic synthesis, technology mapping, placement, and routing. To mitigate the high runtime cost incurred by the complex FPGA implementation process, we devise an efficient parallelization scheme that enables multiple MAB-based autotuners to explore the design space simultaneously. In particular, we propose a dynamic solution space partitioning and resource allocation technique that intelligently allocates computing resources to promising search regions based on the runtime information of search quality from previous iterations. Experiments on academic and commercial FPGA CAD tools demonstrate promising improvements in quality and convergence rate across a variety of real-life designs."
      }
    ]
  },
  {
    "tid":"1254",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficacy of shared-control guidance paradigms for robot-mediated training' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Robot-mediated training has seen a plethora of implementations of shared-control haptic guidance, intended to teach novices to perform dynamic tasks by providing them with real-time visual and haptic feedback from real or virtual experts. The efficacies of these paradigms are difficult to quantify and compare, as the paradigms have typically been developed in an ad-hoc manner to suit specific devices and tasks. This work proposes a novel guidance paradigm taxonomy intended to help classify and compare the multitude of implementations in the literature, as well as a revised proxy rendering model to allow for the implementation of two relatively novel guidance paradigms (in addition to existing paradigms). The efficacies of these two paradigms, plus two more paradigms representing the vast majority of implementations in the literature, are compared in a controlled study with 50 healthy subjects. The results show that none of these paradigms are superior to visual-only guidance (the control condition), and that the two newer paradigms are actually detrimental to training for a target-hitting task. These results contradict many intuitions about how haptic guidance should be implemented and raise doubts about the efficacies of the most commonly-implemented paradigms."
      }
    ]
  },
  {
    "tid":"1255",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A robust incremental clustering-based facial feature tracking.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Display Omitted RICFFT offers a person-independent, automatic facial feature tracking system.17-point feature model enables emotion-specific facial features to capture dynamic facial muscle movement.Two incremental clustering algorithms are developed to build and update the appearance and model incrementally.Results show captured dynamic facial muscle movement with increased accuracy. Emerging significance of person-independent, emotion specific facial feature tracking has been actively tracked in the machine vision society for decades. Among distinct methods, the Constrained Local Model (CLM) has shown significant results in person-independent feature tracking. In this paper, we propose an automatic, efficient, and robust method for emotion specific facial feature detection and tracking from image sequences. A novel tracking system along with 17-point feature model on the frontal face region has also been proposed to facilitate the tracking of human basic facial expressions. The proposed feature tracking system keeps patch images and face shapes till certain number of key frames incorporating CLM-based tracker. After that, incremental patch and shape clustering algorithms is applied to build appearance model and structure model of similar patches and similar shapes respectively. The clusters in each model are built and updated incrementally and online, controlled by amount of facial muscle movement. The overall performance of the proposed Robust Incremental Clustering-based Facial Feature Tracking (RICFFT) is evaluated on the FGnet database and the Extended Cohn-Kanade (CK+) database. RICFFT demonstrates mean tracking accuracy of 97.45% and 96.64% for FGnet and CK+ database respectively. Also, RICFFT is more robust by minimizing average shape distortion error of 0.20% and 1.86% for FGnet and CK+ (apex frame) database, as compared with classic method CLM."
      }
    ]
  },
  {
    "tid":"1256",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A dynamic SVR\u2013ARMA model with improved fruit fly algorithm for the nonlinear fiber stretching process' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":" The fiber stretching process plays the key role in the process of fiber production and its effects is measured by the stretching ratio. The stretching ratio is determined by the relative speed of the winding roller. The stretching ratio has impact on the performance of the final fiber filament and production directly. Focused on the importance of the stretching ratio, the support vector regression (SVR) predictive model, called nonlinear auto-regressive exogenous model, for the fiber stretching rate based on existing industry data is proposed. Furthermore, the fruit fly optimization algorithm inspired by immune mechanism and cooperation functional (IFOA) is presented, and then is used to optimize the parameters in SVR. Furthermore, taking into account the high cost and accurate precision of the fiber stretching process, a time series autoregressive moving average (ARMA) model is introduced to reduce the prediction error of the IFOA\u2013SVR model. Simulations results demonstrate that the proposed IFOA\u2013SVR method can increase the prediction accuracy than the traditional FOA and the SVR method, and the ARMA model is essential to modify the prediction error of the IFOA\u2013SVR model."
      }
    ]
  },
  {
    "tid":"1257",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Weak Singular Hybrid Automata.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"  The framework of Hybrid automata, introduced by Alur, Courcourbetis, Henzinger, and Ho, provides a formal modeling and analysis environment to analyze the interaction between the discrete and the continuous parts of cyber-physical systems. Hybrid automata can be considered as generalizations of finite state automata augmented with a finite set of real-valued variables whose dynamics in each state is governed by a system of ordinary differential equations. Moreover, the discrete transitions of hybrid automata are guarded by constraints over the values of these real-valued variables, and enable discontinuous jumps in the evolution of these variables. Singular hybrid automata are a subclass of hybrid automata where dynamics is specified by state-dependent constant vectors. Henzinger, Kopke, Puri, and Varaiya showed that for even very restricted subclasses of singular hybrid automata, the fundamental verification questions, like reachability and schedulability, are undecidable. In this paper we present \\emph{weak singular hybrid automata} (WSHA), a previously unexplored subclass of singular hybrid automata, and show the decidability (and the exact complexity) of various verification questions for this class including reachability (NP-Complete) and LTL model-checking (PSPACE-Complete). We further show that extending WSHA with a single unrestricted clock or extending WSHA with unrestricted variable updates lead to undecidability of reachability problem. "
      }
    ]
  },
  {
    "tid":"1258",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Geometry-Based Pectoral Muscle Segmentation From MLO Mammogram Views.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Computer-aided diagnosis systems (CADx) play a major role in the early diagnosis of breast cancer. Extracting the breast region precisely from a mammogram is an essential component of CADx for mammography. The appearance of the pectoral muscle on medio-lateral oblique (MLO) views increases the false positive rate in CADx. Therefore, the pectoral muscle should be identified and removed from the bre..."
      }
    ]
  },
  {
    "tid":"1259",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Two-Stage Reranking for Remote Sensing Image Retrieval.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Image reranking is a popular postprocessing method for remote sensing image retrieval (RSIR), which aims at enhancing the initial retrieval performance. In general, it takes either users&#39; opinions or the relationships between images into consideration to find an optimal reranked list based on the initial retrieved results. In this paper, we present a reranking method for improving RSIR, which is n..."
      }
    ]
  },
  {
    "tid":"1260",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Shift-invariant discrete wavelet transform analysis for retinal image classification.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This work involves retinal image classification and a novel analysis system was developed. From the compressed domain, the\n proposed scheme extracts textural features from wavelet coefficients, which describe the relative homogeneity of localized\n areas of the retinal images. Since the discrete wavelet transform (DWT) is shift-variant, a shift-invariant DWT was explored\n to ensure that a robust feature set was extracted. To combat the small database size, linear discriminant analysis classification\n was used with the leave one out method. 38 normal and 48 abnormal (exudates, large drusens, fine drusens, choroidal neovascularization,\n central vein and artery occlusion, histoplasmosis, arteriosclerotic retinopathy, hemi-central retinal vein occlusion and more)\n were used and a specificity of 79% and sensitivity of 85.4% were achieved (the average classification rate is 82.2%). The\n success of the system can be accounted to the highly robust feature set which included translation, scale and semi-rotational,\n features. Additionally, this technique is database independent since the features were specifically tuned to the pathologies\n of the human eye."
      }
    ]
  },
  {
    "tid":"1261",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Novel Channel Estimation in T-DMB Receivers' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes noise reduction methods to improve the channel estimation performance in terrestrial-digital multimedia broadcasting (T-DMB) receivers. In the T-DMB, the channel estimation is performed by the pilot signal extraction and the channel coefficient estimation. Since the performance of the channel estimation highly depends on the estimated channel coefficients whose accuracy is associated with the received signal-to-noise ratio (SNR), it is important to reduce noise existing at the estimated channel coefficients. To reduce such noise components, various noise reduction methods of the estimated channel coefficients based on the Ad-point weighted moving average are proposed. Computer simulations show that the proposed methods improve the estimation performance by more than 2 to 3 dB in terms of the symbol error rate after equalization."
      }
    ]
  },
  {
    "tid":"1262",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'LBlock: a lightweight block cipher' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a new lightweight block cipher called LBlock. Similar to many other lightweight block ciphers, the block size of LBlock is 64-bit and the key size is 80-bit. Our security evaluation shows that LBlock can achieve enough security margin against known attacks, such as differential cryptanalysis, linear cryptanalysis, impossible differential cryptanalysis and related-key attacks etc. Furthermore, LBlock can be implemented efficiently not only in hardware environments but also in software platforms such as 8-bit microcontroller. Our hardware implementation of LBlock requires about 1320 GE on 0.18 \u00b5m technology with a throughput of 200 Kbps at 100 KHz. The software implementation of LBlock on 8-bit microcontroller requires about 3955 clock cycles to encrypt a plaintext block."
      }
    ]
  },
  {
    "tid":"1263",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Preemptive and non-preemptive scheduling of optical switches with configuration delay' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Utilizing optical technologies for the design of packet switches and routers offers several advantages in terms of scalability, high bandwidth, power consumption, and cost. However, the configuration delays of optical crossbars are much longer than that of the electronic counterpart, which makes the conventional slot-by-slot scheduling methods no longer the feasible solution. Therefore, some tradeoff must be found between the empty time slots and configuration overhead. This paper classifies such scheduling problems into preemptive and non-preemptive scenarios, each has its own advantages and disadvantages. Although non-preemptive scheduling is inherently not good at achieving the above-mentioned tradeoff, it is shown, however, that the proposed maximum weight matching (MWM) based greedy algorithm is guaranteed to achieve an approximation 2 for arbitrary configuration delay, and with a relatively low time complexity O(N2). For preemptive scheduling, a novel 2-approximation heuristic is presented. Each time in finding a switch configuration, the 2-approximation heuristic guarantees the covering cost of the remaining traffic matrix to have 2-approximation. Simulation results demonstrate that 2-approximation heuristic (1) performs close to the optimal scheduling, and (2) outperforms ADJUST and DOUBLE in terms of traffic transmission delay and time complexity."
      }
    ]
  },
  {
    "tid":"1264",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'dbPSP: a curated database for protein phosphorylation sites in prokaryotes.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"As one of the most important post-translational modifications, phosphorylation is highly involved in almost all of biological processes through temporally and spatially modifying substrate proteins. Recently, phosphorylation in prokaryotes attracted much attention for its critical roles in various cellular processes such as signal transduction. Thus, an integrative data resource of the prokaryotic phosphorylation will be useful for further analysis. In this study, we presented a curated database of phosphorylation sites in prokaryotes (dbPSP, Database URL: http://dbpsp.biocuckoo.org) for 96 prokaryotic organisms, which belong to 11 phyla in two domains including bacteria and archaea. From the scientific literature, we manually collected experimentally identified phosphorylation sites on seven types of residues, including serine, threonine, tyrosine, aspartic acid, histidine, cysteine and arginine. In total, the dbPSP database contains 7391 phosphorylation sites in 3750 prokaryotic proteins. With the dataset, the sequence preferences of the phosphorylation sites and functional annotations of the phosphoproteins were analyzed, while the results shows that there were obvious differences among the phosphorylation in bacteria, archaea and eukaryotes. All the phosphorylation sites were annotated with original references and other descriptions in the database, which could be easily accessed through user-friendly website interface including various search and browse options. Taken together, the dbPSP database provides a comprehensive data resource for further studies of protein phosphorylation in prokaryotes."
      }
    ]
  },
  {
    "tid":"1265",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Cyclops: an FSO-based wireless link for VR headsets' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The ultimate goal of virtual reality (VR) is to create an experience indistinguishable from actual reality. To provide such a \"life-like\" experience, (i) the VR headset (VRH) should be wireless so that the user can move around freely, and (ii) the wireless link, connecting the VRH to a high-performance renderer, should support high data rates (tens to hundreds of Gbps). Industry is already pushing towards such wireless VRHs; however, these wireless links can only support a few Gbps rates. In general, current radio-frequency (RF) links (including mmWave) are not able to provide desired data rates. In this paper, we build a system, we call Cyclops, which uses free-space optical (FSO) technology to create a high-bandwidth VR wireless link. FSO links are capable of very high data rates (up to Tbps) due to the high frequencies of light waves and narrow beams. The main challenges in developing an effective FSO link are: (i) designing a link with sufficient movement tolerance, and (ii) developing a viable tracking and pointing (TP) mechanism which maintains the link while the VRH moves. As traditional TP approaches seem infeasible in our context, we develop a novel TP approach based on learning techniques, leveraging the VRH's inbuilt tracking system. We build robust 10 Gbps and 25Gbps link prototypes from commodity components, demonstrate their viability for expected movement speeds of a VRH, and show that, with certain custom-built components, we can support much higher movement speeds and bandwidths."
      }
    ]
  },
  {
    "tid":"1266",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient Block-Division Model For Robust Multiple Object Tracking' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Tracking multiple objects under occlusion is one of the most challenging issues in computer vision. Occlusion results in mistaken match when finding the most similar candidate. Adapting to the change of objects is essential for tracking as objects often undergo intrinsic changes, but noise is unavoidably introduced during updating of the object, and this further confuses the tracker. In order to address these problems, a block-division appearance model is introduced to efficiently handle occlusion. In this model, spatial information is introduced to avoid the mistaken match between object and candidate. Based on this model, a selective updating strategy is proposed to incrementally learn the change of the object, avoiding introducing noise when updating. At the same time occlusion is deduced by monitoring the variation of each block. Experimental results in various videos validate the effectiveness of our algorithm in tracking multiple objects under occlusion."
      }
    ]
  },
  {
    "tid":"1267",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multi-pair two-way relay networks: Interference management using lattice codes and amplify and compute relaying' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we consider lattice codes for inter-pair interference cancellation in a multi-pair two-way relay network (TWRN) with amplify-and-compute relaying. We investigate a new method to separate the desired message from the interference signal by computing only two equations at the users. First, we formulate the sum rate for this lattice code based approach and find that it has better sum rate than the existing beamforming based approaches. Next, we analytically obtain the optimum power allocation coefficient for the users' and the relay's power to maximize the sum rate. We show that for larger number of user pairs, optimum power allocation allows more improvement compared to that for smaller number of user pairs."
      }
    ]
  },
  {
    "tid":"1268",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multi-objective evolutionary algorithms for energy-aware scheduling on distributed computing systems.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"HighlightsHeterogeneous, energy-aware precedence constrained (DAG) scheduling problem.Three multi-objective algorithms schemas are adapted: MOCell, NSGAII and IBEA.A representation with correspondi..."
      }
    ]
  },
  {
    "tid":"1269",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Effective Intercarrier Interference Reduction Techniques For Ofdm Underwater Acoustic Communications' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Fast time-varying, phase variation, and Doppler shift constitute the features of the underwater acoustic (UWA) communication channel. As a consequence, the effect of intercarrier interference (ICI) becomes significantly critical for orthogonal frequency division multiplexing (OFDM) based UWA communication systems. In this paper, an ICI countermeasure, termed two-path transmission, is proposed. The key idea is to employ data repetition within two concatenated OFDM blocks according to subcarrier mirror mapping criterion. Through theoretical analysis and numerical results, it is shown that the proposed two-path transmission schemes can effectively improve the carrier-to-interference power ratio (CIR) in the presence of carrier frequency offset (CFO) and additive white Gaussian noise (AWGN). Furthermore, a recent sea experiment conducted in Taiwan in May 2013 verifies the ICI suppression capability of the proposed schemes in UWA channels."
      }
    ]
  },
  {
    "tid":"1270",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mini-fingerprints detect similar activity of receptor ligands previously recognized only by three-dimensional pharmacophore-based methods.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mini-fingerprints (MFPs) are short binary bit string representations of molecular structure and properties, composed of few selected two-dimensional (2D) descriptors and a number of structural keys. MFPs were specifically designed to recognize compounds with similar activity. Here we report that MFPs an capable of detecting similar activities of some druglike molecules, including endothelin A antagonists and cll-adrenergic receptor ligands, the recognition of which was previously thought to depend on the use of multiple point three-dimensional (3D) pharmacophore methods. Thus, in these cases, MFPs and pharmacophore fingerprints produce similar results, although they define, in terms of their complexity, opposite ends of the spectrum of methods currently used to study molecular similarity or diversity. For each of the studied compound classes, comparison of MFP bit settings identified a consensus or signature pattern. Scaling factors can be applied to these bits in order to increase the probability of finding compounds with similar activity by virtual screening."
      }
    ]
  },
  {
    "tid":"1271",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'GPS-Based Message Broadcasting for Inter-vehicle Communication' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Intelligent Transportation Systems (ITS) have become a focus for many countries. To achieve ITS, Inter Vehicle Communication (IVC) is required for the exchange and distribution of data such as congestion or emergency information. If this communication can be done without fixed infrastructure, the systems can be deployed quickly and on a larger scale. Ad hoc networking technologies are one such technology to achieve IVC. However, if generic ad hoc network solutions are applied directly to IVC, performance can degrade quickly as the system scales particularly for broadcast type messages. In this paper, we propose two new broadcast protocols that reduce bandwidth required for broadcast communication by taking advantage of a vehicle's highly directional movement and Global Positioning Information. To show the performance of our new protocols, we compare our approach with generic ad hoc broadcasting techniques. Our results show that it is possible to achieve several hundred percent improvement of bandwidth utilization with very slight sacrifice of reachability."
      }
    ]
  },
  {
    "tid":"1272",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Content-Related Repairing of Inconsistencies in Distributed Data.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Conditional functional dependencies (CFDs) are a critical technique for detecting inconsistencies while they may ignore some potential inconsistencies without considering the content relationship of data. Content-related conditional functional dependencies (CCFDs) are a type of special CFDs, which combine content-related CFDs and detect potential inconsistencies by putting content-related data together. In the process of cleaning inconsistencies, detection and repairing are interactive: 1) detection catches inconsistencies, 2) repairing corrects caught inconsistencies while may bring new inconsistencies. Besides, data are often fragmented and distributed into multiple sites. It consequently costs expensive shipment for inconsistencies cleaning. In this paper, our aim is to repair inconsistencies in distributed content-related data. We propose a framework consisting of an inconsistencies detection method and an inconsistencies repairing method, which work iteratively. The detection method marks the violated CCFDs for computing the inconsistencies which should be repaired preferentially. Based on the repairing-cost model presented in this paper, we prove that the minimum-cost repairing using CCFDs is NP-complete. Therefore, the repairing method heuristically repairs the inconsistencies with minimum cost. To improve the efficiency and accuracy of repairing, we propose distinct values and rules sequences. Distinct values make less data shipments than real data for communication. Rules sequences determine appropriate repairing sequences to avoid some incorrect repairs. Our solution is proved to be more effective than CFDs by empirical evaluation on two real-life datasets."
      }
    ]
  },
  {
    "tid":"1273",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Tension distribution algorithm of a 7-DOF cable-driven robotic arm based on dynamic minimum pre-tightening force.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"From shoulder to wrist, a person's arm has a total of seven degrees of freedom (DOFs), which is the minimum number of DOF for the robot that needs to avoid obstacles and internal singularity. Compared to the traditional 7-DOF robot, the 7-DOF cable-driven robotic arm (CDRA) similar to human muscles parallel drive mode is the hybrid structure of serial-parallel, which possesses a number of promising advantages, such as simple and light-weight mechanical structure, high-loading capacity, and large reachable workspace. Since the cable can only generate tension and cannot stand pressure, cable-driven mechanism must be a redundant drive structure. Both the shoulder and wrist joint are redundant drive mechanism, and the tension of four cables has multiple solutions in the same posture. Hence, iteration Newton-Euler method is adopted to conduct dynamic analysis. Cable tension distribution algorithm based on null space method by solving Pseudo-inverse matrix is proposed, and in order to keep the cable tensional and also reduce energy consumption, the index of dynamic minimum pre-tightening force is originally proposed to realtime adjust the driving force of each cable. To show the accuracy and effectiveness of the proposed cable tension distribution algorithm, several simulation results are illustrated. These lay a good theory foundation for further research on effective control and performance improvement of the CDRA. \u00a9 2011 IEEE."
      }
    ]
  },
  {
    "tid":"1274",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Salient areas in natural scenes are generally regarded as areas which the human eye will typically focus on, and finding these areas is the key step in object detection. In computer vision, many models have been proposed to simulate the behavior of eyes such as SaliencyToolBox (STB), Neuromorphic Vision Toolkit (NVT), and others, but they demand high computational cost and computing useful results mostly relies on their choice of parameters. Although some region-based approaches were proposed to reduce the computational complexity of feature maps, these approaches still were not able to work in real time. Recently, a simple and fast approach called spectral residual (SR) was proposed, which uses the SR of the amplitude spectrum to calculate the image's saliency map. However, in our previous work, we pointed out that it is the phase spectrum, not the amplitude spectrum, of an image's Fourier transform that is key to calculating the location of salient areas, and proposed the phase spectrum of Fourier transform (PFT) model. In this paper, we present a quaternion representation of an image which is composed of intensity, color, and motion features. Based on the principle of PFT, a novel multiresolution spatiotemporal saliency detection model called phase spectrum of quaternion Fourier transform (PQFT) is proposed in this paper to calculate the spatiotemporal saliency map of an image by its quaternion representation. Distinct from other models, the added motion dimension allows the phase spectrum to represent spatiotemporal saliency in order to perform attention selection not only for images but also for videos. In addition, the PQFT model can compute the saliency map of an image under various resolutions from coarse to fine. Therefore, the hierarchical selectivity (HS) framework based on the PQFT model is introduced here to construct the tree structure representation of an image. With the help of HS, a model called multiresolution wavelet domain foveation (MWDF) is proposed in this paper to improve coding efficiency in image and video compression. Extensive tests of videos, natural images, and psychological patterns show that the proposed PQFT model is more effective in saliency detection and can predict eye fixations better than other state-of-the-art models in previous literature. Moreover, our model requires low computational cost and, therefore, can work in real time. Additional experiments on image and video compression show that the HS-MWDF model can achieve higher compression rate than the traditional model."
      }
    ]
  },
  {
    "tid":"1275",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Exploring Load-Balance To Dispatch Mobile Sensors In Wireless Sensor Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a hybrid sensor network consisting of static and mobile sensors is considered, where static sensors are used to detect events, and mobile sensors can move to event locations to conduct more advanced analysis. By exploring the load balance concept, we propose a CentralSD algorithm to efficiently dispatch mobile sensors. Our algorithm is general in that the numbers of mobile sensors and events can be arbitrary. When mobile sensors are more than event locations, we transform the dispatch problem to a maximum-matching problem in a weighted bipartite graph. When there are fewer mobile sensors than event locations, we propose an efficient clustering scheme to group event locations so that the maximum-matching approach can still be applied. To reduce message cost, we also develop a distributed GridSD algorithm. Simulation results are presented to verify the effectiveness of the proposed algorithms."
      }
    ]
  },
  {
    "tid":"1276",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The Design and Evaluation of Techniques for Route Diversity in Distributed Hash Tables' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present a replica placement scheme for any distributed hash table that uses a prefix-matching routing scheme and evaluate the number of replicas necessary to produce a desired number of disjoint routes. We show through simulation that this placement can make a significant improvement in routing robustness over other placements. Furthermore, we consider another route diversity mechanism that we call neighbor set routing and show that, when used with our replica placement, it can successfully route messages to a correct replica even with a quarter of the nodes in the system failed at random. Finally, we demonstrate a family of replica query strategies that can trade off response time and system load. We present a hybrid query strategy that keeps response time low without producing too high a load."
      }
    ]
  },
  {
    "tid":"1277",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Instruction set compiled simulation: a technique for fast and flexible instruction set simulation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Instruction set simulators are critical tools for the exploration and validation of new programmable architectures. Due to increasing complexity of the architectures and time-to-market pressure, performance is the most important feature of an instruction-set simulator. Interpretive simulators are flexible but slow, whereas compiled simulators deliver speed at the cost of flexibility. This paper presents a novel technique for generation of fast instruction set simulators that combines the benefit of both compiled and interpretive simulation. We achieve fast instruction accurate simulation through two mechanisms. First, we move the time consuming decoding process from run-time to compile time while maintaining the flexibility of the interpretive simulation. Second, we use a novel instruction abstraction technique to generate aggressively optimized decoded instructions that further improves simulation performance. Our instruction set compiled simulation (IS-CS) technique delivers upto 40% performance improvement over the best known published result that has the flexibility of interpretive simulation. We illustrate the applicability of the IS-CS technique using the ARM7 embedded processor."
      }
    ]
  },
  {
    "tid":"1278",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Attacking Image Classification Based On Bag-Of-Visual-Words' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Nowadays, with the widespread diffusion of online image databases, the possibility of easily searching, browsing and filtering image content is more than an urge. Typically, this operation is made possible thanks to the use of tags, i.e., textual representations of semantic concepts associated to the images. The tagging process is either performed by users, who manually label the images, or by automatic image classifiers, so as to reach a broader coverage. Typically, these methods rely on the extraction of local descriptors (e.g., SIFT, SURF, HOG, etc.), the construction of a suitable feature-based representation (e.g., bag-of-visual words), and the use of supervised classifiers (e.g., SVM). In this paper, we show that such a classification procedure can be attacked by a malicious user, who might be interested in altering the tags automatically suggested by the classifier. This might be used, for example, by an attacker who is willing to avoid the automatic detection of improper material in a parental control system. More specifically, we show that it is possible to modify an image in order to have it associated to the wrong class, without perceptually affecting the image visual quality. The proposed method is validated against a well known image dataset, and results prove to be promising, highlighting the need to jointly study the problem from the standpoint of both the analyst and the attacker."
      }
    ]
  },
  {
    "tid":"1279",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Enabling coexistence of heterogeneous wireless systems: case for ZigBee and WiFi' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The ISM spectrum is becoming increasingly populated by emerging wireless networks. Spectrum sharing among the same network of devices can be arbitrated by MAC protocols (e.g., CSMA), but the coexistence between heterogeneous networks remains a challenge. The disparate power levels, asynchronous time slots, and incompatible PHY layers of heterogeneous networks severely degrade the effectiveness of traditional MAC. In this paper, we propose a new mechanism, called the Cooperative Busy Tone (CBT), that enables the reliable coexistence between two such networks, ZigBee and WiFi. CBT allows a separate ZigBee node to schedule a busy tone concurrently with the desired transmission, thereby improving the visibility of ZigBee devices to WiFi. Its core components include a frequency flip scheme that prevents the mutual interference between cooperative ZigBee nodes, and a busy tone scheduler that minimizes the interference to WiFi, for both CSMA and TDMA packets. To optimize CBT, we establish an analytical framework that relates its key design parameters to performance and cost. Both the analytical and detailed simulation results demonstrate CBT's significant throughput improvement over the legacy ZigBee protocol, with negligible performance loss to WiFi. The results are validated further by implementing CBT on sensor motes and software radios."
      }
    ]
  },
  {
    "tid":"1280",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Locally adaptive MR intensity models and MRF-based segmentation of multiple sclerosis lesions' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Neuroimaging biomarkers are an important paraclinical tool used to characterize a number of neurological diseases, however, their extraction requires accurate and reliable segmentation of normal and pathological brain structures. For MR images of healthy brains the intensity models of normal-appearing brain tissue (NABT) in combination with Markov random field (MRF) models are known to give reliable and smooth NABT segmentation. However, the presence of pathology, MR intensity bias and natural tissue-dependent intensity variability altogether represent difficult challenges for a reliable estimation of NABT intensity model based on MR images. In this paper, we propose a novel method for segmentation of normal and pathological structures in brain MR images of multiple sclerosis (MS) patients that is based on locally-adaptive NABT model, a robust method for the estimation of model parameters and a MRF-based segmentation framework. Experiments on multi-sequence brain MR images of 27 MS patients show that, compared to whole-brain model and compared to the widely used Expectation-Maximization Segmentation (EMS) method, the locally-adaptive NABT model increases the accuracy of MS lesion segmentation."
      }
    ]
  },
  {
    "tid":"1281",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An efficient hole filling for depth image based rendering' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"An efficient hole filling algorithm is presented providing texture compilation for invisible pixels during depth image based rendering (DIBR). Some pixels cannot be assigned to proper texture values due to occluding foreground object; thus, compilation is required to obtain visually pleasing virtual views from a set of reference views. The proposed algorithm exploits reliable texture transition from visible pixels to the holes among horizontal and vertical directions. Transition is performed by successive weighted summation (SWS) based on color and depth similarities between the neighboring pixels, producing adaptively weighted and connected aggregation. Depending on local depth variation, assignment of proper texture to the hole pixels is achieved by combining horizontal and vertical information. According to the experimental results, proposed approach yields visually pleasing compilation of occluded regions with drastically decreased number of operations per pixel."
      }
    ]
  },
  {
    "tid":"1282",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Balancing Attended and Global Stimuli in Perceived Video Quality Assessment' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The visual attention mechanism plays a key role in the human perception system and it has a significant impact on our assessment of perceived video quality. In spite of receiving less attention from the viewers, unattended stimuli can still contribute to the understanding of the visual content. This paper proposes a quality model based on the late attention selection theory, assuming that the video quality is perceived via two mechanisms: global and local quality assessment. First we model several visual features influencing the visual attention in quality assessment scenarios to derive an attention map using appropriate fusion techniques. The global quality assessment as based on the assumption that viewers allocate their attention equally to the entire visual scene, is modeled by four carefully designed quality features. By employing these same quality features, the local quality model tuned by the attention map considers the degradations on the significantly attended stimuli. To generate the overall video quality score, global and local quality features are combined by a content adaptive linear fusion method and pooled over time, taking the temporal quality variation into consideration. The experimental results have been compared to results from appropriate eye tracking and video quality assessment experiments, demonstrating promising performance."
      }
    ]
  },
  {
    "tid":"1283",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Correlating Adverse Drug Reactions With Biological Pathways In Humans' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"It has been well recognized that adverse drug reactions (ADRs) are a significant cause of morbidity and mortality. There is a growing interest in investigating biological pathways involved in cellular response to drugs. Based on examining the co-occurrence of drugs in pathway activity and ADR profiles, in this paper, we propose a new method to explore the relationship between biological pathways and ADRs at a large scale. Using sparse canonical correlation analysis of 495 drugs with two profiles for 173 pathways and 1385 ADRs, a total of 80 correlated sets of pathways and ADRs were extracted. To evaluate the performance of our method, extracted correlated components were used to retrieve known ADR profiles from drug pathway profiles using a 5-fold cross validation. A relatively high prediction performance (AUC: 0.881) was achieved. This work provides a foundation for future investigation of ADRs in the context of biological pathways under different conditions."
      }
    ]
  },
  {
    "tid":"1284",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Resource Allocation in Energy-Cooperation Enabled Two-Tier NOMA HetNets Toward Green 5G.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper focuses on resource allocation in energy-cooperation enabled two-tier heterogeneous networks (HetNets) with non-orthogonal multiple access (NOMA), where base stations (BSs) are powered by both renewable energy sources and the conventional grid. Each BS can serve multiple users at the same time and frequency band. To deal with the fluctuation of renewable energy harvesting, we consider t..."
      }
    ]
  },
  {
    "tid":"1285",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Robots for Humanity: Using Assistive Robotics to Empower People with Disabilities.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":" Assistive mobile manipulators (AMMs) have the potential to one day serve as surrogates and helpers for people with disabilities, giving them the freedom to perform tasks such as scratching an itch, picking up a cup, or socializing with their families."
      }
    ]
  },
  {
    "tid":"1286",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Software Approach To Protecting Embedded System Memory From Single Event Upsets' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Radiation from radioactive environments, such as those encountered during space flight, can cause damage to embedded systems. One of the most common examples is the single event upset (SEU), which occurs when a high-energy ionizing particle passes through an integrated circuit, changing the value of a single bit by releasing its charge. The SEU could cause damage and potentially fatal failures to spacecraft and satellites. In this paper, we present an approach that extends the AVR-GCC compiler to protect the system stack from SEUs through duplication, validation, and recovery. Three applications are used to verify our approach, and the time and space overhead characteristics are evaluated."
      }
    ]
  },
  {
    "tid":"1287",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'You had me at hello: how phrasing affects memorability' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Understanding the ways in which information achieves widespread public awareness is a research question of significant interest. We consider whether, and how, the way in which the information is phrased --- the choice of words and sentence structure --- can affect this process. To this end, we develop an analysis framework and build a corpus of movie quotes, annotated with memorability information, in which we are able to control for both the speaker and the setting of the quotes. We find that there are significant differences between memorable and non-memorable quotes in several key dimensions, even after controlling for situational and contextual factors. One is lexical distinctiveness: in aggregate, memorable quotes use less common word choices, but at the same time are built upon a scaf-folding of common syntactic patterns. Another is that memorable quotes tend to be more general in ways that make them easy to apply in new contexts --- that is, more portable. We also show how the concept of \\\"memorable language\\\" can be extended across domains."
      }
    ]
  },
  {
    "tid":"1288",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Collaborative Work in Domain-Specific Discrete Event Simulation Software Development: Fleet Anti-air Defense Simulation Software' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Modeing and simulation (M&S) is an important method to evaluate numerous designs and operational concepts for a real-world system. If a system to be modeled is domain-specific, developing the simulation software of the system will require domain knowledge about the system as well as understanding the M&S methodology. This paper describes M&S stakeholders and proposes a collaborative work process in the development of domain-specific simulation software. M&S stakeholders are persons with their own professional knowledge: subject matter experts (SME), domain engineers, M&S engineers, platform engineers, and simulation data analysts. The M&S process consists of eight activities from defining modeling objectives to analyzing simulation data, and diverse M&S stakeholders work together in each activity. The M&S process is applied to develop a real-world M&S software development experience in a military domain. Through the proposed collaborative work process, the capabilities of the M&S stakeholders can be utilized maximally by seamlessly separating yet correlating their works."
      }
    ]
  },
  {
    "tid":"1289",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Intrusion-resilient identity-based signatures: Concrete scheme in the standard model and generic construction.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Key exposure problem is a very serious problem for identity-based signatures. Once the secret key is exposed in identity-based signatures, all signatures generated from this secret key will become insecure. To mitigate this serious damage, the intrusion-resilient mechanism has been introduced into identity-based signatures. However, all existing schemes can only be proven secure in random oracles. As we know, security proofs in random oracles do not always imply the security of actual schemes in the real world. In order to deal with this problem, in this paper, we propose the first intrusion-resilient identity-based signature (IRIBS) scheme that can be proven secure in the standard model. In the proposed scheme, the homomorphic structure in the key update is employed to refresh secret keys in one time period. It makes the scheme achieve the intrusion resilience when key exposure happens. In addition, we also provide the first solution for how to generically construct IRIBS schemes. We make use of the separable structure between the user's key material used for updating and that used for the actual signing. As a result, our solution can produce IRIBS schemes from forward-secure identity-based signature (FSIBS) schemes with a special property. This contribution will also simplify the future designs of IRIBS schemes and FSIBS schemes."
      }
    ]
  },
  {
    "tid":"1290",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Can you see what i hear?: the design and evaluation of a peripheral sound display for the deaf' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We developed two visual displays for providing awareness of environmental audio to deaf individuals. Based on fieldwork with deaf and hearing participants, we focused on supporting awareness of non-speech audio sounds such as ringing phones and knocking in a work environment. Unlike past work, our designs support both monitoring and notification of sounds, support discovery of new sounds, and do not require a priori knowledge of sounds to be detected. Our Spectrograph design shows pitch and amplitude, while our Positional Ripples design shows amplitude and location of sounds. A controlled experiment involving deaf participants found neither display to be significantly distracting. However, users preferred the Positional Ripples display and found that display easier to monitor (notification sounds were detected with 90% success in a laboratory setting). The Spectrograph display also supported successful detection in most cases, and was well received when deployed in the field."
      }
    ]
  },
  {
    "tid":"1291",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Linking the Semantics of BPEL Using Maude.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Web services have become more and more important in these years. It is of key importance for enterprise web applications to combine different services available to accomplish complex business process. BPEL4WS (BPEL) is the OASIS standard for web services composition and orchestration. It contains several distinct features, including scope-based compensation and fault handling mechanism. We have already studied the semantics for BPEL, including the operational semantics, algebraic semantics and their linking theory. This paper considers the mechanical approach to linking the operational semantics and algebraic semantics for BPEL. Our approach is to generate operational semantics from algebraic semantics, and to use equational and rewriting logic system Maude to mechanize the linking between the two semantics. Firstly, we investigate the algebraic laws in the Maude approach. Based on the algebraic semantics, the generation of head normal form is explored. Secondly, we consider the Maude approach to deriving the operational semantics from algebraic semantics, where the derivation strategy is based on the concept of head normal form. Our mechanical approach using Maude can visually show the head normal form of each program, as well as the execution steps of a program based on the derivation strategy. Finally, we also mechanize the derived operational semantics. The results mechanized from the second and third exploration indicate that the transition system of the derived operational semantics is the same as the one based on the derivation strategy."
      }
    ]
  },
  {
    "tid":"1292",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Cochlear Implant Filterbank Design and Optimization: A Simulation Study' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cochlear implants (CIs) are devices capable of restoring hearing function in profoundly-deaf patients to an acceptable degree of performance. An essential processing step in any cochlear implant is frequency analysis, which is usually performed via banks of filters. Here, we simulate and test the suitability of different filters and filterbank architectures for CIs with respect to their performance in speech intelligibility. Four different filters were implemented in an established model of CI hearing, the tone-excited vocoder, namely: GTF (Gammatone Filter), DAPGF (Differentiated All-Pole GTF), OZGF (One-Zero GTF) and BUTF (Butterworth). Three filterbank parameters, the filter order ( N), the filter quality factor ( Q) and the number of channels ( Ch), and their combinations were tested using objective and subjective metrics. Simulation results show that all filters tested are suitable for CI implementation, but that the choice of Q and N parameter values is crucial. For most conditions, optimal ( N,Q) combinations were within few units away from the combination (2, 4)."
      }
    ]
  },
  {
    "tid":"1293",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Robust neural network motion tracking control of piezoelectric actuation systems for micro/nanomanipulation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a robust neural network motion tracking control methodology for piezoelectric actuation systems employed in micro/nanomanipulation. This control methodology is proposed for tracking of desired motion trajectories in the presence of unknown system parameters, nonlinearities including the hysteresis effect and external disturbances in the control systems. In this paper, the related control issues are investigated, and a control methodology is established including the neural networks and a sliding control scheme. In particular, the radial basis function (RBF) neural networks are chosen for function approximations. The stability of the closed-loop system, as well as the convergence of the position and velocity tracking errors to zero, is assured by the control methodology in the presence of the aforementioned conditions. An offline learning procedure is also proposed for the improvement of the motion tracking performance. Precise tracking results of the proposed control methodology for a desired motion trajectory are demonstrated in the experimental study. With such a motion tracking capability, the proposed control methodology promises the realization of high-performance piezoelectric actuated micro/nanomanipulation systems."
      }
    ]
  },
  {
    "tid":"1294",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A simple and effective lossless compression algorithm for video display frames' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We propose a simple and effective lossless compression algorithm for video display frames. It combines a dictionary-based compression algorithm and the Huffman coding method to achieve a high compression ratio. We quantitatively analyze the characteristics of display frame data and propose the algorithm accordingly. We first use a dictionary-based algorithm and an adaptive quotient bit truncation method to generate codewords for all video pixels. Then, we employ the Huffman coding scheme to assign bit values to the codewords. Finally, we apply a simple algorithm to further reduce the size of the codeword bits. Compared with previous works, the proposed algorithm achieves at least 13% improvement in data reduction ratio."
      }
    ]
  },
  {
    "tid":"1295",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Accurate segmentation of ultrasound images using the motion cue' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Accurate segmentation of ultrasound images is desired to measure tissue shapes and sizes. But in some cases, it is difficult to segment ultrasound images purely based on static intensity values. In this paper, we propose a novel segmentation framework by using the motion cue. We integrate a new speckle tracking algorithm into a level set framework to iteratively refine the segmentation and motion estimation at the boundary regions of two different tissues. The balance between local motion contrast of different tissue regions and the smoothness constraint will drive the active contour to the boundaries of different tissues. Experiments on both simulated data and phantom data show that our method has achieved better results than previous methods."
      }
    ]
  },
  {
    "tid":"1296",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Enhancing Dataset Quality using Keys.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The Linked Data principles provide a decentral approach for publishing structured data in RDF on the Web. A consequence of this architectural choice is a high variance in the quality of the RDF datasets which constitute the Linked Data cloud. In this demo paper, we address a particular aspect of quality, i.e., the discriminability of resources. During our demo, we will present our simple three-step approach and interface, which allows data publishers to detect the resources in their dataset that are indistinguishable with respect to a given set of properties. Our approach is highly scalable as it relies on ROCKER, a novel algorithm for key discovery. Our evaluation on DBpedia suggests that even very commonly-used data sources are still in need to significant improvement to abide by the discriminability criterion."
      }
    ]
  },
  {
    "tid":"1297",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient All-to-All Broadcast in Gaussian On-Chip Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"With the development of multiprocessor system on chips (MPSoCs), it is expected that hundreds of computing cores will be operating on a single chip in the near future. This will require high-performance on-chip networks with very low latency to provide a communication substrate for the increasing number of cores. In this paper, we consider Gaussian on-chip networks that are of significant topological advantages over traditional mesh and torus networks in terms of diameter and average hop distance. Many applications on MPSoCs need global data movement and global control to exchange data and synchronize the execution among cores, which require all-to-all broadcast communication. In this paper, we propose an all-to-all broadcast algorithm suitable for on-chip implementation on the Gaussian network topology. The algorithm utilizes controlled message flooding based on a broadcast pattern, which can be described in a formal, generic way for each node in terms of a few simple operations and can be easily built into router hardware. Furthermore, the generic broadcast pattern also ensures a balanced traffic load in all dimensions in the network so that minimum total latency for all-to-all broadcast can be achieved. The algorithm overlaps message switching time with transmission time in a pipelined fashion to further reduce the total communication latency of all-to-all broadcast. Comparison results demonstrate the topological merits of Gaussian networks and ultralow latency of the proposed all-to-all broadcast algorithm."
      }
    ]
  },
  {
    "tid":"1298",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'caCORRECT2: Improving the accuracy and reliability of microarray data in the presence of artifacts.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In previous work, we reported the development of caCORRECT, a novel microarray quality control system built to identify and correct spatial artifacts commonly found on Affymetrix arrays. We have made recent improvements to caCORRECT, including the development of a model-based data-replacement strategy and integration with typical microarray workflows via caCORRECT's web portal and caBIG grid services. In this report, we demonstrate that caCORRECT improves the reproducibility and reliability of experimental results across several common Affymetrix microarray platforms. caCORRECT represents an advance over state-of-art quality control methods such as Harshlighting, and acts to improve gene expression calculation techniques such as PLIER, RMA and MAS5.0, because it incorporates spatial information into outlier detection as well as outlier information into probe normalization. The ability of caCORRECT to recover accurate gene expressions from low quality probe intensity data is assessed using a combination of real and synthetic artifacts with PCR follow-up confirmation and the affycomp spike in data. The caCORRECT tool can be accessed at the website: http://cacorrect.bme.gatech.edu."
      }
    ]
  },
  {
    "tid":"1299",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On multipath balancing and expanding for wireless multimedia sensor networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Multiple disjointed paths have been demonstrated to be effective in delivering multimedia traffic in Wireless Sensor Networks (WSNs), and improving the network performance in terms of bandwidth aggregation, reliability and network lifetime. In this paper, we investigate the use of Directional Geographical Routing (DGR) for multipath construction for multimedia data dissemination, and identify the challenging issue of achieving multipath balancing in proximity to the source/sink. The path construction is divided into expanding phase, parallel phase and converging phase in the proposed scheme, which includes two key algorithms, i.e., the detection algorithm for path construction phase and the deviation angle adjustment algorithm. Simulation results verify the effectiveness of the proposed scheme."
      }
    ]
  },
  {
    "tid":"1300",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A semantic approach to wireless networking testbed infrastructure' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"High market demand for wireless networking services and devices has resulted in increased complexity of an infrastructure of networking testbeds because of the scale, diversity and exploitation levels of the involved resources. In this paper, we address the problem of managing the infrastructure of large wireless networking testbed by adopting a semantics-based approach to knowledge-driven system management. A novel data transformation language, fully based on the semantic web standards, is introduced. Using the introduced language, an XML description of testbed services can be transformed to semantically enriched data. Workflow of the transformation is explained on the example of semantic lifting of the ORBIT testbed infrastructure. The example provides experimental evidence for successful application of the proposed approach."
      }
    ]
  },
  {
    "tid":"1301",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Model Transformation from the Palladio Component Model to Layered Queueing Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"For component-based performance engineering, software component developers individually create performance specifications of their components. Software architects compose these specifications to architectural models. This enables assessing the possible fulfilment of performance requirements without the need to purchase and deploy the component implementations. Many existing performance models do not support component-based performance engineering but offer efficient solvers. On the other hand, component-based performance engineering approaches often lack tool support. We present a model transformation combining the advanced component concepts of the Palladio Component Model (PCM) with the efficient performance solvers of Layered Queueing Networks (LQN). Joining the tool-set for PCM specifications with the tool-set for LQN solution is an important step to carry component-based performance engineering into industrial practice. We validate the correctness of the transformation by mapping the PCM model of a component-based architecture to an LQN and conduct performance predictions."
      }
    ]
  },
  {
    "tid":"1302",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Automatic Abstract Tag Detection for Social Image Tag Refinement and Enrichment' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Collaborative image tagging systems, such as Flickr, are very attractive for supporting keyword-based image retrieval, but some user-provided tags of collaboratively-tagged social images might be imprecise. Some people may use general or high-level words (i.e., abstract tags) to tag their images for saving time and effort, but such general or high-level tags are too abstract to describe the visual content of social images precisely. As a result, users may not be able to find what they need when they use the specific keywords for query specification. To tackle the problem of abstract tags, an ontology with three-level semantics is constructed for detecting the candidates of abstract tags from large-scale social images. Then the image context (nearest neighbors) and tag context (most relevant tags) of social images with abstract tags are used to ultimately confirm whether these candidates are abstract or not and identify the specific tags which can further depict the images with abstract tags. In addition, all the relevant tags, which correspond with intermediate nodes between the abstract tags and specific tags on our concept ontology, are added to enrich the tags of social images so that users can have more choices to select various keywords for query specification. We have tested our proposed algorithms on two types of data sets (revised standard datasets and self-constructed dataset) and compared our approach with other approaches."
      }
    ]
  },
  {
    "tid":"1303",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'ReDAL: An Efficient and Practical Request Distribution Technique for Application Server Clusters' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Modern web-based application infrastructures are based on clustered, multi-tiered architectures, where request distribution occurs in two sequential stages: over a cluster of web servers, and over a cluster of application servers. Much work has focused on strategies for distributing requests across a web server cluster in order to improve the overall throughput across the cluster. The strategies applied at the application layer are the same as those at the web server layer, because itis assumed that they transfer directly.In this paper, we argue that the problem of distributing requests across an application server cluster is fundamentally different from the web server request distribution problem, due to core differences in request processing in web and application servers. We devise an approach for distributing requests across a cluster of application servers such that overall system throughput is enhanced, and load across the application servers is balanced."
      }
    ]
  },
  {
    "tid":"1304",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Towards Adapting Parallel Programs to Different Platforms: Identifying Interaction Patterns' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Modern parallel computing platforms exhibit substantial variation in communication performance between on-socket, on-node and inter-rack locations. When application interaction patterns are irregular, communication-aware process placement on such platforms can be critical for overall runtime. Understanding program interaction patterns can be valuable in order to minimize the impact of capacity variations in communications parameters. Such knowledge can also be useful in selecting the best execution platform from the available options. In this paper we propose an approach based on source code analysis which identifies parallel application communication patterns such as star, ring, mesh, or torus. Our implementation based on ROSE framework was tested on various parallel programs that exhibit different communication patterns. We present the accuracy as well as the limitations of our static identification approach. We envisage augmenting our approach with trace information from pilot runs and best effort approaches to determine a process interaction graph."
      }
    ]
  },
  {
    "tid":"1305",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A comparison of SLAM algorithms based on a graph of relations' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we address the problem of creating an objective benchmark for comparing SLAM approaches. We propose a framework for analyzing the results of SLAM approaches based on a metric for measuring the error of the corrected trajectory. The metric uses only relative relations between poses and does not rely on a global reference frame. The idea is related to graph-based SLAM approaches in the sense that it considers the energy needed to deform the trajectory estimated by a SLAM approach to the ground truth trajectory. Our method enables us to compare SLAM approaches that use different estimation techniques or different sensor modalities since all computations are made based on the corrected trajectory of the robot. We provide sets of relative relations needed to compute our metric for an extensive set of datasets frequently used in the SLAM community. The relations have been obtained by manually matching laser-range observations. We believe that our benchmarking framework allows the user an easy analysis and objective comparisons between different SLAM approaches."
      }
    ]
  },
  {
    "tid":"1306",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Predicting Network Response Times Using Social Information' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Social networks and discussion boards have become a significant outlet where people communicate and express their opinion freely. Although the social networks themselves are usually well-provisioned, the participating users frequently point to external links to substantiate their discussions. Unfortunately, the sudden heavy traffic load imposed on the external, linked web sites causes them to become unresponsive leading to the \"Flash Crowds\" effect. In this paper, we quantify the prevalence of flash crowd events for a popular social discussion board (Digg). We measured the response times of 1289 unique popular websites. We were able to verify that 89% of the popular URLs suffered variations in their response times. By analyzing the content and structure of the social discussions, we were able to forecast accurately for 86% of the popular web sites within 5 minutes of their submission and 95% of the sites when more (5 hours) of social content became available. Our work indicates that we can effectively leverage social activity to forecast network events that will be otherwise infeasible to anticipate."
      }
    ]
  },
  {
    "tid":"1307",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Exploring syntactic features for relation extraction using a convolution tree kernel' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes to use a convolution kernel over parse trees to model syntactic structure information for relation extraction. Our study reveals that the syntactic structure features embedded in a parse tree are very effective for relation extraction and these features can be well captured by the convolution tree kernel. Evaluation on the ACE 2003 corpus shows that the convolution kernel over parse trees can achieve comparable performance with the previous best-reported feature-based methods on the 24 ACE relation subtypes. It also shows that our method significantly outperforms the previous two dependency tree kernels on the 5 ACE relation major types."
      }
    ]
  },
  {
    "tid":"1308",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'DUSC: Dimensionality Unbiased Subspace Clustering' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"To gain insight into today's large data resources, data mining provides automatic aggregation techniques. Clustering aims at grouping data such that objects within groups are similar while objects in different groups are dissimilar. In scenarios with many attributes or with noise, clusters are often hidden in subspaces of the data and do not show up in the full dimensional space. For these applications, subspace clustering methods aim at detecting clusters in any sub- space. Existing subspace clustering approaches fall prey to an effect we call dimensionality bias. As dimensionality of subspaces varies, approaches which do not take this effect into account fail to separate clusters from noise. We give a formal definition of dimensionality bias and analyze consequences for subspace clustering. A dimensionality unbiased subspace clustering (DUSC) definition based on statistical foundations is proposed. In thorough experiments on synthetic and real world data, we show that our approach outperforms existing subspace clustering algorithms."
      }
    ]
  },
  {
    "tid":"1309",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Trend cluster based compression of geographically distributed data streams' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In many real-time applications, such as wireless sensor network monitoring, traffic control or health monitoring systems, it is required to analyze continuous and unbounded geographically distributed streams of data (e.g. temperature or humidity measurements transmitted by sensors of weather stations). Storing and querying geo-referenced stream data poses specific challenges both in time (real-time processing) and in space (limited storage capacity). Summarization algorithms can be used to reduce the amount of data to be permanently stored into a data warehouse without losing information for further subsequent analysis. In this paper we present a framework in which data streams are seen as time-varying realizations of stochastic processes. Signal compression techniques, based on transformed domains, are applied and compared with a geometrical segmentation in terms of compression efficiency and accuracy in the subsequent reconstruction."
      }
    ]
  },
  {
    "tid":"1310",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Finding event-oriented patterns in long temporal sequences' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A major task of traditional temporal event sequence mining is to find all frequent event patterns from a long temporal sequence. In many real applications, however, events are often grouped into different types, and not all types are of equal importance. In this paper, we consider the problem of efficient mining of temporal event sequences which lead to an instance of a specific type of event. Temporal constraints are used to ensure sensibility of the mining results. We will first generalise and formalise the problem of event-oriented temporal sequence data mining. After discussing some unique issues in this new problem, we give a set of criteria, which are adapted from traditional data mining techniques, to measure the quality of patterns to be discovered. Finally we present an algorithm to discover potentially interesting patterns."
      }
    ]
  },
  {
    "tid":"1311",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'RDMS: A hardware task scheduling algorithm for Reconfigurable Computing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Reconfigurable computers (RC) can provide significant performance improvement for domain applications. However, wide acceptance of today's RCs among domain scientist is hindered by the complexity of design tools and the required hardware design experience. Recent developments in HW/SW co-design methodologies for these systems provide the ease of use, but they are not comparable in performance to manual co-design. This paper aims at improving the overall performance of hardware tasks assigned to FPGA devices by minimizing both the communication overhead and configuration overhead, which are introduced by using FPGA devices. The proposed reduced data movement scheduling (RDMS) algorithm takes data dependency among tasks, hardware task resource utilization, and inter-task communication into account during the scheduling process and adopts a dynamic programming approach to reduce the communication between muP and FPGA co-processor and the number of FPGA configurations to a minimum. Compared to two other approaches that consider data dependency and hardware resource utilization only, RDMS algorithm can reduce inter-configuration communication time by 11% and 44% respectively based on simulation using randomly generated data flow graphs. The implementation of RDMS on a real-life application, N-body simulation, verifies the efficiency of RDMS algorithm against other approaches."
      }
    ]
  },
  {
    "tid":"1312",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mechanism design for dynamic P2P streaming' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In dynamic streaming, a user can dynamically choose from different versions of the same video. In P2P dynamic streaming, there is one P2P swarm for each version, and within a swarm, peers can share video chunks with each other, thereby reducing the server's bandwidth cost. Due to economy of scale, cooperation among peers can also reduce the per-peer content price. In this paper, we use cooperative game theory to dynamically assign each peer to a version. To maximally incentivize peer cooperation, we use mechanism design to develop pricing schemes that reflect content and bandwidth cost savings derived from peer cooperation. With this approach, each peer is assigned to a swarm that is commensurate with its upload contribution and the price it is willing to pay. We also develop and simulate a distributed dynamic P2P streaming algorithm, consisting of chunk scheduling, token-based accounting, and video version switching, to dynamically adjust each peer's video version based on the collaborative behaviors of all peers."
      }
    ]
  },
  {
    "tid":"1313",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Remote execution vs. simplification for mobile real-time computer vision' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mobile implementations of computationally complex algorithms are often prohibitive due to performance constraints. There are two possible solutions for this: (1) adopting a faster but less powerful approach which results in a loss of accuracy or robustness. (2) using remote data processing which suffers from limited bandwidth and communication latencies and is difficult to implement in real-time interactive applications. Using the example of a mobile Augmented Reality application, we investigate those two approaches and compare them in terms of performance. We examine different workload balances ranging from extensive remote execution to pure onboard processing. The performance behavior is systematically analyzed under different network qualities and device capabilities. We found that even with a fast network connection, optimizing for maximum offload (thin-client configuration) is at a disadvantage compared to splitting the workload between remote system and client. Compared to remote execution, a simplified onboard algorithm is only preferable if the classification data set is below a certain size."
      }
    ]
  },
  {
    "tid":"1314",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Robust degree of exponential stability in polynomially uncertain overlapping control systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, necessary and sufficient conditions are provided for the existence of a structurally constrained controller in order to achieve a desired rate of exponential stability. It is assumed that a finite-dimensional linear time-invariant (LTI) system with uncertain parameters which belong to a known domain is given. The objective is to check if the poles of the closed-loop system can be placed in a desired region in the complex plane, for all values of the uncertain parameters in the known domain of uncertainty. The problem is investigated in both cases of LTI and linear time-varying (LTV) control laws. The results can be very useful in the design of overlapping control systems, when it is desired to maintain a prescribed transient performance for all permissible values of the uncertain parameters. The efficacy of the proposed technique is demonstrated by an example."
      }
    ]
  },
  {
    "tid":"1315",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Enzymatic target identification with dynamic states' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"As a metabolic network reaches from a state to a steady state, a subset of its fluxes gradually change. The sequence of intermediate states, called the dynamic state, shows the pattern that the underlying network follows to reach the steady state Understanding this pattern is crucial for metabolic engineering as the intermediate states can lead to undesired effects such as toxicity although the final steady state is a desired one. In this paper, we consider the problem of enzymatic target identification in metabolic networks. Unlike existing strategies, we consider the dynamic behavior of the state changes of the networks. Given a goal pattern for the fluxes of a given network, we aim to find the set of enzyme knockouts that will produce a dynamic state similar to that goal pattern. We consider three distance functions to measure the difference between two dynamic states. These are the Euclidean distance, time-warping distance and pattern distance. Euclidean distance restricts the solution space to the exact goal state. Time-warping distance allows for stretching of the goal pattern in the time domain. Pattern distance allows scaling and shifting of the goal flux in addition to stretching in the time domain. We provide a branch and bound method to solve this problem. We also develop a partitioning strategy to reduce the running time of our method. This strategy avoids constructing the entire dynamic state by computing a lower bound to the distance between two dynamic states when the entire dynamic state is not available. Our experiments on the purine metabolism show that our method runs accurately. They also show that our partitioning strategy reduces the number of time intervals computed for dynamic states by a factor of 2 to 6."
      }
    ]
  },
  {
    "tid":"1316",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'APDL: A Processor Description Language For Design Space Exploration of Embedded Processors' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Abstract\u2014This  paper  presents  Anahita  Processor  Description  Language (APDL)  for  generation  of  retargetable  processor  de- sign tool sets. The emphasis,is onthe applicability of the gener- ated tools  in  the ,design  space  exploration  (DSE)  phase  of  de- signing a new,embedded,processor. APDL descriptions can be used for  generating  cycle-accurate  instruction  set  simulators,  assembler/disassembler tools, production quality compilers and architecture verification  tools.  The  paper  first  investigates  the  features required for a language,to be,useful for DSE and then presents APDL constructs along with code samples.  Index Terms\u2014 architecture description languages, application"
      }
    ]
  },
  {
    "tid":"1317",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'SplicingCompass: differential splicing detection using RNA-seq data.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Motivation: Alternative splicing is central for cellular processes and substantially increases transcriptome and proteome diversity. Aberrant splicing events often have pathological consequences and are associated with various diseases and cancer types. The emergence of next-generation RNA sequencing (RNA-seq) provides an exciting new technology to analyse alternative splicing on a large scale. However, algorithms that enable the analysis of alternative splicing from short-read sequencing are not fully established yet and there are still no standard solutions available for a variety of data analysis tasks. Results: We present a new method and software to predict genes that are differentially spliced between two different conditions using RNA-seq data. Our method uses geometric angles between the high dimensional vectors of exon read counts. With this, differential splicing can be detected even if the splicing events are composed of higher complexity and involve previously unknown splicing patterns. We applied our approach to two case studies including neuroblastoma tumour data with favourable and unfavourable clinical courses. We show the validity of our predictions as well as the applicability of our method in the context of patient clustering. We verified our predictions by several methods including simulated experiments and complementary in silico analyses. We found a significant number of exons with specific regulatory splicing factor motifs for predicted genes and a substantial number of publications linking those genes to alternative splicing. Furthermore, we could successfully exploit splicing information to cluster tissues and patients. Finally, we found additional evidence of splicing diversity for many predicted genes in normalized read coverage plots and in reads that span exon-exon junctions."
      }
    ]
  },
  {
    "tid":"1318",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Reading Scene Text in Deep Convolutional Sequences.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We develop a Deep-Text Recurrent Network (DTRN) that regards scene text reading as a sequence labelling problem. We leverage recent advances of deep convolutional neural networks to generate an ordered high-level sequence from a whole word image, avoiding the difficult character segmentation problem. Then a deep recurrent model, building on long short-term memory (LSTM), is developed to robustly recognize the generated CNN sequences, departing from most existing approaches recognising each character independently. Our model has a number of appealing properties in comparison to existing scene text recognition methods: (i) It can recognise highly ambiguous words by leveraging meaningful context information, allowing it to work reliably without either pre- or post-processing; (ii) the deep CNN feature is robust to various image distortions; (iii) it retains the explicit order information in word image, which is essential to discriminate word strings; (iv) the model does not depend on pre-defined dictionary, and it can process unknown words and arbitrary strings. It achieves impressive results on several benchmarks, advancing the-state-of-the-art substantially."
      }
    ]
  },
  {
    "tid":"1319",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'HRF Estimation Improves Sensitivity of fMRI Encoding and Decoding Models.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Extracting activation patterns from functional Magnetic Resonance Images (fMRI) datasets remains challenging in rapid-event designs due to the inherent delay of blood oxygen level-dependent (BOLD) signal. The general linear model (GLM) allows to estimate the activation from a design matrix and a fixed hemodynamic response function (HRF). However, the HRF is known to vary substantially between subjects and brain regions. In this paper, we propose a model for jointly estimating the hemodynamic response function (HRF) and the activation patterns via a low-rank representation of task effects. This model is based on the linearity assumption behind the GLM and can be computed using standard gradient-based solvers. We use the activation patterns computed by our model as input data for encoding and decoding studies and report performance improvement in both settings."
      }
    ]
  },
  {
    "tid":"1320",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title '3D Surface Matching with Mutual Information and Riemann Surface Structures' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Many medical imaging applications require the computa-tion of dense correspondence vector fields that match one surface with another. Surface-based registration is use-ful for tracking brain change, registering functional imag-ing data from multiple subjects, and for creating statistical shape models of anatomy. To avoid the need for a large set of manually-defined landmarks to constrain these surface correspondences, we developed an algorithm to automate the matching of surface features. It extends the mutual in-formation method to automatically match general 3D sur-faces (including surfaces with a branching topology). We use diffeomorphic flows to optimally align the Riemann surface structures of two surfaces. First, we use holomor-phic 1-forms to induce consistent conformal grids on both surfaces. High genus surfaces are mapped to a set of rectan-gles in the Euclidean plane, and closed genus-zero surfaces are mapped to the sphere. Next, we compute stable geo-metric features (mean curvature and the conformal factor) and pull them back as scalar fields onto the 2D parameter domains. Mutual information is used as a cost functional to drive a fluid flow in the parameter domain that optimally aligns these surface features. A diffeomorphic surface-to-surface mapping is then recovered that matches anatomy in 3D. Lastly, we present a spectral method that ensures that the grids induced on the target surface remain conformal when pulled through the correspondence field. Using the chain rule, we express the gradient of the mutual informa-tion between surfaces in the conformal basis of the source surface. This finite-dimensional linear space generates all conformal reparameterizations of the surface. Illustrative experiments show the method applied to hippocampal sur-face registration, a key step in subcortical shape analysis in Alzheimer's disease and schizophrenia. KEY WORDS Surface Matching, Riemann Surface Structure, Mutual In-formation "
      }
    ]
  },
  {
    "tid":"1321",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Combining event-and state-based norms' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Institutions offer a mechanism to regulate the behaviour of agents without the need for these agents to internalise the norms of the system. Current formalisms can be divided in two groups depending on whether norms are expressed on the state of the normative structure or the events that bring about normative change. This paper argues that for complex systems both types are needed. To this extend, we introduce formal, a formal model for institutions incorporating the concepts for both event- and state-based normative modelling. We demonstrate our approach with a simplified legal case-study."
      }
    ]
  },
  {
    "tid":"1322",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Recoverability For Holevo'S Just-As-Good Fidelity' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Holevo's just-as-good fidelity is a similarity measure for quantum states that has found several applications. One of its critical properties is that it obeys a data processing inequality: the measure does not decrease under the action of a quantum channel on the underlying states. In this paper, I prove a refinement of this data processing inequality that includes an additional term related to recoverability. That is, if the increase in the measure is small after the action of a partial trace, then one of the states can be nearly recovered by the Petz recovery channel, while the other state is perfectly recovered by the same channel. The refinement is given in terms of the trace distance of one of the states to its recovered version and also depends on the minimum eigenvalue of the other state. As such, the refinement is universal, in the sense that the recovery channel depends only on one of the states, and it is explicit, given by the Petz recovery channel."
      }
    ]
  },
  {
    "tid":"1323",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Identifying co-regulating microRNA groups.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Current miRNA target prediction tools have the common problem that their false positive rate is high. This renders identification of co-regulating groups of miRNAs and target genes unreliable. In this study, we describe a procedure to identify highly probable co-regulating miRNAs and the corresponding co-regulated gene groups. Our procedure involves a sequence of statistical tests: (1) identify genes that are highly probable miRNA targets; (2) determine for each such gene, the minimum number of miRNAs that co-regulate it with high probability; (3) find, for each such gene, the combination of the determined minimum size of miRNAs that co-regulate it with the lowest p-value; and (4) discover for each such combination of miRNAs, the group of genes that are co-regulated by these miRNAs with the lowest p-value computed based on GO term annotations of the genes.Our method identifies 4, 3 and 2-term miRNA groups that co-regulate gene groups of size at least 3 in human. Our result suggests some interesting hypothesis on the functional role of several miRNAs through a \"guilt by association\" reasoning. For example, miR-130, miR-19 and miR-101 are known neurodegenerative diseases associated miRNAs. Our 3-term miRNA table shows that miR-130/19/101 form a co-regulating group of rank 22 (p-value =1.16 x 10(-2)). Since miR-144 is co-regulating with miR-130, miR-19 and miR-101 of rank 4 (p-value = 1.16 x 10(-2)) in our 4-term miRNA table, this suggests hsa-miR-144 may be neurodegenerative diseases related miRNA.This work identifies highly probable co-regulating miRNAs, which are refined from the prediction by computational tools using (1) signal-to-noise ratio to get high accurate regulating miRNAs for every gene, and (2) Gene Ontology to obtain functional related co-regulating miRNA groups. Our result has partly been supported by biological experiments. Based on prediction by TargetScanS, we found highly probable target gene groups in the Supplementary Information. This result might help biologists to find small set of miRNAs for genes of interest rather than huge amount of miRNA set.http://www.deakin.edu.au/~phoebe/JBCBAnChen/JBCB.htm."
      }
    ]
  },
  {
    "tid":"1324",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Accelerometer-Based Tilt Determination for Rigid Bodies With a Nonaccelerated Pivot Point.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"An estimation algorithm is proposed for determining pitch and roll angles (tilt), angular velocities, and angular accelerations of a rigid body with a nonaccelerated pivot point. The estimation uses only accelerometer measurements. It is based on a kinematic model of the rigid body and is, therefore, independent of its dynamics; only the mounting positions of the sensors need to be known. Simulati..."
      }
    ]
  },
  {
    "tid":"1325",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimal Sleeping Mechanism for Multiple Servers with MMPP-Based Bursty Traffic Arrival.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A fundamental problem in green communications and networking is the operation of servers (routers or base stations) with sleeping mechanism to optimize energy-delay tradeoffs. This problem is very challenging when considering realistic bursty (non-Poisson) traffic. We prove for the first time that the optimal structure of such a sleeping mechanism for multiple servers when the arrival of jobs is m..."
      }
    ]
  },
  {
    "tid":"1326",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient and self-tuning incremental query expansion for top-k query processing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present a novel approach for efficient and self-tuning query expansion that is embedded into a top-k query processor with candidate pruning. Traditional query expansion methods select expansion terms whose thematic similarity to the original query terms is above some specified threshold, thus generating a disjunctive query with much higher dimensionality. This poses three major problems: 1) the need for hand-tuning the expansion threshold, 2) the potential topic dilution with overly aggressive expansion, and 3) the drastically increased execution cost of a high-dimensional query. The method developed in this paper addresses all three problems by dynamically and incrementally merging the inverted lists for the potential expansion terms with the lists for the original query terms. A priority queue is used for maintaining result candidates, the pruning of candidates is based on Fagin's family of top-k algorithms, and optionally probabilistic estimators of candidate scores can be used for additional pruning. Experiments on the TREC collections for the 2004 Robust and Terabyte tracks demonstrate the increased efficiency, effectiveness, and scalability of our approach."
      }
    ]
  },
  {
    "tid":"1327",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On the resource allocation for multi-relay cognitive radio systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we investigate the resource allocation problem in an OFDM-based cognitive radio system with multiple relays. We address the formulated intractable problem by a two-stage procedure which corresponds to two subproblems: subchannel allocation and power distribution. For each subproblem, a fast algorithm is proposed by exploiting its special structure. Our proposed algorithms can achieve performance close to the upper bound. The remarkable characteristic of the proposed algorithm is that the approximate linear complexity is significantly lower than that of standard techniques, making it promising for applications. Simulation results also validate the superior performance of the proposed algorithm."
      }
    ]
  },
  {
    "tid":"1328",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Tuning Adaptive Computations for Performance Improvement of Autonomic Middleware in PaaS Cloud' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In a cloud platform belonging to the PaaS (Platform as a Service) category, autonomic middleware have become the fundamental part of a cloud node. An autonomic middleware can perform adaptive computations for self-management of the system. However, these adaptive computations consume resources such as CPU and memory, and can interfere with each other and also with normal business functions of the system due to resource competition, especially when the system is under heavy load. As a result, the adaptive computations should be tuned from the perspective of resource management. In this position paper, we propose an approach to tuning the autonomic levels and thus controlling the resource costs of the adaptive computations in an autonomic middleware of PaaS cloud, so as to guarantee the system's performance when resources are competed."
      }
    ]
  },
  {
    "tid":"1329",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A DSL for specifying run-time adaptations for embedded systems: an application to vehicle stereo navigation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The traditional approach for specifying adaptive behavior in embedded applications requires developers to engage in error-prone programming tasks. This results in long design cycles and in the inherent inability to explore and evaluate a wide variety of alternative adaptation behaviors, critical for systems exposed to dynamic operational and situational environments. In this paper, we introduce a domain-specific language (DSL) for specifying and implementing run-time adaptable application behavior. We illustrate our approach using a real-life stereo navigation application as a case study, highlighting the impact and benefits of dynamically adapting algorithm parameters. The experiments reveal our approach effective, as such run-time adaptations are easily specified in a higher level by the DSL, and thus at a lower programming effort than when using a general-purpose language such as C."
      }
    ]
  },
  {
    "tid":"1330",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Non-interactive Secure Computation from One-Way Functions.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The notion of non-interactive secure computation (NISC) first introduced in the work of Ishai et al. [EUROCRYPT 2011] studies the following problem: Suppose a receiver R wishes to publish an encryption of her secret input y so that any sender S with input x can then send a message m that reveals f(x, y) to R (for some function f). Here, m can be viewed as an encryption of f(x, y) that can be decrypted by R. NISC requires security against both malicious senders and receivers, and also requires the receiver\u2019s message to be reusable across multiple computations (w.r.t. a fixed input of the receiver)."
      }
    ]
  },
  {
    "tid":"1331",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Compiler supports for VLIW DSP processors with SIMD intrinsics' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"To sustain growing multimedia workload, modern digital signal processing (DSP) processors are commonly equipped with subword instructions to accelerate signal processing. Besides subword, functional units of very long instruction word (VLIW) DSP processors can also be employed to process multiple data streams in parallel. However, because of power and area concerns, many embedded VLIW DSP processors adopt distributed register files to reduce read/write ports and wire connection by privatizing register files for clusters and even for functional units. The distributed design presents great challenges to compilers in distributing single instruction, multiple data (SIMD) workload to functional units. In this paper, we address the issue in supporting SIMD parallelism on VLIW DSP processors with subword instructions and distributed register files. Currently, industrial practices have adopted intrinsics that enable developers to utilize hardware resources and compete with hand-coded assembly in performance. However, it is still an open issue to provide such a solution for VLIW DSP processors with distributed register files. In this work, we provide SIMD intrinsics to allow programmers to write highly optimized codes by following given programming guides. In addition, an enhanced register allocation scheme and data replication optimizations are devised to enable efficient code generation. In our experiments, DSPstone benchmark and a set of H.264 kernels are used to evaluate the proposed programming and optimization schemes. The result shows that by combining SIMD intrinsics and compiler optimizations, one is able to obtain remarkable performance improvements, speedups of 2.9 and 3.5 for DSPstone and H.264 kernels, respectively. Copyright \u00a9 2011 John Wiley & Sons, Ltd."
      }
    ]
  },
  {
    "tid":"1332",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multi-scale Similarity Enhanced Guided Normal Filtering.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a novel mesh denoising scheme in which multi-scale similarity is exploited to improve the performance of non-local normal filtering for feature-preserved mesh restoration. In our scheme, K-ring patches are used to identify multi-scale local structures around faces, and we compare the similarity between patches on multiple levels. The multi-scale similarities are subsequently computed by weighted similarity of patches. Finally, the center faces of similar patches are weighted by similarities in face normal filtering. Experimental results on different models indicate that the proposed method outperforms other local and non-scale-aware similarity based schemes in terms of both objective and subjective evaluations."
      }
    ]
  },
  {
    "tid":"1333",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Stethoscope: A Sustainable Runtime Debugger for Wireless Sensor Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Debugging wireless sensor networks (WSNs) is notoriously difficult, due to the resource constraints on the sensors and distributed running of the debugged programs. Many bugs only manifest themselves during the actual operation of a network, thus requiring runtime debugging of the sensor program. A WSN debugger has to meet two important design criteria, namely saving energy and preserving responsiveness to normal system/network events during debugging. In this paper, we propose Stethoscope, a sustainable runtime debugger for WSNs. In Stethoscope, we devise a new technique called Quick Switch, which enables dynamic binary instrumentation in the RAM instead of the program flash. By incorporating a light-weight hooking mechanism, Stethoscope ensures runtime responsiveness of the debugged program. We implement Stethoscope and demonstrate its advantages with respect to efficacy, energy consumption, and memory cost."
      }
    ]
  },
  {
    "tid":"1334",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A study of diffusion tensor imaging by tissue-specific, smoothing-compensated voxel-based analysis.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Voxel-based analysis (VBA) is commonly used for statistical analysis of image data, including the detection of significant signal differences between groups. Typically, images are co-registered and then smoothed with an isotropic Gaussian kernel to compensate for image misregistration, to improve the signal-to-noise ratio (SNR), to reduce the number of multiple comparisons, and to apply random field theory. Problems with typical implementations of VBA include poor tissue specificity from image misregistration and smoothing. In this study, we developed a new tissue-specific, smoothing-compensated (T-SPOON) method for the VBA of diffusion tensor imaging (DTI) data with improved tissue specificity and compensation for image misregistration and smoothing. When compared with conventional VBA methods, the T-SPOON method introduced substantially less errors in the normalized and smoothed DTI maps. Another confound of the conventional DTI-VBA is that it is difficult to differentiate between differences in morphometry and DTI measures that describe tissue microstructure. T-SPOON VBA decreased the effects of differential morphometry in the DTI VBA studies. T-SPOON and conventional VBA were applied to a DTI study of white matter in autism. T-SPOON VBA results were found to be more consistent with region of interest (ROI) measurements in the corpus callosum and temporal lobe regions. The T-SPOON method may be also applicable to other quantitative imaging maps such as T1 or T2 relaxometry, magnetization transfer, or PET tracer maps."
      }
    ]
  },
  {
    "tid":"1335",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Three-dimensional reconstruction of the bony structures involved in the articular complex of the human shoulder using shape-based interpolation and contour-based extrapolation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Here, we propose a 3D reconstruction approach using shape-based interpolation and contour-based extrapolation. This approach aims to generate a 3D geometric model of a human shoulder from a sequence of MR parallel 2D cross-sections. While interpolation generates intermediate slices between every pair of adjacent input slices, extrapolation performs a smooth closing of the external surface of the model. We propose a new interpolation method based on conditional morphological dilation. Our extrapolation approach is based on surface smoothness constraints and gradually shrinks every extreme 2D cross-section of a bony structure towards a point, respectively. Surface rendering is accomplished through the generation of a triangular mesh using a parametric representation of 2D slice contours. After surface rendering, local surface irregularities are smoothed with Taubin's surface fairing algorithm [G. Taubin, (1995)]."
      }
    ]
  },
  {
    "tid":"1336",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Verifying temporal properties of programs: A parallel approach.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Due to the nature in dealing only with observed executions of a real system, runtime verification is being pursued as a lightweight verification technique. However, the overhead for analyzing the desired temporal properties usually degrades performance greatly. The reasons are: (1) the low efficiency of the interpretive execution in the analyzing process, and (2) nondeterminism of the automata generated from temporal logic properties. To overcome them, this paper presents a parallel approach to verify full regular temporal properties of a program. With this approach, the program is written in Modeling, Simulation and Verification Language (MSVL), and the property is specified by a Propositional Projection Temporal Logic (PPTL) formula. Execution and verification of a program are carried out at the same time. Specifically, each trace generated from executing a program is divided into several segments which are verified in parallel. Also, in the process of verifying each segment, nondeterministic choices in the relative automaton of a temporal property are also handled in parallel. Finally, verification results of all the segments are merged to form the eventual result as well as the counterexample. Our parallel mechanism takes full advantage of the hardware resources and makes temporal property verification efficient in a multi-core system. Experiments show that our approach is practical in verifying temporal properties of large-scale programs in the real world."
      }
    ]
  },
  {
    "tid":"1337",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Improved approximating algorithms for Directed Steiner Forest' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We consider the k-Directed Steiner Forest (k-DSF) problem: given a directed graph G = (V, E) with edge costs, a collection D \u2208 V x V of ordered node pairs, and an integer k \u2264 |D|, find a min-cost subgraph H of G that contains an st-path for (at least) k pairs (s, t) \u2208 D. When k = |D|, we get the Directed Steiner Forest (DSF) problem. The best known approximation ratios for these problems are: \u00d5(k2/3) for k-DSF by Charikar et al. [2], and O(k1/2+\u03b5) for DSF by Chekuri et al. [3]. For DSF we give an O(n\u03b5\u00b7min {n4/5,m2/3})-approximation scheme using a novel LP-relaxation seeking to connect pairs via \"cheap\" paths. This is the first sublinear (in terms of n = |V|) approximation ratio for the problem. For k-DSF we give a simple greedy O(k1/2+\u03b5)-approximation scheme, improving the best known ratio \u00d5(k2/3) by Charikar et al. [2], and (almost) matching, in terms of k, the best ratio known for the undirected variant [11]. Even when used for the particular case DSF, our algorithm favorably compares to the one of [3], which repeatedly solves linear programs, and uses complex time and space consuming transformations."
      }
    ]
  },
  {
    "tid":"1338",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'M-GCF: Multicolor-Green Conflict Free scheduling algorithm for WSN' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Applications for wireless sensor networks (WSNs) are increasing in numbers and are penetrating in areas of increasing importance. The requirements of these applications can only be fulfilled with efficient medium access control (MAC) layer protocols and this paper presents a cluster-based time division multiple access (TDMA) scheduling algorithm, Multicolor-Green Conflict Free (M-GCF), for WSNs. The proposed algorithm finds multiple conflict free slots across a three-hop neighbor view. The algorithm shows better slot sharing with fewer conflicts along with good energy efficiency, throughput and delay as compared with state-of-the-art solutions. The results also include the performance of M-GCF with varying traffic rates, which also shows good energy efficiency, throughput and delay. The contribution of this paper and the main reason for the improved performance with varying number of nodes and traffic rate is the multicoloring approach used."
      }
    ]
  },
  {
    "tid":"1339",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Reactive, model-based monitoring in RFID-enabled manufacturing' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Radio frequency identification (RFID) technology is widely applied throughout the manufacturing lifecycle to track important objects. It is imperative to establish a reactive monitoring system to convert large volumes of RFID data into meaningful information. Two major challenges must therefore be addressed: modeling complexity and monitoring efficiency. When the relationship between high-level ''untagged'' elements (business processes, indicators) and low-level ''tagged'' elements (persons, objects) becomes intricate, it will become difficult to define, maintain and reason by only using traditional rule-based event processing approaches. In order to resolve this problem, the paper proposes a reactive, model-based approach to monitor important elements by estimating their most likely states according to RFID information and a constraint-based model. The approach contains two phases. (1) The model-based programming phase formalizes RFID information and flexibly embeds them into the model that specifies the monitoring rules. (2) The model-based monitoring phase interprets the model and estimates desired states efficiently. Therefore, the RFID-enabled manufacturing units are capable of responding in a timely manner to unexpected disturbances or other important issues."
      }
    ]
  },
  {
    "tid":"1340",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Machining robot with vibrational motion and 3D printer-like data interface' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, a vibration motion control is proposed and implemented on a foamed polystyrene machining robot to suppress the generation of undesirable cusp marks, and the basic performance of the controller is verified through machining experiments of foamed polystyrene. Then, a 3 dimensional (3D) printer-like data interface is proposed for the machining robot. The 3D data interface enables to control the machining robot directly using stereolithography (STL) data without conducting any computer-aided manufacturing (CAM) process. This is done by developing a robotic preprocessor that helps to remove the need for the conventional CAM process by directly converting the STL data into cutter location source data called cutter location (CL) or cutter location source (CLS) data. The STL is a file format proposed by 3D systems, and recently is supported by many computer aided design (CAD)/CAM softwares. The STL is widely used for rapid prototyping with a 3D printer which is a typical additive manufacturing system. The STL deals with a triangular representation of a curved surface geometry. The developed 3D printer-like data interface allows to directly control the machining robot through a zigzag path, rectangular spiral path and circular spiral path generated according to the information included in STL data. The effectiveness and usefulness of the developed system are demonstrated through actual machining experiments."
      }
    ]
  },
  {
    "tid":"1341",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Compound image compression by multi-stage prediction' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Computer generated compound images contain not only photographic images but also text and graphics images, which makes it difficult to be efficiently compressed using exiting image and video coding standards. This paper presents a new compound image coding scheme based on the intra coding framework of the upcoming high efficiency video coding (HEVC) standard. The proposed scheme first decomposes the compound image into color components and structure components. Then two-stage prediction scheme is employed to exploit the correlation among structure components, where the first prediction is generated by directional prediction and second prediction is generated by template matching. Experimental results show that the proposed scheme achieves up to 10.2 dB coding gain on compound images compared with HEVC. And on average it achieves 42.2 percent bitrate saving compared with HEVC."
      }
    ]
  },
  {
    "tid":"1342",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Spectral approximation to point set similarity metric' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In a variety of problems, objects are represented as a collection of feature points {fk} and their spatial positions {pk}. In some cases, feature points doesn't carry enough discriminating information to identify objects so as to raise the question of point set verification, i.e., matching two point sets to identify whether they are match or not. Point set verification problem involves two challenges. The first challenge is to identify a one-to-one mapping between two point sets and the second is to measure the similarity between the two aligned point sets. The first challenge is a well-known one-to-one mapping problem in computer vision with a combinatorial nature and computationally expensive. However, we are able to avoid the computation of one-to-one mapping by directly giving a matching similarity score. The second challenge is attacked with lots of solutions, which shares two disadvantages, i.e., sensitive to both outliners and affine transform. These two challenges are solved simultaneously by our eigenvalue approximation solution. In this work, the point sets are modeled as affinity matrix and the distances between affinity matrices of two point sets are lower bounded by eigenvalue distance. This affinity representation is invariant to scale, translation and rotation and insensitive to outliners and affine transforms. Experiments on both synthetic data and real data shows that this method outperforms both statistics based and geometry based methods."
      }
    ]
  },
  {
    "tid":"1343",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Preventing lapse in performance using a drowsiness monitoring and management system.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Research on public security, especially the safe manipulation and control of vehicles, has gained increasing attention in recent years. This study proposes a closed-loop drowsiness monitoring and management system that can estimate subjects' driving performance. The system observes electroencephalographic (EEG) dynamics and behavioral changes, delivers arousing feedback to individuals experiencing momentary cognitive lapses, and assesses the efficacy of the feedback. Results of this study showed that the arousing feedback immediately improved subject performance, which was accompanied by concurrent theta- and alpha-power suppression in the bilateral occipital areas. This study further demonstrated the feasibility of accurately assessing the efficacy of arousing feedback presented to drowsy participants by monitoring the changes in their EEG power spectra."
      }
    ]
  },
  {
    "tid":"1344",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'ARIES: refactoring support tool for code clone' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we explain our refactoring support tool Aries. Aries characterizes code clones by several metrics, and suggests how to remove them."
      }
    ]
  },
  {
    "tid":"1345",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient fault-tolerant routing algorithms for forward loop backward hop networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Forward loop backward hop (FLBH) networks, characterized by a forward loop connecting all the adjacent nodes and a backward loop connecting nodes separated by a distance of h hops, have a higher connectivity and therefore a higher potential for fault-tolerance than single-loop networks. How to devise efficient fault-tolerant routing algorithms is an important issue on designing FLBH networks. This paper proposes two new fault-tolerant routing algorithms. One is for optimal FLBH networks with h2=N, where N is the number of nodes. The other is for arbitrary FLBH networks (i.e., 1\u2a7dh\u2a7dN-1). Both algorithms are theoretically shown to be able to guarantee that any packet is sure to reach its destination even when a faulty node exists. At the same time, they are simple and merely require local knowledge. A simple and effective rule, called the balancing rule, is used to determine the next node to which the incoming packet should be forwarded. The balancing rule enables the algorithms to find the shortest path between the source and the destination in most cases despite the presence of network faults. The results of computer simulations demonstrate that the average length of paths discovered by the authors' algorithms is shorter than that found by other algorithms and is near optimal solution. As a result, these algorithms are suitable for implementation on practical local computer networks, especially on large ones"
      }
    ]
  },
  {
    "tid":"1346",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Robust supervision using shared-buffers in automated manufacturing systems with unreliable resources' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Try to resolve deadlock issues in a class of AMSs with unreliable resources.Our policy is of the so-called distributing type.Our policy is capable of handling simultaneous multi-resource failures. It has been an active area of research to solve the modeling, analysis, and deadlock control problems for automated manufacturing systems (AMSs). So far, all the system resources are assumed to be reliable in most of the existing approaches for deadlock-free and nonblocking supervisory control. However, many resources of AMSs are subject to failure in the real world. In order to develop a more practical and applicable supervisor, this work takes into consideration of multiple unreliable resources in a class of AMSs. On the basis of two variants of Banker's Algorithm, this paper presents a robust supervisory control policy to avoid deadlock and blocking in these systems. The policy tries to make the best use of buffers of the shared resources to achieve the control objectives. Our controller is qualified to handle simultaneous multi-resource failures. By using formal language and automata theory, we establish its correctness. Moreover, our proposed method is verified via an AMS example, and we make comparison studies between our policy and some of the other similar type of policies in the literature."
      }
    ]
  },
  {
    "tid":"1347",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Scientific Benchmarking with Temporal Logic Decision Procedures' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we propose a hypothesis-driven design of the empirical analysis of difierent decision procedures which we refer to as sci- entiflc benchmarking. The approach is to start by choosing the benchmark problems for which, on the basis of analytical consid- erations, we expect a particular decision pro- cedure to exhibit a behaviour difierent from another decision procedure. Then empiri- cal tests are performed in order to verify the particular hypothesis concerning the decision procedures under consideration. As a case study, we apply this methodology to com- pare difierent decision procedures for propo- sitional temporal logic. We deflne two classes of randomly generated temporal logic formu- lae which we use to investigate the behaviour of two tableaux-based temporal logic ap- proaches using the Logics Workbench, a third tableaux-based approach using the STeP sys- tem, and temporal resolution using a new prover called TRP."
      }
    ]
  },
  {
    "tid":"1348",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Efficient Resource Reservation for Domain Based Mobile IP Networks: Analytical Approach' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a new mechanism to reduce the long resource re-establishment latency and large signalling overhead imposed by the RSVP operation in mobile networks. The proposed scheme architecture comprises of a multi-layer mobility anchor point (MAP) in the Hierarchical Mobile IPv6 (HMIPv6) and the pre-configured RSVP tunnel between them. While having the multi-layer MAP can localise mobility management and resource re-establishment processes to the hierarchically distributed sub-domains, resulting in a decrease in the amount of signalling cost, exploiting the pre-configured RSVP tunnel between two level MAP can alleviate the long resource re-establishment latency, significantly. The analytical framework was developed to make comparisons of the signalling overhead and resource reservation latency between the proposed scheme and RSVPMobility Proxy (RSVP-MP) as a baseline protocol. The numerical results obtained show the superiority of the proposed scheme."
      }
    ]
  },
  {
    "tid":"1349",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Packet-Error-Rate Analysis Using Markov Models of the Signal-to-Interference Ratio for Mobile Packet Systems.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"To analyze the packet error rate (PER) of interference-limited mobile packet systems, in this paper, two Markov models of the time-varying signal-to-interference ratio (SIR) are derived over frequency-selective fading channels. The two Markov models are first-order two-state for the following two cases: 1) infinitesimal interval and 2) finite-time interval. The state and state transition probabilities are obtained from both the level crossing rate and average outage duration of the fluctuating SIR. The PER is analytically derived using the Markov model in the case of a finite time interval, and the analysis can be applied not only to cellular communication systems but to ad hoc mobile networks as well. The analytical results are exact when the average powers of both the desired and interfering signal components are equal for all paths but include an approximation in the other cases. The analytical results of the PER agree well with the results that were obtained by computer simulations. It is also shown that the PER characteristics of cellular systems and of ad hoc networks are almost the same."
      }
    ]
  },
  {
    "tid":"1350",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Reputation Systems for Anonymous Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present a reputation scheme for a pseudonymous peer-to-peer (P2P) system in an anonymous network. Misbehavior is one of the biggest problems in pseudonymous P2P systems, where there is little incentive for proper behavior. In our scheme, using ecash for reputation points, the reputation of each user is closely related to his real identity rather than to his current pseudonym. Thus, our scheme allows an honest user to switch to a new pseudonym keeping his good reputation, while hindering a malicious user from erasing his trail of evil deeds with a new pseudonym."
      }
    ]
  },
  {
    "tid":"1351",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Transmission Strategies for MISO Downlink MC-CDMA Mobile Systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes two space-frequency schemes with a multi-user pre-filtering technique for downlink (DL) multicarrier code division multiple access (MC-CDMA) systems. We consider the use of antenna arrays at the base station (BS) and a single antenna at the mobile terminal (MT) and derive the proposed multi-user pre-filtering technique that modulates the transmitted signal to eliminate the effects of multiple access interference (MAI) and channel distortions at the mobile terminals, while maintaining low MT complexity. Two types of detectors are considered at the MT: simple despreading and single user equalizers. The performances of the proposed schemes are compared to those of other transmit signal design approaches that have been recently proposed for DL MC-CDMA, considering both typical indoor and pedestrian scenarios, and channel coding based on UMTS specifications."
      }
    ]
  },
  {
    "tid":"1352",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Matrix Reordering Methods for Table and Network Visualization.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This survey provides a description of algorithms to reorder visual matrices of tabular data and adjacency matrix of Networks. The goal of this survey is to provide a comprehensive list of reordering algorithms published in different fields such as statistics, bioinformatics, or graph theory. While several of these algorithms are described in publications and others are available in software libraries and programs, there is little awareness of what is done across all fields. Our survey aims at describing these reordering algorithms in a unified manner to enable a wide audience to understand their differences and subtleties. We organize this corpus in a consistent manner, independently of the application or research field. We also provide practical guidance on how to select appropriate algorithms depending on the structure and size of the matrix to reorder, and point to implementations when available."
      }
    ]
  },
  {
    "tid":"1353",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fundamental relationship between NodeDensity and delay in wireless ad hoc networks with unreliable links' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We investigate the fundamental relationship between node density and transmission delay in large-scale wireless ad hoc networks with unreliable links from percolation perspective. Previous works[11][2][10] have already showed the relationship between transmission delay and distance from source to destination. However, it still remains as an open question how transmission delay varies in accordance with node density. Answering this question can provide guidance for determining the number of nodes to meet the delay requirement when designing ad hoc networks. In this paper, we study the impact of node density \u03bb on the ratio of delay and distance, denoted by \u03b3(\u03bb). We analytically characterize the properties of \u03b3(\u03bb) as a function of \u03bb. And then we present upper and lower bounds to \u03b3(\u03bb). Next, we take propagation delay into consideration and obtain further results on the upper and lower bounds of \u03b3(\u03bb). Finally, we make simulations to verify our theoretical analysis."
      }
    ]
  },
  {
    "tid":"1354",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'User- and process-driven dynamic voltage and frequency scaling' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We describe and evaluate two new, independently-applicable power reduction techniques for power management on processors that support dynamic voltage and frequency scaling (DVFS): user-driven frequency scaling (UDFS) and process-driven voltage scaling (PDVS). In PDVS, a CPU-customized profile is derived offline that encodes the minimum voltage needed to achieve stability at each combination of CPU frequency and temperature. On a typical processor, PDVS reduces the voltage below the worst-case minimum operating voltages given in datasheets. UDFS, on the other hand, dynamically adapts CPU frequency to the individual user and the workload through direct user feedback. Our UDFS algorithms dramatically reduce typical operating frequencies and voltages while maintaining performance at a satisfactory level for each user. We evaluate our techniques independently and together through user studies conducted on a Pentium M laptop running Windows applications. We measure the overall system power and temperature reduction achieved by our methods. Combining PDVS and the best UDFS scheme reduces measured system power by 49.9% (27.8% PDVS, 22.1% UDFS), averaged across all our users and applications, compared to Windows XP DVFS. The average temperature of the CPU is decreased by 13.2degC. User trace-driven simulation to evaluate the CPU only indicates average CPU dynamic power savings of 57.3% (32.4% PDVS, 24.9% UDFS), with a maximum reduction of 83.4%. In a multitasking environment, the same UDFS+PDVS technique reduces the CPU dynamic power by 75.7% on average."
      }
    ]
  },
  {
    "tid":"1355",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Functional Encryption for Computational Hiding in Prime Order Groups via Pair Encodings.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Lewko and Waters introduced the  technique in Crypto\u201912. In their technique, two computational assumptions that achieve selective and co-selective security proofs lead to adaptive security of an encryption scheme. Later, pair encoding framework was introduced by Attrapadung in Eurocrypt\u201914. The pair encoding framework generalises the computational hiding technique for functional encryption (FE). It has been used to achieve a number of new FE schemes such as FE for regular languages and unbounded attribute based encryption allowing multi-use of attributes. Nevertheless, the generalised construction of Attrapadung\u2019s pair encoding for those schemes is adaptively secure only in composite order groups, which leads to efficiency loss. It remains a challenging task to explore constructions in prime order groups for gaining efficiency improvement, which leaves the research gap in the existing literature. In this work, we aim to address this drawback by proposing a new generalised construction for pair encodings in prime order groups. Our construction will lead to a number of new FE schemes in prime order groups, which have been previously introduced only in composite order groups by Attrapadung."
      }
    ]
  },
  {
    "tid":"1356",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A hop-bounded single-actor selection algorithm for wireless sensor and actor networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Wireless sensor and actor networks (WSANs) are composed of a large number of sensors and a small number of (mobile) resource-rich actors. Sensors gather information about the physical phenomenon, while actors take decisions and then perform appropriate actions upon the environment.In this paper, a single-actor selection algorithm for WSANs is addressed from hops and energy constraints. The corresponding optimal solution HBMECT (Hop-Bounded and Minimum Energy Cost Tree) is determined by integer linear programming (ILP). And then, a distributed approximate algorithm of HBMECT is provided.In the performance evaluation, the approximate algorithm of HBMECT is compared with MECT (Minimum Energy Cost Tree) and MPCT (Minimum Path Cost Tree) algorithms."
      }
    ]
  },
  {
    "tid":"1357",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'PME: Projected Metric Embedding on Heterogeneous Networks for Link Prediction.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Heterogenous information network embedding aims to embed heterogenous information networks (HINs) into low dimensional spaces, in which each vertex is represented as a low-dimensional vector, and both global and local network structures in the original space are preserved. However, most of existing heterogenous information network embedding models adopt the dot product to measure the proximity in the low dimensional space, and thus they can only preserve the first-order proximity and are insufficient to capture the global structure. Compared with homogenous information networks, there are multiple types of links (i.e., multiple relations) in HINs, and the link distribution w.r.t relations is highly skewed. To address the above challenging issues, we propose a novel heterogenous information network embedding model PME based on the metric learning to capture both first-order and second-order proximities in a unified way. To alleviate the potential geometrical inflexibility of existing metric learning approaches, we propose to build object and relation embeddings in separate object space and relation spaces rather than in a common space. Afterwards, we learn embeddings by firstly projecting vertices from object space to corresponding relation space and then calculate the proximity between projected vertices. To overcome the heavy skewness of the link distribution w.r.t relations and avoid \"over-sampling'' or \"under-sampling'' for each relation, we propose a novel loss-aware adaptive sampling approach for the model optimization. Extensive experiments have been conducted on a large-scale HIN dataset, and the experimental results show superiority of our proposed PME model in terms of prediction accuracy and scalability.\n\n"
      }
    ]
  },
  {
    "tid":"1358",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Return And Risk Model For Efficient Spectrum Sharing In Cognitive Radio Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cognitive Radio technology releases the spectrum from shackles of authorized licenses and facilitates the trading of spectrum bands. In the spectrum market, primary users (PUs) set prices for their vacant bands and sell them for monetary gains, and secondary users (SUs) buy the bands and opportunistically use them to satisfy their service demands when the PUs are not active. However, when there are multiple bands available for the SUs to access, the SU confronts the challenges of how to choose bands and how to split his traffic over them considering both the contention from peer SUs as well as the unpredictable activities of the PUs. In this paper, we propose a return and risk model to represent these concerns of the SU, and help the SU to make appropriate decisions of traffic distribution over available spectrum bands, either the bands belonging to SU itself or the bands shared with PUs. The simulation and analysis show that our spectrum sharing scheme is efficient in terms of maximum return for given risk or minimum risk for given return, and is also effective in improving the spectrum utilization and SUs' satisfactory degrees."
      }
    ]
  },
  {
    "tid":"1359",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Position and Velocity Navigation Systems for Unmanned Vehicles' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This brief presents a new observer synthesis methodology for a class of kinematic systems with application to the estimation of linear motion quantities of mobile platforms (position and linear velocity), in three dimensions, that: 1) presents globally exponentially stable (GES) error dynamics, which are also input-to-state stable (ISS) with respect to angular quantities; 2) minimizes the L 2 induced norm from a generalized disturbance input to a performance variable; 3) provides a systematic design procedure, based upon robust control theory results, that allows for the use of frequency weights to shape the dynamic response of the observer. A practical application is presented, in the field of ocean robotics, that demonstrates the potential and usefulness of the proposed design methodology and simulation results are included that illustrate the observer achievable performance in the presence of both extreme environmental disturbances and realistic sensors' noise."
      }
    ]
  },
  {
    "tid":"1360",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A compressed-domain approach for shot boundary detection on H.264/AVC bit streams' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The amount of digital video content has grown extensively during recent years, resulting in a rising need for the development of systems for automatic indexing, summarization, and semantic analysis. A prerequisite for video content analysis is the ability to discover the temporal structure of a video sequence. In this paper, a novel shot boundary detection technique is introduced that operates completely in the compressed domain using the H.264/AVC video standard. As this specification contains a number of new coding tools, the characteristics of a compressed bit stream are different from prior video specifications. Furthermore, the H.264/AVC specification introduces new coding structures such as hierarchical coding patterns, which can have a major influence on video analysis algorithms. First, a shot boundary detection algorithm is proposed which can be used to segment H.264/AVC bit streams based on temporal dependencies and spatial dissimilarities. This algorithm is further enhanced to exploit hierarchical coding patterns. As these sequences are characterized by a pyramidal structure, only a subset of frames needs to be considered during analysis, allowing the reduction of the computational complexity. Besides the increased efficiency, experimental results also show that the proposed shot boundary detection algorithm achieves a high accuracy."
      }
    ]
  },
  {
    "tid":"1361",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mobile multimedia services' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This panel will mainly focus on the role that media processing can play in creating mobile communications, information, and entertainment services. A major premise of our discussion is that media processing techniques go beyond compression and can be employed to monitor, filter, convert, and repurpose information. Such automated techniques can serve to create personalized information and entertainment services in a cost-effective way, adapt existing content for consumption on mobile devices, and circumvent the inherent limitations of mobile devices. Some examples of the applications of media processing techniques for mobile service generation will be given."
      }
    ]
  },
  {
    "tid":"1362",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Grand challenge: the TechniBall system' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this work we present the solution to the DEBS'2013 Grand Challenge, as crafted by the joint effort of teams from the Technion and TU Dortmund. The paper describes the architecture, details the queries, shows throughput and latency evaluation, and offers our observations regarding the appropriate way to trade-off high-level processing with time constraints."
      }
    ]
  },
  {
    "tid":"1363",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Concise representation of hypergraph minimal transversals: Approach and application on the dependency inference problem' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The problem of extracting the minimal transversals from a hypergraph is known to be particularly difficult. Given that the number of minimal transversals can be exponential even for moderately sized hypergraphs, we propose, in this paper, a concise representation of minimal transversals in order to optimize the computation time and we introduce a new algorithm, called IRRED-ENGINE, which extracts all the minimal transversals from a subset. Experiments carried out on several types of hypergraphs, showed that IRRED-ENGINE obtains very interesting results evaluated through a compactness measure. To illustrate the benefit of our approach, we show how our concise representation can be used to solve the dependency inference problem by computing a concise cover of functional dependencies."
      }
    ]
  },
  {
    "tid":"1364",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An ontology-guided approach to change detection of the semantic web data' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"To achieve improved availability and performance, often, local copies of remote data from autonomous sources are maintained. Web search engines are the primary examples of such services. Increasingly, these services are utilizing the Semantic Web as it is often envisioned as a machine-interpretable web. In order to keep the local repositories current, it is essential to synchronize their content with that of their original sources. Change detection is the first step to accomplish this. It is essential to have efficient change detection mechanisms as the size of the local repositories is often very large. In this paper, we present an approach that exploits the semantic relationships among the concepts in guiding the change detection process. Given changes to some seed instances, a reasoning engine fires a set of pre-defined rules to characterize the profile of the changed target instances. In addition to change detection, our proposed semantics-based approach of utilizing semantic associations can be utilized in other applications such as guiding information discovery for agents, consistency maintenance among distributed information sources, among others."
      }
    ]
  },
  {
    "tid":"1365",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fabrication cost analysis and cost-aware design space exploration for 3-D ICs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"3-D integration technology is emerging as an attractive alternative to increase the transistor count for future chips. The majority of the existing 3-D integrated circuit (IC) research is focused on the performance, power, density, and heterogeneous integration benefits offered by 3-D integration. All such advantages, however, ultimately have to translate into cost evaluation when a design strategy has to be decided. Consequently, system-level cost analysis at early design stages is imperative to decide on whether 3-D integration should be adopted. This paper presents a cost estimation method for 3-D ICs at early design stages and proposes a set of cost models that include wafer cost, 3-D bonding cost, package cost, and cooling cost. The proposed 3-D IC cost estimation method can help designers analyze the cost implication for 3-D ICs during the design space exploration at the early stage, and it enables a cost-driven 3-D IC design flow that can guide the design choice toward a cost-effective direction. Based on the proposed cost estimation method, this paper demonstrates two case studies that explore the cost benefits of 3-D integration for application-specific integrated circuit designs and many-core microprocessor designs style, respectively. Finally, this paper suggests the optimum partitioning strategy for future 3-D IC designs."
      }
    ]
  },
  {
    "tid":"1366",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Generating Chaos Via Feedback Control From A Stable Ts Fuzzy System Through A Sinusoidal Nonlinearity' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"An approach is proposed for making chaotic a given stable Takagi-Sugeno (TS) fuzzy system using state feedback control of arbitrarily small magnitude. The feedback controller chosen among several candidates is a simple sinusoidal function of the system states, which can lead to uniformly bounded state vectors of the controlled system with positive Lyapunov exponents, and satisfy the chaotic mechanisms of stretching and folding, thereby yielding chaotic dynamics. This approach is mathematically proven for rigorous generation of chaos from a stable TS fuzzy system, where the chaos is in the sense of Li and Yorke. A numerical example is included to visualize the theoretical analysis and the controller design."
      }
    ]
  },
  {
    "tid":"1367",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'dlvhex: A Prover for Semantic-Web Reasoning under the Answer-Set Semantics' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We present the system dlvhex, a solver for HEX-programs, which are nonmonotonic logic programs admitting both higher-order atoms as well as external atoms. Higher-order features are widely acknowledged as being useful for various tasks, including meta-reasoning. Furthermore, the possibility to exchange knowledge with external sources in a fully declarative paradigm such as answer-set programming (ASP) becomes increasingly important, in particular in view of applications in the Semantic-Web area. Through external atoms, HEX-programs can deal with external knowledge and reasoners of various nature, such as RDF datasets or description-logics knowledge bases."
      }
    ]
  },
  {
    "tid":"1368",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Network-Based Predictive Control for Constrained Nonlinear Systems With Two-Channel Packet Dropouts.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper investigates the predictive control scheme and the associated stability issue for the constrained nonlinear networked control systems (NCSs), where both the sensor-to-controller packet dropout and the controller-to-actuator packet dropout are considered simultaneously. The model predictive control based framework is proposed to compensate for the two-channel packet dropouts. This framew..."
      }
    ]
  },
  {
    "tid":"1369",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Not at the Dinner Table: Parents\u00bf and Children\u00bfs Perspectives on Family Technology Rules.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Parents and children both use technology actively and increasingly, but prior work shows that concerns about attention, family time, and family relationships abound. We conducted a survey with 249 parent-child pairs distributed across 40 U.S. states to understand the types of technology rules (also known as restrictive mediation) they have established in their family and how effective those rules are perceived to be. Our data robustly show that children (age 10-17) are more likely to follow rules that constrain technology activities (e.g., no Snapchat) than rules that constrain technology use in certain contexts (e.g., no phone at the dinner table). Children find context constraints harder to live up to, parents find them harder to enforce, and parents\u00e2\u0080\u0099 most common challenge when trying to enforce such rules is that children \u00e2\u0080\u009ccan\u00e2\u0080\u0099t put it down.\u00e2\u0080\u009d This is consistent with the idea that banning certain technologies is currently easier than setting more nuanced boundaries. Parents and children agree that parents should also unplug when spending time with family, while children alone express frustration with the common parent practice of posting about children online. Together, our results suggest several mechanisms by which designers and families can improve parent-child relationships around technology use."
      }
    ]
  },
  {
    "tid":"1370",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A spatio-temporal speech enhancement scheme for robust speech recognition in noisy environments' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A new speech enhancement scheme is presented integrating spatial and temporal signal processing methods for robust speech recognition in noisy environments. The scheme first separates spatially localized point sources from noisy speech signals recorded by two microphones. Blind source separation algorithms assuming no a priori knowledge about the sources involved are applied in this spatial processing stage. Then denoising of distributed background noise is achieved in a combined spatial/temporal processing approach. The desired speaker signal is first processed along with an artificially constructed noise signal in a supplementary blind source separation step. It is further denoised by exploiting differences in temporal speech and noise statistics in a wavelet filterbank. The scheme\u2019s performance is illustrated by speech recognition experiments on real recordings in a noisy car environment. In comparison to a common multi-microphone technique like beamforming with spectral subtraction, the scheme is shown to enable more accurate speech recognition in the presence of a highly interfering point source and strong background noise."
      }
    ]
  },
  {
    "tid":"1371",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Pipeline Implementation Of A Watershed Algorithm On Fpga' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The watershed transformation is a popular image segmentation technique for grey scale images. This paper describes a pipeline implementation of a watershed algorithm designed for hardware implementation. In the algorithm, pixels in a Given image are repeatedly scanned from top-left to bottom-right, and then from bottom-right to top-left in order to propagate the value of each pixel to its neighbors. In the implementation, w-sets of k-lines are buffered on the FPGA, and the algorithm is repeatedly applied to w-sets, shifting in a new set from the external memory banks and shifting out the oldest set to other external memory banks. w and k can be chosen according to the number of the external memory banks and the size of the FPGA. Therefore, it is possible to realize the best performance on a given hardware platform."
      }
    ]
  },
  {
    "tid":"1372",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Computing spatiotemporal variogram and covariance model' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"A large number of environmental phenomena may be regarded as the realizations of spatiotemporal random fields. In practice, these environmental phenomena are sparsely sampled generally. In order to research deeply, it is necessary to construct a continuous spatiotemporal data surface, so the prediction or interpolation must be done. The spatiotemporal variogram and covariance model is useful means of describing the spatiotemporal correlation structure. For the straightforward extension of variogram and covariance from pure spatial to spatiotemporal fields, there are a number of statistical studies about theoretical spatiotemporal model but very less research on model computing. After making some theoretical spatiotemporal statistical analysis, this paper focused mainly on the computation of spatiotemporal variogram and covariance model and implement effective variogram and covariance model. Firstly, the spatiotemporal product-sum model is deduced into the form of calculable in theory. Secondly, the most likely variogram model and its parameters of sill, nugget, and range are derived through computing the spatial and temporal variogram respectively. Thirdly, the policy of how to determine the parameters k1,k2 and k3 in the product-sum model are put forward. The objective to introduce k1,k2 and k3 is to ensure the effectiveness of variogram and covariance model. Lastly, the spatiotemporal variogram and covariance model are implemented. The results have shown the positive definite characteristics of the spatiotemporal variogram and covariance varying with the parameter k1 and reverse variation characteristics between variogram and covariance, which proves that the theoretical model chosen is effective and the computing approach about spatiotemporal variogram and covariance model is feasible. The research of this paper has laid the foundation for spatiotemporal prediction or interpolation, because prediction or interpolation can do only basing on suitable variogram or c- - ovariance model."
      }
    ]
  },
  {
    "tid":"1373",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Eligibility-based Round Robin for fair and efficient packet scheduling in wormhole switching networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Interconnection networks of parallel systems are used for servicing traffic generated by different applications, often belonging to different users. When multiple users contend for channel bandwidth, fairness in bandwidth sharing becomes a key requirement. In fact, enforcing a fair sharing of channel bandwidth improves flow isolation, thus preventing misbehaving flows from affecting the performance of other flows. We present a novel packet scheduling algorithm, called eligibility-based round robin (EBRR), devised to provide fair queueing in interconnection networks. In fact, EBRR meets the constraints imposed by wormhole switching, which is the most popular switching technique in interconnection networks of parallel systems. It can also be applied to packet switching wide area networks (WANs), such as IP and ATM. We show that EBRR has O(1) complexity and better delay and fairness properties than existing algorithms of comparable complexity. Here, we also investigate the means for assessing the fairness of a scheduler: we show that using the relative fairness bound as a fairness measure may lead to erroneous results. We then propose an alternative measure, called the generalized relative fairness bound, that allows fairness to be assessed more precisely."
      }
    ]
  },
  {
    "tid":"1374",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A robust iterative refinement clustering algorithm with smoothing search space' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Iterative refinement clustering algorithms are widely used in data mining area, but they are sensitive to the initialization. In the past decades, many modified initialization methods have been proposed to reduce the influence of initialization sensitivity problem. The essence of iterative refinement clustering algorithms is the local search method. The big numbers of the local minimum points which are embedded in the search space make the local search problem hard and sensitive to the initialization. The smaller number of local minimum points, the more robust of initialization for a local search algorithm is. In this paper, we propose a Top-Down Clustering algorithm with Smoothing Search Space (TDCS3) to reduce the influence of initialization. The main steps of TDCS3 are to: (1) dynamically reconstruct a series of smoothed search spaces into a hierarchical structure by 'filling' the local minimum points; (2) at the top level of the hierarchical structure, an existing iterative refinement clustering algorithm is run with random initialization to generate the clustering result; (3) eventually from the second level to the bottom level of the hierarchical structure, the same clustering algorithm is run with the initialization derived from the previous clustering result. Experiment results on 3 synthetic and 10 real world data sets have shown that TDCS3 has significant effects on finding better, robust clustering result and reducing the impact of initialization."
      }
    ]
  },
  {
    "tid":"1375",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A physically-based client-server rendering solution for mobile devices' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mobile devices, also known as small-form-factor (SFF) devices such as mobile phones, PDAs and ultra mobile PCs have continued to grow in popularity. Improvements in SFF hardware has enabled a range of suitable applications such as gaming, interactive visualisation and mobile mapping. Although high-fidelity graphic systems typically have significant computational requirements, the time taken may be largely resolution dependent. The limited resolution of SFFs indicates such platforms are prime candidates for running high-fidelity graphics. Due to the limited hardware available on mobile devices, it is not currently possible to produce high-fidelity graphics in reasonable time. However, most SFFs have some degree of network capability. Using a remote server in conjunction with a mobile device to render high-fidelity graphics on demand allows us to substantially reduce the total rendering time. This paper introduces a client-server framework for minimising rendering times using a cost function to predict optimal distribution of rendering."
      }
    ]
  },
  {
    "tid":"1376",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'ISP Tool Update: Scalable MPI Verification.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We provide a status update of ISP, our dynamic formal verifier for MPI programs. ISP determines and explores all relevant schedules of an MPI program. The highlights of this paper are (i) a recap of ISP's features, and (ii) an overview of work in progress, including a graphical explorer for message passing (GEM) and a distributed MPI analyzer (DMA), an adaptation of ISP for the distributed setting."
      }
    ]
  },
  {
    "tid":"1377",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Exploiting an I-IP for In-Field SOC Test' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Today's complex system-on-chip integrated circuits include a wide variety of functional IPs whose correct manufacturing must be guaranteed by IC producers.Infrastructure IPs are increasingly often inserted to achieve this purpose; such blocks, explicitly designed for test, are coupled with functional IPs both to obtain yield improvement during the manufacturing process and to perform volume production test.In this paper, a new test control schema based on the use of an Infrastructure IP (I-IP) is proposed for the test on-site of SoCs.The proposed in-field test strategy is based on the ability of a single I-IP to periodically monitor the behavior of the system by reusing the test structures introduced for manufacturing test.The feasibility of this approach has been proved for SoCs including microprocessors and memories equipped with P1500 compliant solutions.Experimental results highlight the advantages in term of reusability and scalability, low impact on system availability and reduced area overhead."
      }
    ]
  },
  {
    "tid":"1378",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An Efficient Gradient Mechanism of Directed Diffusion in Wireless Sensor Network' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In Wireless Sensor Network (WSN), sensor node is limited in energy, thus energy efficiency is an important concern in routing protocol design. And some applications in sensor networks have the requirement of timely delivery, real-time also become needed for routing. Directed Diffusion (DD) is a data centric dissemination protocol for sensor networks, but DD cannot support time-sensitive traffic nor perform balance of energy consumption to increase network lifetime. To solve this issue, we design a new kind of gradient mechanism, which considers residual energy of node and the number of hops from the sink to an intermediate node synchronously. It can find a best-effort optimal path in sensor networks for data transfer. Simulation results show that our proposed method achieves more globally energy balancing and lower delay for data transfer than originally DD. It prolongs the lifetime of network efficiently."
      }
    ]
  },
  {
    "tid":"1379",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'HearThere: Networked Sensory Prosthetics Through Auditory Augmented Reality.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper we present a vision for scalable indoor and outdoor auditory augmented reality (AAR), as well as HearThere, a wearable device and infrastructure demonstrating the feasibility of that vision. HearThere preserves the spatial alignment between virtual audio sources and the user's environment, using head tracking and bone conduction headphones to achieve seamless mixing of real and virtual sounds. To scale between indoor, urban, and natural environments, our system supports multi-scale location tracking, using fine-grained (20-cm) Ultra-WideBand (UWB) radio tracking when in range of our infrastructure anchors and mobile GPS otherwise. In our tests, users were able to navigate through an AAR scene and pinpoint audio source locations down to 1m. We found that bone conduction is a viable technology for producing realistic spatial sound, and show that users' audio localization ability is considerably better in UWB coverage zones than with GPS alone. HearThere is a major step towards realizing our vision of networked sensory prosthetics, in which sensor networks serve as collective sensory extensions into the world around us. In our vision, AAR would be used to mix spatialized data sonification with distributed, livestreaming microphones. In this concept, HearThere promises a more expansive perceptual world, or umwelt, where sensor data becomes immediately attributable to extrinsic phenomena, externalized in the wearer's perception. We are motivated by two goals: first, to remedy a fractured state of attention caused by existing mobile and wearable technologies; and second, to bring the distant or often invisible processes underpinning a complex natural environment more directly into human consciousness."
      }
    ]
  },
  {
    "tid":"1380",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'FedX: optimization techniques for federated query processing on linked data' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Motivated by the ongoing success of Linked Data and the growing amount of semantic data sources available on theWeb, new challenges to query processing are emerging. Especially in distributed settings that require joining data provided by multiple sources, sophisticated optimization techniques are necessary for efficient query processing. We propose novel join processing and grouping techniques to minimize the number of remote requests, and develop an effective solution for source selection in the absence of preprocessed metadata. We present FedX, a practical framework that enables efficient SPARQL query processing on heterogeneous, virtually integrated Linked Data sources. In experiments, we demonstrate the practicability and efficiency of our framework on a set of real-world queries and data sources from the Linked Open Data cloud. With FedX we achieve a significant improvement in query performance over state-of-the-art federated query engines."
      }
    ]
  },
  {
    "tid":"1381",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Motion Capture Using Joint Skeleton Tracking And Surface Estimation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes a method for capturing the performance of a human or an animal from a multi-view video sequence. Given an articulated template model and silhouettes from a multi-view image sequence, our approach recovers not only the movement of the skeleton, but also the possibly non-rigid temporal deformation of the 3D surface. While large scale deformations or fast movements are captured by the skeleton pose and approximate surface skinning, true small scale deformations or non-rigid garment motion are captured by fitting the surface to the silhouette. We further propose a novel optimization scheme for skeleton-based pose estimation that exploits the skeleton's tree structure to split the optimization problem into a local one and a lower dimensional global one. We show on various sequences that our approach can capture the 3D motion of animals and humans accurately even in the case of rapid movements and wide apparel like skirts."
      }
    ]
  },
  {
    "tid":"1382",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Load-Balanced IP Fast Failure Recovery' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"As a promising approach to improve network reliability, Proactive Failure Recovery (PFR) re-routes data traffic to backup paths without waiting for the completion of routing convergence after a local link failure. However, the diverted traffic may cause congestion on the backup paths if it is not carefully split over multiple paths according to their available capacity. Existing approach assigns new link weights based on links' load and re-calculates the routing paths, which incurs significant computation overhead and is susceptible to route oscillations. In this paper, we propose an efficient scheme for load balancing in PFR. We choose an adequate number of different types of loop-free backup paths for potential failures, and once a failure happens, the affected traffic is diverted to multiple paths in a well balanced manner. We formulate the traffic allocation problem as a tractable linear programming optimization problem, which can be solved iteratively and incrementally. As a result, only the flows affected by the failures are re-allocated to backup paths incrementally without disturbing flows not directly affected by the failures. Simulation results show that our scheme is computationally efficient, can effectively balance link utilization in the network, and can avoid route oscillations."
      }
    ]
  },
  {
    "tid":"1383",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Cooperative Object Tracking and Event Detection with Wireless Smart Cameras' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Wireless embedded smart cameras not only capture images, but also can perform processing and communication. However,many system- and algorithm-wise challenges remain to be addressed to have operational, battery-powered wireless smart-camera networks, since they have limited processing power, memory, energy and bandwidth. In this paper, we present a wireless, embedded smart camera system for cooperative object tracking and event detection, wherein each camera platform consists of a camera board and a wireless mote. Light-weight background subtraction and tracking algorithms are implemented and run on the camera boards. Cameras communicate in a peer-to-peer manner over wireless links to exchange data, and thus to consistently track objects. In a wireless smart camera system, transferring large amounts of data between cameras should be avoided, since it requires more power, and incurs more communication delay. In the presented system, cameras exchange small-size packets for communication. Also, with wireless smart cameras, it is not viable to transfer all the captured frames to a base station due to limited resources. Instead, we define events of interest beforehand, and embedded smart cameras save only those portions of the live video capture where the defined event scenario occurs. We present results of tracking and detecting objects entering a region of interest, all of which are performed on the microprocessor of camera boards. We also show examples of consistently tracking objects, moving across different camera views, by wireless data exchange."
      }
    ]
  },
  {
    "tid":"1384",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Identity-based Threshold Broadcast Encryption in the Standard Model' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In an identity-based threshold broadcast encryption (IDTHBE) scheme, a broadcaster chooses a set of n recipients and a threshold value t, and the plaintext can be recovered only if at least t receivers cooperate. IDTHBE scheme is different from the standard threshold public key encryption schemes, where the set of receivers and the threshold value are decided from the beginning. This kind of scheme has wide applications in ad hoc networks. Previously proposed IDTHBE schemes have ciphertexts which contain at least n elements. In addition, the security of theses schemes relies on the random oracles. In this paper, we introduce two new constructions of IDTHBE for ad hoc networks. Our first scheme achieves S-size private keys while the modified scheme achieves constant size private keys. Both schemes achieve approximately (n-t)-size ciphertexts. Furthermore, we also show that they are provable security under the decision bilinear Diffie-Hellman Exponent (BDHE) assumption in the standard model."
      }
    ]
  },
  {
    "tid":"1385",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Asymmetric gait for rimless wheel with knee joints' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper investigates the properties of asymmetric gaits of a rimless wheel (RW) with knee joints. The RW consists of one point mass at the hip and eight identical massless legs with knees. Any walking patterns converge to the limit cycle which includes stance and impact phases. The mechanical energy dissipates at the heel strike and is restored by impulsive kicking control. Two types of asymmetric gaits are investigated; one is generated by two different restored mechanical energies and the other is generated by two different strides. The walking speed of the asymmetric gaits is compared with that of symmetric gaits through numerical simulations. It is shown that the constraint on the stride is necessary to realize high-speed walking."
      }
    ]
  },
  {
    "tid":"1386",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mermaid: Integrating Vertex-Centric with Edge-Centric for Real-World Graph Processing.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"There has been increasing interests in processing large-scale real-world graphs, and recently many graph systems have been proposed. Vertex-centric GAS (Gather-Apply-Scatter) and Edge-centric GAS are two graph computation models being widely adopted, and existing graph analytics systems commonly follow only one computation model, which is not the best choice for real-world graph processing. In fact, vertex degrees in real-world graphs often obey skewed power-law distributions: most vertices have relatively few neighbors while a few have many neighbors. We observe that vertex-centric GAS for high-degree vertices and edge-centric GAS for low-degree vertices is a much better choice for real-world graph processing. In this paper, we present Mermaid, a system for processing large-scale real-world graphs on a single machine. Mermaid skillfully integrates vertex-centric GAS with edge-centric GAS through a novel vertex-mapping mechanism, and supports streamlined graph processing. On a total of 6 practical natural graph processing tasks, we demonstrate that, on average, Mermaid achieves 1.83x better performance than the state-of-the-art graph system on a single machine."
      }
    ]
  },
  {
    "tid":"1387",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Implementation of multi agents based system for process supervision in large-scale chemical plants.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"\u2022Modern-day chemical plants exhibit scale-, structure- and dynamic-complexity.\u2022Process supervision systems for such processes should be flexible, cooperative, collaborative, extensible, and scalable.\u2022In this paper, we propose an agent-based architecture for developing and deploying such process supervision systems.\u2022The key insight is that the process descriptors used for developing the supervision models are also dynamic.\u2022The implementation of the agent-based system is described and its benefits demonstrated using an oil & gas case study."
      }
    ]
  },
  {
    "tid":"1388",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multi-Rate Broadcasting: Analysis and Design of Stateless Algorithms' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We look at the problem of network wide broadcast using the multi-rate feature of a wireless ad hoc network. Existing research has primarily focused on achieving minimum latency by construction of minimum weight connected dominating sets (WCDS) based on neighbourhood information. In this paper, we are interested in stateless multi-rate broadcasting algorithms in which nodes determine their broadcasting behaviour based on neighbourhood transmissions. The primary contribution of this paper is that we show how broadcast effectiveness at different data rates are related and how this relationship can be used to optimize algorithm design. We propose three stateless broadcasting algorithms and demonstrate the performance improvements achievable. Our simulation results show that significant benefits can be obtained in terms of minimizing both the number of forwarding nodes as well as the broadcast latency."
      }
    ]
  },
  {
    "tid":"1389",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Inductive Lexical Learning of Class Expressions.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Despite an increase in the number of knowledge bases published according to Semantic Web W3C standards, many of those consist primarily of instance data and lack sophisticated schemata, although the availability of such schemata would allow more powerful querying, consistency checking and debugging as well as improved inference. One of the reasons why schemata are still rare is the effort required to create them. Consequently, numerous ontology learning approaches have been developed to simplify the creation of schemata. Those approaches usually either learn structures from text or existing RDF data. In this submission, we present the first approach combining both sources of evidence, in particular we combine an existing logical learning approach with statistical relevance measures applied on textual resources. We perform an experiment involving a manual evaluation on 100 classes of the DBpedia 3.9 dataset and show that the inclusion of relevance measures leads to a significant improvement of the accuracy over the baseline algorithm."
      }
    ]
  },
  {
    "tid":"1390",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Scalable problems and memory-bounded speedup' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":" In this paper three models of parallel speedup are studied. They are fixed-sizespeedup, fixed-time speedup and memory-bounded speedup. The latter two consider therelationship between speedup and problem scalability. Two sets of speedup formulationsare derived for these three models. One set considers uneven workload allocationand communication overhead and gives more accurate estimation. Another set considersa simplified case and provides a clear picture on the impact of the sequential... "
      }
    ]
  },
  {
    "tid":"1391",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Unassisted thresholding based on multi-objective evolutionary algorithms.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"\u2022\u0391n Unassisted Thresholding (UTH) is presented to perform image segmentation while simultaneously determinating the optimal number of thresholds and their values.\u2022UTH is implemented with NSGA III, MOPSO, and PESA II to evaluate its performance.\u2022A particle with variable length is used to encode threshold values on the population."
      }
    ]
  },
  {
    "tid":"1392",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Weaknesses and improvements of the Yoon-Ryu-Yoo remote user authentication scheme using smart cards' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Remote user authentication scheme is a procedure which allows a server to authenticate a remote user through insecure channel. Recently, Yoon, Ryu and Yoo made an enhancement based on Ku\u2013Chen\u2019s remote user authentication scheme by using smart cards. The scheme has the merits of providing mutual authentication, no verification table, freely choosing password, involving only few hashing operations and parallel session attack resistance. In this paper, we point out security flaws of Yoon\u2013Ryu\u2013Yoo\u2019s protocols against masquerading attack, off-line password guessing attacks and parallel session attack. An improvement to enhance Yoon\u2013Ryu\u2013Yoo\u2019s security scheme is proposed."
      }
    ]
  },
  {
    "tid":"1393",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Demand-driven computation of interprocedural data flow' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a general framework for deriving demand-driven algorithms for interprocedural data flow analysis of imperative programs. The goal of demand-driven analysis is to reduce the time and/or space overhead of conventional exhaustive analysis by avoiding the collection of information that is not needed. In our framework, a demand for data flow information is modeled as a set of date flow queries. The derived demand-driven algorithms find responses to these queries through a partial reversal of the respective data flow analysis. Depending on whether minimizing time or space is of primary concern, result caching may be incorporated in the derived algorithm. Our framework is applicable to interprocedural data flow problems with a finite domain set. If the problem's flow functions are distributive, the derived demand algorithms provide as precise information as the corresponding exhaustive analysis. For problems with monotone but non-distributive flow functions the provided data flow solutions are only approximate. We demonstrate our approach using the example of interprocedural copy constant propagation."
      }
    ]
  },
  {
    "tid":"1394",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multi-scale salient object detection with pyramid spatial pooling.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Salient object detection is a challenging task in complex compositions depicting multiple objects of different scales. Albeit the recent progress thanks to the convolutional neural networks, the state-of-the-art salient object detection methods still fall short to handle such real-life scenarios. In this paper, we propose a new method called MP-SOD that exploits both Multi-Scale feature fusion and Pyramid spatial pooling to detect salient object regions in varying sizes. Our framework consists of a front-end network and two multi-scale fusion modules. The front-end network learns an end-to-end mapping from the input image to a saliency map, where a pyramid spatial pooling is incorporated to aggregate rich context information from different spatial receptive fields. The multi-scale fusion module integrates saliency cues across different layers, that is from low-level detail patterns to high-level semantic information by concatenating feature maps, to segment out salient objects with multiple scales. Extensive experimental results on eight benchmark datasets demonstrate the superior performance of our method compared with existing methods."
      }
    ]
  },
  {
    "tid":"1395",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'The Extraction Method Of The Service Improvement Information From Guests' Review' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The online hotel reservation service becomes so popular that the number of transactions has been growing year by year. Before making such reservation, travelers would find it important to refer other guests' opinions about accomodations. In those reviews, both dissatisfied and satisfied impressions used to appear in the same comments. In order to analyze this tendency, we employ text mining and investigate dissatisfied topic from their expressions. We propose some ways to extract useful information from guest review for accommodation's service improvement. (C) 2013 The Authors. Published by Elsevier B.V."
      }
    ]
  },
  {
    "tid":"1396",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Localization of Lesions in Dermoscopy Images Using Ensembles of Thresholding Methods' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Dermoscopy is one of the major imaging modalities used in the diagnosis of melanoma and other pigmented skin lesions. Due to the difficulty and subjectivity of human interpretation, automated analysis of dermoscopy images has become an important research area. Border detection is often the first step in this analysis. In this article, we present an approximate lesion localization method that serves as a preprocessing step for detecting borders in dermoscopy images. In this method, first the black frame around the image is removed using an iterative algorithm. The approximate location of the lesion is then determined using an ensemble of thresholding algorithms. Experiments on a large set of images demonstrate that the presented method achieves both fast and accurate localization of lesions in dermoscopy images."
      }
    ]
  },
  {
    "tid":"1397",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Forward-Secure Grouping-Proof Protocol For Multiple Rfid Tags' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Designing secure and robust grouping-proof protocols based on RFID characteristics becomes a hotspot in the research of security in Internet of Things (IOT). The proposed grouping-proof protocols recently have security and/or privacy omission and these schemes afford order-dependence by relaying message among tags through an RFID reader. In consequence, aiming at enhancing the robustness, improving scalability, reducing the computation costs on resource-constrained devices, and meanwhile combing Computational Intelligence (CI) with Secure Multiparty Communication (SMC), a Forward-Secure Grouping-Proof Protocol (FSGP) for multiple RFID tags based on Shamir's (n, n) secret sharing is proposed. In comparison with the previous grouping-proof protocols, FSGP has the characteristics of forward-security and order-independence addressing the scalability issue by avoiding relaying message. Our protocol provides security enhancement, performance improvement, and meanwhile controls the computation cost, which equilibrates both security and low cost requirements for RFID tags."
      }
    ]
  },
  {
    "tid":"1398",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Virtual access network embedding in wireless mesh networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Network virtualization of a wireless mesh network (WMN) is an economical way for different subscribers to customize their exclusive access networks through a common network infrastructure. The most critical task of network virtualization is virtual network embedding, which can be divided into two sub-problems: node mapping and link mapping. Although there exist approaches to virtual network embedding in wired networks, the characteristics of WMNs make virtual network embedding become a unique and challenging problem. In this paper, virtual access network embedding is studied for WMNs. To support flexible resource allocation in virtual access network embedding, each access node is designed based on orthogonal frequency division multiple access (OFDMA) dual-radio architecture. Through subcarrier allocation on each link, virtual access networks are gracefully separated from each other. To coordinate channel assignment across different links under the constraint of a limited number of orthogonal channels, a novel channel allocation algorithm is proposed to exploit partially-overlapped channels to improve resource utilization. Since the virtual access network embedding problem is NP-hard, a heuristic algorithm is developed based on an enhanced genetic algorithm to obtain an approximate but effective solution. Simulation results illustrate that the virtual access network embedding framework developed in this paper works effectively in WMNs."
      }
    ]
  },
  {
    "tid":"1399",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Delay optimal low-power circuit clustering for FPGAs with dual supply voltages' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a delay optimal FPGA clustering algorithm targeting low power. We assume that the configurable logic blocks of the FPGA can be programmed using either a high supply voltage (high-Vdd) or a low supply voltage (low-Vdd). We carry out the clustering procedure with the guarantee that the delay of the circuit under the general delay model is optimal, and in the meantime, logic blocks on the non-critical paths can be driven by low-Vdd to save power. We explore a set of dual-Vdd combinations to find the best ratio between low-Vdd and high-Vdd to achieve the largest power reduction. Experimental results show that our clustering algorithm can achieve power savings by 20.3% on average compared to the clustering result for an FPGA with a single high-Vdd. To our knowledge, this is the first work on dual-Vdd clustering for FPGA architectures."
      }
    ]
  },
  {
    "tid":"1400",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Towards preserving results confidentiality in cloud-based scientific workflows' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cloud computing has established itself as a solid computational model that allows for scientists to deploy their simulation-based experiments on distributed virtual resources to execute a wide range of scientific experiments. These experiments can be modeled as scientific workflows. Many of these workflows are data-intensive and produce a large volume of data, which is also stored in the cloud using storage services by Scientific Workflow Management Systems (SWfMS). One main issue regarding cloud storage services is confidentiality of stored data, i.e. if unauthorized people access data files they can infer knowledge about the results or even about the workflow structure. Encryption is a possible solution, but it may not be be sufficient and a new level of security can be added to preserve data confidentiality: data dispersion. In order to reduce this risk, generated data files cannot be stored in the same bucket, or at least sensitive data files have to be distributed across many cloud storage. In this paper, we present IPConf, an approach to preserve workflow results confidentiality in cloud storage. IPConf generates a distribution plan for data files generated during a workflow execution. This plan disperses data files in several cloud storage to preserve confidentiality. This distribution plan is then sent to the SWfMS that effectively stores generated data into specific buckets during workflow execution. Experiments performed using real data from SciPhy workflow executions indicate the potential of the proposed approach.\n\n"
      }
    ]
  },
  {
    "tid":"1401",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Automated Patient Handling Activity Recognition For At-Risk Caregivers Using An Unobtrusive Wearable Sensor' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Patient handling activities with awkward postures expose healthcare providers to a high risk of overexertion injury. The recognition of patient handling activities (PHA) is the first step to reduce injury risk for caregivers. In this paper, we propose a system to solve the problem, which comprises an unobtrusive wearable device and a novel spatio-temporal warping (STW) pattern recognition framework. The wearable device, named Smart Insole 2.0, is equipped with a rich set of sensors and can capture the information of patient handling activities. The STW pattern recognition framework fully exploits the spatial and temporal characteristics of plantar pressure, to quantify the similarity for the purpose of activity recognition. we perform a pilot study with eight subjects, including eight common activities in a nursing room. The experimental results show the overall classification accuracy achieves 91.7%. Meanwhile, the qualitative profile and load level can also be classified with accuracies of 98.3% and 92.5%, respectively."
      }
    ]
  },
  {
    "tid":"1402",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Uplink Component Carrier Selection for LTE-Advanced Systems with Carrier Aggregation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Carrier aggregation (CA) is one of the key features for LTE-Advanced. By means of CA, two or more component carriers (CCs) can be aggregated to form a much wider transmission bandwidth up to 100 MHz in order to meet the IMT-Advanced requirements. With CA, it is possible to schedule a user equipment (UE) on multiple component carriers simultaneously. From radio resource management (RRM) perspective, CC selection plays an important role in optimizing the system performance with CA. In this paper, we investigate the uplink resource allocation for LTE-Advanced Systems, i.e., how to assign CCs to different users, and how to do the power scaling for each user. A pathloss threshold based CC selection algorithm is proposed to distinguish between power-limited and non-power-limited LTE-Advanced UEs, and assign only one CC to power-limited LTE-Advanced UEs, while assign multiple CCs to non-power-limited LTE-Advanced UEs. Simulation results show that the derived pathloss threshold works effectively and efficiently compared to other thresholds, and the proposed CC selection algorithm can achieve much better performance in terms of cell edge, average, and cell center user throughput compared to blindly assigning all LTE-Advanced UEs on all CCs."
      }
    ]
  },
  {
    "tid":"1403",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Joint Decoding and Adaptive Compression with QoS Constraint for Uplinks in Cloud Radio Access Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Cloud Radio Access Network (C-RAN) is a promising candidate for future mobile networks to sustain the exponentially increasing demand for data rate. The centralized architecture enables C-RAN to exploit multi-cell cooperation and interference management effectively. In C-RAN, one baseband unit (BBU) communicates with users through distributed Remote Radio Heads (RRHs) which are connected to the BBU via high capacity, low latency fronthaul links and perform ``soft\" relaying. However, the architecture of C-RAN imposes a shortage of fronthaul bandwidth because raw In-phase/Quadrature-phase (I/Q) samples are exchanged between the RRHs and the BBU. In this paper, we leverage on advanced signal processing to improve the compression efficiency in fronthaul uplinks. Specifically, we propose a joint decoding algorithm at the BBU that exploits the correlation among the RRHs and jointly performs decompressing and decoding. An upper bound of the Block Error Rate (BLER) of the proposed algorithm is derived using pair-wise error probability analysis. Based on the BLER upper bound, we propose an adaptive compression scheme which minimizes the fronthaul transmission rate while satisfying a target quality of service constrain on the BLER. Our proposed adaptive compressor originates from practical scenarios in which most applications tolerate certain non-zero BLER thresholds."
      }
    ]
  },
  {
    "tid":"1404",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Is a Picture worth a Thousand Words?: A Comparative Analysis of Using Textual and Graphical Approaches to Specify Use Cases.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Use cases specifications are artifacts employed in all stages of software development, from the requirements elicitation to implementation. During this process, issues related to ambiguity, redundancy, inconsistency, and incompleteness can affect these specifications. These issues can harm software engineers' understanding and, consequently, affect the software quality. Given this context, this paper describes an empirical study to evaluate two different use cases specifications approaches (textual and graphical-based forms). We compared the approaches by assessing the degree of correctness and the time spent to generate the specifications. In addition, we performed an analysis focusing on evaluating the ease of use and usefulness of each approach. The quantitative results showed that textual form and graphical-based specifications presented similar levels of correctness and the time spent to generate them were also similar. The qualitative results indicated that the subjects had difficulties using both approaches; however, subjects stated that graphic-based specifications were easier and more useful to specify use cases."
      }
    ]
  },
  {
    "tid":"1405",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Generalized security analysis of the random key bits leakage attack' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In CRYPTO 2009, Heninger and Shacham presented a new method of recovering RSA private keys bit by bit given a fraction of private data, and analyzed resistance of RSA against the attack. They obtained a system of relations between RSA private variables and calculated the expected number of solution candidates. As they dealt with only RSA case, we consider the case that the system of equations is given in more general linear form. We show that the complexity of their attack depends only on the number of variables, the number of ambiguous variables, and the degree of freedom. As concrete examples, we apply the attack to Paillier cryptosystem and Takagi's variant of RSA, and analyze their resistance against the attack. In Pailiier's case, its resistance is almost the same as the case when a fraction of three private RSA keys are leaked. In Takagi's case, we find that the asymmetricity in two factors of the modulus give some effects on the resistance against the attack."
      }
    ]
  },
  {
    "tid":"1406",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Unified Metric for Routing and Rate Adaptation in Multi-Rate Wireless Mesh Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a new metric that is applicable both to routing and rate adaptation in multi-rate wireless mesh networks. Unlike many previous efforts, our metric is comprehensive; it considers several factors that affect end-to-end performance such as the effect of the relative positions of the links on a path when choosing the rates of operation and the importance of avoiding congested areas. We call our metric ETM (for Expected Transmission cost in Multi-rate wireless networks). We analytically derive the ETM metric. We show that the ETM metric can be used (a) to determine the best end-to-end path with a greedy routing approach and (b) it can be used to dynamically select the best transmission rate for each link on the path via a dynamic programming approach. Based on ETM, we design and implement the ETM-framework on an indoor wireless mesh network and compare its performance with that of the popular ETT and the recently proposed ETOP metrics. Our experiments show that the ETM-framework yields throughput improvements of up to 253% and 368% as compared with the ETT and ETOP frameworks."
      }
    ]
  },
  {
    "tid":"1407",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Face recognition \u2013 combine generic and specific solutions' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In many realistic face recognition applications, such as surveillance photo identification, the subjects of interest usually have only a limited number of image samples a-priori. This makes the recognition a difficult task, especially when only one image sample is available for each subject. In such a case, the performance of many well known face recognition algorithms will deteriorate rapidly and some of the algorithms even fail to apply. In this paper, we introduced a novel scheme to solve the one training sample problem by combining a specific solution learned from the samples of interested subjects and a generic solution learned from the samples of many other subjects. A multi-learner framework is firstly applied to generate and combine a set of generic base learners followed by a second combination with the specific learner. Extensive experiments based on the FERET database suggests that in the scenario considered here, the proposed solution significantly boosts the recognition performance."
      }
    ]
  },
  {
    "tid":"1408",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Motion-refined rewriting of H.264/AVC-coded video to SVC streams' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we discuss motion-refined rewriting of single-layer H.264/AVC streams to SVC streams with multiple quality layers. First, we elaborate on techniques we developed for efficient rewriting of residual data from H.264/AVC to SVC. We investigate if rate-distortion performance can further be improved by extending these architectures with motion refinement techniques, which exploit the inter-layer motion prediction mechanisms available in SVC. For optimum performance, we discuss a fast rate-distortion technique based on Lagrangian relaxation. Although motion refinement in the transform-domain leads to extra distortion in the bitstream, we show that our rate-distortion model successfully takes into account both base and enhancement layer rate and distortion during optimization. Implementation results show that motion-refined rewriting in the transform domain can increase rate-distortion performance, with gains of up to 0.5dB for the SVC base layer. The presented rewriting architectures significantly reduce the computational complexity when compared to reencoding, with a speed-up by a factor of forty or more, even in the case of motion refinement."
      }
    ]
  },
  {
    "tid":"1409",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Deformation Transition Graphs in Forming Operations of Rheological Objects' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Manipulative operations of rheological objects can be found in many industrial fields such as food industry and medical product industry. Automatic operations of rheological objects are eagerly required in these fields. In this paper, deformation transition graphs are pro- posed for the forming operations of theological objects. First, a novel forming machine with multi degrees of freedom is developed for the automatic forming of rhe- ological objects such as dough and paste. Second, we will introduce a deformation transition graph so that the forming process can be described. We will then de- velop a method to generate the deformation transition graph through experiments. Finally, we will explain a forming control method of a rheological object based on the deformation transition graph."
      }
    ]
  },
  {
    "tid":"1410",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Bi-objective release planning for evolving software systems' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The release planning (RP) problem can be investigated from two dimensions -- what to release and when to release. We investigate the \"what\" to release decision in terms of which new features or change requests should be assigned and implemented in which releases of a software system. RP for evolving systems is challenging, because the new features might require changes to the existing system. A major drawback of existing RP methods is that, they do not consider the existing systems in making RP decisions. In this paper, we present a technique to detect coupling between features based on relatedness of the components that would implement the features. The components implementing the features are derived from change impact analysis. We integrate the results from feature coupling into a RP strategy that encourages the assignment of highly coupled features in the same release. This helps to avoid haphazard implementation of related features. We present a decision support approach that formulates the RP problem as a bi-objective optimization problem. Our Bi-Objective Release Planning for Evolving Systems (BORPES) is aimed at optimizing the value of release plans from both the business perspectives and the implementation perspectives. This paper presents BORPES in detail and reports on a proof-of-concept case study that investigates the applicability of the proposed approach. The bi-objective optimization offers a set of Pareto-optimal solutions."
      }
    ]
  },
  {
    "tid":"1411",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Optimal Aviation Security Screening Strategies With Dynamic Passenger Risk Updates' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Passenger screening is a critical component of aviation security systems. This paper introduces the multistage sequential passenger screening problem (MSPSP), which models passenger and carry-on baggage screening operations in an aviation security system with the capability of dynamically updating the perceived risk of passengers. The passenger screening operation at an airport terminal is subdivided into multiple screening stages, with decisions made to assign each passenger to one of several available security classes at each such stage. Each passenger's assessed threat value (initially determined by an automated passenger prescreening system) is updated after the passenger proceeds through each screening stage. The objective of MSPSP is to maximize the total security of all passenger screening decisions over a fixed time period, given passenger perceived risk levels and security device performance parameters. An optimal policy for screening passengers in MSPSP is obtained using optimal sequential assignment theory. A Monte Carlo simulation-based heuristic is presented and compared with stochastic sequential assignment and feedback control algorithms. Computational analysis of a two-stage security system provides an assessment of the total security performance."
      }
    ]
  },
  {
    "tid":"1412",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Joint scheduling and routing with power control for centralized wireless sensor networks.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We consider a TDMA-based multi-hop wireless sensor network, where nodes send data to a sink, which is aware of received powers at all receivers; the sink is responsible for creating the network topology and assigning time slots to links. Under this centralized approach, we propose two algorithms that jointly define the tree topology connecting nodes to the sink, and assign time slots, avoiding any packet loss. In contrast with previous works, the proposed algorithms accurately account for interference effects; when evaluating the signal-to-interference ratio to establish the tree and schedule transmissions, we consider the sum of all actual interfering signals, a fact of relevance for networks with increasing number of nodes. Optimal selection of transmit powers, minimizing energy consumption, is also applied. Our algorithms are compared to a benchmark solution and other proposals from the literature; it is shown that they bring to better radio resource utilization, higher throughput and lower energy consumption, while keeping the average delay limited."
      }
    ]
  },
  {
    "tid":"1413",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Delta-SimRank computing on MapReduce' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Based on the intuition that \"two objects are similar if they are related to similar objects\", SimRank (proposed by Jeh and Widom in 2002) has become a famous measure to compare the similarity between two nodes using network structure. Although SimRank is applicable to a wide range of areas such as social networks, citation networks, link prediction, etc., it suffers from heavy computational complexity and space requirements. Most existing efforts to accelerate SimRank computation work only for static graphs and on single machines. This paper considers the problem of computing SimRank efficiently in a distributed system while handling dynamic networks which grow with time. We first consider an abstract model called Harmonic Field on Node-pair Graph. We use this model to derive SimRank and the proposed Delta-SimRank, which is demonstrated to fit the nature of distributed computing and can be efficiently implemented using Google's MapReduce paradigm. Delta-SimRank can effectively reduce the computational cost and can also benefit the applications with non-static network structures. Our experimental results on four real world networks show that Delta-SimRank is much more efficient than the distributed SimRank algorithm, and leads to up to 30 times speed-up in the best case1."
      }
    ]
  },
  {
    "tid":"1414",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An efficient closed frequent itemset miner for the MOA stream mining system' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Mining itemsets is a central task in data mining, both in the batch and the streaming paradigms. While robust, efficient, and well-tested implementations exist for batch mining, hardly any publicly available equivalent exists for the streaming scenario. The lack of an efficient, usable tool for the task hinders its use by practitioners and makes it difficult to assess new research in the area. To alleviate this situation, we review the algorithms described in the literature, and implement and evaluate the IncMine algorithm by Cheng, Ke and Ng [J. Intell. Inf. Syst. 31(3) (2008), 191\u0096215] for mining frequent closed itemsets from data streams. Our implementation works on top of the MOA (Massive Online Analysis) stream mining framework to ease its use and integration with other stream mining tasks. We provide a PAC-style rigorous analysis of the quality of the output of IncMine as a function of its parameters; this type of analysis is rare in pattern mining algorithms. As a by-product, the analysis shows how one of the user-provided parameters in the original description can be removed entirely while retaining the performance guarantees. Finally, we experimentally confirm both on synthetic and real data the excellent performance of the algorithm, as reported in the original paper, and its ability to handle concept drift."
      }
    ]
  },
  {
    "tid":"1415",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Explicit web search result diversification' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Queries submitted to a web search engine are typically short and often ambiguous. With the enormous size of the Web, a misunderstanding of the information need underlying an ambiguous query can misguide the search engine, ultimately leading the user to abandon the originally submitted query. In order to overcome this problem, a sensible approach is to diversify the documents retrieved for the user's query. As a result, the likelihood that at least one of these documents will satisfy the user's actual information need is increased. In this thesis, we argue that an ambiguous query should be seen as representing not one, but multiple information needs. Based upon this premise, we propose xQuAD---Explicit Query Aspect Diversification, a novel probabilistic framework for search result diversification. In particular, the xQuAD framework naturally models several dimensions of the search result diversification problem in a principled yet practical manner. To this end, the framework represents the possible information needs underlying a query as a set of keyword-based sub-queries. Moreover, xQuAD accounts for the overall coverage of each retrieved document with respect to the identified sub-queries, so as to rank highly diverse documents first. In addition, it accounts for how well each sub-query is covered by the other retrieved documents, so as to promote novelty--and hence penalise redundancy---in the ranking. The framework also models the importance of each of the identified sub-queries, so as to appropriately cater for the interests of the user population when diversifying the retrieved documents. Finally, since not all queries are equally ambiguous, the xQuAD framework caters for the ambiguity level of different queries, so as to appropriately trade-off relevance for diversity on a per-query basis. The xQuAD framework is general and can be used to instantiate several diversification models, including the most prominent models described in the literature. In particular, within xQuAD, each of the aforementioned dimensions of the search result diversification problem can be tackled in a variety of ways. In this thesis, as additional contributions besides the xQuAD framework, we introduce novel machine learning approaches for addressing each of these dimensions. These include a learning to rank approach for identifying effective subqueries as query suggestions mined from a query log, an intent-aware approach for choosing the ranking models most likely to be effective for estimating the coverage and novelty of multiple documents with respect to a sub-query, and a selective approach for automatically predicting how much to diversify the documents retrieved for each individual query. In addition, we perform the first empirical analysis of the role of novelty as a diversification strategy for web search. As demonstrated throughout this thesis, the principles underlying the xQuAD framework are general, sound, and effective. In particular, to validate the contributions of this thesis, we thoroughly assess the effectiveness of xQuAD under the standard experimentation paradigm provided by the diversity task of the TREC 2009, 2010, and 2011 Web tracks. The results of this investigation demonstrate the effectiveness of our proposed framework. Indeed, xQuAD attains consistent and significant improvements in comparison to the most effective diversification approaches in the literature, and across a range of experimental conditions, comprising multiple input rankings, multiple sub-query generation and coverage estimation mechanisms, as well as queries with multiple levels of ambiguity. Altogether, these results corroborate the state-of-the-art diversification performance of xQuAD. Available online at http://theses.gla.ac.uk/4106/."
      }
    ]
  },
  {
    "tid":"1416",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Exactly Sparse Delayed-State Filters' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents the novel insight that the SLAM information matrix is exactly sparse in a delayed-state framework. Such a framework is used in view-based representations of the environment which rely upon scan-matching raw sensor data. Scan-matching raw data results in virtual observations of robot motion with respect to a place its previously been. The exact sparseness of the delayed-state information matrix is in contrast to other recent feature based SLAM information algorithms like Sparse Extended Information Filters or Thin Junction Tree Filters. These methods have to make approximations in order to force the feature-based SLAM information matrix to be sparse. The benefit of the exact sparseness of the delayed-state framework is that it allows one to take advantage of the information space parameterization without having to make any approximations. Therefore, it can produce equivalent results to the \u201cfull-covariance\u201d solution."
      }
    ]
  },
  {
    "tid":"1417",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Anomaly Detection in Dynamic Networks of Varying Size.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"  Dynamic networks, also called network streams, are an important data representation that applies to many real-world domains. Many sets of network data such as e-mail networks, social networks, or internet traffic networks are best represented by a dynamic network due to the temporal component of the data. One important application in the domain of dynamic network analysis is anomaly detection. Here the task is to identify points in time where the network exhibits behavior radically different from a typical time, either due to some event (like the failure of machines in a computer network) or a shift in the network properties. This problem is made more difficult by the fluid nature of what is considered \"normal\" network behavior. The volume of traffic on a network, for example, can change over the course of a month or even vary based on the time of the day without being considered unusual. Anomaly detection tests using traditional network statistics have difficulty in these scenarios due to their Density Dependence: as the volume of edges changes the value of the statistics changes as well making it difficult to determine if the change in signal is due to the traffic volume or due to some fundamental shift in the behavior of the network. To more accurately detect anomalies in dynamic networks, we introduce the concept of Density-Consistent network statistics. On synthetically generated graphs anomaly detectors using these statistics show a a 20-400% improvement in the recall when distinguishing graphs drawn from different distributions. When applied to several real datasets Density-Consistent statistics recover multiple network events which standard statistics failed to find. "
      }
    ]
  },
  {
    "tid":"1418",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Multi-level RAID for very large disk arrays' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Very Large Disk Arrays - VLDAs have been developed to cope with the rapid increase in the volume of data generated requiring ultrareliable storage. Bricks or Storage Nodes - SNs holding a dozen or more disks are cost effective VLDA building blocks, since they cost less than traditional disk arrays. We utilize the Multilevel RAID - MRAID paradigm for protecting both SNs and their disks. Each SN is a k-disk-failure-tolerant kDFT array, while replication or l-node failure tolerance - lNFTs paradigm is applied at the SN level. For example, RAID1(M)/5(N) denotes a RAID1 at the higher level with a degree of replication M and each virtual disk is an SN configured as a RAID5 with N physical disks. We provide the data layout for RAID5/5 and RAID6/5 MRAIDs and give examples of updating data and recovering lost data. The former requires storage transactions to ensure the atomicity of storage updates. We discuss some weaknesses in reliability modeling in RAID5 and give examples of an asymptotic expansion method to compare the reliability of several MRAID organizations. We outline the reliability analysis of Markov chain models of VLDAs and briefly report on conclusions from simulation results. In Conclusions we outline areas for further research."
      }
    ]
  },
  {
    "tid":"1419",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'An improved information retrieval method and input device using gloves for wearable computers' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we describe glove-based information retrieval method and input device for wearable computers. We suggest an easy and effective alphanumeric input algorithm using gloves and conduct efficiency test. The key to the development of the proposed device is the use of unique operator-to-key mapping method, key-to-symbol mapping method and simple algorithm. We list and discuss traditional algorithm and method using a glove, then describe an improved newly proposed algorithm using gloves. The efficiency test was conducted and the results were compared with other glove based device and algorithm for wearable computers."
      }
    ]
  },
  {
    "tid":"1420",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Multi-Scale Tikhonov Regularization Scheme for Implicit Surface Modelling' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Kernel machines have recently been considered as a promising solution for implicit surface modelling. A key challenge of machine learning solutions is how to fit implicit shape models from large-scale sets of point cloud samples efficiently. In this paper, we propose a fast solution for approximating implicit surfaces based on a multi-scale Tikhonov regularization scheme. The optimization of our scheme is formulated into a sparse linear equation system, which can be efficiently solved by factorization methods. Different from traditional approaches, our scheme does not employ auxiliary off-surface points, which not only saves the computational cost but also avoids the problem of injected noise. To further speedup our solution, we present a multi-scale surface fitting algorithm of coarse to fine modelling. We conduct comprehensive experiments to evaluate the performance of our solution on a number of datasets of different scales. The promising results show that our suggested scheme is considerably more efficient than the state-of-the-art approach."
      }
    ]
  },
  {
    "tid":"1421",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'TF-ICF: A New Term Weighting Scheme for Clustering Dynamic Data Streams' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we propose a new term weighting scheme called Term Frequency -- Inverse Corpus Frequency (TF-ICF). It does not require term frequency information from other documents within the document collection and thus, it enables us to generate the document vectors of N streaming documents in linear time. In the context of a machine learning application, unsupervised document clustering, we evaluated the effectiveness of the proposed approach in comparison to five widely used term weighting schemes through extensive experimentation. Our results show that TF-ICF can produce document clusters that are of comparable quality as those generated by the widely recognized term weighting schemes and it is significantly faster than those methods."
      }
    ]
  },
  {
    "tid":"1422",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Tensor Framework and Combined Symmetry for Hypertext Mining' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Petitcolas has proposed a steganographic technique called MP3Stego which can hide secret messages in a MP3 audio. This technique is well-known because of its high capacity. However, in rare cases, the normal audio encoding process will be terminated ..."
      }
    ]
  },
  {
    "tid":"1423",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Accelerating Comparative Genomics Workflows in a Distributed Environment with Optimized Data Partitioning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The advent of new sequencing technology has generated massive amounts of biological data at unprecedented rates. High-throughput bioinformatics tools are required to keep pace with this. Here, we implement a workflow-based model for parallelizing the data intensive task of genome alignment and variant calling with BWA and GATK's Haplotype Caller. We explore different approaches of partitioning data and how each affect the run time. We observe granularity-based partitioning for BWA and alignment-based partitioning for Halo type Caller to be the optimal choices for the pipeline. We identify the various challenges encountered while developing such an application and provide an insight into addressing them. We report significant performance improvements, from 12 days to 4 hours, while running the BWA-GATK pipeline using 100 nodes for analyzing high-coverage oak tree data."
      }
    ]
  },
  {
    "tid":"1424",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On Spatial-Aware Community Search' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Communities are prevalent in social networks, knowledge graphs, and biological networks. Recently, the topic of community search (CS) has received plenty of attention. The CS problem aims to look for a dense subgraph that contains a query vertex. Existing CS solutions do not consider the spatial extent of a community. They can yield communities whose locations of vertices span large areas. In applications that facilitate setting social events (e.g., finding conference attendees to join a dinner), it is important to find groups of people who are physically close to each other, so it is desirable to have a \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">spatial-aware community</italic>\n (or SAC), whose vertices are close structurally and spatially. Given a graph \n<inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$G$</tex-math><alternatives><inline-graphic xlink:href=\"fang-ieq1-2845414.gif\"/></alternatives></inline-formula>\n and a query vertex \n<inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$q$</tex-math><alternatives><inline-graphic xlink:href=\"fang-ieq2-2845414.gif\"/></alternatives></inline-formula>\n, we develop an exact solution to find the SAC containing \n<inline-formula xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><tex-math notation=\"LaTeX\">$q$</tex-math><alternatives><inline-graphic xlink:href=\"fang-ieq3-2845414.gif\"/></alternatives></inline-formula>\n, but it cannot scale to large datasets, so we design three approximation algorithms. We further study the problem of continuous SAC search on a \u201cdynamic spatial graph,\u201d whose vertices\u2019 locations change with time, and propose three fast solutions. We evaluate the solutions on both real and synthetic datasets, and the results show that SACs are better than communities returned by existing solutions. Moreover, our approximation solutions perform accurately and efficiently."
      }
    ]
  },
  {
    "tid":"1425",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title '/*icomment: bugs or bad comments?*/' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Commenting source code has long been a common practice in software development. Compared to source code, comments are more direct, descriptive and easy-to-understand. Comments and sourcecode provide relatively redundant and independent information regarding a program's semantic behavior. As software evolves, they can easily grow out-of-sync, indicating two problems: (1) bugs -the source code does not follow the assumptions and requirements specified by correct program comments; (2) bad comments - comments that are inconsistent with correct code, which can confuse and mislead programmers to introduce bugs in subsequent versions. Unfortunately, as most comments are written in natural language, no solution has been proposed to automatically analyze commentsand detect inconsistencies between comments and source code. This paper takes the first step in automatically analyzing commentswritten in natural language to extract implicit program rulesand use these rules to automatically detect inconsistencies between comments and source code, indicating either bugs or bad comments. Our solution, iComment, combines Natural Language Processing(NLP), Machine Learning, Statistics and Program Analysis techniques to achieve these goals. We evaluate iComment on four large code bases: Linux, Mozilla, Wine and Apache. Our experimental results show that iComment automatically extracts 1832 rules from comments with 90.8-100% accuracy and detects 60 comment-code inconsistencies, 33 newbugs and 27 bad comments, in the latest versions of the four programs. Nineteen of them (12 bugs and 7 bad comments) have already been confirmed by the corresponding developers while the others are currently being analyzed by the developers."
      }
    ]
  },
  {
    "tid":"1426",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'On TCP and self-similar traffic' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"We re-examine the same TCP trace that was used by Veres et al. [A. Veres, M. Boda, The chaotic nature of TCP congestion control, in: Proceedings of the IEEE INFOCOM, 2000] to claim that TCP creates self-similar traffic. A careful reassessment of their data analysis shows that this claim is not justified and suggests that the TCP trace in question is not consistent with (asymptotic second-order) self-similarity or long-range dependence (LRD). We illustrate the reasons that led to the claim in [A. Veres, M. Boda, The chaotic nature of TCP congestion control, in: Proceedings of the IEEE INFOCOM, 2000] and provide some practical guidelines for assessing a statistical characteristic of trace data such as LRD that is defined in strictly asymptotic terms. Our conclusion is in full agreement with the findings obtained from analyzing a much longer TCP trace (resulting from repeating the same simulation as in [A. Veres, M. Boda, The chaotic nature of TCP congestion control, in: Proceedings of the IEEE INFOCOM, 2000], but running it for a longer period) and with analytical results derived from a detailed Markovian model of TCP. These results show that the traffic generated by a long-lived TCP connection, while exhibiting pronounced correlations over a predictable finite range of time-scales, cannot be (asymptotically second-order) self-similar or exhibit LRD. Our work serves as a reminder of the importance of careful trace analysis and detailed examination (and cross-validation) of alternative explanations when establishing or characterizing the generality of any particular finding about Internet traffic."
      }
    ]
  },
  {
    "tid":"1427",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Efficient concurrent multipath transfer using network coding in wireless networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Concurrent Multipath Transfer (CMT), enabled by Stream Control Transmission Protocol (SCTP), is considered as one preferred data-transport mode due to increased available bandwidth. However, CMT performance degrades seriously in terms of data reordering due to path dissimilarity and frequent packet loss from wireless unreliability. Most relevant solutions follow the packet sequence numbers and thereby focus on strict in-order reception and packet-specific retransmission. Passively adapting to the network conditions, those approaches are not general and well enough responding to the dynamicity of wireless environment. By applying Network Coding (NC) to CMT, this paper proposes a progressive end-to-end solution (CMT-NC) to those problems in heterogeneous wireless networks. CMT-NC is capable of avoiding reordering and compensating lost packets. Further, an innovative group-based transmission management mechanism enhances the robustness and reliability of data transfer. Simulation results show how by using CMT-NC significant improvements in comparison to another state-of-the-art solution are obtained."
      }
    ]
  },
  {
    "tid":"1428",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Curve Skeleton Skinning For Human And Creature Characters' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The skeleton driven skinning technique is still the most popular method for animating deformable human and creature characters. Albeit all industry de facto due to its computational performance and intuitiveness, it suffers from problems like collapsing elbow and candy wrapper joint. To remedy these problems, one needs to formulate the non-linear relationship between the skeleton and the skin shape of a character properly, Which however proves mathematically very challenging. Placing additional joints Where the skin bends increases the sampling rate and is all ad hoc Way of approximating this non-linear relationship. In this paper, we propose a method that is able to accommodate the inherent non-linear relationships between the movement of the skeleton and the skin shape. We use the so-called curve skeletons along With the joint-based skeletons to animate the skin shape. Since the deformation follows the tangent of the curve skeleton and also due to higher sampling rates received from the curve points, collapsing skin and other undesirable skin deformation problems are avoided. The curve skeleton retains the advantages of the current skeleton driven skinning. It is easy to use and allows full control over the animation process. As a further enhancement, it is also fairly simple to build realistic muscle and fat bulge effect. A practical implementation in the form of a Maya plug-in is created to demonstrate the viability of the technique. Copyright (c) 2006 John Wiley & Sons, Ltd."
      }
    ]
  },
  {
    "tid":"1429",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Fast Ellipse Detection Algorithm Using Hough Transform on the GPU' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"GPUs (Graphics Processing Units) are specialized microprocessors that accelerate 3D or 2D graphics operations. Recent GPUs, which have many processing units connected with a global memory, can be used for general purpose parallel computation. To utilize the powerful computing ability, GPUs are widely used for general purpose computing. The main purpose of this paper is an ellipse detection algorithm with Hough transform. The feature of our algorithm is that to reduce computational time and space, the parameter spaces in the Hough transform are decomposed for each parameter and each parameter is computed in series. Also, we implemented our algorithm on a modern GPU system. The experimental results show that, for an input image with size of 2040$\\times$2040, our GPU implementation can achieve a speedup factor of approximately 64 times over the sequential implementation without the GPU support."
      }
    ]
  },
  {
    "tid":"1430",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Improving the Compactness in Social Network Thematic Groups by Exploiting a Multi-dimensional User-to-Group Matching Algorithm' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The internal organization of an Online Social Network is well described by the formation of groups between members. Often groups evolve in a very confusing way, due to occasional interactions between their components. However, it does not necessarily imply the formation of aggregation units in which users have similar interests and behaviour and, contextually, mutually trust with each others. Users can form groups (or join already existing groups) on the basis of shared interests or because dense social connections exist among group members, however, it is not uncommon the birth and growth of thematic groups, i.e., those groups arising from the social aggregation of users around a specific topics of interest. In this context, we argue that a multi-dimensional organization of the social network, in which each dimension represents the projection of the network on a given topic, could facilitate the task of forming compact groups. In this paper, after defining a notion of compactness for a group, that integrates similarity and mutual trust, we propose to provide each user with a software agent associated with each topic of interest for the user, and that represents a user's avatar in the corresponding dimension. This allows the user to delegate to his/her agent the management of group joining requests regarding a given topic, selecting only those interlocutors which appear the most appropriate for their owners. In our approach a Users-to-Group matching algorithm allows the agents to dynamically manage the evolution of the social network organization. Some experiments on real data clearly show the advantages introduced from our approach in assigning the users only to groups compatible with their orientations."
      }
    ]
  },
  {
    "tid":"1431",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A performance comparison of three algorithms for proximity queries relative to convex polyhedra' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents a comparative analysis relative to the experimental performances of an asymptotically fast and incremental algorithm, recently developed to compute collision translations for pairs of convex polyhedra. The algorithm may be worth considering because it solves a proximity problem which is less widely addressed than distance, as well as because of its peculiar computation strategy, well suited to work without initialization, but also endowed with an inherently embedded mechanism to exploit spatial coherence. Numerical data characterizing the behavior of the algorithm with respect to the complexity of the polyhedra have already been discussed elsewhere, thus here the main focus is on contrasting its performances with those of two popular algorithms designed to compute distances between polyhedra. Although the considered \"yardsticks\" answer different proximity queries, and although one of the techniques is meant to deal with general polyhedra, the results presented in this paper should help to assess the efficacy and potential of the approach under analysis. All the three algorithms, indeed, share the same kind of application context; moreover, on the basis of the asymptotic bounds discussed in the literature, distances and collision translations require similar computational efforts. A thorough comparison of the reported query times and, more significantly, of the corresponding trends seems to show that the behavior of the novel algorithm is quite interesting, especially when used without initialization, what should encourage further work on its peculiar approach"
      }
    ]
  },
  {
    "tid":"1432",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Applying Low Discrepancy Sequences for Node-ID Assignment in P2PSIP' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The IETF P2PSIP Working Group is currently designing a standard overlay protocol, named RELOAD, that employs a centralized node identifier (node-id) assignment for security reasons. Given this scenario, we propose the utilization of a Low Discrepancy Sequence (LDS) for the assignment of node-ids in the P2PSIP architecture. We perform an analytical and simulation study considering a Chord DHT that demonstrates that using a LDS-based node-id assignment guarantees a fair distribution of the node's zone of responsibility, even in high churn scenarios. Previous studies have shown that a fairer distribution of the zones of responsibility sizes leads to a fairer distribution of the storage and routing load. Therefore we conclude that the proposed LDS node-id assignment provides these features without adding any extra overhead."
      }
    ]
  },
  {
    "tid":"1433",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Real-Time Rendering of Rough Refraction under Dynamically Varying Environmental Lighting' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"The illumination of rough refraction is challenging due to the complex integrations of scattered lights from multiple directions. Prefiltered environment map is able to eliminate the comprehensive integrations, but the precomputation stage protects it from being adapted into dynamically varying environmental lighting. We propose a real-time method for rendering rough refraction, supporting dynamically changing lighting, geometries and materials. Environment maps are efficiently compressed by summed area table (SAT), whose construction only costs several milliseconds. Meanwhile, rough refraction effects are approximated by our lobe merging method. Solid results of our approach are demonstrated by comparing with ground truth results generated with a path tracing engine."
      }
    ]
  },
  {
    "tid":"1434",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Breaking the Oblivious-RAM Bandwidth Wall' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"PathORAM is a popular security primitive for obfuscating memory access patterns from a secure processor to an insecure main memory. Emerging throughput multicore and GPU processors provide immense memory bandwidth via multiple on-chip memory controllers. PathORAM translates a single off-chip cache line access into ~100 cache lines, thereby stressing the available memory bandwidth. However, current PathORAM scheme shows degradation of bandwidth utilization with an increase in the number of memory controllers. This deprivation in bandwidth utilization is primarily due to the fact that PathORAM falls short in proportionate distribution of memory accesses among all available on-chip memory controllers. This paper presents a novel ORAM path distribution scheme that ensures balanced load distribution among parallel on-chip memory controllers, and consequently improves secure processor performance by ~24% over state-of-the-art PathORAM scheme."
      }
    ]
  },
  {
    "tid":"1435",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Boosting for transfer learning' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Traditional machine learning makes a ba- sic assumption: the training and test data should be under the same distribution. However, in many cases, this identical- distribution assumption does not hold. The assumption might be violated when a task from one new domain comes, while there are only labeled data from a similar old domain. Labeling the new data can be costly and it would also be a waste to throw away all the old data. In this pa- per, we present a novel transfer learning framework called TrAdaBoost, which extends boosting-based learning algorithms (Freund & Schapire, 1997). TrAdaBoost allows users to utilize a small amount of newly labeled data to leverage the old data to construct a high-quality classification model for the new data. We show that this method can allow us to learn an accurate model using only a tiny amount of new data and a large amount of old data, even when the new data are not sufficient to train a model alone. We show that TrAdaBoost allows knowledge to be ef- fectively transferred from the old data to the new. The effectiveness of our algorithm is an- alyzed theoretically and empirically to show that our iterative algorithm can converge well to an accurate model."
      }
    ]
  },
  {
    "tid":"1436",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Characterization of Texture in Images by Using a Cellular Automata Approach.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Spatial patterns in an image that shows a visual perception of roughness or softness of the surface is known as the texture of the image. Most of the analysis and description of texture found in the literature is based on statistical or structural properties of this attribute [2]. The field of cellular automata (CA), which has been developed mainly to model the dynamical behavior of systems, is based on the behavior or arrangements of pixel values and their neighborhood which, according to some rules behaves in different manners [2, 8]. In this paper, within the frame of structural approach, a novel method based on the properties of linear cellular automata is proposed to characterize different sort of textures. To this purpose, it is assumed that a binary version of the image under study was generated by a cellular automata technique. By using this model a number of textural primitives are found which allows the production of a characterizing image. In order to verify the feasibility of the proposed method, texture images generated by CA techniques as well as natural images has been used."
      }
    ]
  },
  {
    "tid":"1437",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Benchmarking of objective quality metrics for HDR image quality assessment' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Abstract Recent advances in high dynamic range (HDR) capture and display technologies have attracted a lot of interest from scientific, professional, and artistic communities. As in any technology, the evaluation of HDR systems in terms of quality of experience is essential. Subjective evaluations are time consuming and expensive, and thus objective quality assessment tools are needed as well. In this paper, we report and analyze the results of an extensive benchmarking of objective quality metrics for HDR image quality assessment. In total, 35 objective metrics were benchmarked on a database of 20 HDR contents encoded with 3 compression algorithms at 4 bit rates, leading to a total of 240 compressed HDR images, using subjective quality scores as ground truth. Performance indexes were computed to assess the accuracy, monotonicity, and consistency of the metric estimation of subjective scores. Statistical analysis was performed on the performance indexes to discriminate small differences between metrics. Results demonstrated that metrics designed for HDR content, i.e., HDR-VDP-2 and HDR-VQM, are the most reliable predictors of perceived quality. Finally, our findings suggested that the performance of most full-reference metrics can be improved by considering non-linearities of the human visual system, while further efforts are necessary to improve performance of no-reference quality metrics for HDR content."
      }
    ]
  },
  {
    "tid":"1438",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Adaptive Neuro-Fuzzy Inference System Based Autonomous Flight Control of Unmanned Air Vehicles' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper proposes ANFIS logic based autonomous flight controller for UAVs (unmanned aerial vehicles). Three fuzzy logic modules are developed for the control of the altitude, the speed, and the roll angle, through which the altitude and the latitude-longitude of the air vehicle is controlled. The implementation framework utilizes MATLAB's standard configuration and the Aerosim Aeronautical Simulation Block Set which provides a complete set of tools for rapid development of detailed 6 degree-of-freedom nonlinear generic manned/unmanned aerial vehicle models. The Aerosonde UAV model is used in the simulations in order to demonstrate the performance and the potential of the controllers. Additionally, Microsoft Flight Simulator and FlightGear Flight Simulator are deployed in order to get visual outputs that aid the designer in the evaluation of the controllers. Despite the simple design procedure, the simulated test flights indicate the capability of the approach in achieving the desired performance."
      }
    ]
  },
  {
    "tid":"1439",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Selecting best viewpoint for human-pose estimation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Estimating human poses is an important step towards developing robots that can understand human motion. Since a human is highly articulated, changing viewpoints of sensors on robots can improve the accuracy of human-pose estimation. We propose a two-phase approach that determines the best viewpoint of a depth sensor for human-pose estimation. The proposed approach measures the quality of potential viewpoints and selects one of them as the best viewpoint for each human pose. Based on the quality of viewpoints, human poses can be directly mapped to the best viewpoint without reconstructing the human body. Thus, the proposed approach provides a discriminative mapping to determine the best viewpoint for estimating different human poses. To measure the quality of a potential viewpoint, the viewpoint is first instantiated by representing the depth sensor of the viewpoint using the finite projective camera model. The quality of the viewpoint is expressed in terms of the error of human-pose estimates. A mapping is derived by minimizing the error in a human-pose estimate among different viewpoints. The proposed two-phase approach has been evaluated on a benchmark database. Experimental results showed that the best viewpoint for a human pose could be determined by evaluating the quality of potential viewpoints. The mean error and standard deviation of human-pose estimates were reduced by using the best viewpoint determined by the proposed two-phase approach."
      }
    ]
  },
  {
    "tid":"1440",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Standalone training of context-dependent deep neural network acoustic models' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Recently, context-dependent (CD) deep neural network (DNN) hidden Markov models (HMMs) have been widely used as acoustic models for speech recognition. However, the standard method to build such models requires target training labels from a system using HMMs with Gaussian mixture model output distributions (GMM-HMMs). In this paper, we introduce a method for training state-of-the-art CD-DNN-HMMs without relying on such a pre-existing system. We achieve this in two steps: build a context-independent (CI) DNN iteratively with word transcriptions, and then cluster the equivalent output distributions of the untied CD-DNN HMM states using the decision tree based state tying approach. Experiments have been performed on the Wall Street Journal corpus and the resulting system gave comparable word error rates (WER) to CD-DNNs built based on GMM-HMM alignments and state-clustering."
      }
    ]
  },
  {
    "tid":"1441",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Cautious Models for General Logic Programs' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":" In this paper, cautious models of general logic programs are investigated. Such modelsare constructed iteratively using a monotonic operator which performs case analysis ontotal interpretations generated by enumerations of atoms. Consequently, every generallogic program has a unique cautious model. The new class of partial models is comparedwith well-founded and stable models of general logic programs. Various extensions of thesemodels are are also addressed. The time complexity of... "
      }
    ]
  },
  {
    "tid":"1442",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Mobi-Sync: Efficient Time Synchronization for Mobile Underwater Sensor Networks' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Time synchronization is a critical service for distributed networks. In this paper, we investigate this problem in underwater sensor networks (UWSNs). We propose a novel time synchronization scheme, called ``Mobi-Sync''. Mobi-Sync effectively utilizes the spatial correlation of underwater mobile sensor nodes to estimate the long and dynamic propagation delays. Simulation results show that Mobi-Sync outperforms existing schemes in both accuracy and energy efficiency."
      }
    ]
  },
  {
    "tid":"1443",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'A Route Planner Service with Recharging Reservation: Electric Itinerary with a Click.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In most countries the share of circulating electric vehicles (EV) is less than 1%. Beside cost issues, the detrimental factors that most discourage people from purchasing EVs are their limited range and the rather poor coverage area of the recharging infrastructure. This clearly indicates the need for higher investments in vehicular technologies and infrastructures; however, software services and ..."
      }
    ]
  },
  {
    "tid":"1444",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Colorization-based compression using optimization.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we formulate the colorization-based coding problem into an optimization problem, i.e., an L1 minimization problem. In colorization-based coding, the encoder chooses a few representative pixels (RP) for which the chrominance values and the positions are sent to the decoder, whereas in the decoder, the chrominance values for all the pixels are reconstructed by colorization methods. Th..."
      }
    ]
  },
  {
    "tid":"1445",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'To be or not to be convex? A study on regularization in hyperspectral image classification' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"Hyperspectral image classification has long been dominated by convex models, which provide accurate decision functions exploiting all the features in the input space. However, the need for high geometrical details, which are often satisfied by using spatial filters, and the need for compact models (i.e. relying on models issued form reduced input spaces) has pushed research to study alternatives such as sparsity inducing regularization, which promotes models using only a subset of the input features. Although successful in reducing the number of active inputs, these models can be biased and sometimes offer sparsity at the cost of reduced accuracy. In this paper, we study the possibility of using non-convex regularization, which limits the bias induced by the regularization. We present and compare four regularizers, and then apply them to hyperspectral classification with different cost functions."
      }
    ]
  },
  {
    "tid":"1446",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'Automatic video editing system using stereo-based head tracking for multiparty conversation' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"This paper presents an automatic video editing system based on head tracking for multiparty conversations. Systems that record meetings and those that support teleconferences are attracting considerable interest. Conventional systems use a fixed-viewpoint camera and simple camera selection based on participants' utterances. However, conventional systems fail to adequately convey who is talking to whom to the viewer. We focus on the participants' head orientation since this information is useful in detecting the speaker and who the speaker is talking to. In order to automatically estimate each participant's head orientation, our system combines several modules for stereo-based head tracking. The system selects the shot of the participant that most participants are looking at, based on majority decision. Experiments confirm the effectiveness of our system in several 3-participant conversations. The results show that our system can more successfully convey who is talking to whom which is an extremely crucial piece of information that allows the viewer to better under-stand conversation content."
      }
    ]
  },
  {
    "tid":"1447",
    "conversations":[
      {
        "from":"user",
        "value":"Compose an abstract for the title 'User-Centric Energy Efficiency Maximization for Wireless Powered Communications.' that matches the author's unique content, style and tone."
      },
      {
        "from":"assistant",
        "value":"In this paper, we consider wireless powered communication networks (WPCNs) where multiple users harvest energy from a dedicated power station and then communicate with an information receiving station in a time-division manner. Thereby, our goal is to maximize the weighted sum of the user energy efficiencies (WSUEEs). In contrast to the existing system-centric approaches, the choice of the weights..."
      }
    ]
  }
]